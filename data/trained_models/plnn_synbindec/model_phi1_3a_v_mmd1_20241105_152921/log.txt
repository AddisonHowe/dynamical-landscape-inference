Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/data_phi1_3a/training', validation_data='data/training_data/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3369565763

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.486292050378344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.486292050378344 | validation: 5.661742208400902]
	TIME [epoch: 254 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.291805863733721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.291805863733721 | validation: 4.093396599836015]
	TIME [epoch: 0.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.193706982873279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.193706982873279 | validation: 4.715609662718447]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.329879986165777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.329879986165777 | validation: 3.7609920474267398]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.941183384281821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.941183384281821 | validation: 3.937045773456872]
	TIME [epoch: 0.691 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.088519475550228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.088519475550228 | validation: 4.0768538307077735]
	TIME [epoch: 0.69 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.958002947473751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.958002947473751 | validation: 3.787455730155888]
	TIME [epoch: 0.689 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.799797733393599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.799797733393599 | validation: 3.6836705604531597]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7519881893946785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7519881893946785 | validation: 3.6685699817996347]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.73309180417661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.73309180417661 | validation: 3.6234455354135005]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.731732826085903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.731732826085903 | validation: 3.7768122568349884]
	TIME [epoch: 0.692 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7714729985902125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7714729985902125 | validation: 3.602659255574219]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.771161309686139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.771161309686139 | validation: 3.649416861794697]
	TIME [epoch: 0.691 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.701000535982175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.701000535982175 | validation: 3.466815936031603]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6294103852842765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6294103852842765 | validation: 3.503702443670588]
	TIME [epoch: 0.69 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.59932703672704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.59932703672704 | validation: 3.417201410604402]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5881915283859325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5881915283859325 | validation: 3.5071056645270744]
	TIME [epoch: 0.689 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.605415164849389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.605415164849389 | validation: 3.4434477399191183]
	TIME [epoch: 0.688 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.601322436865201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.601322436865201 | validation: 3.4522091239068176]
	TIME [epoch: 0.687 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.563162833747061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.563162833747061 | validation: 3.3310500317371905]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4915379573150505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4915379573150505 | validation: 3.3172694962148883]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.459240625351909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.459240625351909 | validation: 3.285486753432784]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.437030608673897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437030608673897 | validation: 3.3271316809259206]
	TIME [epoch: 0.692 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4536533093047685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4536533093047685 | validation: 3.317149806525011]
	TIME [epoch: 0.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4555975595129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4555975595129 | validation: 3.292683920421152]
	TIME [epoch: 0.689 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.415885451397686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.415885451397686 | validation: 3.179333749760204]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.333482424575704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.333482424575704 | validation: 3.1551595686860296]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.305877086796649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.305877086796649 | validation: 3.133137917444346]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2786361395924155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2786361395924155 | validation: 3.1456488033483225]
	TIME [epoch: 0.69 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.289110320254564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.289110320254564 | validation: 3.146511468728228]
	TIME [epoch: 0.69 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.28652683829664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.28652683829664 | validation: 3.1967728380803715]
	TIME [epoch: 0.688 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.321801907130576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.321801907130576 | validation: 3.101545055129721]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.244404303069539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.244404303069539 | validation: 3.0178218816835534]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.163374532088016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163374532088016 | validation: 2.983967449685944]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.124840787814673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124840787814673 | validation: 2.962672671634103]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.104073375466745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104073375466745 | validation: 2.9439891271768617]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.087546421919654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.087546421919654 | validation: 2.933789156142069]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.065348061365257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065348061365257 | validation: 2.9264855337032727]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.055518353969015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.055518353969015 | validation: 2.9269075308002477]
	TIME [epoch: 0.691 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.059878567680173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.059878567680173 | validation: 2.9377514885495026]
	TIME [epoch: 0.689 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.042712399333275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.042712399333275 | validation: 2.9051004519777353]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.031157462578327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.031157462578327 | validation: 2.8677463908597605]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.981527893794564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.981527893794564 | validation: 2.959639769429588]
	TIME [epoch: 0.69 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.045834772409709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.045834772409709 | validation: 3.082349659375547]
	TIME [epoch: 0.69 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2069326262050515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2069326262050515 | validation: 3.0248633936573914]
	TIME [epoch: 0.688 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.068381547044607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.068381547044607 | validation: 2.8111002892788792]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.913535696082166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.913535696082166 | validation: 2.8885179532140444]
	TIME [epoch: 0.691 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9980504071182366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9980504071182366 | validation: 2.88238428464678]
	TIME [epoch: 0.692 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9539406664155057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9539406664155057 | validation: 2.7546954201800675]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.852126388734268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.852126388734268 | validation: 2.7663507112557753]
	TIME [epoch: 0.691 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.871536943927131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.871536943927131 | validation: 2.7908550225854007]
	TIME [epoch: 0.687 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.847898725485893		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.847898725485893 | validation: 2.7151834214419655]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.810655060353124		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.810655060353124 | validation: 2.7140483685651144]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.787644990898728		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.787644990898728 | validation: 2.687925775212302]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.764145012014477		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.764145012014477 | validation: 2.6616761441175942]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.758029449237463		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.758029449237463 | validation: 2.684636717131874]
	TIME [epoch: 0.689 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7495197516276346		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.7495197516276346 | validation: 2.6600725706661743]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7495343267987016		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.7495343267987016 | validation: 2.7692040517755894]
	TIME [epoch: 0.694 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8019726962448357		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.8019726962448357 | validation: 2.698594068553559]
	TIME [epoch: 0.693 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8106560857115257		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.8106560857115257 | validation: 2.689124283618832]
	TIME [epoch: 0.691 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7164716889651395		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.7164716889651395 | validation: 2.609834517842505]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6706100234683756		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.6706100234683756 | validation: 2.58471625897983]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6596326207608305		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.6596326207608305 | validation: 2.6211048407715674]
	TIME [epoch: 0.692 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6538648016475532		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.6538648016475532 | validation: 2.558114079065345]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6295973938194126		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.6295973938194126 | validation: 2.567376596738401]
	TIME [epoch: 0.691 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.608535316508736		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.608535316508736 | validation: 2.547398236198623]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5984322332688383		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.5984322332688383 | validation: 2.556942755201279]
	TIME [epoch: 0.691 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5883904740174706		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.5883904740174706 | validation: 2.5615943918970294]
	TIME [epoch: 0.688 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.630880599377077		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.630880599377077 | validation: 2.7446853060115597]
	TIME [epoch: 0.687 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7341601377134355		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.7341601377134355 | validation: 2.521035013206198]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5596489383469634		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.5596489383469634 | validation: 2.490603893564195]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5203560042922937		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.5203560042922937 | validation: 2.5516394892206113]
	TIME [epoch: 0.689 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5448610553360766		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.5448610553360766 | validation: 2.4764546387377453]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.504505978728117		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.504505978728117 | validation: 2.467273141875539]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4789265256183888		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.4789265256183888 | validation: 2.449016507735327]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4590056841034493		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.4590056841034493 | validation: 2.4211261211865316]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4430600440155685		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.4430600440155685 | validation: 2.447181029424618]
	TIME [epoch: 0.694 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4360180827480344		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.4360180827480344 | validation: 2.4412821131980973]
	TIME [epoch: 0.691 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.468589704046281		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.468589704046281 | validation: 2.777276823724998]
	TIME [epoch: 0.689 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.675764109718365		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.675764109718365 | validation: 2.464368060327054]
	TIME [epoch: 0.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.374059430031812		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.374059430031812 | validation: 2.4322839495295616]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.330545528293503		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.330545528293503 | validation: 2.3860620193068054]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.093629298885487		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.093629298885487 | validation: 2.2509096523854217]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.903358876042631		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.903358876042631 | validation: 2.259451897334207]
	TIME [epoch: 0.692 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8783500956525403		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.8783500956525403 | validation: 2.0494807935797144]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3225388514574683		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.3225388514574683 | validation: 2.0077331810422]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8667712370114509		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.8667712370114509 | validation: 1.474369018488497]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4548599955003283		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.4548599955003283 | validation: 1.230305249097992]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.399764019059392		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.399764019059392 | validation: 1.7530162051357594]
	TIME [epoch: 0.691 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9430805880308697		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.9430805880308697 | validation: 1.0468021263181233]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274325706593544		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.274325706593544 | validation: 0.9327715117797961]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.088921076498599		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.088921076498599 | validation: 1.0396400581331238]
	TIME [epoch: 0.694 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183171562631926		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.183171562631926 | validation: 0.9750048669614597]
	TIME [epoch: 0.694 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638419509840594		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.1638419509840594 | validation: 0.9264207510802471]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0410462530959843		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0410462530959843 | validation: 0.887084412084808]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017164285038632		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.017164285038632 | validation: 0.8777234317906605]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0018811096499287		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.0018811096499287 | validation: 0.869276694065694]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9966025447642318		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9966025447642318 | validation: 0.8425775836756261]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9889345843564914		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9889345843564914 | validation: 0.843075092258543]
	TIME [epoch: 0.689 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9721780587057451		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9721780587057451 | validation: 0.8398647992800495]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682497298686596		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.9682497298686596 | validation: 0.8601089127516458]
	TIME [epoch: 0.689 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964715744342941		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.964715744342941 | validation: 0.8487392956402131]
	TIME [epoch: 0.689 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788149352855245		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9788149352855245 | validation: 0.8813020277052228]
	TIME [epoch: 0.688 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9821882143367094		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9821882143367094 | validation: 0.8673112615289669]
	TIME [epoch: 0.687 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0337477059104079		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.0337477059104079 | validation: 0.9708440115603983]
	TIME [epoch: 0.689 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0264627392423284		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.0264627392423284 | validation: 0.8411450662770452]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9689457981308027		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9689457981308027 | validation: 0.8054879020297346]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238009246457171		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9238009246457171 | validation: 0.8255767833113961]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986073344515717		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8986073344515717 | validation: 0.796633837658428]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9008337882541955		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.9008337882541955 | validation: 0.806458404368108]
	TIME [epoch: 0.689 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009500052634999		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9009500052634999 | validation: 0.7812075852853602]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894114820912472		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.894114820912472 | validation: 0.8053553235983298]
	TIME [epoch: 0.692 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918569092349629		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8918569092349629 | validation: 0.7628739384010229]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945474811118425		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8945474811118425 | validation: 0.9249667353584992]
	TIME [epoch: 0.693 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436782936960273		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9436782936960273 | validation: 0.8131852592842134]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593647552722222		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9593647552722222 | validation: 0.9826762598681125]
	TIME [epoch: 0.691 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0000766013256759		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.0000766013256759 | validation: 0.8461243941914134]
	TIME [epoch: 0.689 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391930293383192		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9391930293383192 | validation: 0.8423411058595447]
	TIME [epoch: 0.693 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666940671351018		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.9666940671351018 | validation: 0.8633988457850365]
	TIME [epoch: 0.689 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357070609940596		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.9357070609940596 | validation: 0.7777914703391954]
	TIME [epoch: 0.689 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635590309781435		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8635590309781435 | validation: 0.7790552197382588]
	TIME [epoch: 0.689 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621074773202436		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8621074773202436 | validation: 0.7955399533790172]
	TIME [epoch: 0.692 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870520808804722		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.870520808804722 | validation: 0.7855928104765906]
	TIME [epoch: 0.691 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8600197588369575		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8600197588369575 | validation: 0.8004905291245934]
	TIME [epoch: 0.691 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682485239733657		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8682485239733657 | validation: 0.8085114538025806]
	TIME [epoch: 0.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656245052550042		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8656245052550042 | validation: 0.7923258169455206]
	TIME [epoch: 0.691 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818989827945525		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8818989827945525 | validation: 0.9334824239139623]
	TIME [epoch: 0.692 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9283674692049662		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9283674692049662 | validation: 0.797481976086015]
	TIME [epoch: 0.692 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9117359790283874		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.9117359790283874 | validation: 0.849350568031182]
	TIME [epoch: 0.692 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746964572087891		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8746964572087891 | validation: 0.7617013453320074]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384418821888593		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8384418821888593 | validation: 0.7851485718955686]
	TIME [epoch: 0.689 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838340188475744		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.838340188475744 | validation: 0.7949454185475903]
	TIME [epoch: 0.688 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8383457116891636		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8383457116891636 | validation: 0.7965565259289517]
	TIME [epoch: 0.69 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8559790013493567		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8559790013493567 | validation: 0.8193924763353233]
	TIME [epoch: 0.688 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815548437612977		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8815548437612977 | validation: 0.7796910952334395]
	TIME [epoch: 0.688 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959178598345722		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8959178598345722 | validation: 0.896981627913572]
	TIME [epoch: 0.687 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003690599858934		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.9003690599858934 | validation: 0.7795389034978171]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576591255886724		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8576591255886724 | validation: 0.8969265360448514]
	TIME [epoch: 0.691 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746461471093414		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8746461471093414 | validation: 0.8217786469522361]
	TIME [epoch: 0.691 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.895380501295074		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.895380501295074 | validation: 0.8301115345258201]
	TIME [epoch: 0.691 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8696768665487664		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8696768665487664 | validation: 0.7757661081823728]
	TIME [epoch: 0.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337553064106881		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8337553064106881 | validation: 0.7863741203787982]
	TIME [epoch: 0.689 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815580125998873		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.815580125998873 | validation: 0.7838011018773946]
	TIME [epoch: 0.693 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161818106625848		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8161818106625848 | validation: 0.7880428308213949]
	TIME [epoch: 0.695 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134302888541286		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8134302888541286 | validation: 0.7704707296420017]
	TIME [epoch: 0.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148017829380507		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8148017829380507 | validation: 0.7669659959693915]
	TIME [epoch: 0.692 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131539391014545		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8131539391014545 | validation: 0.7698597521826831]
	TIME [epoch: 0.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149561237730406		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8149561237730406 | validation: 0.8398282697507704]
	TIME [epoch: 0.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842314919095662		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.842314919095662 | validation: 0.88026721509768]
	TIME [epoch: 0.692 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.986490477248432		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.986490477248432 | validation: 1.0506676230269167]
	TIME [epoch: 0.691 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0037458776342898		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0037458776342898 | validation: 0.777107116341263]
	TIME [epoch: 0.691 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199732406344086		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8199732406344086 | validation: 0.8250739531644863]
	TIME [epoch: 0.692 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032436501423802		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.9032436501423802 | validation: 0.889681781513909]
	TIME [epoch: 0.691 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901986896301511		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8901986896301511 | validation: 0.8140636093157243]
	TIME [epoch: 0.69 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458532071589175		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8458532071589175 | validation: 0.8126147966014395]
	TIME [epoch: 0.691 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985346521258087		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8985346521258087 | validation: 0.81800607927968]
	TIME [epoch: 0.692 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239751676437278		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8239751676437278 | validation: 0.7733935271763146]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017149205589701		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8017149205589701 | validation: 0.75666477152814]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097105699903882		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8097105699903882 | validation: 0.7982592193552052]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063157975219565		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8063157975219565 | validation: 0.7712360893375618]
	TIME [epoch: 0.691 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056017415956103		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8056017415956103 | validation: 0.7694959700591832]
	TIME [epoch: 0.69 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980897025169061		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7980897025169061 | validation: 0.7536017604471655]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070272590513108		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8070272590513108 | validation: 0.8509532735606392]
	TIME [epoch: 0.693 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557066602145659		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8557066602145659 | validation: 0.8133627707319123]
	TIME [epoch: 0.692 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007682535962493		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.9007682535962493 | validation: 0.876614561867371]
	TIME [epoch: 0.691 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992216793866021		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8992216793866021 | validation: 0.7740030177013992]
	TIME [epoch: 0.692 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984749338290013		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7984749338290013 | validation: 0.7681494519425502]
	TIME [epoch: 0.691 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977782495558942		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7977782495558942 | validation: 0.7846847869016682]
	TIME [epoch: 0.691 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114631032979842		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8114631032979842 | validation: 0.7616292840945413]
	TIME [epoch: 0.691 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80958859569254		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.80958859569254 | validation: 0.811027908318361]
	TIME [epoch: 0.693 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022695742454751		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8022695742454751 | validation: 0.7706916626707483]
	TIME [epoch: 0.692 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370373198703959		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8370373198703959 | validation: 0.9832578274599376]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232337272455342		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.9232337272455342 | validation: 0.8169174761877533]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732900696512468		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.8732900696512468 | validation: 0.8280861045546581]
	TIME [epoch: 0.691 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367054091988129		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.8367054091988129 | validation: 0.768719249500363]
	TIME [epoch: 0.689 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082664582385596		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8082664582385596 | validation: 0.7417243665267477]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069454940189615		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8069454940189615 | validation: 0.7894585921916439]
	TIME [epoch: 0.692 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022425585768411		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8022425585768411 | validation: 0.7572205147765798]
	TIME [epoch: 0.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795826798160536		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.795826798160536 | validation: 0.7834935324547695]
	TIME [epoch: 0.688 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086831647799402		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8086831647799402 | validation: 0.7657386668002687]
	TIME [epoch: 0.687 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042969848334052		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8042969848334052 | validation: 0.8074708871348779]
	TIME [epoch: 0.687 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804450815766339		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.804450815766339 | validation: 0.7602993718177657]
	TIME [epoch: 0.687 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022894843282685		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8022894843282685 | validation: 0.7779990369698228]
	TIME [epoch: 0.689 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044250109217725		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8044250109217725 | validation: 0.76681075525513]
	TIME [epoch: 0.687 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290878243386922		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8290878243386922 | validation: 0.7970740916094041]
	TIME [epoch: 0.687 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377119952361065		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8377119952361065 | validation: 0.7857895577412781]
	TIME [epoch: 0.691 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652390812311821		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8652390812311821 | validation: 0.774975298050029]
	TIME [epoch: 0.688 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993539375530908		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7993539375530908 | validation: 0.7951272669353673]
	TIME [epoch: 0.687 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942657670437723		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7942657670437723 | validation: 0.8077656299215283]
	TIME [epoch: 0.688 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667660376995834		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8667660376995834 | validation: 0.9433968928631101]
	TIME [epoch: 0.686 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212367385117259		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9212367385117259 | validation: 0.727490718399284]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844473364528237		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7844473364528237 | validation: 0.7440347900952902]
	TIME [epoch: 0.688 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789486167545353		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.789486167545353 | validation: 0.7764273672691115]
	TIME [epoch: 0.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048702253189547		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.8048702253189547 | validation: 0.7412090231199866]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808566802357598		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7808566802357598 | validation: 0.7555416040663177]
	TIME [epoch: 0.687 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759704176374349		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7759704176374349 | validation: 0.7429106079861798]
	TIME [epoch: 0.689 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776190449067447		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.776190449067447 | validation: 0.7464306467600578]
	TIME [epoch: 0.689 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995414180068794		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7995414180068794 | validation: 0.7915956845366351]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477923393381213		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8477923393381213 | validation: 0.7844262544839724]
	TIME [epoch: 0.688 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529136764960441		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.8529136764960441 | validation: 0.7592731884854892]
	TIME [epoch: 0.688 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726504991747103		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7726504991747103 | validation: 0.7383039228668649]
	TIME [epoch: 266 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7792802778121487		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7792802778121487 | validation: 0.8043515671132587]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180682638272496		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.8180682638272496 | validation: 0.7820861457316122]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557229347107443		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8557229347107443 | validation: 0.883452101053854]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544741149046655		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8544741149046655 | validation: 0.7354278464040273]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775299469259679		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7775299469259679 | validation: 0.712225553258427]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769015548934876		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.769015548934876 | validation: 0.7449034833518864]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691424955536929		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7691424955536929 | validation: 0.7095909027480269]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708686188691263		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7708686188691263 | validation: 0.718376507698197]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800486932037913		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7800486932037913 | validation: 0.7156398755077125]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840031314236079		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7840031314236079 | validation: 0.7793712253218461]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912928298074465		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7912928298074465 | validation: 0.7251158124890593]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842501056212678		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7842501056212678 | validation: 0.7715691813856884]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774497105761169		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.774497105761169 | validation: 0.7194307015396657]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776053197800608		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.776053197800608 | validation: 0.7479011640055231]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769136817789846		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7769136817789846 | validation: 0.7109924915738003]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702460297659073		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7702460297659073 | validation: 0.7632777019410043]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269561637351329		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.8269561637351329 | validation: 0.809155602761056]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623302353894079		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.8623302353894079 | validation: 0.7220662427916701]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156173722857151		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8156173722857151 | validation: 0.7414562024501413]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612954911282451		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.7612954911282451 | validation: 0.7289066781959223]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972980513330191		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7972980513330191 | validation: 0.8043233865078842]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108281581434928		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.8108281581434928 | validation: 0.7054358136779554]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692869162652379		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7692869162652379 | validation: 0.746717078134199]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664217958694721		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7664217958694721 | validation: 0.6891229696605805]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574536924187498		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7574536924187498 | validation: 0.7130550995175908]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754785542334101		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.754785542334101 | validation: 0.6877779639355996]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503506441722051		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7503506441722051 | validation: 0.7420406958608158]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494581174978512		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7494581174978512 | validation: 0.6880479957419663]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591638161850378		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7591638161850378 | validation: 0.7533326666825683]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705687081244581		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7705687081244581 | validation: 0.7382310340862683]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210843504348223		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8210843504348223 | validation: 0.8547724488594821]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427545137230851		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8427545137230851 | validation: 0.7150215115240462]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764939504806456		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7764939504806456 | validation: 0.7169576570010815]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605046769112965		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7605046769112965 | validation: 0.7142053585223899]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787269248561176		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.787269248561176 | validation: 0.7193980290062715]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871543567395923		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7871543567395923 | validation: 0.6983358418324233]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753126696382774		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.753126696382774 | validation: 0.7127237361507368]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449287469782214		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7449287469782214 | validation: 0.6830735808829266]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380356088462731		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7380356088462731 | validation: 0.7181302687798676]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418554874797877		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7418554874797877 | validation: 0.6999132430180535]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518374443042948		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7518374443042948 | validation: 0.759377059424299]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749631120914546		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7749631120914546 | validation: 0.7532801396297674]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977863144461067		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7977863144461067 | validation: 0.7498043482883099]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903754485117267		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.7903754485117267 | validation: 0.714861568683628]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779349607871882		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7779349607871882 | validation: 0.6789518702661358]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609093292400365		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7609093292400365 | validation: 0.6962395334510783]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744598526942585		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.744598526942585 | validation: 0.6774322308694951]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327295403626364		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7327295403626364 | validation: 0.6871772587230452]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184715629170022		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7184715629170022 | validation: 0.6715053580782953]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716650136687525		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.716650136687525 | validation: 0.6911858891274171]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239462685332423		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7239462685332423 | validation: 0.6667595634675383]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7242035220893203		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7242035220893203 | validation: 0.7646039298471665]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626431415598613		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7626431415598613 | validation: 0.7378243504542774]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810377226920192		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.810377226920192 | validation: 0.8060116469194236]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798503656676175		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.798503656676175 | validation: 0.6956121152259507]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403410274045829		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7403410274045829 | validation: 0.6852736577143212]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475803804672569		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7475803804672569 | validation: 0.7199594666831697]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770730949914298		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7770730949914298 | validation: 0.6798556497459994]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659752273910215		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7659752273910215 | validation: 0.681262414721103]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732472098143534		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.732472098143534 | validation: 0.679220587272715]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168412493914087		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7168412493914087 | validation: 0.6696202008722962]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.724328971029813		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.724328971029813 | validation: 0.6976432628537181]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291464663502163		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7291464663502163 | validation: 0.6701254230366089]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229029085796573		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.7229029085796573 | validation: 0.6940441424084072]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394636041574569		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7394636041574569 | validation: 0.7150740697724983]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814994427142488		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7814994427142488 | validation: 0.7418594325970846]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041387808064516		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8041387808064516 | validation: 0.698313721539438]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486209177131697		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7486209177131697 | validation: 0.6703651990504889]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159806849173831		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7159806849173831 | validation: 0.6517958693321472]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063995823230552		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7063995823230552 | validation: 0.6605484931817591]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100566428837829		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7100566428837829 | validation: 0.6674845901831455]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091020606379502		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7091020606379502 | validation: 0.6582867843547997]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7139424550235355		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7139424550235355 | validation: 0.6986214211016607]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449331340020345		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7449331340020345 | validation: 0.7412071166973736]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949898167083265		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7949898167083265 | validation: 0.7491324789872035]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862056510178119		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7862056510178119 | validation: 0.6787068320833433]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150202969462944		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.7150202969462944 | validation: 0.650907349998783]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192587486122497		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7192587486122497 | validation: 0.6910564335523319]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337386972653407		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7337386972653407 | validation: 0.6590382180171663]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368001201403682		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7368001201403682 | validation: 0.666107665766189]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182589913600682		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7182589913600682 | validation: 0.6317416660928956]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027914407915915		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7027914407915915 | validation: 0.6568017777121002]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085972643620921		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.7085972643620921 | validation: 0.6950219314386175]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7146739284635966		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7146739284635966 | validation: 0.6969062163079717]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345966725462186		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7345966725462186 | validation: 0.6972391212649303]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750593858809619		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.750593858809619 | validation: 0.6928695621604393]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553627450117338		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7553627450117338 | validation: 0.6610756188373016]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167429302157244		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7167429302157244 | validation: 0.6566997996188464]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957899670649753		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6957899670649753 | validation: 0.631617860307366]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951785375447642		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6951785375447642 | validation: 0.6554440130488555]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061998687559146		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7061998687559146 | validation: 0.6439784296969884]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068838591398459		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.7068838591398459 | validation: 0.7017055341534342]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333297059737933		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7333297059737933 | validation: 0.6632200646891522]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318708852319221		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7318708852319221 | validation: 0.6620011096999701]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109323385768692		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7109323385768692 | validation: 0.6872846806664454]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341397438682867		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.7341397438682867 | validation: 0.6804490005991327]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523942123990517		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7523942123990517 | validation: 0.6861487838215682]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7206490880524293		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7206490880524293 | validation: 0.6349575597385372]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803623017987295		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6803623017987295 | validation: 0.6406497045949234]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738628335754645		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.6738628335754645 | validation: 0.6444169954331683]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912579077515045		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6912579077515045 | validation: 0.6326339173947165]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901504670804094		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6901504670804094 | validation: 0.6508678482305984]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692647649387876		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.692647649387876 | validation: 0.6287885520116622]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008579158623734		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7008579158623734 | validation: 0.6757519450311305]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125839788476293		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7125839788476293 | validation: 0.7142411859018077]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557657802899257		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7557657802899257 | validation: 0.7517668151870804]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.808887890755305		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.808887890755305 | validation: 0.7034567555048146]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226123876801817		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7226123876801817 | validation: 0.6254943858093283]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786463568164957		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6786463568164957 | validation: 0.6523650042213309]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931373462189222		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6931373462189222 | validation: 0.6555230534373289]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974413974833951		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6974413974833951 | validation: 0.633765051072965]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873391158195473		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6873391158195473 | validation: 0.6304357587074384]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734366009379212		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6734366009379212 | validation: 0.6397680540440813]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789051563052471		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6789051563052471 | validation: 0.6137481445994075]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880655251188034		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6880655251188034 | validation: 0.6438270743640752]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985039352422792		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6985039352422792 | validation: 0.6275538837387349]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7315430996953177		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7315430996953177 | validation: 0.6880669060464988]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268755219219775		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.7268755219219775 | validation: 0.6331908987736223]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831169314382314		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6831169314382314 | validation: 0.6346834435792733]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687068678512222		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.687068678512222 | validation: 0.6744214053655498]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6986943734477322		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6986943734477322 | validation: 0.6338980606660788]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909355542588799		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6909355542588799 | validation: 0.6364177131646821]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763317545561455		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6763317545561455 | validation: 0.6326838638209551]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676292229410503		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.676292229410503 | validation: 0.6286746848771729]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810219576325176		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6810219576325176 | validation: 0.6460564929117105]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980561946554726		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6980561946554726 | validation: 0.6283354822749191]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917737740442368		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6917737740442368 | validation: 0.6364568259597292]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962323799544415		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6962323799544415 | validation: 0.651967223376669]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885341240309759		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6885341240309759 | validation: 0.624042169850483]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818246843328244		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6818246843328244 | validation: 0.6022502301338396]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686558369645381		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6686558369645381 | validation: 0.5955893642990988]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543517488471657		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6543517488471657 | validation: 0.5919874205953051]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643907553626063		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6643907553626063 | validation: 0.6073405556902256]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660935605291541		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.660935605291541 | validation: 0.6122153081524591]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787488732053584		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6787488732053584 | validation: 0.6916535670242581]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285275582071375		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.7285275582071375 | validation: 0.682136882314299]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200833761691908		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.7200833761691908 | validation: 0.6240409314582105]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68505339735652		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.68505339735652 | validation: 0.6076538828055519]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655588697403324		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.655588697403324 | validation: 0.6039901677007606]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488306698021791		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6488306698021791 | validation: 0.6133512341109708]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490911571436111		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6490911571436111 | validation: 0.6042453342728358]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536529572729597		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6536529572729597 | validation: 0.5751780425726011]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478918233513319		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6478918233513319 | validation: 0.6078056754443234]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524406407918529		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6524406407918529 | validation: 0.6126599885829999]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582938770323388		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6582938770323388 | validation: 0.6264331629732262]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683146476035635		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.683146476035635 | validation: 0.6944547673384336]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740652607665291		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.740652607665291 | validation: 0.6106342174601864]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674875993828094		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.674875993828094 | validation: 0.6044092291302299]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64564611941025		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.64564611941025 | validation: 0.5759760443854814]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323820029074059		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6323820029074059 | validation: 0.5832551215026113]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642038939540021		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.642038939540021 | validation: 0.5629991745204911]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380226742761353		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.6380226742761353 | validation: 0.580800461062242]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340975756121552		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6340975756121552 | validation: 0.5942903021432232]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486191559252936		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6486191559252936 | validation: 0.602610987335991]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527799637348994		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6527799637348994 | validation: 0.610324324325645]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608379871695581		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6608379871695581 | validation: 0.569888794544247]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524787246184023		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6524787246184023 | validation: 0.5972730011745288]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420530948525152		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6420530948525152 | validation: 0.553908596904461]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297140562096166		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6297140562096166 | validation: 0.5589649991696811]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188121888420686		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6188121888420686 | validation: 0.5685981501452405]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202109475921062		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6202109475921062 | validation: 0.5646226897504713]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133406148762641		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6133406148762641 | validation: 0.5678477863593369]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208045438958574		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6208045438958574 | validation: 0.5537109459802008]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.62028267251125		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.62028267251125 | validation: 0.5930911193652019]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532405640813985		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6532405640813985 | validation: 0.6399618397122548]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691417004164672		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.691417004164672 | validation: 0.587584723470237]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447730723411096		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6447730723411096 | validation: 0.5711114951864925]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616544276077441		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.616544276077441 | validation: 0.5434615076311199]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5989643352939157		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5989643352939157 | validation: 0.5342016458412663]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5945915814373288		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5945915814373288 | validation: 0.5453997679293592]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936683084453064		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5936683084453064 | validation: 0.520077142823096]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6067123454853921		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6067123454853921 | validation: 0.5255816313627628]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037763209944991		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6037763209944991 | validation: 0.5582021403584166]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6170097052461043		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.6170097052461043 | validation: 0.5773999747041789]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640852172747644		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.640852172747644 | validation: 0.5470064757052832]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286747279261075		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6286747279261075 | validation: 0.5521570333361732]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5968608347299822		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5968608347299822 | validation: 0.5112531989347793]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5831790556806999		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5831790556806999 | validation: 0.5152321859782708]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779548737735872		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.5779548737735872 | validation: 0.5162949628899899]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716104838875048		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5716104838875048 | validation: 0.5077457962537842]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863853578980686		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5863853578980686 | validation: 0.5594935336735778]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587577435733642		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.587577435733642 | validation: 0.5069751939090702]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5771404681756589		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5771404681756589 | validation: 0.5369081199058997]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5681130349692032		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.5681130349692032 | validation: 0.531267592707031]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913445789140183		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5913445789140183 | validation: 0.5343590904686288]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215347419179297		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.6215347419179297 | validation: 0.5869913323613866]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359114637462131		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6359114637462131 | validation: 0.5143069761595336]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5684844413970155		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5684844413970155 | validation: 0.484631736657117]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528324600779303		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5528324600779303 | validation: 0.5178049308218998]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487311497979848		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5487311497979848 | validation: 0.4904104329612331]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5479411947594278		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5479411947594278 | validation: 0.49144365877552104]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5448597919848969		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5448597919848969 | validation: 0.5204195467886915]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543820596459348		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.543820596459348 | validation: 0.48288111637812003]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.538213086804042		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.538213086804042 | validation: 0.5216273802676701]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445229397243833		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.5445229397243833 | validation: 0.5110291886456004]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5462840943717686		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.5462840943717686 | validation: 0.5028069232522194]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5405903204671997		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.5405903204671997 | validation: 0.5381267254688757]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.554547071454828		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.554547071454828 | validation: 0.4637451058416711]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499021738413649		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5499021738413649 | validation: 0.5026063753916952]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5199651940985942		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.5199651940985942 | validation: 0.4720830984038193]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500058681244914		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.500058681244914 | validation: 0.4613427785680811]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5028977127648796		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.5028977127648796 | validation: 0.5119971404888374]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086226013722519		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5086226013722519 | validation: 0.47090230999429566]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509969316291794		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.509969316291794 | validation: 0.499307222374462]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226360959894438		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5226360959894438 | validation: 0.4955744834587306]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301031121921455		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5301031121921455 | validation: 0.48291715525634393]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162163094084727		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5162163094084727 | validation: 0.52696364884951]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5160439787162411		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.5160439787162411 | validation: 0.43504150715288026]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4753027939467922		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4753027939467922 | validation: 0.4649579882362085]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673455641240259		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.4673455641240259 | validation: 0.43709035271278296]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608668439694178		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.4608668439694178 | validation: 0.4306930368796835]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45617961456842765		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.45617961456842765 | validation: 0.5013261550457994]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4642594701720573		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.4642594701720573 | validation: 0.44630725875398425]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4858321882002891		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.4858321882002891 | validation: 0.5207432871832183]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4931109016380154		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4931109016380154 | validation: 0.445171730613064]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4430711676812192		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.4430711676812192 | validation: 0.41583628344010726]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.442227305444459		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.442227305444459 | validation: 0.5024356682711513]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4510292421453217		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.4510292421453217 | validation: 0.40653024724002285]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45170966928233086		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.45170966928233086 | validation: 0.4818066176802773]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43848952506617267		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.43848952506617267 | validation: 0.4024304753652785]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42331293974900447		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.42331293974900447 | validation: 0.4386517134006992]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4273008556305996		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.4273008556305996 | validation: 0.4700413242146218]
	TIME [epoch: 1.37 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47029116606967386		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.47029116606967386 | validation: 0.4801239233738701]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44949810692508335		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.44949810692508335 | validation: 0.4437896340976319]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44114397647871356		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.44114397647871356 | validation: 0.42767903532670215]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40718836844018214		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.40718836844018214 | validation: 0.44463117140274167]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38718302717885666		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.38718302717885666 | validation: 0.3977148171358268]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3836850863723011		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3836850863723011 | validation: 0.44501377479939724]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817423899846669		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.3817423899846669 | validation: 0.38689694908681416]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4073780806138054		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.4073780806138054 | validation: 0.5389973341262555]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43343734182847976		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.43343734182847976 | validation: 0.3795609378933068]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774612591975046		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3774612591975046 | validation: 0.3722688277626728]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35770793705250703		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.35770793705250703 | validation: 0.4662692386673447]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3814427355990604		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.3814427355990604 | validation: 0.4895324665304035]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4776971232571172		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.4776971232571172 | validation: 0.5316684605938654]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45244154570411765		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.45244154570411765 | validation: 0.40151378547009253]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38014377992494314		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.38014377992494314 | validation: 0.3661895470098608]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3600505135001989		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.3600505135001989 | validation: 0.44619046808857016]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36338721367302607		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.36338721367302607 | validation: 0.37259071626729967]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3547695265646007		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.3547695265646007 | validation: 0.3890953010999593]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33668753537922264		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.33668753537922264 | validation: 0.3727980761518652]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32963010558755046		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.32963010558755046 | validation: 0.3898107975018734]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3445686308473937		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.3445686308473937 | validation: 0.41774630116714007]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3924751997988622		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.3924751997988622 | validation: 0.502408786769645]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4144509186002525		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.4144509186002525 | validation: 0.4210828883514525]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4248765639260886		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.4248765639260886 | validation: 0.4586126797114525]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3481374291604443		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.3481374291604443 | validation: 0.385828011080941]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37225043844251815		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.37225043844251815 | validation: 0.3908627369147708]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33984025910432664		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.33984025910432664 | validation: 0.3821616163945234]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31442906386669206		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.31442906386669206 | validation: 0.3561474164673191]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149059485740494		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3149059485740494 | validation: 0.43846849252819126]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33694351850284593		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.33694351850284593 | validation: 0.3525998801101648]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3330503056117044		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.3330503056117044 | validation: 0.47350057745036395]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34435635021278765		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.34435635021278765 | validation: 0.354866557122529]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3719952730157853		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.3719952730157853 | validation: 0.4919450006653128]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37997877790675505		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.37997877790675505 | validation: 0.3974412077242301]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38207266458935357		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.38207266458935357 | validation: 0.3657009208987596]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29700309579991246		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.29700309579991246 | validation: 0.3551365033794207]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29483968856797865		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.29483968856797865 | validation: 0.3284376669001781]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3003483574018841		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.3003483574018841 | validation: 0.4274572143017058]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139808333454111		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.3139808333454111 | validation: 0.3606352348984255]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3692917338807335		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.3692917338807335 | validation: 0.4823950065981361]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3610444727370546		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.3610444727370546 | validation: 0.32787036425439187]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31229183642287794		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.31229183642287794 | validation: 0.38214285316775903]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2845750706027157		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2845750706027157 | validation: 0.3411397559033052]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27640971947951776		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.27640971947951776 | validation: 0.3424132851308132]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2910798076423049		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2910798076423049 | validation: 0.40722175668477106]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3300141404379625		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.3300141404379625 | validation: 0.3831369257419201]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278651672055453		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.3278651672055453 | validation: 0.3271701809533665]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927784735101969		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2927784735101969 | validation: 0.3846586672507717]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927133008383826		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2927133008383826 | validation: 0.37502520196331135]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40316512642801533		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.40316512642801533 | validation: 0.5147656119495111]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36408545287794175		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.36408545287794175 | validation: 0.3098232558508083]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743054543385284		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.2743054543385284 | validation: 0.32796168480334936]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27518394858418227		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.27518394858418227 | validation: 0.33452370540301407]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27931155726335527		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.27931155726335527 | validation: 0.37828889234841717]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29383475612577703		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.29383475612577703 | validation: 0.3017038823744149]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919360063212446		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.2919360063212446 | validation: 0.4132984429040223]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29876121044766146		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.29876121044766146 | validation: 0.30981733550864643]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32340345662911596		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.32340345662911596 | validation: 0.47419484589307104]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31813061767507556		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.31813061767507556 | validation: 0.2949927856581793]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2825154572534945		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.2825154572534945 | validation: 0.3593769565007996]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257660697805111		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.257660697805111 | validation: 0.3022460832144318]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25218962243141335		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.25218962243141335 | validation: 0.3310366957350219]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24439256537797582		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.24439256537797582 | validation: 0.2963058173296815]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24113962343824397		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.24113962343824397 | validation: 0.3473521750135691]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506476791392802		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.2506476791392802 | validation: 0.2884090132535794]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28441134275052316		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.28441134275052316 | validation: 0.48392095400451396]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34629453067517185		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.34629453067517185 | validation: 0.2835108945676589]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29998973106779225		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.29998973106779225 | validation: 0.46389725147679084]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37815019605791267		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.37815019605791267 | validation: 0.37698829985441873]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37829655572057375		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.37829655572057375 | validation: 0.3028604179965441]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23607398671743482		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.23607398671743482 | validation: 0.3423928679683892]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404126287532682		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.2404126287532682 | validation: 0.26966204585866305]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24925763647259		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.24925763647259 | validation: 0.3572353223100661]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250105809071562		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.250105809071562 | validation: 0.2996221659283672]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25819351167244686		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.25819351167244686 | validation: 0.40621733673963467]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871875594324031		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.2871875594324031 | validation: 0.26844642629725163]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2425226195335112		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2425226195335112 | validation: 0.3298071709627046]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23365531161611658		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.23365531161611658 | validation: 0.25841983172884814]
	TIME [epoch: 269 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24422306904711652		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.24422306904711652 | validation: 0.3658403087206158]
	TIME [epoch: 2.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25464890022936876		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.25464890022936876 | validation: 0.2913494917985067]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29756086788883246		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.29756086788883246 | validation: 0.442228870534997]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28576880654621223		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.28576880654621223 | validation: 0.2544386515046496]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747338680357551		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.2747338680357551 | validation: 0.3739176670104915]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27237086271891287		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.27237086271891287 | validation: 0.27286338635068946]
	TIME [epoch: 2.69 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23767029819284588		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.23767029819284588 | validation: 0.2903617292064326]
	TIME [epoch: 2.69 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151321944996567		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.2151321944996567 | validation: 0.31280379291930105]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2175014179308615		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.2175014179308615 | validation: 0.248663998615002]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23668142949460902		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.23668142949460902 | validation: 0.41520294965526383]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26776350342917815		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.26776350342917815 | validation: 0.32719701204775803]
	TIME [epoch: 2.69 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32968230749743305		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.32968230749743305 | validation: 0.436951507558209]
	TIME [epoch: 2.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31873024412159334		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.31873024412159334 | validation: 0.2987335777494136]
	TIME [epoch: 2.69 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2085336882538347		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2085336882538347 | validation: 0.2972621441760568]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661708011700913		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.2661708011700913 | validation: 0.38108872911926495]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771447597168039		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.2771447597168039 | validation: 0.23563856782409306]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22081028094639557		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.22081028094639557 | validation: 0.286604167932058]
	TIME [epoch: 2.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21742483711153582		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.21742483711153582 | validation: 0.242990046067321]
	TIME [epoch: 2.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2102054893037041		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.2102054893037041 | validation: 0.26245610251235707]
	TIME [epoch: 2.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19679138247102262		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.19679138247102262 | validation: 0.26600374860995935]
	TIME [epoch: 2.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191262282202056		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.191262282202056 | validation: 0.22090091338767062]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2072285555748415		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.2072285555748415 | validation: 0.4013387666215611]
	TIME [epoch: 2.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555211506741305		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.2555211506741305 | validation: 0.34228959083637783]
	TIME [epoch: 2.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37642002354577114		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.37642002354577114 | validation: 0.3725990432043865]
	TIME [epoch: 2.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568117136239048		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.2568117136239048 | validation: 0.25338635152962413]
	TIME [epoch: 2.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20480433968080797		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.20480433968080797 | validation: 0.2543983256738883]
	TIME [epoch: 2.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19389317158921876		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.19389317158921876 | validation: 0.2556555197890215]
	TIME [epoch: 2.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19235018453201277		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.19235018453201277 | validation: 0.25954784427794686]
	TIME [epoch: 2.71 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19207283068796027		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.19207283068796027 | validation: 0.22037322225710299]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1932367890043128		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.1932367890043128 | validation: 0.38474416101601394]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24265605111690627		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.24265605111690627 | validation: 0.30088214988282125]
	TIME [epoch: 2.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35050751873021196		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.35050751873021196 | validation: 0.3376310815952997]
	TIME [epoch: 2.69 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2248573143551837		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.2248573143551837 | validation: 0.22987910936525557]
	TIME [epoch: 2.69 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18891530317033955		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.18891530317033955 | validation: 0.2509522232183035]
	TIME [epoch: 2.69 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19777630253040296		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.19777630253040296 | validation: 0.29600389714919506]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21363930299702663		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.21363930299702663 | validation: 0.2589058103793134]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1953374353788241		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.1953374353788241 | validation: 0.238839514176795]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19512467707115658		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.19512467707115658 | validation: 0.3278418589464093]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21635325434774996		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.21635325434774996 | validation: 0.27545914937314386]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792912414569934		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.2792912414569934 | validation: 0.45802061217089957]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29435646411525257		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.29435646411525257 | validation: 0.23597282603344386]
	TIME [epoch: 2.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26129645363231085		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.26129645363231085 | validation: 0.2367127081536513]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18347285478565156		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.18347285478565156 | validation: 0.3384599240015951]
	TIME [epoch: 2.69 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21197465520204023		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.21197465520204023 | validation: 0.2265552827346789]
	TIME [epoch: 2.69 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225223428337167		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.225223428337167 | validation: 0.31421714411871177]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20399385962107267		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.20399385962107267 | validation: 0.23077180945081915]
	TIME [epoch: 2.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18065718259959304		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.18065718259959304 | validation: 0.2636651916413854]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1795850155410894		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.1795850155410894 | validation: 0.21399782266884]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16977309168134766		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.16977309168134766 | validation: 0.2547405209691567]
	TIME [epoch: 2.69 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1861389482095983		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.1861389482095983 | validation: 0.22462015834684107]
	TIME [epoch: 2.69 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21494989595093228		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.21494989595093228 | validation: 0.37523330467605276]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25483714722533163		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.25483714722533163 | validation: 0.18628541522200276]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975236372225077		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.1975236372225077 | validation: 0.261512959920139]
	TIME [epoch: 2.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1735072045463639		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.1735072045463639 | validation: 0.18303359691190035]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16041434483722103		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.16041434483722103 | validation: 0.24322955073967997]
	TIME [epoch: 2.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16653980585257974		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.16653980585257974 | validation: 0.21010077598280366]
	TIME [epoch: 2.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19268364629542187		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.19268364629542187 | validation: 0.3755008097096007]
	TIME [epoch: 2.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24464028636476243		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.24464028636476243 | validation: 0.2231750811503942]
	TIME [epoch: 2.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24914410049438832		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.24914410049438832 | validation: 0.3475139861781121]
	TIME [epoch: 2.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26021543071538333		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.26021543071538333 | validation: 0.18688308299736758]
	TIME [epoch: 2.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19322317887546347		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.19322317887546347 | validation: 0.2426849431476343]
	TIME [epoch: 2.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19173651980611214		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.19173651980611214 | validation: 0.2733930820100427]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20159392068015364		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.20159392068015364 | validation: 0.22938371948082487]
	TIME [epoch: 2.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15271396234352258		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.15271396234352258 | validation: 0.21161094640183584]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16094507960654947		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.16094507960654947 | validation: 0.18566873600690134]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147018018551394		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.147018018551394 | validation: 0.20961788497457065]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15076772323817142		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.15076772323817142 | validation: 0.22170944440576879]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14857821546836142		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.14857821546836142 | validation: 0.18733506597697205]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16398422024496256		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.16398422024496256 | validation: 0.4064257248600825]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2777680599879917		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.2777680599879917 | validation: 0.24501164201204262]
	TIME [epoch: 2.69 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28738262547972826		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.28738262547972826 | validation: 0.21829739536395212]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669363144354731		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.1669363144354731 | validation: 0.19234534189640196]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981821147086688		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.14981821147086688 | validation: 0.21719304226506486]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14077585739459605		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.14077585739459605 | validation: 0.18318838769531376]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16084142376437258		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.16084142376437258 | validation: 0.3965067163987308]
	TIME [epoch: 2.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2432688488157148		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.2432688488157148 | validation: 0.21467030951373567]
	TIME [epoch: 2.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2487487216236371		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.2487487216236371 | validation: 0.2615801055460408]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15906347265757123		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.15906347265757123 | validation: 0.1996937384873081]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349650046987793		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1349650046987793 | validation: 0.17602789762011758]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14501088774126072		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.14501088774126072 | validation: 0.2728892624472236]
	TIME [epoch: 2.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17595017370318175		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.17595017370318175 | validation: 0.2061286773995461]
	TIME [epoch: 2.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20138962047079595		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.20138962047079595 | validation: 0.33147538195644266]
	TIME [epoch: 2.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2214800031746492		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.2214800031746492 | validation: 0.1519330011831741]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15273053002456086		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.15273053002456086 | validation: 0.24605392609467058]
	TIME [epoch: 2.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14877751345830573		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.14877751345830573 | validation: 0.17017829613704105]
	TIME [epoch: 2.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15386185484558726		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.15386185484558726 | validation: 0.2731179868652451]
	TIME [epoch: 2.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1938539939652075		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.1938539939652075 | validation: 0.18055953786653742]
	TIME [epoch: 2.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17894589498886712		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.17894589498886712 | validation: 0.20373804069951829]
	TIME [epoch: 2.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13746507384086068		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.13746507384086068 | validation: 0.1560197442922818]
	TIME [epoch: 2.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118631640179199		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.13118631640179199 | validation: 0.19045060362907426]
	TIME [epoch: 2.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12694543708192707		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.12694543708192707 | validation: 0.19493835063701603]
	TIME [epoch: 2.71 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13824720852357225		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.13824720852357225 | validation: 0.23561116559195905]
	TIME [epoch: 2.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17346707160443953		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.17346707160443953 | validation: 0.20833241252623888]
	TIME [epoch: 2.71 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18921548442952407		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.18921548442952407 | validation: 0.28834138652329905]
	TIME [epoch: 2.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21236255526331113		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.21236255526331113 | validation: 0.16553629236265907]
	TIME [epoch: 2.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16858872245992004		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.16858872245992004 | validation: 0.3182439052666588]
	TIME [epoch: 2.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20047325863379578		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.20047325863379578 | validation: 0.1823560251001193]
	TIME [epoch: 2.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19620451071182385		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.19620451071182385 | validation: 0.23528269843040195]
	TIME [epoch: 2.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1640510866079122		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1640510866079122 | validation: 0.18481788510026176]
	TIME [epoch: 2.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267469636775198		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.1267469636775198 | validation: 0.17788664811128913]
	TIME [epoch: 2.69 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244971357557952		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.12244971357557952 | validation: 0.15301146097759827]
	TIME [epoch: 2.69 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296673076885488		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.1296673076885488 | validation: 0.22602118551621642]
	TIME [epoch: 2.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13619952175303135		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.13619952175303135 | validation: 0.17653696441107536]
	TIME [epoch: 2.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16709935733268289		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.16709935733268289 | validation: 0.29919298555673207]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2186559779276438		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.2186559779276438 | validation: 0.1449778248043648]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13201371880109208		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.13201371880109208 | validation: 0.1781059478289975]
	TIME [epoch: 2.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129462374659338		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.129462374659338 | validation: 0.18299525814764464]
	TIME [epoch: 2.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13862743153125212		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.13862743153125212 | validation: 0.1760530359553527]
	TIME [epoch: 2.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18041088552656173		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.18041088552656173 | validation: 0.30691381136614854]
	TIME [epoch: 2.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21568674404975474		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.21568674404975474 | validation: 0.15969763177007035]
	TIME [epoch: 2.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18538685362404828		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.18538685362404828 | validation: 0.15123912018795585]
	TIME [epoch: 2.69 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12029338052317241		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.12029338052317241 | validation: 0.1533315458547747]
	TIME [epoch: 2.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669684028747448		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.10669684028747448 | validation: 0.13383268676413437]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647768416847071		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.10647768416847071 | validation: 0.1615159240285086]
	TIME [epoch: 2.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11172366142520167		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.11172366142520167 | validation: 0.13589526290081114]
	TIME [epoch: 2.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11095762555606652		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.11095762555606652 | validation: 0.17452536108582323]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204107451291203		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1204107451291203 | validation: 0.18948269462054732]
	TIME [epoch: 2.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677117085938623		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.1677117085938623 | validation: 0.3279826596933504]
	TIME [epoch: 2.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24258295841877453		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.24258295841877453 | validation: 0.17767935481410133]
	TIME [epoch: 2.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18767000250832208		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.18767000250832208 | validation: 0.225562673774637]
	TIME [epoch: 2.69 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381139985943029		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1381139985943029 | validation: 0.12719284466959194]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10185963024654719		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.10185963024654719 | validation: 0.13761790325038734]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10993499262745186		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.10993499262745186 | validation: 0.15082971659492259]
	TIME [epoch: 2.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11353003872998113		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.11353003872998113 | validation: 0.18011262694919924]
	TIME [epoch: 2.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12392382312592341		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.12392382312592341 | validation: 0.1426005598987585]
	TIME [epoch: 2.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14938452920216372		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.14938452920216372 | validation: 0.2420181028565903]
	TIME [epoch: 2.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16045149982361082		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.16045149982361082 | validation: 0.17381006153654238]
	TIME [epoch: 2.69 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923463550898148		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.1923463550898148 | validation: 0.268463645331055]
	TIME [epoch: 2.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15897454396449207		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15897454396449207 | validation: 0.13587912389201445]
	TIME [epoch: 2.71 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11565726551835809		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.11565726551835809 | validation: 0.154410737653085]
	TIME [epoch: 2.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775516541654088		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.11775516541654088 | validation: 0.14947116325409388]
	TIME [epoch: 2.69 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10670149441247372		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.10670149441247372 | validation: 0.16047200071709564]
	TIME [epoch: 2.69 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11263844723956351		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.11263844723956351 | validation: 0.14457237677350313]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12830687151745043		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.12830687151745043 | validation: 0.21061987188799539]
	TIME [epoch: 2.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15162545156428528		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.15162545156428528 | validation: 0.13640789192117472]
	TIME [epoch: 2.69 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15089704606969412		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.15089704606969412 | validation: 0.1786229848108451]
	TIME [epoch: 2.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13071082340440943		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.13071082340440943 | validation: 0.12161469232649813]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396228478235454		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10396228478235454 | validation: 0.12253612061156671]
	TIME [epoch: 2.69 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830587867721706		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.09830587867721706 | validation: 0.13297045798100401]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916922662921909		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.09916922662921909 | validation: 0.1325936439634409]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10756383434652228		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.10756383434652228 | validation: 0.16933466703291952]
	TIME [epoch: 2.69 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15262231297105455		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.15262231297105455 | validation: 0.27420820376616295]
	TIME [epoch: 2.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22446761241930113		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.22446761241930113 | validation: 0.1529189977078379]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1570258914194715		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.1570258914194715 | validation: 0.17105958143141908]
	TIME [epoch: 2.69 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11778971553052887		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.11778971553052887 | validation: 0.16979228165481874]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11404424785462393		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.11404424785462393 | validation: 0.17603750384868624]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18402306211481168		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.18402306211481168 | validation: 0.1649114737516909]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11588265056086293		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.11588265056086293 | validation: 0.12696362278576165]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09821405154696254		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09821405154696254 | validation: 0.11606971189786482]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872658087002888		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.0872658087002888 | validation: 0.1214124741304375]
	TIME [epoch: 2.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09250904053845335		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.09250904053845335 | validation: 0.14199607311208215]
	TIME [epoch: 2.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039602034935884		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.1039602034935884 | validation: 0.1291184330823411]
	TIME [epoch: 2.71 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14235491703545525		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.14235491703545525 | validation: 0.27658120063476804]
	TIME [epoch: 2.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21152309869237992		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.21152309869237992 | validation: 0.21405409103944148]
	TIME [epoch: 2.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2183214877899291		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.2183214877899291 | validation: 0.12510971498020526]
	TIME [epoch: 2.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10657710709498334		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.10657710709498334 | validation: 0.25920803190526337]
	TIME [epoch: 2.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18680676165564372		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.18680676165564372 | validation: 0.12503049677834882]
	TIME [epoch: 2.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12388628301218249		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.12388628301218249 | validation: 0.11298642059779702]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796485813162292		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.09796485813162292 | validation: 0.11247682936322426]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848394075070092		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0848394075070092 | validation: 0.10931804393851628]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464347345440192		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.09464347345440192 | validation: 0.1297992639215134]
	TIME [epoch: 2.68 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1010370880915082		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.1010370880915082 | validation: 0.11761209704668403]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327269843009102		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.11327269843009102 | validation: 0.12512173931747367]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10500816282184847		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.10500816282184847 | validation: 0.1075212982088112]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181888193424831		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.09181888193424831 | validation: 0.1283436851626224]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003036696714847		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.09003036696714847 | validation: 0.10890249778672348]
	TIME [epoch: 2.69 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603409142392931		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.09603409142392931 | validation: 0.15323018347829576]
	TIME [epoch: 2.69 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954442736374477		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.11954442736374477 | validation: 0.1308796492592418]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12358904059883491		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.12358904059883491 | validation: 0.15896414450235596]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11426878200622018		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.11426878200622018 | validation: 0.10675226175214694]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245783254038714		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.10245783254038714 | validation: 0.1304576086211012]
	TIME [epoch: 2.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08746628977938588		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.08746628977938588 | validation: 0.10631358859406873]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07989068767335863		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07989068767335863 | validation: 0.10923881855690128]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845884530972277		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.0845884530972277 | validation: 0.09468782010511033]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279754082292828		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09279754082292828 | validation: 0.2283465709781436]
	TIME [epoch: 2.7 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680575272379504		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.1680575272379504 | validation: 0.17078999624330132]
	TIME [epoch: 2.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17393276679932693		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.17393276679932693 | validation: 0.1341056522573895]
	TIME [epoch: 2.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960256426372043		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.09960256426372043 | validation: 0.11023032257989691]
	TIME [epoch: 2.7 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08108104492939028		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.08108104492939028 | validation: 0.092869633069051]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08763500442058433		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.08763500442058433 | validation: 0.12409572283414508]
	TIME [epoch: 2.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09030485443137003		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09030485443137003 | validation: 0.11380539960732412]
	TIME [epoch: 2.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10069531595326395		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.10069531595326395 | validation: 0.18485060224692718]
	TIME [epoch: 2.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424504055711696		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.1424504055711696 | validation: 0.10656134611185933]
	TIME [epoch: 2.7 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10640299060474442		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10640299060474442 | validation: 0.13846796805087233]
	TIME [epoch: 2.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10177647077849229		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.10177647077849229 | validation: 0.09707033477696492]
	TIME [epoch: 2.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10159135665601353		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.10159135665601353 | validation: 0.15288096969136902]
	TIME [epoch: 2.71 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11891546203598212		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.11891546203598212 | validation: 0.10841420420361894]
	TIME [epoch: 2.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10609426274875744		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.10609426274875744 | validation: 0.12840119035661365]
	TIME [epoch: 2.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918430838239246		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.0918430838239246 | validation: 0.09942435690960856]
	TIME [epoch: 2.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09102294556578627		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.09102294556578627 | validation: 0.11314235616730631]
	TIME [epoch: 2.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817628101351385		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.08817628101351385 | validation: 0.08582033677024366]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325096885035776		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.08325096885035776 | validation: 0.11308882065217317]
	TIME [epoch: 2.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08393335760575499		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.08393335760575499 | validation: 0.08935752177455025]
	TIME [epoch: 2.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08656217397844056		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.08656217397844056 | validation: 0.1456214199857242]
	TIME [epoch: 2.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09791271406503946		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.09791271406503946 | validation: 0.16975707918693064]
	TIME [epoch: 2.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17226457169297624		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.17226457169297624 | validation: 0.1512196836697086]
	TIME [epoch: 2.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1076470742946678		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1076470742946678 | validation: 0.1289607052222765]
	TIME [epoch: 2.7 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09989142850865446		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.09989142850865446 | validation: 0.12900197308475034]
	TIME [epoch: 2.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862993857285438		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.10862993857285438 | validation: 0.08143502295356703]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08026545411540116		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.08026545411540116 | validation: 0.11129205413192132]
	TIME [epoch: 2.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07504742546055224		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.07504742546055224 | validation: 0.08466481979417684]
	TIME [epoch: 2.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861327292305798		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.06861327292305798 | validation: 0.08666167447940779]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06985160167690739		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06985160167690739 | validation: 0.07452787156289663]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07572346995308765		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.07572346995308765 | validation: 0.11456807792755136]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08749929182789863		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.08749929182789863 | validation: 0.12134442058392568]
	TIME [epoch: 2.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13240857621809876		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.13240857621809876 | validation: 0.20090813229372612]
	TIME [epoch: 2.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15647562657598368		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.15647562657598368 | validation: 0.08338724574230247]
	TIME [epoch: 2.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07456600869843585		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.07456600869843585 | validation: 0.11195635094521156]
	TIME [epoch: 2.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08339740888757963		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.08339740888757963 | validation: 0.14373433889989357]
	TIME [epoch: 2.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405620024494784		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.10405620024494784 | validation: 0.1052772967734859]
	TIME [epoch: 2.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11432222255089386		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.11432222255089386 | validation: 0.11625380812419964]
	TIME [epoch: 2.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08473723168577657		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.08473723168577657 | validation: 0.08877848346895736]
	TIME [epoch: 2.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704225845556907		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.0704225845556907 | validation: 0.07349312518737737]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697658321299194		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06697658321299194 | validation: 0.08861883084478438]
	TIME [epoch: 2.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06404453984657237		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06404453984657237 | validation: 0.07956440566785133]
	TIME [epoch: 2.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06288524770661329		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.06288524770661329 | validation: 0.08549666280288434]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0702213761664529		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0702213761664529 | validation: 0.07429417123330083]
	TIME [epoch: 2.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035299819358174		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.08035299819358174 | validation: 0.15801623431911987]
	TIME [epoch: 2.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11901335525599548		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.11901335525599548 | validation: 0.1404786240866026]
	TIME [epoch: 2.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1505690308628127		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.1505690308628127 | validation: 0.10780057179792957]
	TIME [epoch: 2.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587484745400505		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.09587484745400505 | validation: 0.07390626827578817]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925313294332322		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.06925313294332322 | validation: 0.06896542702834495]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540196198987781		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06540196198987781 | validation: 0.08401866299127744]
	TIME [epoch: 2.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564703515069006		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06564703515069006 | validation: 0.07029120449342317]
	TIME [epoch: 2.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06428011952319844		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.06428011952319844 | validation: 0.09439262521670894]
	TIME [epoch: 2.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06901433391171011		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.06901433391171011 | validation: 0.08278454218551615]
	TIME [epoch: 2.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397902416077323		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.08397902416077323 | validation: 0.14774784574592323]
	TIME [epoch: 2.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12411367534197922		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.12411367534197922 | validation: 0.11031190465538424]
	TIME [epoch: 2.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11407045636926219		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.11407045636926219 | validation: 0.09210765955910404]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748366201799453		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.07748366201799453 | validation: 0.07267545968333564]
	TIME [epoch: 2.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06245399353308185		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.06245399353308185 | validation: 0.0753133289232718]
	TIME [epoch: 2.69 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096293801536836		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.06096293801536836 | validation: 0.0642349709021716]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812568219912542		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05812568219912542 | validation: 0.07032728311579282]
	TIME [epoch: 2.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0601981739486766		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.0601981739486766 | validation: 0.06275503203762853]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05933009170692076		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.05933009170692076 | validation: 0.06631102082756013]
	TIME [epoch: 2.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06180337928515961		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.06180337928515961 | validation: 0.10181549816178236]
	TIME [epoch: 2.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08704608560282895		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.08704608560282895 | validation: 0.11582824395721561]
	TIME [epoch: 2.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122287334980468		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.12122287334980468 | validation: 0.15437550824316315]
	TIME [epoch: 2.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12803065773405733		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.12803065773405733 | validation: 0.09344721521262372]
	TIME [epoch: 2.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895632862081305		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.0895632862081305 | validation: 0.07035187212479477]
	TIME [epoch: 2.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454145012250288		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.06454145012250288 | validation: 0.07220804802869421]
	TIME [epoch: 2.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131372772714233		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.06131372772714233 | validation: 0.06629733209892193]
	TIME [epoch: 2.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057809695705126604		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.057809695705126604 | validation: 0.08564152198531422]
	TIME [epoch: 2.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0669784601146191		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.0669784601146191 | validation: 0.07214081214176794]
	TIME [epoch: 2.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07503524204903206		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.07503524204903206 | validation: 0.11440399169567442]
	TIME [epoch: 2.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08786481616968232		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.08786481616968232 | validation: 0.0818673548799118]
	TIME [epoch: 2.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829376458959135		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.0829376458959135 | validation: 0.09395987630547974]
	TIME [epoch: 2.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0760343033790005		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.0760343033790005 | validation: 0.07342719698767157]
	TIME [epoch: 2.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012839464256033		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.08012839464256033 | validation: 0.07662249873668757]
	TIME [epoch: 2.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710843996145741		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.07710843996145741 | validation: 0.08620527429738872]
	TIME [epoch: 2.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07536273930784483		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07536273930784483 | validation: 0.08983955520792418]
	TIME [epoch: 2.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763659297388241		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.0763659297388241 | validation: 0.06898754756941487]
	TIME [epoch: 2.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06048229482333572		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.06048229482333572 | validation: 0.07063794336368016]
	TIME [epoch: 2.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057554514664144685		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.057554514664144685 | validation: 0.06631908643972999]
	TIME [epoch: 2.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633830596090574		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0633830596090574 | validation: 0.1355220315465933]
	TIME [epoch: 2.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10811117323742814		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.10811117323742814 | validation: 0.11832149554763483]
	TIME [epoch: 2.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11269700277002047		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.11269700277002047 | validation: 0.08736134556354379]
	TIME [epoch: 2.71 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07099501459413592		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07099501459413592 | validation: 0.06637326785523961]
	TIME [epoch: 2.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783297415247481		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.05783297415247481 | validation: 0.06185814811602391]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05293875551665277		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05293875551665277 | validation: 0.05636277284029371]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055018891699060164		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.055018891699060164 | validation: 0.06406283639487245]
	TIME [epoch: 2.69 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601601121060092		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.05601601121060092 | validation: 0.05641996577888873]
	TIME [epoch: 2.69 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0528108141334091		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0528108141334091 | validation: 0.058003530690559996]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135739417313438		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05135739417313438 | validation: 0.06702197465031444]
	TIME [epoch: 2.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823291962726279		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.05823291962726279 | validation: 0.11934583616762434]
	TIME [epoch: 2.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10617924775991217		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.10617924775991217 | validation: 0.11564818645077309]
	TIME [epoch: 2.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11206698773360535		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.11206698773360535 | validation: 0.13520470327549017]
	TIME [epoch: 2.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10371270221499128		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.10371270221499128 | validation: 0.05876871399187068]
	TIME [epoch: 2.69 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829481905720551		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05829481905720551 | validation: 0.05165337586529309]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054124740743236296		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.054124740743236296 | validation: 0.06471104829070601]
	TIME [epoch: 2.69 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05036419461347955		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.05036419461347955 | validation: 0.057144922586676805]
	TIME [epoch: 2.7 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05340796354504262		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.05340796354504262 | validation: 0.05777368119862447]
	TIME [epoch: 2.69 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053777498230657896		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.053777498230657896 | validation: 0.06378405045656717]
	TIME [epoch: 2.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054206286140639465		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.054206286140639465 | validation: 0.06873721141149791]
	TIME [epoch: 2.7 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059698229371364916		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.059698229371364916 | validation: 0.07155069397400064]
	TIME [epoch: 2.7 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06699630640928102		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.06699630640928102 | validation: 0.09061005178557036]
	TIME [epoch: 2.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125073857346543		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08125073857346543 | validation: 0.0907574781733902]
	TIME [epoch: 2.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10019993367999697		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.10019993367999697 | validation: 0.10784316310914846]
	TIME [epoch: 2.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183209748881691		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.10183209748881691 | validation: 0.05713094358853114]
	TIME [epoch: 2.69 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056653143859953056		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.056653143859953056 | validation: 0.0523945657786171]
	TIME [epoch: 2.69 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507509025783148		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.0507509025783148 | validation: 0.0647607448594175]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05563198566346261		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05563198566346261 | validation: 0.06477551762553517]
	TIME [epoch: 2.69 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06494599094993561		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06494599094993561 | validation: 0.10773939643004948]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848791111026139		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.0848791111026139 | validation: 0.06256542937218448]
	TIME [epoch: 2.69 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056910668515138596		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.056910668515138596 | validation: 0.06524032387103278]
	TIME [epoch: 2.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047867432187891544		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.047867432187891544 | validation: 0.04563712674264411]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691724870165093		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.04691724870165093 | validation: 0.051735339266076596]
	TIME [epoch: 2.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05045032250648765		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.05045032250648765 | validation: 0.0553470502586282]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04935093744228839		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04935093744228839 | validation: 0.06549932337252402]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060623447824787464		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.060623447824787464 | validation: 0.09315328155451139]
	TIME [epoch: 2.69 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088104897365124		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.088104897365124 | validation: 0.1054796437678348]
	TIME [epoch: 2.69 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740724303321613		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.09740724303321613 | validation: 0.07057745580217765]
	TIME [epoch: 2.69 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06448189223255202		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.06448189223255202 | validation: 0.046984693987003306]
	TIME [epoch: 2.69 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617585221250492		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04617585221250492 | validation: 0.04692321109421144]
	TIME [epoch: 2.69 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048107138843830714		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.048107138843830714 | validation: 0.0530136125941324]
	TIME [epoch: 2.69 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05060428059167766		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.05060428059167766 | validation: 0.054369045510805375]
	TIME [epoch: 2.69 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251723525378974		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.05251723525378974 | validation: 0.07903974768551116]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06302192727135687		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.06302192727135687 | validation: 0.08114191863498443]
	TIME [epoch: 2.69 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08183178368329508		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.08183178368329508 | validation: 0.07375767763780654]
	TIME [epoch: 2.69 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968267400783118		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.06968267400783118 | validation: 0.05654599687668869]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049781449259722166		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.049781449259722166 | validation: 0.045919256849116255]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04352843906881976		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.04352843906881976 | validation: 0.04620717739626676]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04495900968913268		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.04495900968913268 | validation: 0.042808983184048105]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0444860733910749		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.0444860733910749 | validation: 0.05001138440779662]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04098633438679697		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.04098633438679697 | validation: 0.04750044758358152]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592871841486727		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.04592871841486727 | validation: 0.07122778997231664]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05944359627362115		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.05944359627362115 | validation: 0.08958484463221963]
	TIME [epoch: 2.69 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0914755763772167		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.0914755763772167 | validation: 0.09931591864892259]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686250457916637		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.08686250457916637 | validation: 0.05178519623508104]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05411911637261818		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.05411911637261818 | validation: 0.048575139882938]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045060323787889		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.045060323787889 | validation: 0.052986183809863856]
	TIME [epoch: 2.69 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597538117525936		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04597538117525936 | validation: 0.04965842216391049]
	TIME [epoch: 2.69 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701325420147662		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.04701325420147662 | validation: 0.04828733272630151]
	TIME [epoch: 2.69 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044980743962786375		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.044980743962786375 | validation: 0.04513033033768255]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556438505179447		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.04556438505179447 | validation: 0.06052462952192331]
	TIME [epoch: 2.69 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276729602021532		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.06276729602021532 | validation: 0.09349137073162103]
	TIME [epoch: 2.69 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09249355573323337		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.09249355573323337 | validation: 0.10909474471389419]
	TIME [epoch: 2.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000110486238279		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.09000110486238279 | validation: 0.048624315797755585]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109249069671822		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.05109249069671822 | validation: 0.04518256735495599]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04357716469523632		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.04357716469523632 | validation: 0.04844260025692454]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141143321466196		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.05141143321466196 | validation: 0.056588629407735486]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05893568076989345		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.05893568076989345 | validation: 0.05715376307497925]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563130378344011		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.0563130378344011 | validation: 0.047903464356195934]
	TIME [epoch: 2.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046941310210475866		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.046941310210475866 | validation: 0.05409814137531621]
	TIME [epoch: 2.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668838235323939		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.04668838235323939 | validation: 0.04021842284175707]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043830988275428806		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.043830988275428806 | validation: 0.05117033782388572]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050165600718129165		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.050165600718129165 | validation: 0.06107079817038537]
	TIME [epoch: 2.69 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058870381691938416		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.058870381691938416 | validation: 0.06519825955921014]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847356913946725		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.05847356913946725 | validation: 0.06078363466466914]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05676757076570157		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.05676757076570157 | validation: 0.049054554858416366]
	TIME [epoch: 2.69 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05034976802116841		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.05034976802116841 | validation: 0.03912178720926617]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04192928226353915		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.04192928226353915 | validation: 0.05306339608164315]
	TIME [epoch: 2.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671952631215758		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04671952631215758 | validation: 0.04399908959993378]
	TIME [epoch: 2.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259784810350272		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04259784810350272 | validation: 0.04064979720402481]
	TIME [epoch: 2.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03948613879212723		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03948613879212723 | validation: 0.03929398372655683]
	TIME [epoch: 2.69 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020677291385116		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04020677291385116 | validation: 0.046675927104563794]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041166827976873835		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.041166827976873835 | validation: 0.05562988006129719]
	TIME [epoch: 2.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054450187769243076		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.054450187769243076 | validation: 0.07200821414569233]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07423034845702857		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.07423034845702857 | validation: 0.0811716404231363]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07683852296152424		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07683852296152424 | validation: 0.05711376960853907]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07166613592438108		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07166613592438108 | validation: 0.0418314856204414]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042063842388128665		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.042063842388128665 | validation: 0.04834629620963708]
	TIME [epoch: 2.68 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041272978778941476		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.041272978778941476 | validation: 0.045704780810786494]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024670626924441		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.04024670626924441 | validation: 0.045105858438202966]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04319876478877416		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04319876478877416 | validation: 0.04365272503541357]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04032501554071872		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.04032501554071872 | validation: 0.04042545979878323]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008488297675257		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.04008488297675257 | validation: 0.04338180005331049]
	TIME [epoch: 2.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040110883874385356		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.040110883874385356 | validation: 0.04406980779803327]
	TIME [epoch: 2.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041867192491819555		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.041867192491819555 | validation: 0.04587321385449798]
	TIME [epoch: 2.7 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044409141436810885		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.044409141436810885 | validation: 0.051577506918863786]
	TIME [epoch: 2.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05076193137293492		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.05076193137293492 | validation: 0.04939115423299082]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05451774272795056		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.05451774272795056 | validation: 0.06977254096259909]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06732874881284892		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.06732874881284892 | validation: 0.06185867611850554]
	TIME [epoch: 2.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362050332852673		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.06362050332852673 | validation: 0.044973818662221415]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858781877417398		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.04858781877417398 | validation: 0.04050872256778059]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04305198632537195		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04305198632537195 | validation: 0.05037336620687394]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564206848190528		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.04564206848190528 | validation: 0.04020533968239704]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03835713505103261		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.03835713505103261 | validation: 0.040947848175358036]
	TIME [epoch: 2.68 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03694980743502593		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.03694980743502593 | validation: 0.03871239375171899]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568575958939747		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.03568575958939747 | validation: 0.0411360686077876]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03619233127045529		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.03619233127045529 | validation: 0.0430314145052895]
	TIME [epoch: 2.7 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04027023768287121		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.04027023768287121 | validation: 0.05896441493237801]
	TIME [epoch: 2.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05761309493072561		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.05761309493072561 | validation: 0.07769348423255282]
	TIME [epoch: 2.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965896213870252		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.07965896213870252 | validation: 0.05289576274204782]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052412815224972924		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.052412815224972924 | validation: 0.041148044284757115]
	TIME [epoch: 2.69 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039058375294439576		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.039058375294439576 | validation: 0.03426955341359433]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03527328602694937		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.03527328602694937 | validation: 0.04172089694556502]
	TIME [epoch: 2.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03730514898552749		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.03730514898552749 | validation: 0.0410451121343291]
	TIME [epoch: 2.71 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126951617920864		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.04126951617920864 | validation: 0.039193548141252824]
	TIME [epoch: 2.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04698164659958128		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04698164659958128 | validation: 0.05816280283955758]
	TIME [epoch: 2.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05424646854068324		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.05424646854068324 | validation: 0.061295257312459356]
	TIME [epoch: 2.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05752652890082731		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.05752652890082731 | validation: 0.04319386713880064]
	TIME [epoch: 2.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04400645398906356		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.04400645398906356 | validation: 0.03586222678909886]
	TIME [epoch: 2.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038071253735251505		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.038071253735251505 | validation: 0.036091211083012435]
	TIME [epoch: 2.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038811884642431446		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.038811884642431446 | validation: 0.04222536375261633]
	TIME [epoch: 2.7 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321933994385473		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.04321933994385473 | validation: 0.0421570001942935]
	TIME [epoch: 2.71 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421876879747234		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.04421876879747234 | validation: 0.051618015323145694]
	TIME [epoch: 2.71 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04787707848283744		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04787707848283744 | validation: 0.04759956639446783]
	TIME [epoch: 2.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046365818769143814		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.046365818769143814 | validation: 0.04329747509092969]
	TIME [epoch: 2.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043812238549564494		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.043812238549564494 | validation: 0.04021204464530262]
	TIME [epoch: 2.71 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040431262187809215		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.040431262187809215 | validation: 0.046259176739174235]
	TIME [epoch: 2.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321893059749846		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.04321893059749846 | validation: 0.042833681828608944]
	TIME [epoch: 2.71 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041313946877953374		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.041313946877953374 | validation: 0.038521276584484496]
	TIME [epoch: 2.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983300295147012		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03983300295147012 | validation: 0.04005389908620452]
	TIME [epoch: 2.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909929129307883		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03909929129307883 | validation: 0.040174323761415796]
	TIME [epoch: 2.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03851757777276575		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.03851757777276575 | validation: 0.04047130566407839]
	TIME [epoch: 2.71 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039809156953909096		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.039809156953909096 | validation: 0.03619742013136604]
	TIME [epoch: 2.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03793627651824282		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.03793627651824282 | validation: 0.03891295839238093]
	TIME [epoch: 2.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036306575972906374		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.036306575972906374 | validation: 0.04225191978749208]
	TIME [epoch: 2.7 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03696804705302276		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.03696804705302276 | validation: 0.048470964257079946]
	TIME [epoch: 2.71 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044952190317323885		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.044952190317323885 | validation: 0.06278581714387872]
	TIME [epoch: 2.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262334352196765		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.06262334352196765 | validation: 0.06100590867617311]
	TIME [epoch: 2.71 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06489701433459823		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.06489701433459823 | validation: 0.03980375215415991]
	TIME [epoch: 2.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042515485446451375		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.042515485446451375 | validation: 0.04159141458524574]
	TIME [epoch: 2.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03508679708277078		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.03508679708277078 | validation: 0.03390321965802522]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353243002018233		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.03353243002018233 | validation: 0.03547570714872567]
	TIME [epoch: 2.7 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035920543782086145		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.035920543782086145 | validation: 0.043472749336124744]
	TIME [epoch: 2.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039603076594840365		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.039603076594840365 | validation: 0.04222284227984366]
	TIME [epoch: 2.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042046232363145676		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.042046232363145676 | validation: 0.037046486605155]
	TIME [epoch: 2.7 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491102216657216		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.03491102216657216 | validation: 0.030127915274143737]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321254337284633		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03321254337284633 | validation: 0.029667627876485192]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034552223453116995		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.034552223453116995 | validation: 0.03263014227477636]
	TIME [epoch: 2.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032745007381134936		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.032745007381134936 | validation: 0.04131001972439011]
	TIME [epoch: 2.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378682047655598		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.0378682047655598 | validation: 0.048911644149096226]
	TIME [epoch: 2.7 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346774615081266		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.05346774615081266 | validation: 0.07399020946186256]
	TIME [epoch: 2.7 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06634578933203064		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.06634578933203064 | validation: 0.03285968290019655]
	TIME [epoch: 2.71 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04283027977176751		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04283027977176751 | validation: 0.037895167097051284]
	TIME [epoch: 2.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037830942173241046		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.037830942173241046 | validation: 0.032156472409764054]
	TIME [epoch: 2.7 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03499667550369532		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.03499667550369532 | validation: 0.02814804139623034]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035090478577528214		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.035090478577528214 | validation: 0.033397257998741735]
	TIME [epoch: 2.7 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03500436533869329		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03500436533869329 | validation: 0.03313048326203129]
	TIME [epoch: 2.7 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032047313673300624		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.032047313673300624 | validation: 0.034579758490760636]
	TIME [epoch: 2.7 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03444051178025663		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03444051178025663 | validation: 0.03610023534589087]
	TIME [epoch: 2.7 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813394567267769		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03813394567267769 | validation: 0.04764501201678425]
	TIME [epoch: 2.7 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041995864511080463		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.041995864511080463 | validation: 0.050699655013763036]
	TIME [epoch: 2.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048642221648748006		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.048642221648748006 | validation: 0.04921321576729703]
	TIME [epoch: 2.7 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626029630381238		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.04626029630381238 | validation: 0.04385692025770306]
	TIME [epoch: 2.7 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040922541613177855		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.040922541613177855 | validation: 0.032613322032035874]
	TIME [epoch: 2.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033771496347131474		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.033771496347131474 | validation: 0.032781322141828946]
	TIME [epoch: 2.7 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030695633225503408		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.030695633225503408 | validation: 0.03194976417635051]
	TIME [epoch: 2.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03085501073622267		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03085501073622267 | validation: 0.02460210722865493]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_922.pth
	Model improved!!!
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03003913142709992		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03003913142709992 | validation: 0.026258204284063377]
	TIME [epoch: 2.7 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031995117636926265		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.031995117636926265 | validation: 0.02941478973773337]
	TIME [epoch: 2.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032099783977113924		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.032099783977113924 | validation: 0.038175485433388806]
	TIME [epoch: 2.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03415892829198497		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03415892829198497 | validation: 0.0345508567247755]
	TIME [epoch: 2.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249210970483027		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.04249210970483027 | validation: 0.06742808284278821]
	TIME [epoch: 2.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735595314383645		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.06735595314383645 | validation: 0.06382197049235849]
	TIME [epoch: 2.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636412733742731		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.0636412733742731 | validation: 0.035576257674130196]
	TIME [epoch: 2.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035147643196013566		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.035147643196013566 | validation: 0.03623924177421267]
	TIME [epoch: 2.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03318771765514108		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.03318771765514108 | validation: 0.03347845318269124]
	TIME [epoch: 2.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843975030653823		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03843975030653823 | validation: 0.03744213401034249]
	TIME [epoch: 2.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392944222222378		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0392944222222378 | validation: 0.03571777591716338]
	TIME [epoch: 2.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406548532155622		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03406548532155622 | validation: 0.03350152215629906]
	TIME [epoch: 2.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03095261954373055		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03095261954373055 | validation: 0.03032220708457948]
	TIME [epoch: 2.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02898109409697537		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.02898109409697537 | validation: 0.028500632747141763]
	TIME [epoch: 2.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316463397306254		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.0316463397306254 | validation: 0.02911349569414981]
	TIME [epoch: 2.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030732028679737074		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.030732028679737074 | validation: 0.03169770380474013]
	TIME [epoch: 2.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03339809654113598		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.03339809654113598 | validation: 0.031546755511121335]
	TIME [epoch: 2.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03392473254502173		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.03392473254502173 | validation: 0.039771087868714125]
	TIME [epoch: 2.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039265117582755166		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.039265117582755166 | validation: 0.05228087069675222]
	TIME [epoch: 2.69 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05631234587059801		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.05631234587059801 | validation: 0.05513306958313844]
	TIME [epoch: 2.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375727296502204		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.05375727296502204 | validation: 0.02895634934004365]
	TIME [epoch: 2.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03078822470599996		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.03078822470599996 | validation: 0.027582702029220037]
	TIME [epoch: 2.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032710285824695824		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.032710285824695824 | validation: 0.04133612187292168]
	TIME [epoch: 2.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037967190832444576		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.037967190832444576 | validation: 0.03720328919336913]
	TIME [epoch: 2.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460305775105277		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.03460305775105277 | validation: 0.03448988966048302]
	TIME [epoch: 2.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03452324686300958		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03452324686300958 | validation: 0.03181252394478043]
	TIME [epoch: 2.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03248522264737193		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03248522264737193 | validation: 0.030633774582929797]
	TIME [epoch: 2.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032453307073152575		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.032453307073152575 | validation: 0.02597352930138257]
	TIME [epoch: 2.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0319050708398429		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.0319050708398429 | validation: 0.026807980434083358]
	TIME [epoch: 2.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0303337485681975		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0303337485681975 | validation: 0.032064960274476674]
	TIME [epoch: 2.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030809390058925513		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.030809390058925513 | validation: 0.03512091259257172]
	TIME [epoch: 2.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03716478152783502		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03716478152783502 | validation: 0.035186529368678575]
	TIME [epoch: 2.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037834435020276676		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.037834435020276676 | validation: 0.03823288919571765]
	TIME [epoch: 2.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04296663229269855		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.04296663229269855 | validation: 0.03980234245140911]
	TIME [epoch: 2.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030557213683825		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.04030557213683825 | validation: 0.03146939502173135]
	TIME [epoch: 2.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03137635603902899		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03137635603902899 | validation: 0.028152016170098933]
	TIME [epoch: 2.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029206642231004994		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.029206642231004994 | validation: 0.032916795092822436]
	TIME [epoch: 2.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032108259819325066		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.032108259819325066 | validation: 0.03241464139178809]
	TIME [epoch: 2.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03737114692374529		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.03737114692374529 | validation: 0.05211466484193113]
	TIME [epoch: 2.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04172584077243416		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.04172584077243416 | validation: 0.042357347191801085]
	TIME [epoch: 2.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327247730208951		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.04327247730208951 | validation: 0.034436138518740105]
	TIME [epoch: 2.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035585754373169715		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.035585754373169715 | validation: 0.026140838138711743]
	TIME [epoch: 2.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028686007863627478		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.028686007863627478 | validation: 0.023351474792142526]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_965.pth
	Model improved!!!
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029912180370789895		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.029912180370789895 | validation: 0.024903665567503674]
	TIME [epoch: 2.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02877571553437064		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.02877571553437064 | validation: 0.029043675241422175]
	TIME [epoch: 2.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03070498679325416		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03070498679325416 | validation: 0.0234115557366685]
	TIME [epoch: 2.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02912744581420011		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.02912744581420011 | validation: 0.033086727821122375]
	TIME [epoch: 2.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156598613998893		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.03156598613998893 | validation: 0.023151701790804155]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02988471474865117		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.02988471474865117 | validation: 0.02461693235403283]
	TIME [epoch: 2.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028779363403171124		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.028779363403171124 | validation: 0.03211544469544497]
	TIME [epoch: 2.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0296064740863636		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.0296064740863636 | validation: 0.026101205611547565]
	TIME [epoch: 2.72 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02876536037737127		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.02876536037737127 | validation: 0.026206983765625837]
	TIME [epoch: 2.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030559923855221802		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.030559923855221802 | validation: 0.044039119443615375]
	TIME [epoch: 2.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979703364688449		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03979703364688449 | validation: 0.05411394624076005]
	TIME [epoch: 2.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062916300868565		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.062916300868565 | validation: 0.05518337402746756]
	TIME [epoch: 2.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05148870599971748		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05148870599971748 | validation: 0.030078692497336135]
	TIME [epoch: 2.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03085528922119151		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.03085528922119151 | validation: 0.02536743515368768]
	TIME [epoch: 2.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030827042229035557		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.030827042229035557 | validation: 0.03335547307278809]
	TIME [epoch: 2.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029778968545242108		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.029778968545242108 | validation: 0.02928602030260696]
	TIME [epoch: 2.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02926606056558094		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.02926606056558094 | validation: 0.030280235651327328]
	TIME [epoch: 2.7 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028837191148576916		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.028837191148576916 | validation: 0.023637029996818888]
	TIME [epoch: 2.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02936124940853377		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.02936124940853377 | validation: 0.023795648547075027]
	TIME [epoch: 2.69 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026903655276386256		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.026903655276386256 | validation: 0.02853913844658822]
	TIME [epoch: 2.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02599857138958981		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.02599857138958981 | validation: 0.02697428796914053]
	TIME [epoch: 2.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029598394890009462		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.029598394890009462 | validation: 0.026044013648051784]
	TIME [epoch: 2.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193297690491322		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03193297690491322 | validation: 0.03769195365002531]
	TIME [epoch: 2.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03375930340965273		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.03375930340965273 | validation: 0.03159433953261042]
	TIME [epoch: 2.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038354476322861264		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.038354476322861264 | validation: 0.03205992752892322]
	TIME [epoch: 2.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350932769356624		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0350932769356624 | validation: 0.022250084925695003]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_991.pth
	Model improved!!!
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473932253673186		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03473932253673186 | validation: 0.026996755608888746]
	TIME [epoch: 2.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030728516478476565		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.030728516478476565 | validation: 0.02560350984351791]
	TIME [epoch: 2.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027969701430418024		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.027969701430418024 | validation: 0.02914026707840326]
	TIME [epoch: 2.69 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030392734054236918		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.030392734054236918 | validation: 0.02493394041021221]
	TIME [epoch: 2.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02965844984946421		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.02965844984946421 | validation: 0.0263430300642611]
	TIME [epoch: 2.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028694388836358376		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.028694388836358376 | validation: 0.026430287853411873]
	TIME [epoch: 2.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254110408812		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.03254110408812 | validation: 0.04508710519271141]
	TIME [epoch: 2.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042360460733027065		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.042360460733027065 | validation: 0.04116690764648396]
	TIME [epoch: 2.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044139676431204776		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.044139676431204776 | validation: 0.027769366213016247]
	TIME [epoch: 2.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03053257982452988		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.03053257982452988 | validation: 0.02676451308778985]
	TIME [epoch: 276 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030165756824490134		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.030165756824490134 | validation: 0.023450207030652977]
	TIME [epoch: 5.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030491351619742525		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.030491351619742525 | validation: 0.031352590334272366]
	TIME [epoch: 5.78 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187587097950202		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03187587097950202 | validation: 0.024304777376556886]
	TIME [epoch: 5.79 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02786363868043665		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.02786363868043665 | validation: 0.023379094451332096]
	TIME [epoch: 5.78 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026742601732515983		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.026742601732515983 | validation: 0.02267041372641019]
	TIME [epoch: 5.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026169027039912952		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.026169027039912952 | validation: 0.020860187093419386]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1007.pth
	Model improved!!!
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025477840449939823		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.025477840449939823 | validation: 0.026266570678768017]
	TIME [epoch: 5.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028634917851446322		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.028634917851446322 | validation: 0.02599650050445955]
	TIME [epoch: 5.78 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028928224960346		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.028928224960346 | validation: 0.02579503483025324]
	TIME [epoch: 5.79 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03171403546066071		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.03171403546066071 | validation: 0.04067293341768144]
	TIME [epoch: 5.78 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612131540418994		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.04612131540418994 | validation: 0.04833981314892221]
	TIME [epoch: 5.79 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050909937445070275		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.050909937445070275 | validation: 0.026088211969810635]
	TIME [epoch: 5.79 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027877442065690438		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.027877442065690438 | validation: 0.0309690613806914]
	TIME [epoch: 5.79 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028356463415327857		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.028356463415327857 | validation: 0.02236340565124008]
	TIME [epoch: 5.78 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027038072364081064		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.027038072364081064 | validation: 0.02643493275013071]
	TIME [epoch: 5.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027723780767011767		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.027723780767011767 | validation: 0.02341808486080918]
	TIME [epoch: 5.78 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02936823816530758		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.02936823816530758 | validation: 0.024410112198066127]
	TIME [epoch: 5.78 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026602444854731778		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.026602444854731778 | validation: 0.02835862419503798]
	TIME [epoch: 5.78 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02812995732288282		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.02812995732288282 | validation: 0.027611433679512517]
	TIME [epoch: 5.79 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030974292325609314		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.030974292325609314 | validation: 0.03517715825862598]
	TIME [epoch: 5.79 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562321640920241		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03562321640920241 | validation: 0.026673046812500736]
	TIME [epoch: 5.79 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463279595918384		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.03463279595918384 | validation: 0.028418889137233328]
	TIME [epoch: 5.78 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028846081910912286		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.028846081910912286 | validation: 0.026088964125741833]
	TIME [epoch: 5.79 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025481867414507635		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.025481867414507635 | validation: 0.021910959829376454]
	TIME [epoch: 5.79 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027270210099100512		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.027270210099100512 | validation: 0.029817557038569694]
	TIME [epoch: 5.79 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028223690065733022		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.028223690065733022 | validation: 0.02295465174960817]
	TIME [epoch: 5.79 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032551552984904836		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.032551552984904836 | validation: 0.03803212833369158]
	TIME [epoch: 5.79 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03862751165835526		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.03862751165835526 | validation: 0.024957023976015166]
	TIME [epoch: 5.77 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032767048596798125		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.032767048596798125 | validation: 0.023436544030647783]
	TIME [epoch: 5.79 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025459860264634832		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.025459860264634832 | validation: 0.031088138107112268]
	TIME [epoch: 5.78 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030312721549870653		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.030312721549870653 | validation: 0.02455395592544355]
	TIME [epoch: 5.79 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030846479830098892		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.030846479830098892 | validation: 0.026518561011282877]
	TIME [epoch: 5.79 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028905189719522984		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.028905189719522984 | validation: 0.021771173067960788]
	TIME [epoch: 5.79 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026303050239753087		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.026303050239753087 | validation: 0.02039265523226107]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1035.pth
	Model improved!!!
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02575623855474447		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.02575623855474447 | validation: 0.02466358235876863]
	TIME [epoch: 5.78 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027326817893760298		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.027326817893760298 | validation: 0.0266199767985726]
	TIME [epoch: 5.78 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03246973861535615		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03246973861535615 | validation: 0.035428316099748176]
	TIME [epoch: 5.79 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0333344846521889		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.0333344846521889 | validation: 0.02749569300386482]
	TIME [epoch: 5.78 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03567805406315955		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03567805406315955 | validation: 0.026667374479260982]
	TIME [epoch: 5.78 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027548625405561026		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.027548625405561026 | validation: 0.02222089796365019]
	TIME [epoch: 5.78 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02486568202668007		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.02486568202668007 | validation: 0.025797099680977667]
	TIME [epoch: 5.78 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02779218163794309		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.02779218163794309 | validation: 0.02650117993304508]
	TIME [epoch: 5.79 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028798286494185796		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.028798286494185796 | validation: 0.022759704365632977]
	TIME [epoch: 5.78 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02695135951558412		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.02695135951558412 | validation: 0.024037772306665147]
	TIME [epoch: 5.79 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029008096278400296		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.029008096278400296 | validation: 0.026090699152141402]
	TIME [epoch: 5.78 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027431932449276206		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.027431932449276206 | validation: 0.032538395543587936]
	TIME [epoch: 5.78 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032787684878439124		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.032787684878439124 | validation: 0.0293856308309339]
	TIME [epoch: 5.79 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03184697874562589		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.03184697874562589 | validation: 0.029453844612885662]
	TIME [epoch: 5.78 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028046016183312785		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.028046016183312785 | validation: 0.021938999886531824]
	TIME [epoch: 5.78 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02573537220894446		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.02573537220894446 | validation: 0.02678259307888863]
	TIME [epoch: 5.79 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027382958852290886		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.027382958852290886 | validation: 0.023609409082755037]
	TIME [epoch: 5.78 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028123666868147223		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.028123666868147223 | validation: 0.029001777285664533]
	TIME [epoch: 5.78 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026299006776604853		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.026299006776604853 | validation: 0.02175400830777223]
	TIME [epoch: 5.78 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635483911533752		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.02635483911533752 | validation: 0.024417639032293464]
	TIME [epoch: 5.78 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02678863932197926		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.02678863932197926 | validation: 0.02138485065021808]
	TIME [epoch: 5.79 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030498803129213065		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.030498803129213065 | validation: 0.029561693261899988]
	TIME [epoch: 5.78 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030704690197668045		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.030704690197668045 | validation: 0.030651217245360046]
	TIME [epoch: 5.79 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032918695624271195		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.032918695624271195 | validation: 0.034499696076332145]
	TIME [epoch: 5.78 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030585124497734786		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.030585124497734786 | validation: 0.024078084064212114]
	TIME [epoch: 5.79 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02713611648807282		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.02713611648807282 | validation: 0.02521281953654439]
	TIME [epoch: 5.78 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025917341561377566		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.025917341561377566 | validation: 0.021639562919504]
	TIME [epoch: 5.79 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024715245116077283		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.024715245116077283 | validation: 0.022321746187799897]
	TIME [epoch: 5.78 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02509205723865992		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.02509205723865992 | validation: 0.017869021392815432]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026539895710332564		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.026539895710332564 | validation: 0.023492690307945808]
	TIME [epoch: 5.78 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025404679233281664		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.025404679233281664 | validation: 0.023614442880064687]
	TIME [epoch: 5.79 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028017212222001514		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.028017212222001514 | validation: 0.02437175393683047]
	TIME [epoch: 5.79 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026100089461454805		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.026100089461454805 | validation: 0.028408360405002653]
	TIME [epoch: 5.79 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030193247083253008		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.030193247083253008 | validation: 0.027106139286849736]
	TIME [epoch: 5.79 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026886100132181616		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.026886100132181616 | validation: 0.01967169422006529]
	TIME [epoch: 5.78 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025116666021861564		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.025116666021861564 | validation: 0.021451998326701627]
	TIME [epoch: 5.78 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025875331205344434		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.025875331205344434 | validation: 0.02412215253020127]
	TIME [epoch: 5.79 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393766467610373		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.02393766467610373 | validation: 0.02832091294388372]
	TIME [epoch: 5.78 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027313686991063728		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.027313686991063728 | validation: 0.025007250165858794]
	TIME [epoch: 5.79 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031347503201421656		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.031347503201421656 | validation: 0.04686450967050558]
	TIME [epoch: 5.79 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04219445561800271		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.04219445561800271 | validation: 0.019755848037254338]
	TIME [epoch: 5.78 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027906114670441604		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.027906114670441604 | validation: 0.024317059518129538]
	TIME [epoch: 5.79 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026719202916455513		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.026719202916455513 | validation: 0.03287784482192652]
	TIME [epoch: 5.79 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901590451821613		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.02901590451821613 | validation: 0.023139902973302304]
	TIME [epoch: 5.78 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028389230761042602		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.028389230761042602 | validation: 0.01975479461532095]
	TIME [epoch: 5.79 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02599816197826821		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.02599816197826821 | validation: 0.022184176925414523]
	TIME [epoch: 5.78 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024190612283075487		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.024190612283075487 | validation: 0.022665861813642066]
	TIME [epoch: 5.79 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026499713857177466		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.026499713857177466 | validation: 0.031758421961023]
	TIME [epoch: 5.78 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027600847793183253		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.027600847793183253 | validation: 0.022527769307482265]
	TIME [epoch: 5.79 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026996156918321038		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.026996156918321038 | validation: 0.023946655253487728]
	TIME [epoch: 5.79 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025535922768427425		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.025535922768427425 | validation: 0.02756617568533777]
	TIME [epoch: 5.79 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025714984138545453		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.025714984138545453 | validation: 0.027317540418512882]
	TIME [epoch: 5.79 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026168008245480847		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.026168008245480847 | validation: 0.023306348074206423]
	TIME [epoch: 5.79 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02864080764619034		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.02864080764619034 | validation: 0.026746104669481525]
	TIME [epoch: 5.79 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02593464594936103		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.02593464594936103 | validation: 0.018559821796618792]
	TIME [epoch: 5.78 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025783049560553		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.025783049560553 | validation: 0.01954737207362501]
	TIME [epoch: 5.78 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024618987981571513		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.024618987981571513 | validation: 0.023544759834227033]
	TIME [epoch: 5.77 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024895061826380634		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.024895061826380634 | validation: 0.024359674900459982]
	TIME [epoch: 5.79 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02395841352524758		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.02395841352524758 | validation: 0.02312531225677136]
	TIME [epoch: 5.78 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852025095623406		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.02852025095623406 | validation: 0.03158040925917365]
	TIME [epoch: 5.78 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036928006306210355		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.036928006306210355 | validation: 0.022193899497081324]
	TIME [epoch: 5.79 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031331136918126584		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.031331136918126584 | validation: 0.023679440671749232]
	TIME [epoch: 5.79 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02412999515650685		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02412999515650685 | validation: 0.026369158639838075]
	TIME [epoch: 5.78 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026297856185356513		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.026297856185356513 | validation: 0.022575088183344996]
	TIME [epoch: 5.78 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026453537470212216		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.026453537470212216 | validation: 0.02038529711940004]
	TIME [epoch: 5.78 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022804574726259158		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.022804574726259158 | validation: 0.02223271236934897]
	TIME [epoch: 5.78 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023768077648306535		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.023768077648306535 | validation: 0.021098517082270736]
	TIME [epoch: 5.79 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507288231994858		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.02507288231994858 | validation: 0.0218217122626037]
	TIME [epoch: 5.79 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02339169508672689		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02339169508672689 | validation: 0.020999375648985436]
	TIME [epoch: 5.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024781951903189558		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.024781951903189558 | validation: 0.020759887412372433]
	TIME [epoch: 5.79 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026316315252869326		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.026316315252869326 | validation: 0.03083816593368004]
	TIME [epoch: 5.79 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030286286632379086		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.030286286632379086 | validation: 0.025106031678071933]
	TIME [epoch: 5.79 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187744280352434		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.03187744280352434 | validation: 0.021417737747185223]
	TIME [epoch: 5.79 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026693798792751373		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.026693798792751373 | validation: 0.02295131140810482]
	TIME [epoch: 5.79 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023461761488440046		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.023461761488440046 | validation: 0.021837894337160282]
	TIME [epoch: 5.79 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024167398286660813		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.024167398286660813 | validation: 0.023496350329858348]
	TIME [epoch: 5.79 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026173919517836028		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.026173919517836028 | validation: 0.018636552115863854]
	TIME [epoch: 5.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02282484519163192		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.02282484519163192 | validation: 0.022008890406365125]
	TIME [epoch: 5.79 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023233343119004976		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.023233343119004976 | validation: 0.018249286197473315]
	TIME [epoch: 5.79 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02380799016751067		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.02380799016751067 | validation: 0.019251340649020583]
	TIME [epoch: 5.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025767979121021786		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.025767979121021786 | validation: 0.019986442463504013]
	TIME [epoch: 5.79 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024021726940106163		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.024021726940106163 | validation: 0.019071716461033325]
	TIME [epoch: 5.79 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024697796814977618		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.024697796814977618 | validation: 0.025147672069156093]
	TIME [epoch: 5.78 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647157977926641		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02647157977926641 | validation: 0.02012208744176712]
	TIME [epoch: 5.79 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026615365559132968		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.026615365559132968 | validation: 0.02700392273141147]
	TIME [epoch: 5.79 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02757208896241944		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.02757208896241944 | validation: 0.019253813222624934]
	TIME [epoch: 5.79 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030202277785286186		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.030202277785286186 | validation: 0.025270482135092018]
	TIME [epoch: 5.78 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029483704702909803		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.029483704702909803 | validation: 0.02018900098111814]
	TIME [epoch: 5.79 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02576872494730368		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.02576872494730368 | validation: 0.02337884558343018]
	TIME [epoch: 5.79 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023561953055948257		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.023561953055948257 | validation: 0.01958655237839843]
	TIME [epoch: 5.79 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021748539026327292		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.021748539026327292 | validation: 0.018332916121589354]
	TIME [epoch: 5.79 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024120437710767554		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.024120437710767554 | validation: 0.024430557043392465]
	TIME [epoch: 5.79 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0246174847804767		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0246174847804767 | validation: 0.02111566860718793]
	TIME [epoch: 5.78 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02181746152795749		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.02181746152795749 | validation: 0.026205395217033814]
	TIME [epoch: 5.79 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025897898528895735		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.025897898528895735 | validation: 0.019799832784394473]
	TIME [epoch: 5.79 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024214539346105367		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.024214539346105367 | validation: 0.04471712121422745]
	TIME [epoch: 5.8 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040174880926682525		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.040174880926682525 | validation: 0.02027637085749867]
	TIME [epoch: 5.79 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023836979363518973		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.023836979363518973 | validation: 0.023088082034026708]
	TIME [epoch: 5.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024865851536589118		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.024865851536589118 | validation: 0.02072322570300668]
	TIME [epoch: 5.79 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023814649413150354		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.023814649413150354 | validation: 0.0189748353891407]
	TIME [epoch: 5.79 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023384628189293304		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.023384628189293304 | validation: 0.018895348790633915]
	TIME [epoch: 5.78 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022958790131331234		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.022958790131331234 | validation: 0.01835363614187006]
	TIME [epoch: 5.79 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02322963371389128		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.02322963371389128 | validation: 0.01928644551791386]
	TIME [epoch: 5.78 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022033342085807473		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.022033342085807473 | validation: 0.01644762597664068]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025170495677942217		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.025170495677942217 | validation: 0.02361627410505944]
	TIME [epoch: 5.79 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024819738880311297		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.024819738880311297 | validation: 0.0165560177298621]
	TIME [epoch: 5.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02471046030977913		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.02471046030977913 | validation: 0.02000924543055805]
	TIME [epoch: 5.79 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243697673723692		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.02243697673723692 | validation: 0.02035101880667935]
	TIME [epoch: 5.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023002094888557114		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.023002094888557114 | validation: 0.019838942043559755]
	TIME [epoch: 5.79 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026265740467464386		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.026265740467464386 | validation: 0.023989356395655068]
	TIME [epoch: 5.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024764453601771534		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.024764453601771534 | validation: 0.02018992187406874]
	TIME [epoch: 5.79 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026701669217476725		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.026701669217476725 | validation: 0.0271056483049642]
	TIME [epoch: 5.8 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026780390139263446		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.026780390139263446 | validation: 0.018674243174032098]
	TIME [epoch: 5.8 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02561404449812571		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02561404449812571 | validation: 0.02373722716836344]
	TIME [epoch: 5.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021931974999765586		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.021931974999765586 | validation: 0.021474023677495558]
	TIME [epoch: 5.79 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021275591380666644		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.021275591380666644 | validation: 0.016887804220944748]
	TIME [epoch: 5.8 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02318135689975479		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.02318135689975479 | validation: 0.021694688047730556]
	TIME [epoch: 5.79 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026599475438829127		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.026599475438829127 | validation: 0.017705518954426187]
	TIME [epoch: 5.8 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024916660149453627		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.024916660149453627 | validation: 0.024703781530121052]
	TIME [epoch: 5.8 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026997726690667783		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.026997726690667783 | validation: 0.01907910035969944]
	TIME [epoch: 5.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025970163479324668		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.025970163479324668 | validation: 0.021636089392329163]
	TIME [epoch: 5.8 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022143656238617523		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.022143656238617523 | validation: 0.019497333083634324]
	TIME [epoch: 5.8 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021668268411834904		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.021668268411834904 | validation: 0.018634170311110882]
	TIME [epoch: 5.79 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02333149940951504		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.02333149940951504 | validation: 0.024364918305852884]
	TIME [epoch: 5.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023440267848813173		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.023440267848813173 | validation: 0.019221464569363335]
	TIME [epoch: 5.79 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024535818558400837		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.024535818558400837 | validation: 0.02610933290699008]
	TIME [epoch: 5.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439145165414117		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.02439145165414117 | validation: 0.016396662359479144]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1162.pth
	Model improved!!!
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022334676557830866		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.022334676557830866 | validation: 0.01638136128518914]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023473935055610743		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.023473935055610743 | validation: 0.024656058553141152]
	TIME [epoch: 5.78 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023273083313944964		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.023273083313944964 | validation: 0.018442669612295348]
	TIME [epoch: 5.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025677137748709066		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.025677137748709066 | validation: 0.021141522890009243]
	TIME [epoch: 5.78 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025838612752487004		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.025838612752487004 | validation: 0.01919232256218649]
	TIME [epoch: 5.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026651401586925347		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.026651401586925347 | validation: 0.023075279709869903]
	TIME [epoch: 5.79 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02383308059165637		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.02383308059165637 | validation: 0.02006533092466968]
	TIME [epoch: 5.78 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02332168049511031		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02332168049511031 | validation: 0.01637391084183834]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1170.pth
	Model improved!!!
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021692329420036557		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.021692329420036557 | validation: 0.01974760768038816]
	TIME [epoch: 5.76 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024588679917086572		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.024588679917086572 | validation: 0.02131930999092087]
	TIME [epoch: 5.75 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025527799411268877		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.025527799411268877 | validation: 0.018625644917464972]
	TIME [epoch: 5.76 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023793597613359654		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.023793597613359654 | validation: 0.019607409503139873]
	TIME [epoch: 5.75 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02329808160062677		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.02329808160062677 | validation: 0.01746499269645004]
	TIME [epoch: 5.76 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022595175230564656		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.022595175230564656 | validation: 0.0196862111177092]
	TIME [epoch: 5.75 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02268252075872165		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.02268252075872165 | validation: 0.01803295877209889]
	TIME [epoch: 5.76 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02028318728026127		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.02028318728026127 | validation: 0.022542582641798693]
	TIME [epoch: 5.75 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02320671898717253		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.02320671898717253 | validation: 0.01829829800082845]
	TIME [epoch: 5.78 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024269555852353762		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.024269555852353762 | validation: 0.016149534999144687]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021324833253678187		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.021324833253678187 | validation: 0.01956580089071286]
	TIME [epoch: 5.76 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022059465176715785		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.022059465176715785 | validation: 0.023090950203927277]
	TIME [epoch: 5.76 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024008989929552586		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.024008989929552586 | validation: 0.014707031143850352]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1183.pth
	Model improved!!!
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02137400498919237		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02137400498919237 | validation: 0.02413038826616762]
	TIME [epoch: 5.77 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025732800353803365		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.025732800353803365 | validation: 0.022527775372242306]
	TIME [epoch: 5.79 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02839365085344799		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.02839365085344799 | validation: 0.019804912446026446]
	TIME [epoch: 5.78 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02642901647624483		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.02642901647624483 | validation: 0.01598059205647234]
	TIME [epoch: 5.79 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022644690714994545		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.022644690714994545 | validation: 0.020676994940020118]
	TIME [epoch: 5.75 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02074496089358917		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.02074496089358917 | validation: 0.01876974427832312]
	TIME [epoch: 5.75 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02237190806652118		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.02237190806652118 | validation: 0.019253707849828158]
	TIME [epoch: 5.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02273212253404266		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.02273212253404266 | validation: 0.019091354595375754]
	TIME [epoch: 5.77 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022305731755841985		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.022305731755841985 | validation: 0.01428774721893727]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1192.pth
	Model improved!!!
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023196918046662764		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.023196918046662764 | validation: 0.02198995419677798]
	TIME [epoch: 5.78 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02147016008678172		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.02147016008678172 | validation: 0.018036305742844694]
	TIME [epoch: 5.78 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02607822378757766		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.02607822378757766 | validation: 0.024807952626589216]
	TIME [epoch: 5.75 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025906779727848762		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.025906779727848762 | validation: 0.017130556232653256]
	TIME [epoch: 5.76 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024745489830686366		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.024745489830686366 | validation: 0.02167631728036695]
	TIME [epoch: 5.76 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022717555410914567		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.022717555410914567 | validation: 0.01710292832547492]
	TIME [epoch: 5.75 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02353650520808508		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.02353650520808508 | validation: 0.017552754268746185]
	TIME [epoch: 5.76 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021286812207753646		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.021286812207753646 | validation: 0.01819713068083643]
	TIME [epoch: 5.76 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02211889941766176		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.02211889941766176 | validation: 0.0229105733733759]
	TIME [epoch: 5.79 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02225955102612584		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02225955102612584 | validation: 0.018035636338247736]
	TIME [epoch: 5.77 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02110059392676105		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.02110059392676105 | validation: 0.01689874690968431]
	TIME [epoch: 5.78 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02644375410661706		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.02644375410661706 | validation: 0.02434335041674648]
	TIME [epoch: 5.77 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02785319659874948		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.02785319659874948 | validation: 0.016230921227340236]
	TIME [epoch: 5.78 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022764250046701493		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.022764250046701493 | validation: 0.017521670292703823]
	TIME [epoch: 5.77 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022434066948860458		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.022434066948860458 | validation: 0.021937051172944866]
	TIME [epoch: 5.78 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022471959406278767		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.022471959406278767 | validation: 0.021066551434787435]
	TIME [epoch: 5.78 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02340996871239853		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02340996871239853 | validation: 0.01796137179428885]
	TIME [epoch: 5.78 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02218738233636815		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.02218738233636815 | validation: 0.017486087938362146]
	TIME [epoch: 5.78 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021268377260765893		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.021268377260765893 | validation: 0.019473014538085864]
	TIME [epoch: 5.79 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021759658679724177		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.021759658679724177 | validation: 0.018675125904243414]
	TIME [epoch: 5.78 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02138393936021867		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.02138393936021867 | validation: 0.01734699103458556]
	TIME [epoch: 5.79 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021422038009719802		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.021422038009719802 | validation: 0.017405220618952656]
	TIME [epoch: 5.78 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024710124324545845		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.024710124324545845 | validation: 0.01301325969008111]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1215.pth
	Model improved!!!
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024330204234723735		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.024330204234723735 | validation: 0.018812810961352417]
	TIME [epoch: 5.76 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022675534377500268		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.022675534377500268 | validation: 0.018477400497298815]
	TIME [epoch: 5.78 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021666338353421823		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.021666338353421823 | validation: 0.018151632821659647]
	TIME [epoch: 5.8 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02090522467824715		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.02090522467824715 | validation: 0.015830773815531104]
	TIME [epoch: 5.79 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021878404780223164		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.021878404780223164 | validation: 0.015836381529305454]
	TIME [epoch: 5.79 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02097555993993625		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.02097555993993625 | validation: 0.01785245997138497]
	TIME [epoch: 5.79 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023465092519382218		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.023465092519382218 | validation: 0.01375176302443325]
	TIME [epoch: 5.79 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178926364399328		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.02178926364399328 | validation: 0.01963540515709819]
	TIME [epoch: 5.78 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024150658740236013		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.024150658740236013 | validation: 0.015736630751535058]
	TIME [epoch: 5.78 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023238247959219617		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.023238247959219617 | validation: 0.026335492586540762]
	TIME [epoch: 5.78 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023417561139541423		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.023417561139541423 | validation: 0.01802119702541276]
	TIME [epoch: 5.79 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027032231623270786		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.027032231623270786 | validation: 0.02486188392558174]
	TIME [epoch: 5.78 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02234969285532437		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.02234969285532437 | validation: 0.0191556203322408]
	TIME [epoch: 5.79 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022282099788648292		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.022282099788648292 | validation: 0.012201414275631529]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1229.pth
	Model improved!!!
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02189780288468495		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02189780288468495 | validation: 0.02722820749981457]
	TIME [epoch: 5.76 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243851080778864		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.02243851080778864 | validation: 0.017223799015629315]
	TIME [epoch: 5.77 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022222106065757388		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.022222106065757388 | validation: 0.016023522789853195]
	TIME [epoch: 5.76 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020698028006966743		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.020698028006966743 | validation: 0.015168624963391893]
	TIME [epoch: 5.77 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022095816691355682		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.022095816691355682 | validation: 0.016133371764803808]
	TIME [epoch: 5.76 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019655256782469262		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.019655256782469262 | validation: 0.019012590275698724]
	TIME [epoch: 5.77 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02177099296705417		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02177099296705417 | validation: 0.015332107026199505]
	TIME [epoch: 5.76 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022426520784658007		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.022426520784658007 | validation: 0.021337775785637437]
	TIME [epoch: 5.76 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02444894723041571		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.02444894723041571 | validation: 0.01476590071973264]
	TIME [epoch: 5.76 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021783455075546104		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.021783455075546104 | validation: 0.014716556197712961]
	TIME [epoch: 5.76 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022063482035409025		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.022063482035409025 | validation: 0.019077859075345968]
	TIME [epoch: 5.77 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02275792145190181		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.02275792145190181 | validation: 0.0170713863579269]
	TIME [epoch: 5.78 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021512648097090158		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.021512648097090158 | validation: 0.014758042153796109]
	TIME [epoch: 5.76 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021346614636696383		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.021346614636696383 | validation: 0.016062839985540846]
	TIME [epoch: 5.78 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02058398110772666		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.02058398110772666 | validation: 0.020260836582453087]
	TIME [epoch: 5.77 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02217819452493023		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02217819452493023 | validation: 0.017892183292715115]
	TIME [epoch: 5.77 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021839637500510913		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.021839637500510913 | validation: 0.01857674327615825]
	TIME [epoch: 5.77 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021806878835906335		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.021806878835906335 | validation: 0.015577386910453834]
	TIME [epoch: 5.77 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021690200675252073		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.021690200675252073 | validation: 0.019500589963146867]
	TIME [epoch: 5.77 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02100642100792205		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.02100642100792205 | validation: 0.015404426728025711]
	TIME [epoch: 5.77 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02173164817852305		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.02173164817852305 | validation: 0.020960524173017088]
	TIME [epoch: 5.78 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021941875197433302		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.021941875197433302 | validation: 0.0160322861040482]
	TIME [epoch: 5.77 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022742447812726895		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.022742447812726895 | validation: 0.022237512353518247]
	TIME [epoch: 5.77 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02252308113540182		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.02252308113540182 | validation: 0.015838162237508613]
	TIME [epoch: 5.75 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021606747189755807		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.021606747189755807 | validation: 0.017713229360887175]
	TIME [epoch: 5.76 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02048764007972037		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.02048764007972037 | validation: 0.018208531474403062]
	TIME [epoch: 5.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02007796938016163		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.02007796938016163 | validation: 0.015995253008368477]
	TIME [epoch: 5.75 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02101810008214958		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.02101810008214958 | validation: 0.017336029016131366]
	TIME [epoch: 5.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02172118929551025		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.02172118929551025 | validation: 0.02113986291983533]
	TIME [epoch: 5.75 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022938483792918712		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.022938483792918712 | validation: 0.017179445297832645]
	TIME [epoch: 5.77 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023057925064030968		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.023057925064030968 | validation: 0.015143758786385226]
	TIME [epoch: 5.75 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02169989675576824		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.02169989675576824 | validation: 0.017450406601159957]
	TIME [epoch: 5.76 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022156103466481315		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.022156103466481315 | validation: 0.017451860226799436]
	TIME [epoch: 5.75 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021488182581935127		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.021488182581935127 | validation: 0.01662136975861504]
	TIME [epoch: 5.77 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021764819226152605		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.021764819226152605 | validation: 0.017452440108968688]
	TIME [epoch: 5.76 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02150183404634974		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.02150183404634974 | validation: 0.01930430506946288]
	TIME [epoch: 5.77 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021549615867909707		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.021549615867909707 | validation: 0.016082122303148812]
	TIME [epoch: 5.77 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020080668825009466		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.020080668825009466 | validation: 0.01993093486313173]
	TIME [epoch: 5.77 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0207787856524427		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.0207787856524427 | validation: 0.01700938344922741]
	TIME [epoch: 5.79 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021085823012827753		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.021085823012827753 | validation: 0.018973930135604385]
	TIME [epoch: 5.78 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199223223009633		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.0199223223009633 | validation: 0.01585577305649241]
	TIME [epoch: 5.79 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02188692350337277		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.02188692350337277 | validation: 0.01965399456544614]
	TIME [epoch: 5.79 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02278303207953912		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.02278303207953912 | validation: 0.013025826265700197]
	TIME [epoch: 5.77 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023286439555579914		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.023286439555579914 | validation: 0.0234586230582857]
	TIME [epoch: 5.79 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02308469551856775		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.02308469551856775 | validation: 0.015647309157720012]
	TIME [epoch: 5.78 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020418186772717988		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.020418186772717988 | validation: 0.01986106539478745]
	TIME [epoch: 5.79 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019902500029654265		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.019902500029654265 | validation: 0.01510022006925605]
	TIME [epoch: 5.78 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020423778000255854		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.020423778000255854 | validation: 0.01579113464445085]
	TIME [epoch: 5.79 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02078534852252231		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.02078534852252231 | validation: 0.017713266638949156]
	TIME [epoch: 5.79 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019123083620170397		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.019123083620170397 | validation: 0.017625061732806653]
	TIME [epoch: 5.78 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02085175812099682		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02085175812099682 | validation: 0.015733529010229324]
	TIME [epoch: 5.79 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02229226743053063		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.02229226743053063 | validation: 0.017017177214178586]
	TIME [epoch: 5.79 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01959270689534752		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.01959270689534752 | validation: 0.0197959285779381]
	TIME [epoch: 5.79 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023043084240661545		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.023043084240661545 | validation: 0.021507888043905826]
	TIME [epoch: 5.78 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024201761178717824		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.024201761178717824 | validation: 0.019336304775930166]
	TIME [epoch: 5.79 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022887594804997916		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.022887594804997916 | validation: 0.017438388417679026]
	TIME [epoch: 5.79 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021776530187388055		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.021776530187388055 | validation: 0.01674676702271291]
	TIME [epoch: 5.78 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020832254957993505		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.020832254957993505 | validation: 0.015950995997573658]
	TIME [epoch: 5.79 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02086822810905677		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.02086822810905677 | validation: 0.01404906279126486]
	TIME [epoch: 5.77 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178079727584138		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02178079727584138 | validation: 0.02102674906952707]
	TIME [epoch: 5.77 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018601266002416086		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.018601266002416086 | validation: 0.018542475477739784]
	TIME [epoch: 5.77 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02012667514144672		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02012667514144672 | validation: 0.01767569287754698]
	TIME [epoch: 5.77 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019327694105760858		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.019327694105760858 | validation: 0.017777927800456467]
	TIME [epoch: 5.76 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02143250421003773		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.02143250421003773 | validation: 0.01434442518532052]
	TIME [epoch: 5.78 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020931312313219787		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.020931312313219787 | validation: 0.01779370804008101]
	TIME [epoch: 5.76 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020453194852263875		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.020453194852263875 | validation: 0.016798876218302002]
	TIME [epoch: 5.76 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02059624650431564		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.02059624650431564 | validation: 0.017236880566268993]
	TIME [epoch: 5.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020703528547903728		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.020703528547903728 | validation: 0.018924733635063556]
	TIME [epoch: 5.76 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021698718910939597		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.021698718910939597 | validation: 0.02305210897578104]
	TIME [epoch: 5.76 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023135229887744874		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.023135229887744874 | validation: 0.014290662844700526]
	TIME [epoch: 5.76 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02174467833117559		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.02174467833117559 | validation: 0.018184289290963006]
	TIME [epoch: 5.75 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019785961751184605		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.019785961751184605 | validation: 0.01755482897988312]
	TIME [epoch: 5.78 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020109347882499736		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.020109347882499736 | validation: 0.015488014595299616]
	TIME [epoch: 5.78 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018519097382963395		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.018519097382963395 | validation: 0.013658773396190023]
	TIME [epoch: 5.78 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02078570462245617		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.02078570462245617 | validation: 0.01580283913162498]
	TIME [epoch: 5.78 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018841485229829348		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.018841485229829348 | validation: 0.018165178113301195]
	TIME [epoch: 5.77 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01987460178636167		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.01987460178636167 | validation: 0.013481719362437273]
	TIME [epoch: 5.78 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019846779801069726		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.019846779801069726 | validation: 0.01608314755253499]
	TIME [epoch: 5.77 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02098898417879277		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02098898417879277 | validation: 0.019158609830831442]
	TIME [epoch: 5.78 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019899209137596595		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.019899209137596595 | validation: 0.019529934664586687]
	TIME [epoch: 5.77 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0215709097256739		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.0215709097256739 | validation: 0.019065642978870707]
	TIME [epoch: 5.78 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022014939660682593		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.022014939660682593 | validation: 0.012170139383404367]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1311.pth
	Model improved!!!
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020594131167219826		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.020594131167219826 | validation: 0.015001168866595188]
	TIME [epoch: 5.76 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019483777119295883		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.019483777119295883 | validation: 0.01710513206924006]
	TIME [epoch: 5.77 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019726856259863902		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.019726856259863902 | validation: 0.017190866951721374]
	TIME [epoch: 5.77 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02140369283260098		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.02140369283260098 | validation: 0.019833398922313242]
	TIME [epoch: 5.77 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022532219499018406		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.022532219499018406 | validation: 0.01882823872446185]
	TIME [epoch: 5.78 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020279262104734155		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.020279262104734155 | validation: 0.017420929943097543]
	TIME [epoch: 5.77 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019968830534738795		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.019968830534738795 | validation: 0.01939071358393717]
	TIME [epoch: 5.79 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01910303614277543		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.01910303614277543 | validation: 0.012506476093779152]
	TIME [epoch: 5.78 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019642789271900746		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.019642789271900746 | validation: 0.018927813571885145]
	TIME [epoch: 5.78 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021208515985087148		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.021208515985087148 | validation: 0.016633406355307478]
	TIME [epoch: 5.78 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020878262415069307		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.020878262415069307 | validation: 0.01648179290057979]
	TIME [epoch: 5.77 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02128224801319454		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02128224801319454 | validation: 0.01617230412111088]
	TIME [epoch: 5.77 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020799774287449252		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.020799774287449252 | validation: 0.01700139662310794]
	TIME [epoch: 5.77 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022733887584532573		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.022733887584532573 | validation: 0.015964456440567588]
	TIME [epoch: 5.76 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01879534695340092		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.01879534695340092 | validation: 0.014776340914086285]
	TIME [epoch: 5.76 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02000463982597641		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.02000463982597641 | validation: 0.020846664922767932]
	TIME [epoch: 5.76 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022010040041426402		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.022010040041426402 | validation: 0.016861289266995382]
	TIME [epoch: 5.77 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01891896176192529		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.01891896176192529 | validation: 0.016318912277259318]
	TIME [epoch: 5.77 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019017652115248238		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.019017652115248238 | validation: 0.021274091964774096]
	TIME [epoch: 5.76 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02217630062200886		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.02217630062200886 | validation: 0.013803022992085967]
	TIME [epoch: 5.76 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021012147138599873		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.021012147138599873 | validation: 0.015612947121529497]
	TIME [epoch: 5.77 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022626503747607014		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.022626503747607014 | validation: 0.016622241357787112]
	TIME [epoch: 5.77 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020868249069455485		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.020868249069455485 | validation: 0.017202962797544526]
	TIME [epoch: 5.78 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020954231667942143		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.020954231667942143 | validation: 0.01646912222092628]
	TIME [epoch: 5.76 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954782941562425		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.01954782941562425 | validation: 0.017553112784474356]
	TIME [epoch: 5.77 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967826283611913		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.01967826283611913 | validation: 0.017043102743149876]
	TIME [epoch: 5.76 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01964181819633599		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.01964181819633599 | validation: 0.018020944020724933]
	TIME [epoch: 5.77 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019544772546957995		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.019544772546957995 | validation: 0.01774390606637349]
	TIME [epoch: 5.77 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02020801351953024		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.02020801351953024 | validation: 0.01750756752555854]
	TIME [epoch: 5.76 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020738606910115864		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.020738606910115864 | validation: 0.015723379162155462]
	TIME [epoch: 5.76 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01891134224756763		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.01891134224756763 | validation: 0.020924453094006756]
	TIME [epoch: 5.76 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019674066952196725		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.019674066952196725 | validation: 0.013652935603312828]
	TIME [epoch: 5.77 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019664716926645114		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.019664716926645114 | validation: 0.012081776243569499]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1344.pth
	Model improved!!!
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019743920699939594		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.019743920699939594 | validation: 0.015342942020289569]
	TIME [epoch: 5.77 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02023000565133055		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.02023000565133055 | validation: 0.01680964293177873]
	TIME [epoch: 5.78 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020338069599805737		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.020338069599805737 | validation: 0.021044952948170118]
	TIME [epoch: 5.78 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01901103043278911		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.01901103043278911 | validation: 0.01664721085529125]
	TIME [epoch: 5.78 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02051186857065106		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.02051186857065106 | validation: 0.013521831088437176]
	TIME [epoch: 5.78 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020202957686717708		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.020202957686717708 | validation: 0.013094789238726607]
	TIME [epoch: 5.79 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018193494582030536		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.018193494582030536 | validation: 0.01671718728631056]
	TIME [epoch: 5.78 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020677917734724854		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.020677917734724854 | validation: 0.012969500297232428]
	TIME [epoch: 5.79 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018628166379855805		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.018628166379855805 | validation: 0.017130326361737485]
	TIME [epoch: 5.78 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020825689205687174		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.020825689205687174 | validation: 0.016209935505636788]
	TIME [epoch: 5.79 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021336464139309914		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.021336464139309914 | validation: 0.015114472324416352]
	TIME [epoch: 5.77 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01854777616846916		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.01854777616846916 | validation: 0.015506151900394172]
	TIME [epoch: 5.78 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02097083715779614		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.02097083715779614 | validation: 0.013496710490383784]
	TIME [epoch: 5.77 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020119500368832702		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.020119500368832702 | validation: 0.014371769094816928]
	TIME [epoch: 5.78 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02085626304427568		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.02085626304427568 | validation: 0.019638274510078765]
	TIME [epoch: 5.78 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020297639175650087		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.020297639175650087 | validation: 0.014707049926982347]
	TIME [epoch: 5.79 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019296560913584712		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.019296560913584712 | validation: 0.01305863371213803]
	TIME [epoch: 5.77 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019946770884536377		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.019946770884536377 | validation: 0.012531007075480539]
	TIME [epoch: 5.79 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018610035831690974		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.018610035831690974 | validation: 0.013329894408558958]
	TIME [epoch: 5.78 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01958987312110792		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.01958987312110792 | validation: 0.015480551405505527]
	TIME [epoch: 5.78 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021413936757281857		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.021413936757281857 | validation: 0.020792237656846096]
	TIME [epoch: 5.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019383325633653206		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.019383325633653206 | validation: 0.015038523118809372]
	TIME [epoch: 5.77 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02047414328996741		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.02047414328996741 | validation: 0.012476918089611967]
	TIME [epoch: 5.77 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017435513936297428		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.017435513936297428 | validation: 0.016449780884830945]
	TIME [epoch: 5.78 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018550312818399833		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.018550312818399833 | validation: 0.021046193199792586]
	TIME [epoch: 5.78 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021276164384260048		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.021276164384260048 | validation: 0.01961181709200493]
	TIME [epoch: 5.78 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020705341170139984		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.020705341170139984 | validation: 0.0178441070870826]
	TIME [epoch: 5.78 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019036776528441983		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.019036776528441983 | validation: 0.014719417582347329]
	TIME [epoch: 5.79 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02071457686576129		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.02071457686576129 | validation: 0.021287711750957512]
	TIME [epoch: 5.79 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019792007468922672		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.019792007468922672 | validation: 0.016785720634773904]
	TIME [epoch: 5.78 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019304718793531697		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.019304718793531697 | validation: 0.012615312475435398]
	TIME [epoch: 5.78 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020805951577477604		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.020805951577477604 | validation: 0.014841944706687999]
	TIME [epoch: 5.78 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019190638680650657		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.019190638680650657 | validation: 0.015185128606744525]
	TIME [epoch: 5.77 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01852590614654383		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.01852590614654383 | validation: 0.015207060982465588]
	TIME [epoch: 5.76 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834069201212072		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.01834069201212072 | validation: 0.01220701100634452]
	TIME [epoch: 5.76 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021260001011575786		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.021260001011575786 | validation: 0.014117128822671]
	TIME [epoch: 5.77 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01931136330692517		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.01931136330692517 | validation: 0.01299380473364009]
	TIME [epoch: 5.75 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020167673692848255		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.020167673692848255 | validation: 0.020365649433328886]
	TIME [epoch: 5.76 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02056634039655591		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.02056634039655591 | validation: 0.0169008987000705]
	TIME [epoch: 5.76 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019795373583787433		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.019795373583787433 | validation: 0.019794812008893777]
	TIME [epoch: 5.75 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02040422527382846		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.02040422527382846 | validation: 0.012265694976783077]
	TIME [epoch: 5.76 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01796066985166822		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.01796066985166822 | validation: 0.016648411817993675]
	TIME [epoch: 5.76 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019421541151921632		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.019421541151921632 | validation: 0.017211325029047236]
	TIME [epoch: 5.79 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02054660450882511		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.02054660450882511 | validation: 0.01654119329876036]
	TIME [epoch: 5.78 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019167285334048483		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.019167285334048483 | validation: 0.018392355386376503]
	TIME [epoch: 5.78 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018527523916757955		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.018527523916757955 | validation: 0.017422911071761894]
	TIME [epoch: 5.78 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020966236254559237		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.020966236254559237 | validation: 0.01094503974297863]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1391.pth
	Model improved!!!
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021375127816578268		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.021375127816578268 | validation: 0.01763951388392784]
	TIME [epoch: 5.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021489776102199258		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.021489776102199258 | validation: 0.016229577507125743]
	TIME [epoch: 5.77 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01960583584191355		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.01960583584191355 | validation: 0.014398240612531794]
	TIME [epoch: 5.78 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0198734537480822		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.0198734537480822 | validation: 0.01239035089595355]
	TIME [epoch: 5.76 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018776634950157044		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.018776634950157044 | validation: 0.015036676476510781]
	TIME [epoch: 5.75 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019132710010116495		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.019132710010116495 | validation: 0.014024178286101053]
	TIME [epoch: 5.76 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020623090808143184		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.020623090808143184 | validation: 0.012843432816118994]
	TIME [epoch: 5.76 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020189640759558744		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.020189640759558744 | validation: 0.01982354030572598]
	TIME [epoch: 5.76 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02000657278752084		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.02000657278752084 | validation: 0.018713576357803692]
	TIME [epoch: 5.76 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020588395402795036		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.020588395402795036 | validation: 0.018214385078126744]
	TIME [epoch: 5.78 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01938768302572359		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.01938768302572359 | validation: 0.014086428621807085]
	TIME [epoch: 5.78 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01943393212906723		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.01943393212906723 | validation: 0.011718719631503505]
	TIME [epoch: 5.78 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021596740695670125		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.021596740695670125 | validation: 0.014820367366681492]
	TIME [epoch: 5.79 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019286462518892256		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.019286462518892256 | validation: 0.015446340666311254]
	TIME [epoch: 5.78 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01798337779437605		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.01798337779437605 | validation: 0.013314556660858269]
	TIME [epoch: 5.78 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01885384047546869		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.01885384047546869 | validation: 0.014436632313741815]
	TIME [epoch: 5.78 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01903342318385771		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.01903342318385771 | validation: 0.015691571298602332]
	TIME [epoch: 5.77 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0187265362149422		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.0187265362149422 | validation: 0.021512525251517284]
	TIME [epoch: 5.78 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01974851538625943		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.01974851538625943 | validation: 0.013374191096828986]
	TIME [epoch: 5.78 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019203041979922807		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.019203041979922807 | validation: 0.01756947814473583]
	TIME [epoch: 5.78 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020210300170109843		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.020210300170109843 | validation: 0.013594387420166483]
	TIME [epoch: 5.79 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021001762291632114		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.021001762291632114 | validation: 0.01079818115734172]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1413.pth
	Model improved!!!
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02064492835804018		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.02064492835804018 | validation: 0.01722879694652841]
	TIME [epoch: 5.76 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017057206268808144		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.017057206268808144 | validation: 0.014209234836079465]
	TIME [epoch: 5.76 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876136722924743		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.01876136722924743 | validation: 0.013625260257760043]
	TIME [epoch: 5.77 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020290820193023022		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.020290820193023022 | validation: 0.01685115074470255]
	TIME [epoch: 5.77 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01989497849300554		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.01989497849300554 | validation: 0.015826672841482294]
	TIME [epoch: 5.77 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019322098778012387		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.019322098778012387 | validation: 0.018278253972830765]
	TIME [epoch: 5.76 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01920819395870576		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.01920819395870576 | validation: 0.010984142136462788]
	TIME [epoch: 5.76 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01798492500111538		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.01798492500111538 | validation: 0.015034495926465136]
	TIME [epoch: 5.77 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018699480009648987		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.018699480009648987 | validation: 0.014662305127739656]
	TIME [epoch: 5.76 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018491417876232933		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.018491417876232933 | validation: 0.012057573338404272]
	TIME [epoch: 5.77 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020265316423130945		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.020265316423130945 | validation: 0.020969791759670005]
	TIME [epoch: 5.77 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020479264458912967		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.020479264458912967 | validation: 0.017907578601142973]
	TIME [epoch: 5.76 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018859782627908044		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.018859782627908044 | validation: 0.013112596628014783]
	TIME [epoch: 5.77 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01936556391561366		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.01936556391561366 | validation: 0.017035593918571398]
	TIME [epoch: 5.77 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018685838640448222		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.018685838640448222 | validation: 0.01793832406161543]
	TIME [epoch: 5.76 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019084013998272725		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.019084013998272725 | validation: 0.013054056253117058]
	TIME [epoch: 5.76 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017638317569137793		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.017638317569137793 | validation: 0.01691535886301413]
	TIME [epoch: 5.77 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020128512549231613		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.020128512549231613 | validation: 0.012904068277295733]
	TIME [epoch: 5.77 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0195280386209993		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.0195280386209993 | validation: 0.014036752031639038]
	TIME [epoch: 5.78 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01895345697389034		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.01895345697389034 | validation: 0.015737564360197073]
	TIME [epoch: 5.77 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018404055198655102		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.018404055198655102 | validation: 0.014188678929496573]
	TIME [epoch: 5.78 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019113020860940626		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.019113020860940626 | validation: 0.012017781670565704]
	TIME [epoch: 5.77 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018929694139708977		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.018929694139708977 | validation: 0.014449795121886239]
	TIME [epoch: 5.76 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018337005146755913		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.018337005146755913 | validation: 0.017020152781245464]
	TIME [epoch: 5.76 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019756480948875974		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.019756480948875974 | validation: 0.014728641072079364]
	TIME [epoch: 5.76 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018678828042931833		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.018678828042931833 | validation: 0.01506641635614897]
	TIME [epoch: 5.76 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02009180702741287		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.02009180702741287 | validation: 0.01738346978688296]
	TIME [epoch: 5.76 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018588729245481113		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.018588729245481113 | validation: 0.017140959723256226]
	TIME [epoch: 5.77 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019238263719917704		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.019238263719917704 | validation: 0.013485111440543719]
	TIME [epoch: 5.77 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01868712766660732		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.01868712766660732 | validation: 0.013718506529192032]
	TIME [epoch: 5.76 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019771902484184242		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.019771902484184242 | validation: 0.015099758056232966]
	TIME [epoch: 5.78 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016968107446046904		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.016968107446046904 | validation: 0.01768262487912181]
	TIME [epoch: 5.77 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018892067925777595		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.018892067925777595 | validation: 0.018900569894751647]
	TIME [epoch: 5.79 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01929967309543827		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.01929967309543827 | validation: 0.012232934925928446]
	TIME [epoch: 5.77 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01844163573484711		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.01844163573484711 | validation: 0.014042053020880447]
	TIME [epoch: 5.77 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01750744120331343		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.01750744120331343 | validation: 0.01968632550141022]
	TIME [epoch: 5.75 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019482496050269832		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.019482496050269832 | validation: 0.013718406148992125]
	TIME [epoch: 5.77 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02043565126264651		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.02043565126264651 | validation: 0.017799753510660387]
	TIME [epoch: 5.77 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019145000346532375		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.019145000346532375 | validation: 0.014856195552218204]
	TIME [epoch: 5.77 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019976650548640274		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.019976650548640274 | validation: 0.0183641205502476]
	TIME [epoch: 5.76 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018116645739098006		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.018116645739098006 | validation: 0.013822198657887669]
	TIME [epoch: 5.77 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0189069607013967		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.0189069607013967 | validation: 0.015302936255151523]
	TIME [epoch: 5.76 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01887755724622808		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.01887755724622808 | validation: 0.014063395247044192]
	TIME [epoch: 5.77 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0208526868928058		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.0208526868928058 | validation: 0.011489271060771578]
	TIME [epoch: 5.78 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018659564041380534		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.018659564041380534 | validation: 0.018443525972002762]
	TIME [epoch: 5.77 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019098501110484837		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.019098501110484837 | validation: 0.012960126483378798]
	TIME [epoch: 5.77 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01765315729286605		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.01765315729286605 | validation: 0.012974483926310166]
	TIME [epoch: 5.78 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019515653476567824		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.019515653476567824 | validation: 0.014195796569582381]
	TIME [epoch: 5.77 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018659492464373074		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.018659492464373074 | validation: 0.015068927460390775]
	TIME [epoch: 5.77 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019080668555572146		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.019080668555572146 | validation: 0.014474081176282756]
	TIME [epoch: 5.77 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019243968303996092		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.019243968303996092 | validation: 0.012363113972896612]
	TIME [epoch: 5.79 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018182145890892926		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.018182145890892926 | validation: 0.012623386233432444]
	TIME [epoch: 5.79 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018131466690275124		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.018131466690275124 | validation: 0.01548827991452269]
	TIME [epoch: 5.79 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018890730958296704		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.018890730958296704 | validation: 0.016664119843961257]
	TIME [epoch: 5.78 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897170185164087		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.01897170185164087 | validation: 0.015079638072100582]
	TIME [epoch: 5.78 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841578484809791		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.01841578484809791 | validation: 0.011159933442409897]
	TIME [epoch: 5.79 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017253425700454548		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.017253425700454548 | validation: 0.012935739224359394]
	TIME [epoch: 5.8 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01805443671442043		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.01805443671442043 | validation: 0.0166333686868203]
	TIME [epoch: 5.77 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841451909520695		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.01841451909520695 | validation: 0.014742398838839876]
	TIME [epoch: 5.79 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017898694772076573		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.017898694772076573 | validation: 0.015309404092664753]
	TIME [epoch: 5.79 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01751957650353234		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.01751957650353234 | validation: 0.013957778425203848]
	TIME [epoch: 5.79 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016945444959761007		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.016945444959761007 | validation: 0.015278528723981883]
	TIME [epoch: 5.79 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01934914648840895		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.01934914648840895 | validation: 0.015422657924432287]
	TIME [epoch: 5.79 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019109786283135925		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.019109786283135925 | validation: 0.014468343203127383]
	TIME [epoch: 5.79 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019299534423892463		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.019299534423892463 | validation: 0.015598948194107905]
	TIME [epoch: 5.79 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01879022491585219		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.01879022491585219 | validation: 0.015787118677116307]
	TIME [epoch: 5.79 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019308900868703183		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.019308900868703183 | validation: 0.013368541196829432]
	TIME [epoch: 5.8 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018167477149722153		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.018167477149722153 | validation: 0.012272315753491109]
	TIME [epoch: 5.87 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01957407571138055		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.01957407571138055 | validation: 0.018567741710588847]
	TIME [epoch: 5.79 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018965420306527266		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.018965420306527266 | validation: 0.011846569619615645]
	TIME [epoch: 5.8 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897594306576374		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.01897594306576374 | validation: 0.013374073046165658]
	TIME [epoch: 5.79 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01857507050439716		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.01857507050439716 | validation: 0.014259884812872055]
	TIME [epoch: 5.8 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018488137327655616		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.018488137327655616 | validation: 0.014706395368460957]
	TIME [epoch: 5.79 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019217223806368765		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.019217223806368765 | validation: 0.01549776995016884]
	TIME [epoch: 5.79 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017945594152555217		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.017945594152555217 | validation: 0.013459129398712223]
	TIME [epoch: 5.79 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018156447456948416		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.018156447456948416 | validation: 0.013327694500262461]
	TIME [epoch: 5.8 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017716612520143053		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.017716612520143053 | validation: 0.01513590918061531]
	TIME [epoch: 5.76 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016529251641905492		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.016529251641905492 | validation: 0.015396675119196741]
	TIME [epoch: 5.77 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954981510863686		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.01954981510863686 | validation: 0.010518665834142389]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1492.pth
	Model improved!!!
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01796105168322804		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.01796105168322804 | validation: 0.021226641352532324]
	TIME [epoch: 5.83 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01940594824164689		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.01940594824164689 | validation: 0.01020220094677049]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1494.pth
	Model improved!!!
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018253263491031787		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.018253263491031787 | validation: 0.011639898866745026]
	TIME [epoch: 5.76 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01654083444146201		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.01654083444146201 | validation: 0.01454541354935397]
	TIME [epoch: 5.77 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018209985791127547		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.018209985791127547 | validation: 0.014334535111038271]
	TIME [epoch: 5.77 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017784532957690943		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.017784532957690943 | validation: 0.01302920796734135]
	TIME [epoch: 5.79 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01948325477869662		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.01948325477869662 | validation: 0.017004651176142145]
	TIME [epoch: 5.78 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019535248786742034		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.019535248786742034 | validation: 0.013155433459833594]
	TIME [epoch: 5.79 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018072419411352784		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.018072419411352784 | validation: 0.013170368701612773]
	TIME [epoch: 5.76 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017667969064754725		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.017667969064754725 | validation: 0.02019335814391269]
	TIME [epoch: 5.78 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01857533565122964		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.01857533565122964 | validation: 0.01164993253037664]
	TIME [epoch: 5.75 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018876733498419222		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.018876733498419222 | validation: 0.01891299112724919]
	TIME [epoch: 5.78 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019091321321675358		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.019091321321675358 | validation: 0.01380304545427561]
	TIME [epoch: 5.76 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016984369892453076		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.016984369892453076 | validation: 0.013381319519884883]
	TIME [epoch: 5.77 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01853075435771628		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.01853075435771628 | validation: 0.012994389132041446]
	TIME [epoch: 5.77 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017465329893772023		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.017465329893772023 | validation: 0.01471425525905833]
	TIME [epoch: 5.78 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018711119848076004		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.018711119848076004 | validation: 0.01381928805315008]
	TIME [epoch: 5.76 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018740390868272172		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.018740390868272172 | validation: 0.011306038058911722]
	TIME [epoch: 5.77 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018343495862027163		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.018343495862027163 | validation: 0.0134752662944237]
	TIME [epoch: 5.76 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017317396286134764		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.017317396286134764 | validation: 0.011715460805315143]
	TIME [epoch: 5.78 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01819631715388472		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.01819631715388472 | validation: 0.011456800976787342]
	TIME [epoch: 5.78 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01883086198801377		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.01883086198801377 | validation: 0.014034052418021699]
	TIME [epoch: 5.78 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016854332565750516		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.016854332565750516 | validation: 0.011204539117108376]
	TIME [epoch: 5.76 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017714138199319482		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.017714138199319482 | validation: 0.015827620229746982]
	TIME [epoch: 5.77 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01830518616788449		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.01830518616788449 | validation: 0.014990531444731316]
	TIME [epoch: 5.76 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019413470133066327		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.019413470133066327 | validation: 0.012826614701332718]
	TIME [epoch: 5.77 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018345061309332725		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.018345061309332725 | validation: 0.01437107943139217]
	TIME [epoch: 5.77 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0176118353531956		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.0176118353531956 | validation: 0.012792700948780773]
	TIME [epoch: 5.78 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018624718314856927		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.018624718314856927 | validation: 0.013304150525454273]
	TIME [epoch: 5.78 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01741707427980022		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.01741707427980022 | validation: 0.016003763923050415]
	TIME [epoch: 5.77 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019948191383330538		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.019948191383330538 | validation: 0.012534775950927158]
	TIME [epoch: 5.78 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01860464329535625		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.01860464329535625 | validation: 0.015478377712262292]
	TIME [epoch: 5.78 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018940924433773695		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.018940924433773695 | validation: 0.014616293277479042]
	TIME [epoch: 5.77 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01870615892087762		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.01870615892087762 | validation: 0.011080152569284758]
	TIME [epoch: 5.78 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017321981524009978		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.017321981524009978 | validation: 0.013033031843596938]
	TIME [epoch: 5.77 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019676325105533864		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.019676325105533864 | validation: 0.011846183423430202]
	TIME [epoch: 5.77 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01773853456686804		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.01773853456686804 | validation: 0.009203714122860496]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1529.pth
	Model improved!!!
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019944721153629285		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.019944721153629285 | validation: 0.017255919242275437]
	TIME [epoch: 5.77 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020251964446134148		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.020251964446134148 | validation: 0.013471570632890685]
	TIME [epoch: 5.77 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017606863261566297		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.017606863261566297 | validation: 0.014994225584126575]
	TIME [epoch: 5.77 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020719564938499187		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.020719564938499187 | validation: 0.018615512381825572]
	TIME [epoch: 5.77 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018675553236751685		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.018675553236751685 | validation: 0.01301286070471029]
	TIME [epoch: 5.77 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017971538460579525		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.017971538460579525 | validation: 0.018269781763951203]
	TIME [epoch: 5.76 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01924547166695673		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.01924547166695673 | validation: 0.01339902860483282]
	TIME [epoch: 5.77 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018281258686471024		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.018281258686471024 | validation: 0.012701721963812041]
	TIME [epoch: 5.76 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019288051750920118		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.019288051750920118 | validation: 0.014270245826113038]
	TIME [epoch: 5.77 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018085061560436426		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.018085061560436426 | validation: 0.015593576045798025]
	TIME [epoch: 5.77 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018074561057891497		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.018074561057891497 | validation: 0.012998748005878608]
	TIME [epoch: 5.77 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01835425976917882		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.01835425976917882 | validation: 0.014131449728294311]
	TIME [epoch: 5.77 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016862346446081627		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.016862346446081627 | validation: 0.01669247349114349]
	TIME [epoch: 5.77 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018100993353200426		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.018100993353200426 | validation: 0.014624938102092727]
	TIME [epoch: 5.78 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01662552640138105		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.01662552640138105 | validation: 0.014025579243867782]
	TIME [epoch: 5.77 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01866291802695299		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.01866291802695299 | validation: 0.01648360870254415]
	TIME [epoch: 5.78 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01751416303392099		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.01751416303392099 | validation: 0.014706333643298692]
	TIME [epoch: 5.77 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018995231462090933		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.018995231462090933 | validation: 0.01110615786118392]
	TIME [epoch: 5.76 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01930481019975825		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.01930481019975825 | validation: 0.011479585713618057]
	TIME [epoch: 5.76 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018232537549714097		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.018232537549714097 | validation: 0.014127460624522099]
	TIME [epoch: 5.77 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018779043973376958		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.018779043973376958 | validation: 0.013977236742052307]
	TIME [epoch: 5.77 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01973462665983702		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.01973462665983702 | validation: 0.01375981630371096]
	TIME [epoch: 5.77 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018437715978268928		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.018437715978268928 | validation: 0.017311760011679334]
	TIME [epoch: 5.78 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017388815449559277		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.017388815449559277 | validation: 0.012569341434539189]
	TIME [epoch: 5.79 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018898431545024016		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.018898431545024016 | validation: 0.012595675095922099]
	TIME [epoch: 5.78 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017826264799149994		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.017826264799149994 | validation: 0.016729835094551705]
	TIME [epoch: 5.76 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01679805042180185		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.01679805042180185 | validation: 0.01549208302085775]
	TIME [epoch: 5.76 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017800430983832966		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.017800430983832966 | validation: 0.01314109512394694]
	TIME [epoch: 5.77 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018196789976616543		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.018196789976616543 | validation: 0.012433989008931412]
	TIME [epoch: 5.77 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01780826043879882		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.01780826043879882 | validation: 0.01640641103531562]
	TIME [epoch: 5.77 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018612619566383656		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.018612619566383656 | validation: 0.016541383092987017]
	TIME [epoch: 5.77 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017597380044349745		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.017597380044349745 | validation: 0.012584768645336525]
	TIME [epoch: 5.77 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017719556052653254		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.017719556052653254 | validation: 0.013678012072974172]
	TIME [epoch: 5.78 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01671679917084299		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.01671679917084299 | validation: 0.014285901118753726]
	TIME [epoch: 5.78 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017900014185241515		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.017900014185241515 | validation: 0.014313644473772182]
	TIME [epoch: 5.78 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018790499049567543		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.018790499049567543 | validation: 0.011211759585281]
	TIME [epoch: 5.76 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018103944528850693		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.018103944528850693 | validation: 0.011805209957392855]
	TIME [epoch: 5.76 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017878083030766285		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.017878083030766285 | validation: 0.013174686280370763]
	TIME [epoch: 5.76 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01908666916727829		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.01908666916727829 | validation: 0.012676675968745627]
	TIME [epoch: 5.77 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019300836612568727		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.019300836612568727 | validation: 0.01332587036003598]
	TIME [epoch: 5.76 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019115902868491932		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.019115902868491932 | validation: 0.01404625062961079]
	TIME [epoch: 5.77 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017899434763173655		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.017899434763173655 | validation: 0.011825330618001978]
	TIME [epoch: 5.78 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017323707925971116		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.017323707925971116 | validation: 0.01741867925082859]
	TIME [epoch: 5.77 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01737026799271666		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.01737026799271666 | validation: 0.011216476785102604]
	TIME [epoch: 5.77 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02030450540128065		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.02030450540128065 | validation: 0.013337715891532527]
	TIME [epoch: 5.77 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018633890118786668		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.018633890118786668 | validation: 0.014251135623648098]
	TIME [epoch: 5.77 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019576052288755665		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.019576052288755665 | validation: 0.012541825344912771]
	TIME [epoch: 5.76 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019474288975908624		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.019474288975908624 | validation: 0.015539329650514645]
	TIME [epoch: 5.76 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01898273481804613		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.01898273481804613 | validation: 0.0128485665777625]
	TIME [epoch: 5.77 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017382722357029295		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.017382722357029295 | validation: 0.013815155639877936]
	TIME [epoch: 5.77 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017556995136933123		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.017556995136933123 | validation: 0.011845753965654671]
	TIME [epoch: 5.77 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01753339375341902		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.01753339375341902 | validation: 0.01659561019634675]
	TIME [epoch: 5.77 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018381797613213086		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.018381797613213086 | validation: 0.013226993596454146]
	TIME [epoch: 5.76 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017720814747720248		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.017720814747720248 | validation: 0.011911311434830813]
	TIME [epoch: 5.76 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016980957185291527		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.016980957185291527 | validation: 0.014650524760880796]
	TIME [epoch: 5.76 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01803675325260041		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.01803675325260041 | validation: 0.012715272813787933]
	TIME [epoch: 5.76 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019380731770650857		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.019380731770650857 | validation: 0.0103047374161599]
	TIME [epoch: 5.77 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01776888683107469		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.01776888683107469 | validation: 0.01469637327579535]
	TIME [epoch: 5.76 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018116975315944688		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.018116975315944688 | validation: 0.013901170270421244]
	TIME [epoch: 5.76 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01770601585636703		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.01770601585636703 | validation: 0.011398412525638358]
	TIME [epoch: 5.77 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01886079220685634		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.01886079220685634 | validation: 0.011431279941863721]
	TIME [epoch: 5.77 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01786999480694302		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.01786999480694302 | validation: 0.013755303113877283]
	TIME [epoch: 5.78 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018089788798676704		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.018089788798676704 | validation: 0.013881418565109882]
	TIME [epoch: 5.76 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017331063840409593		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.017331063840409593 | validation: 0.01217993363575981]
	TIME [epoch: 5.76 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018096592771712376		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.018096592771712376 | validation: 0.014115992774028958]
	TIME [epoch: 5.77 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018888926731852926		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.018888926731852926 | validation: 0.011960170056357716]
	TIME [epoch: 5.76 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018558506521416437		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.018558506521416437 | validation: 0.009425376313973617]
	TIME [epoch: 5.76 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017010360833322035		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.017010360833322035 | validation: 0.011384714382592637]
	TIME [epoch: 5.77 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01772652194428466		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.01772652194428466 | validation: 0.012955546189222067]
	TIME [epoch: 5.77 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019218778549256		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.019218778549256 | validation: 0.01726289593831807]
	TIME [epoch: 5.77 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017843100916272482		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.017843100916272482 | validation: 0.013809115834459946]
	TIME [epoch: 5.77 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01744327581938571		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.01744327581938571 | validation: 0.01352909384369932]
	TIME [epoch: 5.77 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018453572419423116		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.018453572419423116 | validation: 0.013661049351122357]
	TIME [epoch: 5.77 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01881475502897163		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.01881475502897163 | validation: 0.013107851712104351]
	TIME [epoch: 5.76 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01867256241934608		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.01867256241934608 | validation: 0.012889822574265315]
	TIME [epoch: 5.76 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01742924597797908		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.01742924597797908 | validation: 0.015870276556648768]
	TIME [epoch: 5.76 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017170450646161937		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.017170450646161937 | validation: 0.012654031684839279]
	TIME [epoch: 5.76 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0180563327463497		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.0180563327463497 | validation: 0.014676508469195394]
	TIME [epoch: 5.76 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016678656295316077		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.016678656295316077 | validation: 0.015692530365811853]
	TIME [epoch: 5.76 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018533327842313298		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.018533327842313298 | validation: 0.01500712786016057]
	TIME [epoch: 5.76 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017034107003017652		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.017034107003017652 | validation: 0.011595596108927181]
	TIME [epoch: 5.76 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018906838490469178		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.018906838490469178 | validation: 0.016738057546335734]
	TIME [epoch: 5.77 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0179155795501347		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.0179155795501347 | validation: 0.012801645339631003]
	TIME [epoch: 5.76 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018615377568368282		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.018615377568368282 | validation: 0.015169257143931037]
	TIME [epoch: 5.76 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016971980178726084		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.016971980178726084 | validation: 0.014876875197527657]
	TIME [epoch: 5.77 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017441852902223116		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.017441852902223116 | validation: 0.01663816471970892]
	TIME [epoch: 5.76 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017425422647909628		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.017425422647909628 | validation: 0.012020769133354271]
	TIME [epoch: 5.76 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019203928096623104		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.019203928096623104 | validation: 0.01685169257004344]
	TIME [epoch: 5.77 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01736029987638953		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.01736029987638953 | validation: 0.01166168678493716]
	TIME [epoch: 5.77 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018363865627024082		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.018363865627024082 | validation: 0.012845938355586057]
	TIME [epoch: 5.77 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018176861883335397		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.018176861883335397 | validation: 0.012277306000856292]
	TIME [epoch: 5.77 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018086618076285116		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.018086618076285116 | validation: 0.014115731324692455]
	TIME [epoch: 5.77 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01766315216355212		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.01766315216355212 | validation: 0.016355269125380146]
	TIME [epoch: 5.77 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01855864268693002		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.01855864268693002 | validation: 0.014415736257504842]
	TIME [epoch: 5.77 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018006064644749357		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.018006064644749357 | validation: 0.015332269005445966]
	TIME [epoch: 5.77 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01737665127407966		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.01737665127407966 | validation: 0.010720702656167575]
	TIME [epoch: 5.77 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017280894450905526		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.017280894450905526 | validation: 0.01000754391373433]
	TIME [epoch: 5.77 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01805434239805633		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.01805434239805633 | validation: 0.01200304074818766]
	TIME [epoch: 5.77 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018065644403505627		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.018065644403505627 | validation: 0.016382203072587122]
	TIME [epoch: 5.78 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01820577139835264		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.01820577139835264 | validation: 0.016235306059754106]
	TIME [epoch: 5.77 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01651368020871586		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.01651368020871586 | validation: 0.014346416994157257]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_152921/states/model_phi1_3a_v_mmd1_1630.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6750.012 seconds.
