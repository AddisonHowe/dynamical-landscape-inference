Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2517734177

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.714655509043923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.714655509043923 | validation: 5.679933357766318]
	TIME [epoch: 184 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.509676312989268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.509676312989268 | validation: 5.455585403987692]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1350987690883585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1350987690883585 | validation: 5.525432250535696]
	TIME [epoch: 2.75 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1439417608639895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1439417608639895 | validation: 5.239765907677512]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.756464121223051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756464121223051 | validation: 5.154453149230963]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.640268250428942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.640268250428942 | validation: 4.842019065041992]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.323004367977006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323004367977006 | validation: 4.355728218580085]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.173226634541483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.173226634541483 | validation: 4.748283136335782]
	TIME [epoch: 2.74 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.46782139594896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.46782139594896 | validation: 4.210637779869077]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7782742801147347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7782742801147347 | validation: 4.541748467537959]
	TIME [epoch: 2.75 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5258256743022045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5258256743022045 | validation: 4.012338139288564]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5953598349379288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5953598349379288 | validation: 4.295858955129133]
	TIME [epoch: 2.74 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.813403994324206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.813403994324206 | validation: 3.9077389257792507]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4677563165812098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4677563165812098 | validation: 3.8246575934448384]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6576515789867483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6576515789867483 | validation: 3.9072820346967063]
	TIME [epoch: 2.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4486689097468854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4486689097468854 | validation: 3.744805677643207]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2407735047620507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2407735047620507 | validation: 3.5608077629833943]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.092781580423836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.092781580423836 | validation: 3.339993808530734]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8752889898595595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8752889898595595 | validation: 3.058085673260496]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.649239167971085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.649239167971085 | validation: 3.3755499190342295]
	TIME [epoch: 2.74 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.623733548958901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.623733548958901 | validation: 3.0326482864362085]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6464438921407747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6464438921407747 | validation: 3.4182327408645556]
	TIME [epoch: 2.74 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2066190119822524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2066190119822524 | validation: 2.7008341885838294]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4166765122532197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4166765122532197 | validation: 2.79301969276588]
	TIME [epoch: 2.75 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.426790655087777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.426790655087777 | validation: 2.3040761759288975]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.097544097907305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.097544097907305 | validation: 2.2280248847294324]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0093307118218537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0093307118218537 | validation: 2.0332058367795742]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8777824826812435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8777824826812435 | validation: 2.0780666780280987]
	TIME [epoch: 2.74 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8393396135945457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8393396135945457 | validation: 1.9112254303703602]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.711025246134901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.711025246134901 | validation: 1.7834838769101993]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9058832390282796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9058832390282796 | validation: 2.389158772208533]
	TIME [epoch: 2.75 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0498883291341876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0498883291341876 | validation: 2.0272323414921507]
	TIME [epoch: 2.75 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.81154037467307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.81154037467307 | validation: 1.6086904014142662]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.607333956457695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.607333956457695 | validation: 1.8533524388317706]
	TIME [epoch: 2.76 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9119116417666022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9119116417666022 | validation: 1.5387330132789439]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6279773801317203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6279773801317203 | validation: 1.8620501169552008]
	TIME [epoch: 2.74 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6498684537017658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6498684537017658 | validation: 1.4174959877267594]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3239943332415516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3239943332415516 | validation: 1.6096531483532972]
	TIME [epoch: 2.74 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7095864999035915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7095864999035915 | validation: 1.5200655286948583]
	TIME [epoch: 2.74 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4021276810169112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4021276810169112 | validation: 1.3582805482534388]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268675368853367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.268675368853367 | validation: 1.215181818332292]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3931399125596409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3931399125596409 | validation: 1.2560512995889195]
	TIME [epoch: 2.74 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1654807217542287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1654807217542287 | validation: 1.3665750863194452]
	TIME [epoch: 2.74 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2889804128254785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2889804128254785 | validation: 1.115442544877842]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2438281023890994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2438281023890994 | validation: 1.0607445678751775]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1269577846789165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1269577846789165 | validation: 1.3161240015231286]
	TIME [epoch: 2.74 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274247496259806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.274247496259806 | validation: 1.107663878558172]
	TIME [epoch: 2.73 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2290235422738973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2290235422738973 | validation: 1.0189089630015637]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1502403442427067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1502403442427067 | validation: 1.16097221828518]
	TIME [epoch: 2.74 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0727551399074304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0727551399074304 | validation: 1.112856565575165]
	TIME [epoch: 2.73 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0363346463895933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0363346463895933 | validation: 1.1511153597346861]
	TIME [epoch: 2.74 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057352116662301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.057352116662301 | validation: 1.005652161517606]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0727624716082076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0727624716082076 | validation: 0.9902138258430299]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355297739390212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0355297739390212 | validation: 1.177141382117706]
	TIME [epoch: 2.75 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.07987919234562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.07987919234562 | validation: 1.0802976987104713]
	TIME [epoch: 2.74 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192327849636977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0192327849636977 | validation: 1.0008161761393115]
	TIME [epoch: 2.74 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.041372690542853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.041372690542853 | validation: 1.0466375972106396]
	TIME [epoch: 2.74 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0060069925749426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0060069925749426 | validation: 0.9815167357682306]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862181797658224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9862181797658224 | validation: 1.0474231194006511]
	TIME [epoch: 2.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9794662799308617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9794662799308617 | validation: 1.012026513482083]
	TIME [epoch: 2.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1071221174323116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1071221174323116 | validation: 1.0973574974396432]
	TIME [epoch: 2.75 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0477520363359043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0477520363359043 | validation: 0.9121406507393275]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0072038643142516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0072038643142516 | validation: 1.0127702865362544]
	TIME [epoch: 2.75 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863459093053368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9863459093053368 | validation: 0.9119123722795828]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9920982372500197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9920982372500197 | validation: 0.9042176804333668]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9545534835829196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9545534835829196 | validation: 0.8659313879760163]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9289383966393436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9289383966393436 | validation: 0.8670521626168388]
	TIME [epoch: 2.75 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9186628359923869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9186628359923869 | validation: 0.8457690215573056]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127616266068609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127616266068609 | validation: 0.8598670418869911]
	TIME [epoch: 2.75 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9079757774306362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9079757774306362 | validation: 0.9809433517684383]
	TIME [epoch: 2.75 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1697462572694424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1697462572694424 | validation: 1.2116827655366675]
	TIME [epoch: 2.75 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1716758820711721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1716758820711721 | validation: 0.8323467809692442]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9476255110251086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9476255110251086 | validation: 0.8706292042611481]
	TIME [epoch: 2.75 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9786141706529826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9786141706529826 | validation: 0.8687726680392711]
	TIME [epoch: 2.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9351758435274724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9351758435274724 | validation: 0.8068624769079951]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9004446598847744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9004446598847744 | validation: 0.865003585461793]
	TIME [epoch: 2.75 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420708259426607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9420708259426607 | validation: 0.9114198636088112]
	TIME [epoch: 2.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0366867570168774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0366867570168774 | validation: 0.8395703939986325]
	TIME [epoch: 2.75 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251894878525968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9251894878525968 | validation: 0.7784111225919174]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885485790692486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885485790692486 | validation: 0.7788654282527466]
	TIME [epoch: 2.75 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963551049155052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8963551049155052 | validation: 1.026316334242138]
	TIME [epoch: 2.75 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0582129348619482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0582129348619482 | validation: 0.8507270258228843]
	TIME [epoch: 2.75 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643020744270885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9643020744270885 | validation: 0.7975323598553967]
	TIME [epoch: 2.76 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923340291647831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923340291647831 | validation: 0.7706143196801852]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766957366018291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8766957366018291 | validation: 0.7722089807731033]
	TIME [epoch: 2.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891302781931399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891302781931399 | validation: 0.8151209542351229]
	TIME [epoch: 2.76 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159057707774373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9159057707774373 | validation: 0.8270931277057336]
	TIME [epoch: 2.75 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9798157853129758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9798157853129758 | validation: 0.8647662670692751]
	TIME [epoch: 2.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472228226976441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9472228226976441 | validation: 0.7743155186592627]
	TIME [epoch: 2.76 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8911297529931489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8911297529931489 | validation: 0.7691520665981921]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890965257066643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8890965257066643 | validation: 0.7988052142494348]
	TIME [epoch: 2.76 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811836867770726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811836867770726 | validation: 0.7709371799151528]
	TIME [epoch: 2.75 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9082128158488533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9082128158488533 | validation: 0.7974772125139659]
	TIME [epoch: 2.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959470602789585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8959470602789585 | validation: 0.7675706942347266]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888073101686874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888073101686874 | validation: 0.7908799541636615]
	TIME [epoch: 2.75 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894139447623287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.894139447623287 | validation: 0.8584851239845297]
	TIME [epoch: 2.75 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940090847266271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9940090847266271 | validation: 0.8550548417044372]
	TIME [epoch: 2.75 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9670833366361844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9670833366361844 | validation: 0.8418004509108052]
	TIME [epoch: 2.75 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9399875444279564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9399875444279564 | validation: 0.8353650415137344]
	TIME [epoch: 2.75 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9503253391290784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9503253391290784 | validation: 0.7641985936836773]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724972672038334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724972672038334 | validation: 0.7711403750403607]
	TIME [epoch: 2.74 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794774294798937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794774294798937 | validation: 0.7916568782656294]
	TIME [epoch: 2.74 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885930746503233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885930746503233 | validation: 0.8268735063635841]
	TIME [epoch: 2.74 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971494632546552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.971494632546552 | validation: 0.8128699835826545]
	TIME [epoch: 2.74 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278233320698367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278233320698367 | validation: 0.78873254862131]
	TIME [epoch: 2.74 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944768846166551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944768846166551 | validation: 0.7822216217870999]
	TIME [epoch: 2.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038514608948819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038514608948819 | validation: 0.7754177159812841]
	TIME [epoch: 2.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973686916370052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973686916370052 | validation: 0.7542049521517399]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632839591828472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8632839591828472 | validation: 0.7566604221120079]
	TIME [epoch: 2.75 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572440455034374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572440455034374 | validation: 0.7575092661520384]
	TIME [epoch: 2.75 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625902038457585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625902038457585 | validation: 0.8733735962137413]
	TIME [epoch: 2.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047009789315141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.047009789315141 | validation: 0.9499635540485478]
	TIME [epoch: 2.74 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0649475292853767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0649475292853767 | validation: 0.7479198334230477]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779750488049847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8779750488049847 | validation: 0.8218901019622553]
	TIME [epoch: 2.74 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9246124544558691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9246124544558691 | validation: 0.8025891662643846]
	TIME [epoch: 2.74 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291622268135693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291622268135693 | validation: 0.8175461390892228]
	TIME [epoch: 2.74 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980710804928285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8980710804928285 | validation: 0.7864471097650052]
	TIME [epoch: 2.75 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506550468617266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9506550468617266 | validation: 0.8217157214128282]
	TIME [epoch: 2.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9454969578888216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9454969578888216 | validation: 0.7704757266611578]
	TIME [epoch: 2.75 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9073066875745545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9073066875745545 | validation: 0.7547701197904826]
	TIME [epoch: 2.74 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860197803518252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860197803518252 | validation: 0.7480373524244214]
	TIME [epoch: 2.74 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726818319738459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8726818319738459 | validation: 0.7601928141776035]
	TIME [epoch: 2.74 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982773467475568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8982773467475568 | validation: 0.7911443374648562]
	TIME [epoch: 2.74 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105070262065519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9105070262065519 | validation: 0.8027034730703533]
	TIME [epoch: 2.74 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9484598974615744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9484598974615744 | validation: 0.7937317069653398]
	TIME [epoch: 2.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957811639682582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957811639682582 | validation: 0.8296389899547618]
	TIME [epoch: 2.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053731038742596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053731038742596 | validation: 0.7801021842647785]
	TIME [epoch: 2.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9547188883586347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9547188883586347 | validation: 0.7624825825341517]
	TIME [epoch: 2.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757639960955362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8757639960955362 | validation: 0.7637907402922046]
	TIME [epoch: 2.74 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707366886041229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8707366886041229 | validation: 0.766531899802658]
	TIME [epoch: 2.74 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.880634993276763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.880634993276763 | validation: 0.794184588080112]
	TIME [epoch: 2.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990215122061136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8990215122061136 | validation: 0.8764747083442632]
	TIME [epoch: 2.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9837237996950026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9837237996950026 | validation: 0.7351326267771683]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734420051797834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734420051797834 | validation: 0.7733749637713316]
	TIME [epoch: 2.74 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734825453111723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734825453111723 | validation: 0.7646104457486116]
	TIME [epoch: 2.75 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9076038770058688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9076038770058688 | validation: 0.7905513257518039]
	TIME [epoch: 2.75 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086735955276558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9086735955276558 | validation: 0.7657950093278949]
	TIME [epoch: 2.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049416631793054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049416631793054 | validation: 0.7902427199419323]
	TIME [epoch: 2.75 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89800309621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.89800309621 | validation: 0.7390465348659909]
	TIME [epoch: 2.75 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606776566787597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8606776566787597 | validation: 0.7750352029300838]
	TIME [epoch: 2.74 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869733953002292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869733953002292 | validation: 0.8033522373753285]
	TIME [epoch: 2.74 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032270681164309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032270681164309 | validation: 0.7640362321697224]
	TIME [epoch: 2.74 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547394418826122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547394418826122 | validation: 0.7368075893048148]
	TIME [epoch: 2.74 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509567050066149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8509567050066149 | validation: 0.7523221079451128]
	TIME [epoch: 2.74 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8753604709891585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8753604709891585 | validation: 0.8163798104004538]
	TIME [epoch: 2.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9473209625673065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9473209625673065 | validation: 0.8406859414424294]
	TIME [epoch: 2.74 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9713335870617809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9713335870617809 | validation: 0.8165896340568527]
	TIME [epoch: 2.74 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921676548314712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.921676548314712 | validation: 0.9115829385582205]
	TIME [epoch: 2.74 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.984207980025456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.984207980025456 | validation: 0.7572938756304717]
	TIME [epoch: 2.74 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834256189497828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834256189497828 | validation: 0.7680677453601712]
	TIME [epoch: 2.74 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512077210000135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512077210000135 | validation: 0.7386613729520587]
	TIME [epoch: 2.77 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90606850128338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.90606850128338 | validation: 0.7671125483826682]
	TIME [epoch: 2.75 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616342604713333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8616342604713333 | validation: 0.7530231834974738]
	TIME [epoch: 2.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935851603907407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8935851603907407 | validation: 0.7737965492790506]
	TIME [epoch: 2.74 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652907620686204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8652907620686204 | validation: 0.7410682611568092]
	TIME [epoch: 2.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873848939640945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873848939640945 | validation: 0.8921712886263515]
	TIME [epoch: 2.74 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357244938442232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9357244938442232 | validation: 0.7850078459668416]
	TIME [epoch: 2.74 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804151665604303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8804151665604303 | validation: 0.8338352810186342]
	TIME [epoch: 2.74 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532306687997592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532306687997592 | validation: 0.9629088254271534]
	TIME [epoch: 2.74 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024349374846273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.024349374846273 | validation: 0.9234391503637348]
	TIME [epoch: 2.74 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0329628120348189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0329628120348189 | validation: 0.7170466295166733]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246330154494181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8246330154494181 | validation: 0.7986925920280064]
	TIME [epoch: 2.75 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947107983473983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8947107983473983 | validation: 0.7289737799555638]
	TIME [epoch: 2.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644425905262475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644425905262475 | validation: 0.7132777646538152]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057113404461103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057113404461103 | validation: 0.7512795820206617]
	TIME [epoch: 2.74 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8118560122179804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8118560122179804 | validation: 0.7279463560243344]
	TIME [epoch: 2.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926027353360049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7926027353360049 | validation: 0.8181529112546055]
	TIME [epoch: 2.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604906213741085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604906213741085 | validation: 1.7029681852406349]
	TIME [epoch: 2.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6299796496674464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6299796496674464 | validation: 1.0428873815251507]
	TIME [epoch: 2.74 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1121037840559183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1121037840559183 | validation: 0.7724407893926326]
	TIME [epoch: 2.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588046574098749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588046574098749 | validation: 0.7832538565031871]
	TIME [epoch: 2.74 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090640524726792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9090640524726792 | validation: 0.7007580813335912]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437169156211343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8437169156211343 | validation: 0.7654976697996104]
	TIME [epoch: 2.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588874187527029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588874187527029 | validation: 0.7122747540724215]
	TIME [epoch: 2.74 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291732830315265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8291732830315265 | validation: 0.7186539267838056]
	TIME [epoch: 2.74 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241345984811017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241345984811017 | validation: 0.7164640825155957]
	TIME [epoch: 2.74 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156676248490705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8156676248490705 | validation: 0.7226857512101641]
	TIME [epoch: 2.74 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159800068310628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8159800068310628 | validation: 0.7176181383100049]
	TIME [epoch: 2.74 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109405985111365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109405985111365 | validation: 0.741730441463986]
	TIME [epoch: 2.74 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293671614857505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8293671614857505 | validation: 0.7228953830543977]
	TIME [epoch: 2.74 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495345375287429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8495345375287429 | validation: 0.7207488656934901]
	TIME [epoch: 2.74 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837444496134239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837444496134239 | validation: 0.6897078115150825]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778152172087907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778152172087907 | validation: 0.7285611890629631]
	TIME [epoch: 2.74 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745385524512755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745385524512755 | validation: 0.883968001589378]
	TIME [epoch: 2.74 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049894149397897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049894149397897 | validation: 1.0636012082200614]
	TIME [epoch: 2.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1118225290983457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1118225290983457 | validation: 0.73527283594315]
	TIME [epoch: 2.74 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422429698197335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8422429698197335 | validation: 0.7155386589206594]
	TIME [epoch: 2.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612315155870437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612315155870437 | validation: 0.7763811117761759]
	TIME [epoch: 2.74 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036348834911594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036348834911594 | validation: 0.7863169162493739]
	TIME [epoch: 2.74 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489601503146458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8489601503146458 | validation: 0.8611528198851559]
	TIME [epoch: 2.74 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868987076680582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8868987076680582 | validation: 0.8405628197124664]
	TIME [epoch: 2.74 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848661864400322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848661864400322 | validation: 0.673457582074086]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376851994472177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7376851994472177 | validation: 0.6609127858572637]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.70442000648218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.70442000648218 | validation: 0.7683051942668074]
	TIME [epoch: 2.74 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515567973287387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7515567973287387 | validation: 1.0116667277307845]
	TIME [epoch: 2.73 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0307216064249851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0307216064249851 | validation: 0.8981790889136848]
	TIME [epoch: 2.74 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070241902737084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9070241902737084 | validation: 0.6877921561545133]
	TIME [epoch: 2.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73811706551847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73811706551847 | validation: 0.6981096400651592]
	TIME [epoch: 2.74 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484991543184806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484991543184806 | validation: 0.647294976776265]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150581368728947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7150581368728947 | validation: 0.7092855946183078]
	TIME [epoch: 2.74 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990338048025884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6990338048025884 | validation: 0.7012276165167642]
	TIME [epoch: 183 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889713542263683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889713542263683 | validation: 0.9553364845341099]
	TIME [epoch: 5.88 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9721572193171198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9721572193171198 | validation: 0.6859334041323917]
	TIME [epoch: 5.87 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762695246439956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7762695246439956 | validation: 0.6292972922594663]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190071663812085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190071663812085 | validation: 0.691969204611954]
	TIME [epoch: 5.87 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952969591918463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6952969591918463 | validation: 0.8097802742278714]
	TIME [epoch: 5.87 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034983742779324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8034983742779324 | validation: 1.1273367272331158]
	TIME [epoch: 5.88 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14901800161762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.14901800161762 | validation: 0.7034289974662878]
	TIME [epoch: 5.88 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322894597960349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8322894597960349 | validation: 0.7662746355675167]
	TIME [epoch: 5.88 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822275490925978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822275490925978 | validation: 0.7304103827497055]
	TIME [epoch: 5.88 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739989872857968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739989872857968 | validation: 0.6226265488703004]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688685752948511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688685752948511 | validation: 0.6549056474295505]
	TIME [epoch: 5.88 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6460240488897847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6460240488897847 | validation: 0.6490807200431767]
	TIME [epoch: 5.88 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501653246962532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501653246962532 | validation: 0.7374317672865855]
	TIME [epoch: 5.87 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215201251937998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215201251937998 | validation: 0.9579281587952704]
	TIME [epoch: 5.88 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9756619227187883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9756619227187883 | validation: 0.6998563883470447]
	TIME [epoch: 5.87 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279530443572561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279530443572561 | validation: 0.683488102962183]
	TIME [epoch: 5.88 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227832009666145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227832009666145 | validation: 0.7267281710454736]
	TIME [epoch: 5.88 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276793891208214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7276793891208214 | validation: 0.7362849969264882]
	TIME [epoch: 5.88 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864610182071661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864610182071661 | validation: 0.8265039302681212]
	TIME [epoch: 5.87 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292688349021404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292688349021404 | validation: 0.6574630471465631]
	TIME [epoch: 5.88 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053423368695917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053423368695917 | validation: 0.5975890808650686]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6807229956235827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6807229956235827 | validation: 0.6367060111367903]
	TIME [epoch: 5.88 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6216324341197376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6216324341197376 | validation: 0.7433165842278738]
	TIME [epoch: 5.87 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859358270756916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6859358270756916 | validation: 0.9450024570666193]
	TIME [epoch: 5.88 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9559783393834478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9559783393834478 | validation: 0.6874153159937955]
	TIME [epoch: 5.87 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467394139601183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467394139601183 | validation: 0.6811356378443089]
	TIME [epoch: 5.87 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771747287793679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6771747287793679 | validation: 0.5709958962824827]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447722109824757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6447722109824757 | validation: 0.6120563435099133]
	TIME [epoch: 5.87 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062419292601721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6062419292601721 | validation: 0.6062346377613465]
	TIME [epoch: 5.87 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6131665284170071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6131665284170071 | validation: 0.7574638427562882]
	TIME [epoch: 5.88 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841617483848019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841617483848019 | validation: 0.8823097996918015]
	TIME [epoch: 5.88 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9790526200403724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9790526200403724 | validation: 0.677487839680754]
	TIME [epoch: 5.89 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410553296350093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7410553296350093 | validation: 0.6832194888516949]
	TIME [epoch: 5.87 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227889683132633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227889683132633 | validation: 0.6309161069493616]
	TIME [epoch: 5.89 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972737794141464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6972737794141464 | validation: 0.5837972437515877]
	TIME [epoch: 5.87 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021497755515314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6021497755515314 | validation: 0.6352224313944191]
	TIME [epoch: 5.88 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.60287168212533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.60287168212533 | validation: 0.634506899271082]
	TIME [epoch: 5.88 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385117521038525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6385117521038525 | validation: 0.8495462522416619]
	TIME [epoch: 5.87 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576211285768756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8576211285768756 | validation: 0.9177432503172475]
	TIME [epoch: 5.88 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9594271254877628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9594271254877628 | validation: 0.6419333416718637]
	TIME [epoch: 5.87 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363844370233201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363844370233201 | validation: 0.645822898757677]
	TIME [epoch: 5.87 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181212064133372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181212064133372 | validation: 0.6331604906958059]
	TIME [epoch: 5.87 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586174469292498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586174469292498 | validation: 0.6060623715436737]
	TIME [epoch: 5.88 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6877157497255588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6877157497255588 | validation: 0.8221904982117121]
	TIME [epoch: 5.87 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083541514615774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083541514615774 | validation: 0.8189876618790901]
	TIME [epoch: 5.87 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328416532735892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328416532735892 | validation: 0.7832681987112111]
	TIME [epoch: 5.87 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218144545883763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8218144545883763 | validation: 0.6229318368388693]
	TIME [epoch: 5.87 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66830977348888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66830977348888 | validation: 0.6207618125168262]
	TIME [epoch: 5.88 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710412760314657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710412760314657 | validation: 0.5774913672260275]
	TIME [epoch: 5.87 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6041493206833181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6041493206833181 | validation: 0.5651160992438794]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855970478865804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5855970478865804 | validation: 0.6413146257515532]
	TIME [epoch: 5.87 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6080180439750844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6080180439750844 | validation: 0.5634768393045361]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372215576519296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6372215576519296 | validation: 0.6689990945307697]
	TIME [epoch: 5.87 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322629715521828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322629715521828 | validation: 0.5592244744634781]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428142688567079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428142688567079 | validation: 0.7486746256772379]
	TIME [epoch: 5.87 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366378963447052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366378963447052 | validation: 0.6497460041678974]
	TIME [epoch: 5.87 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681300919263302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.681300919263302 | validation: 0.5872263756111453]
	TIME [epoch: 5.87 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079687319143069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6079687319143069 | validation: 0.5017170686958927]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5428390519611273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5428390519611273 | validation: 0.5499254347988087]
	TIME [epoch: 5.87 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411249578379378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5411249578379378 | validation: 0.514868319355285]
	TIME [epoch: 5.87 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439245303925511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5439245303925511 | validation: 0.607527703038871]
	TIME [epoch: 5.87 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6060532717934285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6060532717934285 | validation: 0.8271074830013968]
	TIME [epoch: 5.87 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469927358662014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469927358662014 | validation: 0.619804143264695]
	TIME [epoch: 5.87 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378566090745043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378566090745043 | validation: 0.5439093498753785]
	TIME [epoch: 5.87 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489480921151322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5489480921151322 | validation: 0.567971299709303]
	TIME [epoch: 5.87 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489462274561894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5489462274561894 | validation: 0.583371697371778]
	TIME [epoch: 5.87 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498146995724653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498146995724653 | validation: 0.7870746390488534]
	TIME [epoch: 5.87 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573631467434811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573631467434811 | validation: 0.8652790339533605]
	TIME [epoch: 5.88 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052711002467345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052711002467345 | validation: 0.562772066812532]
	TIME [epoch: 5.87 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421139445935687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6421139445935687 | validation: 0.6324108943601875]
	TIME [epoch: 5.87 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659680814399504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.659680814399504 | validation: 0.6804365537727645]
	TIME [epoch: 5.87 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235250718274759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235250718274759 | validation: 0.5909119140306573]
	TIME [epoch: 5.88 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065101709373917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6065101709373917 | validation: 0.5537099390544115]
	TIME [epoch: 5.88 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530160923972648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530160923972648 | validation: 0.5183574297501515]
	TIME [epoch: 5.88 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526954525537251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.526954525537251 | validation: 0.5189862747416981]
	TIME [epoch: 5.87 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139173753153242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139173753153242 | validation: 0.4847053761194897]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030747640393708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030747640393708 | validation: 0.4945291058992203]
	TIME [epoch: 5.87 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4868488828426278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4868488828426278 | validation: 0.49333827650298034]
	TIME [epoch: 5.87 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4860650679391087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4860650679391087 | validation: 0.46571336964090215]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48909863605876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48909863605876 | validation: 0.6118031291606233]
	TIME [epoch: 5.89 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045820975082263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6045820975082263 | validation: 0.8704219797646213]
	TIME [epoch: 5.89 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0277589968723166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0277589968723166 | validation: 0.6356778795911118]
	TIME [epoch: 5.89 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879541344934753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879541344934753 | validation: 0.8269271494220757]
	TIME [epoch: 5.89 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9274277045658061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9274277045658061 | validation: 0.6351310924332596]
	TIME [epoch: 5.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596665535366282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6596665535366282 | validation: 0.5911779336683876]
	TIME [epoch: 5.89 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319471238404236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6319471238404236 | validation: 0.500904628734561]
	TIME [epoch: 5.89 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068600672261144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5068600672261144 | validation: 0.5263610124267792]
	TIME [epoch: 5.89 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5230353876419312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5230353876419312 | validation: 0.4690161955756832]
	TIME [epoch: 5.88 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49512999018679593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49512999018679593 | validation: 0.4842775261773835]
	TIME [epoch: 5.87 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4944388407408084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4944388407408084 | validation: 0.5505983629757276]
	TIME [epoch: 5.87 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5602443184159167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5602443184159167 | validation: 0.7378000822365007]
	TIME [epoch: 5.87 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275336224134601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7275336224134601 | validation: 0.5449885097251107]
	TIME [epoch: 5.88 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328565300793051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6328565300793051 | validation: 0.45588227487513777]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47800593714779493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47800593714779493 | validation: 0.5489493916163916]
	TIME [epoch: 5.9 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5333812328151731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5333812328151731 | validation: 0.4898368080117127]
	TIME [epoch: 5.89 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5509780216832989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5509780216832989 | validation: 0.5384152728935783]
	TIME [epoch: 5.9 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5543358095773573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5543358095773573 | validation: 0.5403613466782738]
	TIME [epoch: 5.89 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437684487606397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437684487606397 | validation: 0.4730583939167387]
	TIME [epoch: 5.89 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4804228481272807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4804228481272807 | validation: 0.45603777516942134]
	TIME [epoch: 5.89 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4563752115197158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4563752115197158 | validation: 0.5247414702101638]
	TIME [epoch: 5.88 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5120074126983056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5120074126983056 | validation: 0.5973776616213233]
	TIME [epoch: 5.87 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618600653682098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.618600653682098 | validation: 0.6402662173615252]
	TIME [epoch: 5.88 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586841323555356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586841323555356 | validation: 0.48883380853778846]
	TIME [epoch: 5.87 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5339425548487798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5339425548487798 | validation: 0.40413629871723417]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43728262561773423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43728262561773423 | validation: 0.4804217542778334]
	TIME [epoch: 5.9 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48499954293220493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48499954293220493 | validation: 0.43232636004329167]
	TIME [epoch: 5.9 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4740113824715988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4740113824715988 | validation: 0.5218124994037645]
	TIME [epoch: 5.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258137062700632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5258137062700632 | validation: 0.5224553026383657]
	TIME [epoch: 5.9 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521599202740648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.521599202740648 | validation: 0.5010318096272148]
	TIME [epoch: 5.9 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5245833735700309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5245833735700309 | validation: 0.42806475947695355]
	TIME [epoch: 5.91 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673193074267083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673193074267083 | validation: 0.5343831972217251]
	TIME [epoch: 5.9 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.53548318947873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.53548318947873 | validation: 0.47974443865105293]
	TIME [epoch: 5.91 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5770644962705774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5770644962705774 | validation: 0.49024948100554905]
	TIME [epoch: 5.9 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47713118778625463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47713118778625463 | validation: 0.5317575979348609]
	TIME [epoch: 5.89 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49927743340680625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49927743340680625 | validation: 0.44348669158771864]
	TIME [epoch: 5.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4705973402139972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4705973402139972 | validation: 0.45702406428339554]
	TIME [epoch: 5.9 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4382546753990626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4382546753990626 | validation: 0.41540155820775715]
	TIME [epoch: 5.9 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42072909482564397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42072909482564397 | validation: 0.3944822625641268]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42003217229858225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42003217229858225 | validation: 0.472078895318345]
	TIME [epoch: 5.87 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45883133934458625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45883133934458625 | validation: 0.48588589184808095]
	TIME [epoch: 5.87 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5216125197487788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5216125197487788 | validation: 0.5596259795670674]
	TIME [epoch: 5.86 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.597146338673521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.597146338673521 | validation: 0.43049663776676594]
	TIME [epoch: 5.87 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5337469228417341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5337469228417341 | validation: 0.39022704132799196]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871988596852269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871988596852269 | validation: 0.47771350528832673]
	TIME [epoch: 5.87 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4783993361121585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4783993361121585 | validation: 0.4374780207334869]
	TIME [epoch: 5.87 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494868961599794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494868961599794 | validation: 0.3966426624523696]
	TIME [epoch: 5.87 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42890627082075455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42890627082075455 | validation: 0.6991526991476296]
	TIME [epoch: 5.87 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286536669274911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6286536669274911 | validation: 0.4697856663697537]
	TIME [epoch: 5.87 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5124194145871853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5124194145871853 | validation: 0.4528174012596986]
	TIME [epoch: 5.86 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5570947467116559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5570947467116559 | validation: 0.38523193763651]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3961099677263258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3961099677263258 | validation: 0.525138961937654]
	TIME [epoch: 5.89 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45382885513680876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45382885513680876 | validation: 0.49380507690210784]
	TIME [epoch: 5.91 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49860417298972076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49860417298972076 | validation: 0.556000765769917]
	TIME [epoch: 5.86 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5917674605459383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5917674605459383 | validation: 0.45594318853315613]
	TIME [epoch: 5.87 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48428440157851244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48428440157851244 | validation: 0.35517451131305067]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796850896144243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3796850896144243 | validation: 0.35064371378594567]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3614467473508964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3614467473508964 | validation: 0.4161335404881985]
	TIME [epoch: 5.88 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39308160692949334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39308160692949334 | validation: 0.3877046187343765]
	TIME [epoch: 5.9 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44684281172375173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44684281172375173 | validation: 0.44832491723134427]
	TIME [epoch: 5.9 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45741060107610354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45741060107610354 | validation: 0.45125459945397073]
	TIME [epoch: 5.89 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4824605927458029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4824605927458029 | validation: 0.37272237283766263]
	TIME [epoch: 5.9 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3808037377120582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3808037377120582 | validation: 0.33700547830590744]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323511908416263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.323511908416263 | validation: 0.30932174181831323]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33559838686571153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33559838686571153 | validation: 0.3792501119359967]
	TIME [epoch: 5.9 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35142497698234565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35142497698234565 | validation: 0.41920036182590015]
	TIME [epoch: 5.9 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43802554228499374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43802554228499374 | validation: 0.5165797464262404]
	TIME [epoch: 5.9 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5490180467003041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490180467003041 | validation: 0.4620447900285356]
	TIME [epoch: 5.9 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625613168416769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5625613168416769 | validation: 0.3980229715072233]
	TIME [epoch: 5.9 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36949116210674804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36949116210674804 | validation: 0.42954390578207974]
	TIME [epoch: 5.9 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41925888395940897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41925888395940897 | validation: 0.3764363448031995]
	TIME [epoch: 5.9 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39818739234872313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39818739234872313 | validation: 0.35402417514028817]
	TIME [epoch: 5.91 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3535312685978289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3535312685978289 | validation: 0.2982873804987646]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905460746443272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2905460746443272 | validation: 0.2936384626637142]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2956621222279023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2956621222279023 | validation: 0.34971248919549014]
	TIME [epoch: 5.9 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3103008693832555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3103008693832555 | validation: 0.31917245023433516]
	TIME [epoch: 5.91 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3335262057638586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3335262057638586 | validation: 0.3856792947020735]
	TIME [epoch: 5.9 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3495165981397168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3495165981397168 | validation: 0.5202092183599873]
	TIME [epoch: 5.9 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5720508262122922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5720508262122922 | validation: 0.41142804086221824]
	TIME [epoch: 5.9 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5268541533541479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5268541533541479 | validation: 0.48716703493435215]
	TIME [epoch: 5.9 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168984494068454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4168984494068454 | validation: 0.36170224068243295]
	TIME [epoch: 5.9 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33712234842146543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33712234842146543 | validation: 0.3689407495905959]
	TIME [epoch: 5.9 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.420712900016582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.420712900016582 | validation: 0.41432660942917743]
	TIME [epoch: 5.9 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43330905243988854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43330905243988854 | validation: 0.2624919968886298]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970938559540299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2970938559540299 | validation: 0.30223482382546873]
	TIME [epoch: 5.9 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2934382578923156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2934382578923156 | validation: 0.3772960298506258]
	TIME [epoch: 5.9 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34514803873942274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34514803873942274 | validation: 0.29417915169878245]
	TIME [epoch: 5.91 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227809385103326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227809385103326 | validation: 0.27289429379518365]
	TIME [epoch: 5.9 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749871065773807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749871065773807 | validation: 0.32279265569975335]
	TIME [epoch: 5.9 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041449194134211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3041449194134211 | validation: 0.421471029995633]
	TIME [epoch: 5.92 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43876650167161596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43876650167161596 | validation: 0.3058432168222763]
	TIME [epoch: 5.9 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32247799394909293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32247799394909293 | validation: 0.2939281570475999]
	TIME [epoch: 5.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28833864652129776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28833864652129776 | validation: 0.3331842581121104]
	TIME [epoch: 5.9 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3366180488142044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3366180488142044 | validation: 0.37073854186902966]
	TIME [epoch: 5.9 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39066542195497816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39066542195497816 | validation: 0.37292099776071697]
	TIME [epoch: 5.9 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3600567928207712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3600567928207712 | validation: 0.20747781315981806]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22917721320670573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22917721320670573 | validation: 0.24165849175967274]
	TIME [epoch: 5.91 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22161376189383145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22161376189383145 | validation: 0.25380656539260693]
	TIME [epoch: 5.9 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27275444240774976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27275444240774976 | validation: 0.7328600608845923]
	TIME [epoch: 5.9 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224706818024649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224706818024649 | validation: 0.26353370712524155]
	TIME [epoch: 5.89 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661244331591508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2661244331591508 | validation: 0.480518103197594]
	TIME [epoch: 5.89 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417036962490953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5417036962490953 | validation: 0.47270428668974385]
	TIME [epoch: 5.89 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45243165047132666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45243165047132666 | validation: 0.22035813839887708]
	TIME [epoch: 5.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24151941467973756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24151941467973756 | validation: 0.2243302037620025]
	TIME [epoch: 5.89 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2351528155050963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2351528155050963 | validation: 0.20853838248768797]
	TIME [epoch: 5.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2095386058156653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2095386058156653 | validation: 0.20608457385381743]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19884608489553907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19884608489553907 | validation: 0.18603427363119798]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19372844681351745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19372844681351745 | validation: 0.21792316361960712]
	TIME [epoch: 5.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19576204638687184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19576204638687184 | validation: 0.24155535414040163]
	TIME [epoch: 5.91 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26093782908929536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26093782908929536 | validation: 0.49051013464543236]
	TIME [epoch: 5.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537686647208158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5537686647208158 | validation: 0.3563506552811082]
	TIME [epoch: 5.9 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4035768618520744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4035768618520744 | validation: 0.5486450857723035]
	TIME [epoch: 5.9 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40129374281634783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40129374281634783 | validation: 0.532033602374157]
	TIME [epoch: 5.9 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445418264239668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445418264239668 | validation: 0.28684493558787255]
	TIME [epoch: 5.9 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35848525856691005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35848525856691005 | validation: 0.4494212742930273]
	TIME [epoch: 5.9 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37968916694537563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37968916694537563 | validation: 0.31395123722230744]
	TIME [epoch: 5.9 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28308449507378264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28308449507378264 | validation: 0.21250990987766058]
	TIME [epoch: 5.9 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23198885622075976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23198885622075976 | validation: 0.21990996712930128]
	TIME [epoch: 5.9 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2113394103204417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2113394103204417 | validation: 0.1791385174511777]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19064319358766596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19064319358766596 | validation: 0.19849407467712216]
	TIME [epoch: 5.9 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18598625511591912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18598625511591912 | validation: 0.20757701836501885]
	TIME [epoch: 5.91 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20629130914720606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20629130914720606 | validation: 0.3588094666710176]
	TIME [epoch: 5.9 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266701927306559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3266701927306559 | validation: 0.34798073398262874]
	TIME [epoch: 5.91 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37013069874384114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37013069874384114 | validation: 0.4084710904898364]
	TIME [epoch: 5.9 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3494368607252972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3494368607252972 | validation: 0.23172716655065778]
	TIME [epoch: 5.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25184009502444776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25184009502444776 | validation: 0.18432047678446983]
	TIME [epoch: 5.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18823789263641036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18823789263641036 | validation: 0.23419126140074403]
	TIME [epoch: 5.9 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19418006766171936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19418006766171936 | validation: 0.16908768554572048]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1970443879931625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1970443879931625 | validation: 0.3776345832140724]
	TIME [epoch: 5.9 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886181937292312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2886181937292312 | validation: 0.3681183805761363]
	TIME [epoch: 5.9 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093805705404368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4093805705404368 | validation: 0.1958771188697494]
	TIME [epoch: 5.9 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20434429789320915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20434429789320915 | validation: 0.32238061968344145]
	TIME [epoch: 5.9 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2738298508159515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2738298508159515 | validation: 0.46070547545143975]
	TIME [epoch: 5.9 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47072551972599647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47072551972599647 | validation: 0.24949059883117553]
	TIME [epoch: 5.9 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2877886793008812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2877886793008812 | validation: 0.3558305225822358]
	TIME [epoch: 5.9 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2773504269484327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2773504269484327 | validation: 0.22725070146611387]
	TIME [epoch: 5.9 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271088652034013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271088652034013 | validation: 0.19657625586566022]
	TIME [epoch: 5.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21251199041588123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21251199041588123 | validation: 0.3792734388965653]
	TIME [epoch: 5.91 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143350461906263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143350461906263 | validation: 0.34272880145549134]
	TIME [epoch: 5.9 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35665878589270333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35665878589270333 | validation: 0.253404273567168]
	TIME [epoch: 5.9 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24545652000559584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24545652000559584 | validation: 0.20846744866336397]
	TIME [epoch: 5.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19438797415581555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19438797415581555 | validation: 0.1711743274891996]
	TIME [epoch: 5.9 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2011567763623457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2011567763623457 | validation: 0.22308114916056743]
	TIME [epoch: 5.9 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17877988162691985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17877988162691985 | validation: 0.14965521547379013]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17011970871435697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17011970871435697 | validation: 0.16935011507675252]
	TIME [epoch: 5.87 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15350755832386162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15350755832386162 | validation: 0.20296139762977228]
	TIME [epoch: 5.87 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935598826662828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1935598826662828 | validation: 0.4112266056959573]
	TIME [epoch: 5.87 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4030337589700514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4030337589700514 | validation: 0.3239460597687527]
	TIME [epoch: 5.87 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701049481097406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3701049481097406 | validation: 0.3843821196377034]
	TIME [epoch: 5.87 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762691502880729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2762691502880729 | validation: 0.2127295451977533]
	TIME [epoch: 5.89 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613667317501606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2613667317501606 | validation: 0.2304545784357518]
	TIME [epoch: 5.88 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22479592719839248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22479592719839248 | validation: 0.16534165489991393]
	TIME [epoch: 5.88 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18802017232897883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18802017232897883 | validation: 0.18954390652777375]
	TIME [epoch: 5.87 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16759847493407068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16759847493407068 | validation: 0.17412589075730797]
	TIME [epoch: 5.87 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19141201859087736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19141201859087736 | validation: 0.24551663351953013]
	TIME [epoch: 5.88 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18953771570835404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18953771570835404 | validation: 0.18698550132539601]
	TIME [epoch: 5.88 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21736071397678064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21736071397678064 | validation: 0.2783068483435278]
	TIME [epoch: 5.88 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20482448365705133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20482448365705133 | validation: 0.2834990751428958]
	TIME [epoch: 5.87 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806002011231552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2806002011231552 | validation: 0.1806009839362966]
	TIME [epoch: 5.88 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20866429072686152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20866429072686152 | validation: 0.34630218714209426]
	TIME [epoch: 5.87 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26451947712940466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26451947712940466 | validation: 0.21098588363037793]
	TIME [epoch: 5.87 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21841527065475647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21841527065475647 | validation: 0.192442241605211]
	TIME [epoch: 5.87 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815182615165901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815182615165901 | validation: 0.2247961493903267]
	TIME [epoch: 5.88 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18572890277862622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18572890277862622 | validation: 0.19357683191213604]
	TIME [epoch: 5.88 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23442288653799664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23442288653799664 | validation: 0.2573765597281358]
	TIME [epoch: 5.88 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20405099296946166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20405099296946166 | validation: 0.19376330289455249]
	TIME [epoch: 5.88 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21850611556985108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21850611556985108 | validation: 0.27049367992880063]
	TIME [epoch: 5.88 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1973140495711394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1973140495711394 | validation: 0.17118753795249628]
	TIME [epoch: 5.88 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20097429138753975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20097429138753975 | validation: 0.1740121630784334]
	TIME [epoch: 5.88 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1537961989245341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1537961989245341 | validation: 0.16035146136934778]
	TIME [epoch: 5.88 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519105157011682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1519105157011682 | validation: 0.2509009647959057]
	TIME [epoch: 5.88 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24384794945988095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24384794945988095 | validation: 0.5118168543492736]
	TIME [epoch: 5.88 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4975262580002205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4975262580002205 | validation: 0.15496001981992735]
	TIME [epoch: 5.88 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18285701856008857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18285701856008857 | validation: 0.16611526318927258]
	TIME [epoch: 5.88 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17286810167249592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17286810167249592 | validation: 0.18818177251950985]
	TIME [epoch: 5.88 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19494787125793556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19494787125793556 | validation: 0.1698325230599448]
	TIME [epoch: 5.87 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17687161843512325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17687161843512325 | validation: 0.15449720894082677]
	TIME [epoch: 5.87 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15724019369896072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15724019369896072 | validation: 0.18399009914609832]
	TIME [epoch: 5.87 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15111977184326503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15111977184326503 | validation: 0.14970568533694414]
	TIME [epoch: 5.87 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17502732088460246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17502732088460246 | validation: 0.22752195271668674]
	TIME [epoch: 5.88 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17455539534499123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17455539534499123 | validation: 0.202072460273633]
	TIME [epoch: 5.87 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22080274640887435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22080274640887435 | validation: 0.19350914278307144]
	TIME [epoch: 5.88 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15140065531082594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15140065531082594 | validation: 0.14437162241740284]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15588079482490455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15588079482490455 | validation: 0.31630140863753914]
	TIME [epoch: 5.91 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21305655023013437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21305655023013437 | validation: 0.37358464741124275]
	TIME [epoch: 5.91 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399578105630786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4399578105630786 | validation: 0.36672789536629724]
	TIME [epoch: 5.9 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3322848638196736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3322848638196736 | validation: 0.1970581879055049]
	TIME [epoch: 5.9 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832329043782732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832329043782732 | validation: 0.1415506842598153]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15694837992833502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15694837992833502 | validation: 0.6601518833958199]
	TIME [epoch: 5.88 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44176805815875486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44176805815875486 | validation: 0.24369410389614013]
	TIME [epoch: 5.86 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2048296767795005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2048296767795005 | validation: 0.20023295849871714]
	TIME [epoch: 5.88 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2476792787611664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2476792787611664 | validation: 0.14785103007540643]
	TIME [epoch: 5.86 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14432442657741393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14432442657741393 | validation: 0.1778171834909494]
	TIME [epoch: 5.85 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15336412511940345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15336412511940345 | validation: 0.1259982522886506]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14541408349844137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14541408349844137 | validation: 0.14780678533692268]
	TIME [epoch: 5.85 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13651214261606454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13651214261606454 | validation: 0.16583527015428318]
	TIME [epoch: 5.84 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13359384362877386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13359384362877386 | validation: 0.15414847604021517]
	TIME [epoch: 5.85 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16938338521017174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16938338521017174 | validation: 0.24089411357101786]
	TIME [epoch: 5.85 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21816850728024523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21816850728024523 | validation: 0.3561515672909316]
	TIME [epoch: 5.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3478754737931222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3478754737931222 | validation: 0.11547080407859021]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15147156123804828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15147156123804828 | validation: 0.27910503314902]
	TIME [epoch: 5.86 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21698115121048417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21698115121048417 | validation: 0.20659499755479216]
	TIME [epoch: 5.85 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2338156694712906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2338156694712906 | validation: 0.13548521835311084]
	TIME [epoch: 5.85 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13889739142795912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13889739142795912 | validation: 0.12534449778204285]
	TIME [epoch: 5.85 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12849613462087556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12849613462087556 | validation: 0.1461961339275055]
	TIME [epoch: 5.85 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.144141117969192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.144141117969192 | validation: 0.2642367922649839]
	TIME [epoch: 5.85 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2770624476172577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2770624476172577 | validation: 0.518494841460744]
	TIME [epoch: 5.86 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4702477012326649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4702477012326649 | validation: 0.1251925729236097]
	TIME [epoch: 5.85 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1512121774208779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1512121774208779 | validation: 0.22750689113165556]
	TIME [epoch: 5.86 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22628111124212377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22628111124212377 | validation: 0.15962667600499014]
	TIME [epoch: 5.85 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16954179333010516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16954179333010516 | validation: 0.11891749060334705]
	TIME [epoch: 5.85 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15771341416766566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15771341416766566 | validation: 0.15942372018980172]
	TIME [epoch: 5.85 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14336813796830242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14336813796830242 | validation: 0.129669597975388]
	TIME [epoch: 5.85 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896629451025598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14896629451025598 | validation: 0.21844824848245753]
	TIME [epoch: 5.86 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15936305240328674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15936305240328674 | validation: 0.13028521444415517]
	TIME [epoch: 5.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16605279983717403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16605279983717403 | validation: 0.36420406904128266]
	TIME [epoch: 5.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23641845130699785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23641845130699785 | validation: 0.23938335866977326]
	TIME [epoch: 5.86 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2648309647268247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2648309647268247 | validation: 0.29103660611518534]
	TIME [epoch: 5.85 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24798759977996238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24798759977996238 | validation: 0.15671762365853742]
	TIME [epoch: 5.85 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15183130550786442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15183130550786442 | validation: 0.13959034268503498]
	TIME [epoch: 5.86 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15154323008098738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15154323008098738 | validation: 0.13894047429014408]
	TIME [epoch: 186 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435535225124152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1435535225124152 | validation: 0.16147084302664327]
	TIME [epoch: 12.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148298238959893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.148298238959893 | validation: 0.13958687391615873]
	TIME [epoch: 12.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16132342680053546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16132342680053546 | validation: 0.19120872446352788]
	TIME [epoch: 12.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1392303198851434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1392303198851434 | validation: 0.13851460617440345]
	TIME [epoch: 12.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16334293766378088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16334293766378088 | validation: 0.23531850337020585]
	TIME [epoch: 12.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18292459518275853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18292459518275853 | validation: 0.25938625705260243]
	TIME [epoch: 12.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24654776411826126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24654776411826126 | validation: 0.14097837051493953]
	TIME [epoch: 12.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18498110841207216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18498110841207216 | validation: 0.493857733278404]
	TIME [epoch: 12.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31740001528242173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31740001528242173 | validation: 0.13608457828593035]
	TIME [epoch: 12.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15718512167973273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15718512167973273 | validation: 0.15362673154010473]
	TIME [epoch: 12.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742491497729194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742491497729194 | validation: 0.2781153732520518]
	TIME [epoch: 12.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19673991838729576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19673991838729576 | validation: 0.1274423727730114]
	TIME [epoch: 12.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673371901748497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673371901748497 | validation: 0.15284948550617394]
	TIME [epoch: 12.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538762028713404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538762028713404 | validation: 0.1455126981557752]
	TIME [epoch: 12.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14551898767089652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14551898767089652 | validation: 0.20045026863995005]
	TIME [epoch: 12.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18116682005388782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18116682005388782 | validation: 0.21087841416285338]
	TIME [epoch: 12.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089512899100631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2089512899100631 | validation: 0.2770880645897599]
	TIME [epoch: 12.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21618141951544875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21618141951544875 | validation: 0.17301335676552476]
	TIME [epoch: 12.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20749745088047697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20749745088047697 | validation: 0.3527555132879794]
	TIME [epoch: 12.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215283442598453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2215283442598453 | validation: 0.155922211264966]
	TIME [epoch: 12.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589257525687928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589257525687928 | validation: 0.1262808278869357]
	TIME [epoch: 12.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434229197125467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1434229197125467 | validation: 0.12143883873103532]
	TIME [epoch: 12.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12165549827912124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12165549827912124 | validation: 0.12351717000459668]
	TIME [epoch: 12.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13245857411577475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13245857411577475 | validation: 0.17526526013092059]
	TIME [epoch: 12.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15613072908849193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15613072908849193 | validation: 0.21396346916179707]
	TIME [epoch: 12.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2231149916110153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2231149916110153 | validation: 0.1454162965487864]
	TIME [epoch: 12.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451547059849185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1451547059849185 | validation: 0.2696660212083125]
	TIME [epoch: 12.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19749027514760076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19749027514760076 | validation: 0.2470427822435012]
	TIME [epoch: 12.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889961082076298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2889961082076298 | validation: 0.30870450117914777]
	TIME [epoch: 12.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20288104225465609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20288104225465609 | validation: 0.18724849195299081]
	TIME [epoch: 12.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17897101134502003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17897101134502003 | validation: 0.11900510192731235]
	TIME [epoch: 12.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14209347127812838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14209347127812838 | validation: 0.13550582302716183]
	TIME [epoch: 12.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315303322740799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1315303322740799 | validation: 0.1092381426376806]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663616262844788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11663616262844788 | validation: 0.09907802718678539]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803819421875184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803819421875184 | validation: 0.12280829386682027]
	TIME [epoch: 12.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584976185003754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10584976185003754 | validation: 0.09100777961057095]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12246470065072139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12246470065072139 | validation: 0.3536090683851619]
	TIME [epoch: 12.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21425373044307883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21425373044307883 | validation: 0.31596687516373834]
	TIME [epoch: 12.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35922112763991804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35922112763991804 | validation: 0.29819267262050114]
	TIME [epoch: 12.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21446923837970056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21446923837970056 | validation: 0.23851759102150663]
	TIME [epoch: 12.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17807358031366968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17807358031366968 | validation: 0.11978614699937809]
	TIME [epoch: 12.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16149345425170927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16149345425170927 | validation: 0.1207857194539006]
	TIME [epoch: 12.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14081825741712511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14081825741712511 | validation: 0.12968714351519456]
	TIME [epoch: 12.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309571904899435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309571904899435 | validation: 0.13786383170506353]
	TIME [epoch: 12.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1238746309157285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1238746309157285 | validation: 0.11910966581533178]
	TIME [epoch: 12.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452573026589989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1452573026589989 | validation: 0.26578503419762195]
	TIME [epoch: 12.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17498246322219735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17498246322219735 | validation: 0.21450433001038946]
	TIME [epoch: 12.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22579913328365714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22579913328365714 | validation: 0.12602811642110087]
	TIME [epoch: 12.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11710648982559985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11710648982559985 | validation: 0.13160441024543373]
	TIME [epoch: 12.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693112739134326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693112739134326 | validation: 0.1959287456641356]
	TIME [epoch: 12.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18025562783644675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18025562783644675 | validation: 0.21039339706740157]
	TIME [epoch: 12.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20488002381996495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20488002381996495 | validation: 0.31003910942704443]
	TIME [epoch: 12.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24759986854508445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24759986854508445 | validation: 0.12135542387582413]
	TIME [epoch: 12.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1319784015540914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1319784015540914 | validation: 0.10838953252271727]
	TIME [epoch: 12.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12283431668919999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12283431668919999 | validation: 0.1491231154980827]
	TIME [epoch: 12.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13255058212358967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13255058212358967 | validation: 0.12736476265243313]
	TIME [epoch: 12.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11829760941970935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11829760941970935 | validation: 0.10711341290563277]
	TIME [epoch: 12.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15312182449138698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15312182449138698 | validation: 0.3956969393848625]
	TIME [epoch: 12.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2398155094021803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2398155094021803 | validation: 0.19643563359986138]
	TIME [epoch: 12.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711823276535104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2711823276535104 | validation: 0.12636438875517883]
	TIME [epoch: 12.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519764712495559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519764712495559 | validation: 0.27354499432974805]
	TIME [epoch: 12.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.237090466777092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.237090466777092 | validation: 0.19743121252467727]
	TIME [epoch: 12.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16845377593427788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16845377593427788 | validation: 0.13501407998364762]
	TIME [epoch: 12.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15049773093114333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15049773093114333 | validation: 0.21356024963474704]
	TIME [epoch: 12.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16339374952910482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16339374952910482 | validation: 0.10401308081452801]
	TIME [epoch: 12.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012582507851693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12012582507851693 | validation: 0.09633676553276938]
	TIME [epoch: 12.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1108355235190178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1108355235190178 | validation: 0.12409596935188234]
	TIME [epoch: 12.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12222294263322107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12222294263322107 | validation: 0.178994524142048]
	TIME [epoch: 12.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487225930822382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1487225930822382 | validation: 0.217778897798748]
	TIME [epoch: 12.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.208187383773137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.208187383773137 | validation: 0.4158740185952162]
	TIME [epoch: 12.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220665728643665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3220665728643665 | validation: 0.11129964676471005]
	TIME [epoch: 12.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1613749861010165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1613749861010165 | validation: 0.1657778657059569]
	TIME [epoch: 12.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15049871436936005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15049871436936005 | validation: 0.11552124928175472]
	TIME [epoch: 12.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032045113264804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13032045113264804 | validation: 0.09739140646204275]
	TIME [epoch: 12.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11396159451099724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11396159451099724 | validation: 0.11619411953048697]
	TIME [epoch: 12.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10610869298649547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10610869298649547 | validation: 0.0893010206802651]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1124792286639881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1124792286639881 | validation: 0.2628364779861316]
	TIME [epoch: 12.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15355659514770303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15355659514770303 | validation: 0.14805756601212758]
	TIME [epoch: 12.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20390060829428094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20390060829428094 | validation: 0.3521739291269619]
	TIME [epoch: 12.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21911822941891085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21911822941891085 | validation: 0.1140443257329354]
	TIME [epoch: 12.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14004899703981585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14004899703981585 | validation: 0.17793664090925443]
	TIME [epoch: 12.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17786663509140638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17786663509140638 | validation: 0.1608365124402839]
	TIME [epoch: 12.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12363556694345108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12363556694345108 | validation: 0.10157440677944934]
	TIME [epoch: 12.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635725929900172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11635725929900172 | validation: 0.15045997398162886]
	TIME [epoch: 12.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11800215003996722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11800215003996722 | validation: 0.10583770070194523]
	TIME [epoch: 12.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12709406623451955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12709406623451955 | validation: 0.25414645787504003]
	TIME [epoch: 12.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17429885364309544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17429885364309544 | validation: 0.22045136142121008]
	TIME [epoch: 12.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24018441055163123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24018441055163123 | validation: 0.096414039359177]
	TIME [epoch: 12.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11724950177028755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11724950177028755 | validation: 0.3485846758762289]
	TIME [epoch: 12.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20776452881498703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20776452881498703 | validation: 0.12112070168821726]
	TIME [epoch: 12.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14826616755978494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14826616755978494 | validation: 0.118556465047048]
	TIME [epoch: 12.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11183625920024494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11183625920024494 | validation: 0.13574268467534775]
	TIME [epoch: 12.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11596025232659786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11596025232659786 | validation: 0.11216891352122374]
	TIME [epoch: 12.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13119378984114513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119378984114513 | validation: 0.2476765536843294]
	TIME [epoch: 12.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18649236007565506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18649236007565506 | validation: 0.23714348937802862]
	TIME [epoch: 12.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2411027816094572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2411027816094572 | validation: 0.24734708385936754]
	TIME [epoch: 12.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19877671918985582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19877671918985582 | validation: 0.09841198574663197]
	TIME [epoch: 12.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11471227428874803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11471227428874803 | validation: 0.11788726267969625]
	TIME [epoch: 12.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12709662753358575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12709662753358575 | validation: 0.09947847884277856]
	TIME [epoch: 12.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11236290167437925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11236290167437925 | validation: 0.14749777117308657]
	TIME [epoch: 12.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1125243651384523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1125243651384523 | validation: 0.09792666188104401]
	TIME [epoch: 12.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1647472113452976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1647472113452976 | validation: 0.5259972878769608]
	TIME [epoch: 12.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33020210342557205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33020210342557205 | validation: 0.09576608693918942]
	TIME [epoch: 12.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10575481581916979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10575481581916979 | validation: 0.10534149008221279]
	TIME [epoch: 12.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15088554815221283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15088554815221283 | validation: 0.2922022052124829]
	TIME [epoch: 12.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19153418565817198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19153418565817198 | validation: 0.1315825336198432]
	TIME [epoch: 12.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064116943101328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13064116943101328 | validation: 0.1180102248450433]
	TIME [epoch: 12.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17672220291670704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17672220291670704 | validation: 0.23472672205393647]
	TIME [epoch: 12.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1469658054094118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1469658054094118 | validation: 0.13865579745065457]
	TIME [epoch: 12.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12208022917908305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12208022917908305 | validation: 0.133751572047997]
	TIME [epoch: 12.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13383002929182988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13383002929182988 | validation: 0.13638860475714806]
	TIME [epoch: 12.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12367842491079416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12367842491079416 | validation: 0.16782250479065297]
	TIME [epoch: 12.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14793308016469317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14793308016469317 | validation: 0.13900740039392567]
	TIME [epoch: 12.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15459746618625594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15459746618625594 | validation: 0.12350497805700883]
	TIME [epoch: 12.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10628079659926654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10628079659926654 | validation: 0.08714620710202525]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10077948393573877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10077948393573877 | validation: 0.25239120456234954]
	TIME [epoch: 12.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14631294149986881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14631294149986881 | validation: 0.21600849052991045]
	TIME [epoch: 12.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24998692605022527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24998692605022527 | validation: 0.2411325031343232]
	TIME [epoch: 12.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17024356139836816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17024356139836816 | validation: 0.14742310184205334]
	TIME [epoch: 12.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860303814602098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11860303814602098 | validation: 0.10295270749351718]
	TIME [epoch: 12.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14200593968821154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14200593968821154 | validation: 0.14622322629248205]
	TIME [epoch: 12.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13908370620828703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13908370620828703 | validation: 0.17261006560201808]
	TIME [epoch: 12.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15095423784575782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15095423784575782 | validation: 0.19472160630078406]
	TIME [epoch: 12.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815748785348045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815748785348045 | validation: 0.20692526884147436]
	TIME [epoch: 12.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20247278855831596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20247278855831596 | validation: 0.12205242783361997]
	TIME [epoch: 12.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156187546818181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156187546818181 | validation: 0.19421799971600184]
	TIME [epoch: 12.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17555413938217318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17555413938217318 | validation: 0.12067559496275718]
	TIME [epoch: 12.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14459805205195794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14459805205195794 | validation: 0.153826584698179]
	TIME [epoch: 12.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11626162572671818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11626162572671818 | validation: 0.08917921063138534]
	TIME [epoch: 12.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1091654046120733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1091654046120733 | validation: 0.153314570133356]
	TIME [epoch: 12.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10610164240219602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10610164240219602 | validation: 0.08768903180820374]
	TIME [epoch: 12.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11865346642891282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11865346642891282 | validation: 0.2541028741544324]
	TIME [epoch: 12.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13983425404681246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13983425404681246 | validation: 0.159852383764799]
	TIME [epoch: 12.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21931504348813044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21931504348813044 | validation: 0.32327373007219995]
	TIME [epoch: 12.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2006963036508315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2006963036508315 | validation: 0.16100381761645058]
	TIME [epoch: 12.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295525430153632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1295525430153632 | validation: 0.13085531713090312]
	TIME [epoch: 12.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16232042877484118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16232042877484118 | validation: 0.1305903093619425]
	TIME [epoch: 12.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12887097492204702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12887097492204702 | validation: 0.16116504353608227]
	TIME [epoch: 12.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12644784585671295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12644784585671295 | validation: 0.10412234716621085]
	TIME [epoch: 12.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037747671914736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1037747671914736 | validation: 0.11687512138135232]
	TIME [epoch: 12.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11113684956934076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11113684956934076 | validation: 0.20838038471515838]
	TIME [epoch: 12.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1567662598786258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1567662598786258 | validation: 0.25717939213132124]
	TIME [epoch: 12.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614669628294931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614669628294931 | validation: 0.37893325946945966]
	TIME [epoch: 12.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42715561709987443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42715561709987443 | validation: 0.31772112995449747]
	TIME [epoch: 12.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22188320279232923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22188320279232923 | validation: 0.15330333305022723]
	TIME [epoch: 12.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15606774838396525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15606774838396525 | validation: 0.1340759294541869]
	TIME [epoch: 12.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15108950145788355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15108950145788355 | validation: 0.11147960850745992]
	TIME [epoch: 12.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12823202254221877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12823202254221877 | validation: 0.09937629658489928]
	TIME [epoch: 12.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825241500366289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11825241500366289 | validation: 0.11117122070139861]
	TIME [epoch: 12.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10574728273762973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10574728273762973 | validation: 0.11751310574008299]
	TIME [epoch: 12.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10197209956471659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10197209956471659 | validation: 0.10408161085131272]
	TIME [epoch: 12.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11207094214266589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11207094214266589 | validation: 0.1867514136666166]
	TIME [epoch: 12.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11341665546528563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11341665546528563 | validation: 0.0910825375328433]
	TIME [epoch: 12.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292482425845989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292482425845989 | validation: 0.37177706653677584]
	TIME [epoch: 12.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20929331808258958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20929331808258958 | validation: 0.1004975756896237]
	TIME [epoch: 12.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15528714257785944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15528714257785944 | validation: 0.13760536543009083]
	TIME [epoch: 12.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746156470674986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10746156470674986 | validation: 0.10430952933623011]
	TIME [epoch: 12.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043444709935109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10043444709935109 | validation: 0.0982482395639033]
	TIME [epoch: 12.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11699195102823223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11699195102823223 | validation: 0.27239705226191785]
	TIME [epoch: 12.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15874216782791947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15874216782791947 | validation: 0.1250648316295706]
	TIME [epoch: 12.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15327452672870867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15327452672870867 | validation: 0.26750534629991546]
	TIME [epoch: 12.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29392421904818394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29392421904818394 | validation: 0.2544177685624918]
	TIME [epoch: 12.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17672866385278943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17672866385278943 | validation: 0.11665934766883478]
	TIME [epoch: 12.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10969882024135345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10969882024135345 | validation: 0.11032639519083694]
	TIME [epoch: 12.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13904345863246753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13904345863246753 | validation: 0.1448609806194978]
	TIME [epoch: 12.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11048356594656322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11048356594656322 | validation: 0.09731483650514372]
	TIME [epoch: 12.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09986350842647028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09986350842647028 | validation: 0.11471868807478819]
	TIME [epoch: 12.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005021941853031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1005021941853031 | validation: 0.0898688455810385]
	TIME [epoch: 12.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09599046536372681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09599046536372681 | validation: 0.16284227114376876]
	TIME [epoch: 12.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1105883904897185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1105883904897185 | validation: 0.13947915548725395]
	TIME [epoch: 12.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19845404779815026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19845404779815026 | validation: 0.7105263838083347]
	TIME [epoch: 12.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5761747971482575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5761747971482575 | validation: 0.5088174974334809]
	TIME [epoch: 12.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36485814204071104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36485814204071104 | validation: 0.1456736858254156]
	TIME [epoch: 12.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17579162433824272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17579162433824272 | validation: 0.11817429200262763]
	TIME [epoch: 12.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19595177869287939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19595177869287939 | validation: 0.10270950051191337]
	TIME [epoch: 12.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315173340137653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12315173340137653 | validation: 0.14753359446111336]
	TIME [epoch: 12.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122205172834179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12122205172834179 | validation: 0.11314401755240229]
	TIME [epoch: 12.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330796447547007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10330796447547007 | validation: 0.09873625218679971]
	TIME [epoch: 12.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260671329085991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260671329085991 | validation: 0.09848496520371404]
	TIME [epoch: 12.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10336133222822406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10336133222822406 | validation: 0.16917063597846718]
	TIME [epoch: 12.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12492393433369901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12492393433369901 | validation: 0.17365566590360748]
	TIME [epoch: 12.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17290122777657488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17290122777657488 | validation: 0.36041595438651375]
	TIME [epoch: 12.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3409239753958962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3409239753958962 | validation: 0.13627627035306247]
	TIME [epoch: 12.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12280296336102105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12280296336102105 | validation: 0.10903585043109972]
	TIME [epoch: 12.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954625459132711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10954625459132711 | validation: 0.10055966945752166]
	TIME [epoch: 12.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11952505421872246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11952505421872246 | validation: 0.09387445893489603]
	TIME [epoch: 12.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09495266488292643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09495266488292643 | validation: 0.11391359770157024]
	TIME [epoch: 12.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09441626962174873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09441626962174873 | validation: 0.08234297383245445]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08847895647180444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08847895647180444 | validation: 0.14216268441491314]
	TIME [epoch: 12.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09292817124463298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09292817124463298 | validation: 0.08344781305989465]
	TIME [epoch: 12.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12011278858576285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12011278858576285 | validation: 0.3620371863172643]
	TIME [epoch: 12.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19812263858070228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19812263858070228 | validation: 0.13293254335166957]
	TIME [epoch: 12.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18434555175714018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18434555175714018 | validation: 0.31843008867891387]
	TIME [epoch: 12.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22164413939902036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22164413939902036 | validation: 0.21695237142691606]
	TIME [epoch: 12.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854853359925317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854853359925317 | validation: 0.12276672343706299]
	TIME [epoch: 12.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15278950251916518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15278950251916518 | validation: 0.09877232485626164]
	TIME [epoch: 12.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13216809881874594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13216809881874594 | validation: 0.14606094538974507]
	TIME [epoch: 12.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11049773602621478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11049773602621478 | validation: 0.12012534012067971]
	TIME [epoch: 12.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10313224543092185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10313224543092185 | validation: 0.10097429326913321]
	TIME [epoch: 12.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10026027981681086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10026027981681086 | validation: 0.11630816384748888]
	TIME [epoch: 12.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11470832462194416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11470832462194416 | validation: 0.2505370673381934]
	TIME [epoch: 12.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15923665671114712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15923665671114712 | validation: 0.15402827675284203]
	TIME [epoch: 12.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2040288250168384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2040288250168384 | validation: 0.2500518568451117]
	TIME [epoch: 12.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16083797706019945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16083797706019945 | validation: 0.16340643956188178]
	TIME [epoch: 12.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14438859370044732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14438859370044732 | validation: 0.11627052510943972]
	TIME [epoch: 12.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332794758198783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1332794758198783 | validation: 0.12684244298202074]
	TIME [epoch: 12.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09531911177626036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09531911177626036 | validation: 0.10611222854946858]
	TIME [epoch: 12.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08972881111110574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08972881111110574 | validation: 0.11498251412322635]
	TIME [epoch: 12.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08938058870819024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08938058870819024 | validation: 0.10581948492592613]
	TIME [epoch: 12.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08789166803427598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08789166803427598 | validation: 0.08234775950668526]
	TIME [epoch: 12.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945350910055677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09945350910055677 | validation: 0.3518420285103689]
	TIME [epoch: 12.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21270518634875907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21270518634875907 | validation: 0.21552539846057286]
	TIME [epoch: 12.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28764038857095253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28764038857095253 | validation: 0.20234680069042874]
	TIME [epoch: 12.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15733296441638392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15733296441638392 | validation: 0.19433284766504755]
	TIME [epoch: 12.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15109271690036244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15109271690036244 | validation: 0.0964583992149277]
	TIME [epoch: 12.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12564707079052392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12564707079052392 | validation: 0.09254224767493768]
	TIME [epoch: 12.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10321814269445673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10321814269445673 | validation: 0.11956012552645064]
	TIME [epoch: 12.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09702817817927617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09702817817927617 | validation: 0.2642728975799382]
	TIME [epoch: 12.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2924568047726189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2924568047726189 | validation: 0.176449454693405]
	TIME [epoch: 12.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15923719244895845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15923719244895845 | validation: 0.1670734797547288]
	TIME [epoch: 12.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1604577095693273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1604577095693273 | validation: 0.1168519938962312]
	TIME [epoch: 12.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13481327860983858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13481327860983858 | validation: 0.10151331380213868]
	TIME [epoch: 12.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1252764597527375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1252764597527375 | validation: 0.10997859468883808]
	TIME [epoch: 12.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10212555653063957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10212555653063957 | validation: 0.14182153075432025]
	TIME [epoch: 12.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10578096809266103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10578096809266103 | validation: 0.11438544555177939]
	TIME [epoch: 12.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14158695672320773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14158695672320773 | validation: 0.3016987325611544]
	TIME [epoch: 12.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19637618152337724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19637618152337724 | validation: 0.17022085367600595]
	TIME [epoch: 12.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1695247346735148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1695247346735148 | validation: 0.09393963807110357]
	TIME [epoch: 12.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09428709541335646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09428709541335646 | validation: 0.11978669478148525]
	TIME [epoch: 12.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09912495171874745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09912495171874745 | validation: 0.09561137810770354]
	TIME [epoch: 12.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11606948401693794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11606948401693794 | validation: 0.22170648804202947]
	TIME [epoch: 12.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12077519697769112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12077519697769112 | validation: 0.08272941818114443]
	TIME [epoch: 12.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10544258134773607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10544258134773607 | validation: 0.15231426352675062]
	TIME [epoch: 12.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09560218274494214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09560218274494214 | validation: 0.07520178975961672]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09519181888156686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09519181888156686 | validation: 0.1028794068236897]
	TIME [epoch: 12.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08265265859084484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08265265859084484 | validation: 0.11455273505502249]
	TIME [epoch: 12.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859797871251833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0859797871251833 | validation: 0.13573560247472086]
	TIME [epoch: 12.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519258578431057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519258578431057 | validation: 0.29981734296665846]
	TIME [epoch: 12.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904656661931463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2904656661931463 | validation: 0.11419293812855608]
	TIME [epoch: 12.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11189872120248812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11189872120248812 | validation: 0.162668726065068]
	TIME [epoch: 12.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11283889339918939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11283889339918939 | validation: 0.1606214042954681]
	TIME [epoch: 12.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1724181837180161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1724181837180161 | validation: 0.3497298138420375]
	TIME [epoch: 12.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25250262835359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25250262835359 | validation: 0.137828870051905]
	TIME [epoch: 12.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16872460572937661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16872460572937661 | validation: 0.13983044463398675]
	TIME [epoch: 12.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16309020155325088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16309020155325088 | validation: 0.09165323506089264]
	TIME [epoch: 12.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11737603508171399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11737603508171399 | validation: 0.10670948297295833]
	TIME [epoch: 12.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10412117941107432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10412117941107432 | validation: 0.12011073954096788]
	TIME [epoch: 12.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09936546712597387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09936546712597387 | validation: 0.1350252145674494]
	TIME [epoch: 12.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635661110840064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10635661110840064 | validation: 0.1197150165547227]
	TIME [epoch: 12.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1403328231464869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1403328231464869 | validation: 0.3955963876182783]
	TIME [epoch: 12.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2259861451200532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2259861451200532 | validation: 0.1633363485074818]
	TIME [epoch: 12.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23138698246596584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23138698246596584 | validation: 0.13377545449090777]
	TIME [epoch: 12.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994137958216039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0994137958216039 | validation: 0.17115493152815395]
	TIME [epoch: 12.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11596247137627723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11596247137627723 | validation: 0.09435201520247323]
	TIME [epoch: 12.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949623240316694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10949623240316694 | validation: 0.11296522850231008]
	TIME [epoch: 12.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09081231796285141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09081231796285141 | validation: 0.11939888678982907]
	TIME [epoch: 12.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09291998698632661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09291998698632661 | validation: 0.10600052064670772]
	TIME [epoch: 12.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10995471265825767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10995471265825767 | validation: 0.18614620769543624]
	TIME [epoch: 12.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14203345518966287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14203345518966287 | validation: 0.22252425569589818]
	TIME [epoch: 12.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18847325602177495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18847325602177495 | validation: 0.14773820612831354]
	TIME [epoch: 12.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17430451467307856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17430451467307856 | validation: 0.2450041219837769]
	TIME [epoch: 12.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16466018011817304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16466018011817304 | validation: 0.12203573116757478]
	TIME [epoch: 12.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13188759400535197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13188759400535197 | validation: 0.11067814652586733]
	TIME [epoch: 12.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09812994424559339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09812994424559339 | validation: 0.1193321150241421]
	TIME [epoch: 12.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436520532436145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11436520532436145 | validation: 0.27163719412711507]
	TIME [epoch: 12.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849290000030949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849290000030949 | validation: 0.09526227444042074]
	TIME [epoch: 12.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12313582547666091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12313582547666091 | validation: 0.1278601145159026]
	TIME [epoch: 12.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14365076034778343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14365076034778343 | validation: 0.25719408942022054]
	TIME [epoch: 12.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19801417965645093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19801417965645093 | validation: 0.11561557531965434]
	TIME [epoch: 12.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111151411401043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111151411401043 | validation: 0.08900504828850969]
	TIME [epoch: 12.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10998465335874372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10998465335874372 | validation: 0.2990533115512827]
	TIME [epoch: 12.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557541376277258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1557541376277258 | validation: 0.10313100475345044]
	TIME [epoch: 12.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1406104516632572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1406104516632572 | validation: 0.1129096272932741]
	TIME [epoch: 12.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302386353547334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09302386353547334 | validation: 0.1014994112603918]
	TIME [epoch: 12.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08492764827745934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08492764827745934 | validation: 0.09072503173155914]
	TIME [epoch: 12.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08355685583959954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08355685583959954 | validation: 0.12124236698712805]
	TIME [epoch: 12.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10590248946964696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10590248946964696 | validation: 0.21576470562632527]
	TIME [epoch: 12.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885486856625802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1885486856625802 | validation: 0.22823555351042937]
	TIME [epoch: 12.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516676187975583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2516676187975583 | validation: 0.11224185535948711]
	TIME [epoch: 12.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11085618923973277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11085618923973277 | validation: 0.1521640888345226]
	TIME [epoch: 12.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13634573492050833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13634573492050833 | validation: 0.1356218408512196]
	TIME [epoch: 12.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16874633110594384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16874633110594384 | validation: 0.3458269032645597]
	TIME [epoch: 12.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19953135963159355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19953135963159355 | validation: 0.07783080218447902]
	TIME [epoch: 12.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067996331322531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09067996331322531 | validation: 0.07971377430951647]
	TIME [epoch: 12.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0963996132501263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0963996132501263 | validation: 0.1326120043209312]
	TIME [epoch: 12.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09423417368838688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09423417368838688 | validation: 0.06767721624190091]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08052490932275178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08052490932275178 | validation: 0.19526565610140345]
	TIME [epoch: 12.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.116873064895811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.116873064895811 | validation: 0.14140481320501672]
	TIME [epoch: 12.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21658714768920512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21658714768920512 | validation: 0.2798911813552928]
	TIME [epoch: 12.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15885863241135253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15885863241135253 | validation: 0.13135766840160912]
	TIME [epoch: 12.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12904366844394033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12904366844394033 | validation: 0.10751398536800116]
	TIME [epoch: 12.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11347099955060364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11347099955060364 | validation: 0.08760206613886021]
	TIME [epoch: 12.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339692285933389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09339692285933389 | validation: 0.09818461232009747]
	TIME [epoch: 12.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08401691313768794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08401691313768794 | validation: 0.22356860096908848]
	TIME [epoch: 12.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17918297470816683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17918297470816683 | validation: 0.12180346045520477]
	TIME [epoch: 12.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11753022611037293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11753022611037293 | validation: 0.07578173599156428]
	TIME [epoch: 12.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244417624230641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09244417624230641 | validation: 0.21912464600271]
	TIME [epoch: 12.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11941971325646841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11941971325646841 | validation: 0.0927755041231601]
	TIME [epoch: 12.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14630212192362424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14630212192362424 | validation: 0.48857432210322777]
	TIME [epoch: 12.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014973906517289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3014973906517289 | validation: 0.11240256119799522]
	TIME [epoch: 12.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09704351368660134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09704351368660134 | validation: 0.09673062081299225]
	TIME [epoch: 12.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13750078050271186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13750078050271186 | validation: 0.0912753696927554]
	TIME [epoch: 12.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224791554548568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09224791554548568 | validation: 0.1922294419945752]
	TIME [epoch: 12.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13559728400666415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13559728400666415 | validation: 0.1229145100106413]
	TIME [epoch: 12.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13987012976816093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13987012976816093 | validation: 0.18060177413534795]
	TIME [epoch: 12.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375028935527701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375028935527701 | validation: 0.14832127021652936]
	TIME [epoch: 12.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1105480947083651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1105480947083651 | validation: 0.13227636499015577]
	TIME [epoch: 12.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15245728890230972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15245728890230972 | validation: 0.14940308978754518]
	TIME [epoch: 12.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11549250645098556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11549250645098556 | validation: 0.10644237064293782]
	TIME [epoch: 12.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08320918124094753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08320918124094753 | validation: 0.06848226179950229]
	TIME [epoch: 12.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894175147516521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894175147516521 | validation: 0.2348408749013546]
	TIME [epoch: 12.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12545563096447274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12545563096447274 | validation: 0.10274425899833349]
	TIME [epoch: 12.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15025150554883918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15025150554883918 | validation: 0.26940358066411857]
	TIME [epoch: 12.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445678814414663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1445678814414663 | validation: 0.0968030349556684]
	TIME [epoch: 12.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0951592144871838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0951592144871838 | validation: 0.07720782885625395]
	TIME [epoch: 12.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679080471156716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09679080471156716 | validation: 0.33362740854980877]
	TIME [epoch: 12.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2108206796947507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2108206796947507 | validation: 0.09398685874641473]
	TIME [epoch: 12.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09770914780059017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09770914780059017 | validation: 0.07766439512417522]
	TIME [epoch: 12.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10925787603805358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10925787603805358 | validation: 0.16397522034841414]
	TIME [epoch: 12.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11502910777152887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11502910777152887 | validation: 0.11348241137139313]
	TIME [epoch: 12.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212229131433218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11212229131433218 | validation: 0.18612560372798428]
	TIME [epoch: 12.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21695157180758123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21695157180758123 | validation: 0.12982474877812633]
	TIME [epoch: 12.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132260165544158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1132260165544158 | validation: 0.17798035091539882]
	TIME [epoch: 12.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1357165620750463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1357165620750463 | validation: 0.13021373511338466]
	TIME [epoch: 12.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1748594173523686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1748594173523686 | validation: 0.1090237516796965]
	TIME [epoch: 12.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11306491169523042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11306491169523042 | validation: 0.14724805591067086]
	TIME [epoch: 12.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11704017630811081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11704017630811081 | validation: 0.11877886559229996]
	TIME [epoch: 12.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10098159218505164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10098159218505164 | validation: 0.07883732893650218]
	TIME [epoch: 12.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10671619620932052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10671619620932052 | validation: 0.24384470442610065]
	TIME [epoch: 12.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13848604926846725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13848604926846725 | validation: 0.08887983411407722]
	TIME [epoch: 12.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12528371040180436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12528371040180436 | validation: 0.21477336091881136]
	TIME [epoch: 12.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14004000527839808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14004000527839808 | validation: 0.09565931058659283]
	TIME [epoch: 12.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019518435949188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019518435949188 | validation: 0.12420340686329552]
	TIME [epoch: 12.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222870003482832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09222870003482832 | validation: 0.0949093972921839]
	TIME [epoch: 12.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09297158222590701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09297158222590701 | validation: 0.22386408714890274]
	TIME [epoch: 12.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13857820638394558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13857820638394558 | validation: 0.15820942823158488]
	TIME [epoch: 12.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13746944732878083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13746944732878083 | validation: 0.08066372913720352]
	TIME [epoch: 12.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08517537741799706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08517537741799706 | validation: 0.12441098103134003]
	TIME [epoch: 12.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09189962298102182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09189962298102182 | validation: 0.08475198879657365]
	TIME [epoch: 12.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11623532567657616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11623532567657616 | validation: 0.3163477512679379]
	TIME [epoch: 12.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17156756824853162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17156756824853162 | validation: 0.1317812734592056]
	TIME [epoch: 12.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649926623360917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1649926623360917 | validation: 0.1626947328021706]
	TIME [epoch: 12.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1215829246513045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1215829246513045 | validation: 0.18255065149929137]
	TIME [epoch: 12.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157600809942847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157600809942847 | validation: 0.10138067359115395]
	TIME [epoch: 12.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11268148153227095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11268148153227095 | validation: 0.0960569889488381]
	TIME [epoch: 12.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860359128240442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860359128240442 | validation: 0.12698645498064015]
	TIME [epoch: 12.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11047101274799115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11047101274799115 | validation: 0.10360598143932742]
	TIME [epoch: 12.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11231117086703578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11231117086703578 | validation: 0.12557278918390558]
	TIME [epoch: 12.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09472909833010512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09472909833010512 | validation: 0.14193958674123922]
	TIME [epoch: 12.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14677853002371763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14677853002371763 | validation: 0.2561500863652375]
	TIME [epoch: 12.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17035235069817625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17035235069817625 | validation: 0.09101646246931841]
	TIME [epoch: 12.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15666429910248703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15666429910248703 | validation: 0.18103512100335895]
	TIME [epoch: 12.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12468762722992197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12468762722992197 | validation: 0.175517900085945]
	TIME [epoch: 12.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975747393317011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1975747393317011 | validation: 0.1466731311845571]
	TIME [epoch: 12.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13593319328756526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13593319328756526 | validation: 0.1602529465471807]
	TIME [epoch: 12.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11878341315392293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11878341315392293 | validation: 0.15194082270695933]
	TIME [epoch: 12.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13586470321397257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13586470321397257 | validation: 0.08157090940745121]
	TIME [epoch: 12.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09684623660519108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09684623660519108 | validation: 0.10079021359776275]
	TIME [epoch: 12.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0830591582442429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0830591582442429 | validation: 0.13219714401727006]
	TIME [epoch: 12.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264071976141608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09264071976141608 | validation: 0.07900616644512497]
	TIME [epoch: 12.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339672784520174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09339672784520174 | validation: 0.12745991959875821]
	TIME [epoch: 12.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10168959663011133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10168959663011133 | validation: 0.11334398391362072]
	TIME [epoch: 12.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12984587460259137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12984587460259137 | validation: 0.201029433526834]
	TIME [epoch: 12.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14448088926026859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14448088926026859 | validation: 0.27012731157267916]
	TIME [epoch: 12.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24072066067951298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24072066067951298 | validation: 0.11314898049578821]
	TIME [epoch: 12.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11886927890788297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11886927890788297 | validation: 0.13935530296972196]
	TIME [epoch: 12.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13292286329588784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13292286329588784 | validation: 0.09383808668357388]
	TIME [epoch: 12.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11077989923502354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11077989923502354 | validation: 0.07144328891489223]
	TIME [epoch: 12.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10036501147362037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10036501147362037 | validation: 0.11274018895413124]
	TIME [epoch: 12.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09182740190619097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09182740190619097 | validation: 0.07533945839342152]
	TIME [epoch: 12.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10981794705321203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10981794705321203 | validation: 0.4233250425798211]
	TIME [epoch: 12.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2283912237149963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2283912237149963 | validation: 0.08623099044865967]
	TIME [epoch: 12.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11822973993454912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11822973993454912 | validation: 0.1966435985355204]
	TIME [epoch: 12.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14889853754669832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14889853754669832 | validation: 0.08663748710145097]
	TIME [epoch: 12.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373687714829127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373687714829127 | validation: 0.05774067652174422]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_876.pth
	Model improved!!!
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582998829308017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582998829308017 | validation: 0.1285553040116932]
	TIME [epoch: 12.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203926897297692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08203926897297692 | validation: 0.0701071392021098]
	TIME [epoch: 12.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08963515076528104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08963515076528104 | validation: 0.16941946963749488]
	TIME [epoch: 12.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860103010980026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11860103010980026 | validation: 0.12743992831594816]
	TIME [epoch: 12.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16103190157037353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16103190157037353 | validation: 0.1792512770443156]
	TIME [epoch: 12.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11773293174908801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11773293174908801 | validation: 0.06381757747411224]
	TIME [epoch: 12.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935124256808037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0935124256808037 | validation: 0.12236760296859327]
	TIME [epoch: 12.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09081508904736968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09081508904736968 | validation: 0.06354519396735875]
	TIME [epoch: 12.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075984882149753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08075984882149753 | validation: 0.20342905930985813]
	TIME [epoch: 12.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12037958221876008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12037958221876008 | validation: 0.09855236567050872]
	TIME [epoch: 12.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13530204068922214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13530204068922214 | validation: 0.1862307686745217]
	TIME [epoch: 12.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10881636000512863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10881636000512863 | validation: 0.09456593309386314]
	TIME [epoch: 12.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002235914132386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11002235914132386 | validation: 0.18260329732948583]
	TIME [epoch: 12.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15232310613494526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15232310613494526 | validation: 0.17592253619148926]
	TIME [epoch: 12.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12215090879889687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12215090879889687 | validation: 0.06695020681811538]
	TIME [epoch: 12.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927873989252632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09927873989252632 | validation: 0.09236090638141616]
	TIME [epoch: 12.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106509217744645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08106509217744645 | validation: 0.06176187845495758]
	TIME [epoch: 12.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867963013984301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07867963013984301 | validation: 0.1415459251616976]
	TIME [epoch: 12.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12786685269415116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12786685269415116 | validation: 0.1287116663588678]
	TIME [epoch: 12.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15090494899921833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15090494899921833 | validation: 0.5472795695891869]
	TIME [epoch: 12.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39226682140879776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39226682140879776 | validation: 0.0932886669136468]
	TIME [epoch: 12.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932121889827823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10932121889827823 | validation: 0.1630439519548775]
	TIME [epoch: 12.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2337561429042947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2337561429042947 | validation: 0.2352918964366445]
	TIME [epoch: 12.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16173635696343325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16173635696343325 | validation: 0.13472961990452645]
	TIME [epoch: 12.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12063777920991456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12063777920991456 | validation: 0.08483047837679666]
	TIME [epoch: 12.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663061034475444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08663061034475444 | validation: 0.09190340587440649]
	TIME [epoch: 12.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846396270669498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0846396270669498 | validation: 0.07334860368723596]
	TIME [epoch: 12.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09392434373769605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09392434373769605 | validation: 0.3195498078634629]
	TIME [epoch: 12.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16151840186776692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16151840186776692 | validation: 0.11601326200334024]
	TIME [epoch: 12.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2008475044433317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2008475044433317 | validation: 0.0990448535225597]
	TIME [epoch: 12.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890624737867926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0890624737867926 | validation: 0.11162349727723249]
	TIME [epoch: 12.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09176088670894199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09176088670894199 | validation: 0.06757659376405321]
	TIME [epoch: 12.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257166029807465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08257166029807465 | validation: 0.6166140565848525]
	TIME [epoch: 12.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829092335173157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.829092335173157 | validation: 0.45200592494894565]
	TIME [epoch: 12.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4780598579841508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4780598579841508 | validation: 0.2540618475414621]
	TIME [epoch: 12.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938672806190845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2938672806190845 | validation: 0.22960636530728706]
	TIME [epoch: 12.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25858527791475766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25858527791475766 | validation: 0.19977793817696543]
	TIME [epoch: 12.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2240464156308519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2240464156308519 | validation: 0.1648806623634222]
	TIME [epoch: 12.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18276723167592224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18276723167592224 | validation: 0.11810423599605153]
	TIME [epoch: 12.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367627186152673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367627186152673 | validation: 0.10595616177405667]
	TIME [epoch: 12.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11641561603832071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11641561603832071 | validation: 0.11355420202604703]
	TIME [epoch: 12.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09546766680703245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09546766680703245 | validation: 0.1092641209542197]
	TIME [epoch: 12.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08341804417041976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08341804417041976 | validation: 0.07794679136001076]
	TIME [epoch: 12.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08475401571830564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08475401571830564 | validation: 0.12227494406268676]
	TIME [epoch: 12.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09188807803451632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09188807803451632 | validation: 0.09871053107191916]
	TIME [epoch: 12.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10940272152195042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10940272152195042 | validation: 0.23800006047174735]
	TIME [epoch: 12.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16341016146259685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16341016146259685 | validation: 0.15924603822197758]
	TIME [epoch: 12.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1962175064338808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1962175064338808 | validation: 0.11458996016908507]
	TIME [epoch: 12.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10042283319129475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10042283319129475 | validation: 0.11643943451195088]
	TIME [epoch: 12.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11691520994104761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11691520994104761 | validation: 0.15224503975667186]
	TIME [epoch: 12.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12365408910012958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12365408910012958 | validation: 0.264161061501118]
	TIME [epoch: 12.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32237961737611115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32237961737611115 | validation: 0.16909527584868314]
	TIME [epoch: 12.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21329012812512432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21329012812512432 | validation: 0.15889527995616165]
	TIME [epoch: 12.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18258623742290137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18258623742290137 | validation: 0.1515030469144293]
	TIME [epoch: 12.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17122206523065564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17122206523065564 | validation: 0.12322809066000387]
	TIME [epoch: 12.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13234506068226945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13234506068226945 | validation: 0.10004175129018203]
	TIME [epoch: 12.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11183612433904695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11183612433904695 | validation: 0.09207662837766901]
	TIME [epoch: 12.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09903036703395035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09903036703395035 | validation: 0.12290062145841614]
	TIME [epoch: 12.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09112429486277229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09112429486277229 | validation: 0.06441770076806538]
	TIME [epoch: 12.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09019448613328723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09019448613328723 | validation: 0.1259265443058507]
	TIME [epoch: 12.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09450878335914481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09450878335914481 | validation: 0.08179292116006065]
	TIME [epoch: 12.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10995307380075833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10995307380075833 | validation: 0.2584870407344731]
	TIME [epoch: 12.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1515316389409784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1515316389409784 | validation: 0.10924902580401713]
	TIME [epoch: 12.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13798170244379124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13798170244379124 | validation: 0.27999605634804653]
	TIME [epoch: 12.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28643879966941593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28643879966941593 | validation: 0.19004378731514804]
	TIME [epoch: 12.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13819879323037745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13819879323037745 | validation: 0.16448082699995306]
	TIME [epoch: 12.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299545447547918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1299545447547918 | validation: 0.11824435898543961]
	TIME [epoch: 12.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12560716303059308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12560716303059308 | validation: 0.08365251317361516]
	TIME [epoch: 12.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279111547155423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09279111547155423 | validation: 0.948005437804571]
	TIME [epoch: 12.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885998611089727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7885998611089727 | validation: 0.9952338549755386]
	TIME [epoch: 12.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8702409214884259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8702409214884259 | validation: 0.7435945092117745]
	TIME [epoch: 12.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270242036207281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6270242036207281 | validation: 0.5045312593893971]
	TIME [epoch: 12.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4604320212522313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4604320212522313 | validation: 0.3567510036547532]
	TIME [epoch: 12.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3966458878439217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3966458878439217 | validation: 0.29146817678159737]
	TIME [epoch: 12.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31454377662915933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31454377662915933 | validation: 0.2719177773231306]
	TIME [epoch: 12.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26057824226003684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26057824226003684 | validation: 0.22258821511511712]
	TIME [epoch: 12.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20605242396468718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20605242396468718 | validation: 0.1693147392431814]
	TIME [epoch: 12.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16848605319068405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16848605319068405 | validation: 0.15422518864795154]
	TIME [epoch: 12.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13995932630927146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13995932630927146 | validation: 0.13809711302566038]
	TIME [epoch: 12.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12604762608938763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12604762608938763 | validation: 0.1855785867844862]
	TIME [epoch: 12.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13593891116175313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13593891116175313 | validation: 0.12883236039778795]
	TIME [epoch: 12.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1208728968103787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1208728968103787 | validation: 0.12987186890006805]
	TIME [epoch: 12.4 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14032528926486426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14032528926486426 | validation: 0.1329530100003806]
	TIME [epoch: 12.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11962524740788447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11962524740788447 | validation: 0.13382635932242234]
	TIME [epoch: 12.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11133742648911284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11133742648911284 | validation: 0.0794738884101109]
	TIME [epoch: 12.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10550622923588986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10550622923588986 | validation: 0.12652914424546347]
	TIME [epoch: 12.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08700923415309487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08700923415309487 | validation: 0.07660910421599273]
	TIME [epoch: 12.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288931898893377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08288931898893377 | validation: 0.22862887749245342]
	TIME [epoch: 12.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1173075855346718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1173075855346718 | validation: 0.09598272045291965]
	TIME [epoch: 12.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487699019277947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08487699019277947 | validation: 0.06477509812572324]
	TIME [epoch: 12.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11800824273878836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11800824273878836 | validation: 0.18453268581433446]
	TIME [epoch: 12.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947416539636642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0947416539636642 | validation: 0.11526768755134974]
	TIME [epoch: 12.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0933897849671825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0933897849671825 | validation: 0.1440338397810382]
	TIME [epoch: 12.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14714056853451316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14714056853451316 | validation: 0.15311676974233276]
	TIME [epoch: 12.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14222618460154052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14222618460154052 | validation: 0.22383543413057005]
	TIME [epoch: 12.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15732370488976188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15732370488976188 | validation: 0.11808530959729438]
	TIME [epoch: 12.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15569605681802565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15569605681802565 | validation: 0.07587003694015865]
	TIME [epoch: 12.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616359507720181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616359507720181 | validation: 0.20261024747492856]
	TIME [epoch: 12.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11889125264538027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11889125264538027 | validation: 0.07742317418259967]
	TIME [epoch: 12.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199688297812862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09199688297812862 | validation: 0.11521266856908624]
	TIME [epoch: 12.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10946521804619792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10946521804619792 | validation: 0.09811583518951224]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241113_145920/states/model_phi1_4c_v_mmd2_977.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8869.903 seconds.
