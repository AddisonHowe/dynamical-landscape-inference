Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/basic/data_phi1_1a/training', validation_data='data/training_data/basic/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3709500554

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.616038277994274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.616038277994274 | validation: 5.678920241529987]
	TIME [epoch: 118 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.970185211076615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.970185211076615 | validation: 5.372753966862014]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.498745229799401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.498745229799401 | validation: 4.680191921887704]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.220613075998893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.220613075998893 | validation: 4.497165301062559]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8583765086967325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8583765086967325 | validation: 4.392950874328345]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.633264417163084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.633264417163084 | validation: 3.9750767142565095]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337735955320093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.337735955320093 | validation: 4.112253630442526]
	TIME [epoch: 7.51 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.132625956457783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.132625956457783 | validation: 3.481306709465704]
	TIME [epoch: 7.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.800447989563582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.800447989563582 | validation: 3.064810580097548]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7215513749047386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7215513749047386 | validation: 2.8537142791177272]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5670520840910704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5670520840910704 | validation: 2.9029510466499255]
	TIME [epoch: 7.55 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.474506259824484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.474506259824484 | validation: 2.5824466089175933]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.368462534742558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.368462534742558 | validation: 2.5185566423883445]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3390040501985228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3390040501985228 | validation: 2.5624742198680006]
	TIME [epoch: 7.5 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312655226986872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.312655226986872 | validation: 2.4066840910228278]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340781911445758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.340781911445758 | validation: 2.3984076952816062]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2547973933772285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2547973933772285 | validation: 2.3160185320032722]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8842339860285544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8842339860285544 | validation: 2.1045174802841995]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5256657166858516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5256657166858516 | validation: 2.2191210171435616]
	TIME [epoch: 7.51 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.781214941921719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.781214941921719 | validation: 2.084149631495288]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5619747971283149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5619747971283149 | validation: 1.5909476638328806]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3682296759151586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3682296759151586 | validation: 1.3754115181908189]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3411032624084473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3411032624084473 | validation: 1.647213857823823]
	TIME [epoch: 7.51 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3427884134654342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3427884134654342 | validation: 2.0148636275678755]
	TIME [epoch: 7.52 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3926593439638109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3926593439638109 | validation: 1.292714160976382]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1016388038041103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1016388038041103 | validation: 1.035682546888813]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.246229449495177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.246229449495177 | validation: 1.5628623202245053]
	TIME [epoch: 7.51 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3742003543363726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3742003543363726 | validation: 0.974432032622378]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0538649694465678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0538649694465678 | validation: 1.1017036857237508]
	TIME [epoch: 7.55 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0104222609913092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0104222609913092 | validation: 1.1328848585933617]
	TIME [epoch: 7.54 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0722452011723207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0722452011723207 | validation: 1.3241930546970484]
	TIME [epoch: 7.52 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1455909573557526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1455909573557526 | validation: 1.113482015402544]
	TIME [epoch: 7.52 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9868196563245213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9868196563245213 | validation: 1.313303290799578]
	TIME [epoch: 7.52 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0977127060546024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0977127060546024 | validation: 1.040353881879229]
	TIME [epoch: 7.54 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.780929591036195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.780929591036195 | validation: 1.178315866726093]
	TIME [epoch: 7.53 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9015663176849166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015663176849166 | validation: 0.9328505365919145]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7956180566222488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7956180566222488 | validation: 0.8473764272820001]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.71493731271379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71493731271379 | validation: 1.2376623214203644]
	TIME [epoch: 7.52 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.917567384850805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917567384850805 | validation: 0.9691668748661209]
	TIME [epoch: 7.57 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8242166527861505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8242166527861505 | validation: 0.6599647977510459]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8760539063015251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760539063015251 | validation: 0.7071575194283698]
	TIME [epoch: 7.52 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7723343782013484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723343782013484 | validation: 0.8335342130616656]
	TIME [epoch: 7.53 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5741159915490368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5741159915490368 | validation: 0.83182014426479]
	TIME [epoch: 7.52 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.687711785036988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687711785036988 | validation: 0.5286212243738235]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6737634893577649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6737634893577649 | validation: 0.7626122246035267]
	TIME [epoch: 7.52 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5804742477216442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5804742477216442 | validation: 0.8513645716982506]
	TIME [epoch: 7.51 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7140997351971251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140997351971251 | validation: 0.6885818084486959]
	TIME [epoch: 7.52 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6818443254881572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6818443254881572 | validation: 1.3794177623850863]
	TIME [epoch: 7.52 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7331308384145037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331308384145037 | validation: 0.6557696119497132]
	TIME [epoch: 7.57 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5854951839831091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5854951839831091 | validation: 0.4159605705546398]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49861437063482666		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.49861437063482666 | validation: 0.9078629349265395]
	TIME [epoch: 7.52 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5371673468670494		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.5371673468670494 | validation: 0.4039379848029866]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5173812239142767		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.5173812239142767 | validation: 0.6404461652072058]
	TIME [epoch: 7.51 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5932456910864329		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.5932456910864329 | validation: 0.4763304785175847]
	TIME [epoch: 7.56 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47940267802632136		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.47940267802632136 | validation: 0.39301595727168226]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38320464675753824		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.38320464675753824 | validation: 0.37940032214220903]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.464354338429098		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.464354338429098 | validation: 1.0819944603540321]
	TIME [epoch: 7.52 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7104028495337942		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.7104028495337942 | validation: 0.3539984431672691]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4936680063070495		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.4936680063070495 | validation: 0.5264555621196928]
	TIME [epoch: 7.56 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44140468963886953		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.44140468963886953 | validation: 0.507395988812517]
	TIME [epoch: 7.52 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5011901514430152		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5011901514430152 | validation: 0.49661750263164645]
	TIME [epoch: 7.52 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3499742064800913		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.3499742064800913 | validation: 0.6437516826654867]
	TIME [epoch: 7.52 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46352645016672606		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.46352645016672606 | validation: 0.4980750315611693]
	TIME [epoch: 7.53 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42133440006492007		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.42133440006492007 | validation: 0.39993585550338506]
	TIME [epoch: 7.56 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3846660605219606		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.3846660605219606 | validation: 0.4559698607734335]
	TIME [epoch: 7.52 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3540757141982347		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.3540757141982347 | validation: 0.34539451092064205]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44553904509102477		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.44553904509102477 | validation: 0.5765966022515363]
	TIME [epoch: 7.52 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43580464993947504		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.43580464993947504 | validation: 0.3924956268274855]
	TIME [epoch: 7.54 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.354958367539694		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.354958367539694 | validation: 0.35252144385526485]
	TIME [epoch: 7.54 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3663013112642193		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.3663013112642193 | validation: 0.5874511216998064]
	TIME [epoch: 7.52 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4441544965180251		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.4441544965180251 | validation: 0.4453723446850469]
	TIME [epoch: 7.52 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34151422991513414		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.34151422991513414 | validation: 0.3550262958699206]
	TIME [epoch: 7.52 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3820513485243402		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.3820513485243402 | validation: 0.3366965037938029]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3712997274034524		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.3712997274034524 | validation: 0.4327946034949263]
	TIME [epoch: 7.55 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33639878757078173		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.33639878757078173 | validation: 0.36543201643718914]
	TIME [epoch: 7.54 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3538116999254606		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.3538116999254606 | validation: 0.5339614738895323]
	TIME [epoch: 7.52 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41393324659753306		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.41393324659753306 | validation: 0.3716516878549569]
	TIME [epoch: 7.54 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40485787659564976		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.40485787659564976 | validation: 0.32421232330061944]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3668941729633093		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.3668941729633093 | validation: 0.39110371903981267]
	TIME [epoch: 7.53 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.366463025274518		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.366463025274518 | validation: 0.3822053965098958]
	TIME [epoch: 7.52 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3059604111310802		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.3059604111310802 | validation: 0.31194677404190196]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35401591476330385		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.35401591476330385 | validation: 0.3242480416052128]
	TIME [epoch: 7.51 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.338984057169749		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.338984057169749 | validation: 0.34563769541917544]
	TIME [epoch: 7.55 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34360164118568653		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.34360164118568653 | validation: 0.3456603729449775]
	TIME [epoch: 7.51 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3311126128850016		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.3311126128850016 | validation: 0.4054552001774081]
	TIME [epoch: 7.52 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3991982312666568		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.3991982312666568 | validation: 0.30145888718265823]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32629101526599835		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.32629101526599835 | validation: 0.5138055678423712]
	TIME [epoch: 7.52 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44888114939280366		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.44888114939280366 | validation: 0.30460383613466147]
	TIME [epoch: 7.56 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30469774158407137		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.30469774158407137 | validation: 0.3394965374949471]
	TIME [epoch: 7.52 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3252723040667635		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.3252723040667635 | validation: 0.30273570529046234]
	TIME [epoch: 7.51 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674741741004182		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.2674741741004182 | validation: 0.4428673328473334]
	TIME [epoch: 7.51 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39084055251392597		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.39084055251392597 | validation: 0.3647072832907443]
	TIME [epoch: 7.52 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3069877067251882		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.3069877067251882 | validation: 0.27428815867327466]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43804122450922		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.43804122450922 | validation: 0.33902147261124205]
	TIME [epoch: 7.53 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3462047545727265		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.3462047545727265 | validation: 0.40237270346768855]
	TIME [epoch: 7.52 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30828285449230347		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.30828285449230347 | validation: 0.332071158927412]
	TIME [epoch: 7.52 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34131541437799057		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.34131541437799057 | validation: 0.495336867395652]
	TIME [epoch: 7.53 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3119151322824649		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.3119151322824649 | validation: 0.24432456041607095]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781749547579636		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.2781749547579636 | validation: 0.29041903363468197]
	TIME [epoch: 7.53 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29822204748727016		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.29822204748727016 | validation: 0.306762621272033]
	TIME [epoch: 7.52 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.309022548033111		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.309022548033111 | validation: 0.4048316109096879]
	TIME [epoch: 7.53 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4006534716350069		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4006534716350069 | validation: 0.428221503871529]
	TIME [epoch: 7.54 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37511027877609654		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.37511027877609654 | validation: 0.27973021561459677]
	TIME [epoch: 7.56 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2795194760756215		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.2795194760756215 | validation: 0.4119292109018639]
	TIME [epoch: 7.53 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3074842196486228		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.3074842196486228 | validation: 0.26115916963719005]
	TIME [epoch: 7.53 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2908165552410627		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.2908165552410627 | validation: 0.3929318260398874]
	TIME [epoch: 7.53 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2842166183651711		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2842166183651711 | validation: 0.2318665579981979]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3131406053797543		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3131406053797543 | validation: 0.32707526090842565]
	TIME [epoch: 7.57 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35757525581607397		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.35757525581607397 | validation: 0.2942889268905946]
	TIME [epoch: 7.55 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25754590808217154		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.25754590808217154 | validation: 0.2855092523978269]
	TIME [epoch: 7.55 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26929187185468784		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.26929187185468784 | validation: 0.5178564913047559]
	TIME [epoch: 7.52 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31200732980513307		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.31200732980513307 | validation: 0.2926442566220123]
	TIME [epoch: 7.54 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3588368095287968		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.3588368095287968 | validation: 0.3084903721222938]
	TIME [epoch: 7.57 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30366754549239483		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.30366754549239483 | validation: 0.25057819309714946]
	TIME [epoch: 7.54 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32459137953124595		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.32459137953124595 | validation: 0.28177689945260875]
	TIME [epoch: 7.53 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24304153292391698		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.24304153292391698 | validation: 0.24275988855894348]
	TIME [epoch: 7.53 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27936466339008814		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.27936466339008814 | validation: 0.4320516820974969]
	TIME [epoch: 7.55 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3309562868412517		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.3309562868412517 | validation: 0.2277286425482632]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28317613256931273		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.28317613256931273 | validation: 0.2615425698830158]
	TIME [epoch: 7.55 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24177266622477114		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.24177266622477114 | validation: 0.28157778207509343]
	TIME [epoch: 7.55 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2978981402606514		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2978981402606514 | validation: 0.21106857589390876]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23595349675809082		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.23595349675809082 | validation: 0.2274051647221831]
	TIME [epoch: 7.56 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2670617452732086		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.2670617452732086 | validation: 0.34192529325767057]
	TIME [epoch: 7.53 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3615773346963022		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.3615773346963022 | validation: 0.2739006762683091]
	TIME [epoch: 7.52 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2815196080855794		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2815196080855794 | validation: 0.2391719745115431]
	TIME [epoch: 7.51 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.213395320904195		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.213395320904195 | validation: 0.3048531191330731]
	TIME [epoch: 7.52 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2602363737224407		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2602363737224407 | validation: 0.47579820644956194]
	TIME [epoch: 7.56 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.310856041587515		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.310856041587515 | validation: 0.23193715063327486]
	TIME [epoch: 7.53 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609806816091321		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2609806816091321 | validation: 0.2445410717454797]
	TIME [epoch: 7.51 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22376900733990593		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.22376900733990593 | validation: 0.3090445803031914]
	TIME [epoch: 7.51 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2570993922510311		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.2570993922510311 | validation: 0.3144768900779771]
	TIME [epoch: 7.52 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29816990869319365		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.29816990869319365 | validation: 0.36283192832205996]
	TIME [epoch: 7.55 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28093058989955205		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.28093058989955205 | validation: 0.20653275041245736]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28440848788617745		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.28440848788617745 | validation: 0.4204836856200129]
	TIME [epoch: 7.52 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3075803495555157		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3075803495555157 | validation: 0.24832971137502613]
	TIME [epoch: 7.51 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2060292889396595		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2060292889396595 | validation: 0.2101646928504872]
	TIME [epoch: 7.52 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23406402386700328		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.23406402386700328 | validation: 0.29454152950398405]
	TIME [epoch: 7.55 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33928122650324566		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.33928122650324566 | validation: 0.2659304940047497]
	TIME [epoch: 7.52 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24233212008106553		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.24233212008106553 | validation: 0.2915768728921533]
	TIME [epoch: 7.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2139765101001398		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2139765101001398 | validation: 0.25537367697516666]
	TIME [epoch: 7.51 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20978560151411968		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.20978560151411968 | validation: 0.18132948165384632]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636047108623342		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.2636047108623342 | validation: 0.17620345890128608]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20986161502441858		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.20986161502441858 | validation: 0.2152505180268388]
	TIME [epoch: 7.54 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2262553629322439		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2262553629322439 | validation: 0.22010065864102601]
	TIME [epoch: 7.52 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24181188978001486		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.24181188978001486 | validation: 0.2290677253066255]
	TIME [epoch: 7.54 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24611867834513262		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.24611867834513262 | validation: 0.2485918340124169]
	TIME [epoch: 7.52 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22103192881677705		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.22103192881677705 | validation: 0.2849320721526511]
	TIME [epoch: 7.57 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25450405664708914		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.25450405664708914 | validation: 0.2505529587159638]
	TIME [epoch: 7.52 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2197823915658389		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2197823915658389 | validation: 0.3077402641521586]
	TIME [epoch: 7.53 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3071376934494467		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.3071376934494467 | validation: 0.2370699681895392]
	TIME [epoch: 7.53 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2743195808181692		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2743195808181692 | validation: 0.18000102172370036]
	TIME [epoch: 7.52 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21854626815005923		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.21854626815005923 | validation: 0.18178521896599575]
	TIME [epoch: 7.57 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18384515371243332		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.18384515371243332 | validation: 0.22418779191620813]
	TIME [epoch: 7.52 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2189996334730423		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.2189996334730423 | validation: 0.23858548175549862]
	TIME [epoch: 7.51 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20249163353307176		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.20249163353307176 | validation: 0.18181909137094707]
	TIME [epoch: 7.52 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20552790057920595		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.20552790057920595 | validation: 0.15824377831875272]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18641990808312364		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.18641990808312364 | validation: 0.19082782414094834]
	TIME [epoch: 7.57 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20267386682528124		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.20267386682528124 | validation: 0.1746204405009491]
	TIME [epoch: 7.53 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3241230636387264		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.3241230636387264 | validation: 0.3713254147302171]
	TIME [epoch: 7.54 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29928852411090734		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.29928852411090734 | validation: 0.1730513346766953]
	TIME [epoch: 7.52 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17228475488942727		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.17228475488942727 | validation: 0.143580743461266]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18996414145252283		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.18996414145252283 | validation: 0.27401523586330834]
	TIME [epoch: 7.58 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20976814554945372		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.20976814554945372 | validation: 0.3711576380243716]
	TIME [epoch: 7.54 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23253831885450976		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.23253831885450976 | validation: 0.23692992292215365]
	TIME [epoch: 7.53 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20081035879939374		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.20081035879939374 | validation: 0.19583580805164597]
	TIME [epoch: 7.53 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2041699520011131		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.2041699520011131 | validation: 0.16517327751410138]
	TIME [epoch: 7.53 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17606796839557368		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.17606796839557368 | validation: 0.27742540937044086]
	TIME [epoch: 7.58 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22975420987718953		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.22975420987718953 | validation: 0.2442290946310039]
	TIME [epoch: 7.53 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16787346290907312		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.16787346290907312 | validation: 0.3707207058725939]
	TIME [epoch: 7.54 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2509852774522681		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.2509852774522681 | validation: 0.2152064997512435]
	TIME [epoch: 7.53 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.202526908556249		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.202526908556249 | validation: 0.202977147665156]
	TIME [epoch: 7.55 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20819326232121935		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.20819326232121935 | validation: 0.1692404023541225]
	TIME [epoch: 7.58 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20168317122773155		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.20168317122773155 | validation: 0.1825125657014467]
	TIME [epoch: 7.55 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18569032722872858		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.18569032722872858 | validation: 0.1515936210303065]
	TIME [epoch: 7.52 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16064471510078837		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.16064471510078837 | validation: 0.24433701149532733]
	TIME [epoch: 7.54 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20457344359116814		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.20457344359116814 | validation: 0.23986842615280018]
	TIME [epoch: 7.54 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19661137773168522		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.19661137773168522 | validation: 0.24307234612015846]
	TIME [epoch: 7.56 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19153416404975626		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.19153416404975626 | validation: 0.1830272728013658]
	TIME [epoch: 7.53 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1881342337049679		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.1881342337049679 | validation: 0.30894901838433353]
	TIME [epoch: 7.53 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2536517941041313		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.2536517941041313 | validation: 0.22427173871810158]
	TIME [epoch: 7.54 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22408080504826527		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.22408080504826527 | validation: 0.15165874354096187]
	TIME [epoch: 7.53 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17437187559285253		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.17437187559285253 | validation: 0.1485458066542158]
	TIME [epoch: 7.58 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.168390095757088		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.168390095757088 | validation: 0.2744076101185751]
	TIME [epoch: 7.52 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19810869940311968		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.19810869940311968 | validation: 0.16161440710633332]
	TIME [epoch: 7.53 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17076450818144695		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.17076450818144695 | validation: 0.14925764092547525]
	TIME [epoch: 7.53 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2755084123887256		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.2755084123887256 | validation: 0.21570607700871322]
	TIME [epoch: 7.54 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19706065927916452		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.19706065927916452 | validation: 0.14291875295589337]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15711868550995012		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.15711868550995012 | validation: 0.4472432378803869]
	TIME [epoch: 7.56 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3039820075084496		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.3039820075084496 | validation: 0.19006275827145314]
	TIME [epoch: 7.54 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22734111757872721		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.22734111757872721 | validation: 0.12727687975753088]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15227128580987337		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.15227128580987337 | validation: 0.14837288783429567]
	TIME [epoch: 7.55 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1381628990628562		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.1381628990628562 | validation: 0.16323890545152125]
	TIME [epoch: 7.53 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2462241356934357		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.2462241356934357 | validation: 0.15615378507492045]
	TIME [epoch: 7.51 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15633221787608745		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.15633221787608745 | validation: 0.16328699282098796]
	TIME [epoch: 7.51 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18949712235647104		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.18949712235647104 | validation: 0.1492269902076176]
	TIME [epoch: 7.51 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1681727843900473		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1681727843900473 | validation: 0.22367167029728297]
	TIME [epoch: 7.53 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15997259578401662		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.15997259578401662 | validation: 0.14747200343343614]
	TIME [epoch: 7.53 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1377760031253803		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.1377760031253803 | validation: 0.11568493858651807]
	TIME [epoch: 7.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14791126456155057		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.14791126456155057 | validation: 0.28200539898011057]
	TIME [epoch: 7.51 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2372964077890618		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.2372964077890618 | validation: 0.23082736472048615]
	TIME [epoch: 7.51 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1878167277155748		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.1878167277155748 | validation: 0.25404242832560336]
	TIME [epoch: 120 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18120917557053415		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.18120917557053415 | validation: 0.15530704105006482]
	TIME [epoch: 14.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20566069664630365		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.20566069664630365 | validation: 0.18877230620292262]
	TIME [epoch: 14.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16526120622438556		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.16526120622438556 | validation: 0.19929586852170694]
	TIME [epoch: 14.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1489323595044037		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.1489323595044037 | validation: 0.11219080672780803]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1587343211698906		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.1587343211698906 | validation: 0.16141303091798037]
	TIME [epoch: 14.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21807513119882685		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.21807513119882685 | validation: 0.1468149445571641]
	TIME [epoch: 14.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15687051317788359		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.15687051317788359 | validation: 0.13470829144899116]
	TIME [epoch: 14.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13062221664618903		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.13062221664618903 | validation: 0.24648487533909183]
	TIME [epoch: 14.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1910613176937137		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1910613176937137 | validation: 0.18638358848821374]
	TIME [epoch: 14.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25219728178632994		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.25219728178632994 | validation: 0.28581036762543277]
	TIME [epoch: 14.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19322096962730145		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.19322096962730145 | validation: 0.13438221199514788]
	TIME [epoch: 14.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13279352965857746		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.13279352965857746 | validation: 0.2400103214374742]
	TIME [epoch: 14.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22540073594977197		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.22540073594977197 | validation: 0.15583731758018432]
	TIME [epoch: 14.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15099776304136098		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.15099776304136098 | validation: 0.12471669312605677]
	TIME [epoch: 14.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14031018653901312		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.14031018653901312 | validation: 0.2026003129113656]
	TIME [epoch: 14.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1901735060355396		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.1901735060355396 | validation: 0.17142977091764341]
	TIME [epoch: 14.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17892332218199153		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.17892332218199153 | validation: 0.1062244020318181]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12355956792503875		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.12355956792503875 | validation: 0.14879934942340337]
	TIME [epoch: 14.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18211214954748037		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.18211214954748037 | validation: 0.15022475395204765]
	TIME [epoch: 14.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16653224275016806		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.16653224275016806 | validation: 0.14026331072248632]
	TIME [epoch: 14.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14512237487727264		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.14512237487727264 | validation: 0.1112352306730242]
	TIME [epoch: 14.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12186858661669117		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.12186858661669117 | validation: 0.09552874559402627]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15291095743348038		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.15291095743348038 | validation: 0.12573628381326912]
	TIME [epoch: 14.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14108021688934827		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.14108021688934827 | validation: 0.18248873232460217]
	TIME [epoch: 14.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1722753125094468		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.1722753125094468 | validation: 0.16993177300657034]
	TIME [epoch: 14.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13207939844369837		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.13207939844369837 | validation: 0.09472851094441888]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12280442336220757		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.12280442336220757 | validation: 0.143721226104238]
	TIME [epoch: 14.9 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17363207077517448		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.17363207077517448 | validation: 0.19190904310059737]
	TIME [epoch: 14.8 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15584313229933802		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.15584313229933802 | validation: 0.19166128572025234]
	TIME [epoch: 14.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1374374983522929		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.1374374983522929 | validation: 0.10710355454076466]
	TIME [epoch: 14.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11582311048448762		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.11582311048448762 | validation: 0.1137569369281677]
	TIME [epoch: 14.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14459277257646538		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.14459277257646538 | validation: 0.14849851696551142]
	TIME [epoch: 14.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13666396968247319		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.13666396968247319 | validation: 0.09362969775005464]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12153000202687972		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.12153000202687972 | validation: 0.1496469455235363]
	TIME [epoch: 14.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13953287523794586		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.13953287523794586 | validation: 0.11828341868076225]
	TIME [epoch: 14.9 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1254907199338486		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.1254907199338486 | validation: 0.23912589862430234]
	TIME [epoch: 14.9 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13577865328298824		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.13577865328298824 | validation: 0.1109985879557942]
	TIME [epoch: 14.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10661765430745708		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.10661765430745708 | validation: 0.21801318684235185]
	TIME [epoch: 14.9 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20414620967779595		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.20414620967779595 | validation: 0.16056896563987544]
	TIME [epoch: 14.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13710993939378593		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.13710993939378593 | validation: 0.15314235889133462]
	TIME [epoch: 14.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10825342366892203		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.10825342366892203 | validation: 0.09649706147445139]
	TIME [epoch: 14.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11450174180952132		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11450174180952132 | validation: 0.1214040408162962]
	TIME [epoch: 14.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16301666241351842		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.16301666241351842 | validation: 0.09576712737712213]
	TIME [epoch: 14.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09749335543985732		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.09749335543985732 | validation: 0.09619800591005356]
	TIME [epoch: 14.9 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10886002529034738		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.10886002529034738 | validation: 0.11989415728728772]
	TIME [epoch: 14.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13194901418909855		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.13194901418909855 | validation: 0.15900174864588834]
	TIME [epoch: 14.9 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15157122743439205		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.15157122743439205 | validation: 0.12531842824409659]
	TIME [epoch: 14.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13693445251528372		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.13693445251528372 | validation: 0.1008421170526729]
	TIME [epoch: 14.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12889087953803885		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.12889087953803885 | validation: 0.084347597783791]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1469861341952334		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.1469861341952334 | validation: 0.12515309907539957]
	TIME [epoch: 14.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12917513457343227		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.12917513457343227 | validation: 0.13520593398906944]
	TIME [epoch: 14.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12752082129600498		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.12752082129600498 | validation: 0.15369614742296983]
	TIME [epoch: 14.9 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12648860221447		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.12648860221447 | validation: 0.12498176526430976]
	TIME [epoch: 14.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10399855558518929		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.10399855558518929 | validation: 0.08981387106409999]
	TIME [epoch: 14.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.103902848037172		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.103902848037172 | validation: 0.12049104503209096]
	TIME [epoch: 14.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10400477422607578		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.10400477422607578 | validation: 0.1817766656962237]
	TIME [epoch: 14.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11783029446363597		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.11783029446363597 | validation: 0.06499855625440593]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1116030772859831		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.1116030772859831 | validation: 0.08652205100139629]
	TIME [epoch: 14.9 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09723626441121612		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.09723626441121612 | validation: 0.08637267653226113]
	TIME [epoch: 14.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13686913106428775		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.13686913106428775 | validation: 0.1354702378430475]
	TIME [epoch: 14.9 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12809779193394402		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.12809779193394402 | validation: 0.0901242211316589]
	TIME [epoch: 14.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0991954850505926		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.0991954850505926 | validation: 0.06863488592451455]
	TIME [epoch: 14.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08759811380445054		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.08759811380445054 | validation: 0.09571713631033742]
	TIME [epoch: 14.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09312714438858119		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.09312714438858119 | validation: 0.0968846308932837]
	TIME [epoch: 14.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12547901041661158		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.12547901041661158 | validation: 0.09339007869931329]
	TIME [epoch: 14.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1303416026291905		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.1303416026291905 | validation: 0.11102615257192763]
	TIME [epoch: 14.9 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09545218818271416		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.09545218818271416 | validation: 0.10059084712735197]
	TIME [epoch: 14.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10544625986737294		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.10544625986737294 | validation: 0.11879999202625116]
	TIME [epoch: 14.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10445150807008352		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.10445150807008352 | validation: 0.12769405760384137]
	TIME [epoch: 14.9 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12493743752536719		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.12493743752536719 | validation: 0.08386479019102476]
	TIME [epoch: 14.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11056729832529036		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.11056729832529036 | validation: 0.08339597755557102]
	TIME [epoch: 14.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09104002652298124		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.09104002652298124 | validation: 0.07617263369854532]
	TIME [epoch: 14.9 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14200473388090148		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.14200473388090148 | validation: 0.11291514222477617]
	TIME [epoch: 14.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11479768761073496		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.11479768761073496 | validation: 0.08298556909001659]
	TIME [epoch: 14.9 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07603135572750261		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.07603135572750261 | validation: 0.06852757863887551]
	TIME [epoch: 14.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07673512434649105		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.07673512434649105 | validation: 0.08676904345779266]
	TIME [epoch: 14.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10761254038237902		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.10761254038237902 | validation: 0.1010277694411423]
	TIME [epoch: 14.9 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11648602419909666		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11648602419909666 | validation: 0.06648862368996794]
	TIME [epoch: 14.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08913154445775084		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.08913154445775084 | validation: 0.10925218988391804]
	TIME [epoch: 14.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09719959995013602		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.09719959995013602 | validation: 0.11620344447519086]
	TIME [epoch: 14.9 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10684917248413528		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.10684917248413528 | validation: 0.06295963586208023]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07739386471787918		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.07739386471787918 | validation: 0.05927075086843779]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08343017255948246		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.08343017255948246 | validation: 0.18441015746296652]
	TIME [epoch: 14.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14710086609130227		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.14710086609130227 | validation: 0.12084122773859302]
	TIME [epoch: 14.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09311468310139633		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.09311468310139633 | validation: 0.054201146962501896]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1092290333131477		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.1092290333131477 | validation: 0.06071976200859032]
	TIME [epoch: 14.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08027078987975864		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.08027078987975864 | validation: 0.0725617429089293]
	TIME [epoch: 14.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0838486897020377		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.0838486897020377 | validation: 0.10229389769326651]
	TIME [epoch: 14.9 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06933267920765934		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.06933267920765934 | validation: 0.05694813972689376]
	TIME [epoch: 14.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08201880063942395		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.08201880063942395 | validation: 0.13180300757729696]
	TIME [epoch: 14.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1320262364771203		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.1320262364771203 | validation: 0.05669025210509071]
	TIME [epoch: 14.9 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1028326627607421		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.1028326627607421 | validation: 0.1052182209033725]
	TIME [epoch: 14.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08751852664599948		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.08751852664599948 | validation: 0.08911237372764197]
	TIME [epoch: 14.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07932335185839694		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.07932335185839694 | validation: 0.05654579582387924]
	TIME [epoch: 14.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0617619856682919		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.0617619856682919 | validation: 0.1250460503538907]
	TIME [epoch: 14.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10133704183621267		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.10133704183621267 | validation: 0.09159960976104094]
	TIME [epoch: 14.9 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07651524989897564		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.07651524989897564 | validation: 0.16222178269547333]
	TIME [epoch: 14.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10216733682081519		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.10216733682081519 | validation: 0.06546288815129336]
	TIME [epoch: 14.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07464128800217165		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.07464128800217165 | validation: 0.060185481702394995]
	TIME [epoch: 14.9 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06465807690915602		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.06465807690915602 | validation: 0.06946329536143096]
	TIME [epoch: 14.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07742212808942343		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.07742212808942343 | validation: 0.07864265111137925]
	TIME [epoch: 14.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09117894224905503		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.09117894224905503 | validation: 0.11422175830667378]
	TIME [epoch: 14.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1452675135993839		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.1452675135993839 | validation: 0.08018878923260075]
	TIME [epoch: 14.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06502835663515294		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.06502835663515294 | validation: 0.05325829751546038]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06866997962060631		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.06866997962060631 | validation: 0.08069536251657847]
	TIME [epoch: 14.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06736750809327773		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.06736750809327773 | validation: 0.0827497780279658]
	TIME [epoch: 14.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06237248908066352		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.06237248908066352 | validation: 0.09697526574888221]
	TIME [epoch: 14.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07326309861066482		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.07326309861066482 | validation: 0.055193028355464954]
	TIME [epoch: 14.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05539393086484898		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.05539393086484898 | validation: 0.051330175793140406]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06820510765743912		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.06820510765743912 | validation: 0.20538506612376434]
	TIME [epoch: 14.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11350212817449336		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.11350212817449336 | validation: 0.08099001930715141]
	TIME [epoch: 14.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06567573494721718		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.06567573494721718 | validation: 0.04351369909889176]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05134257002181757		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.05134257002181757 | validation: 0.11684832994881739]
	TIME [epoch: 14.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07821896162411344		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.07821896162411344 | validation: 0.061037193757049885]
	TIME [epoch: 14.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06262306098294247		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.06262306098294247 | validation: 0.08722718099559268]
	TIME [epoch: 14.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06567576702460613		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.06567576702460613 | validation: 0.0535564318206916]
	TIME [epoch: 14.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336423045692921		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.06336423045692921 | validation: 0.055520025038409374]
	TIME [epoch: 14.9 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06570069751638716		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.06570069751638716 | validation: 0.05389941090087184]
	TIME [epoch: 14.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04554615097750143		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.04554615097750143 | validation: 0.0585263027544764]
	TIME [epoch: 14.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09872537771220183		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.09872537771220183 | validation: 0.07012466552622584]
	TIME [epoch: 14.9 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06508131560471891		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.06508131560471891 | validation: 0.09661561337633437]
	TIME [epoch: 14.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07406417290978037		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.07406417290978037 | validation: 0.07301669692823845]
	TIME [epoch: 14.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07165243254395906		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.07165243254395906 | validation: 0.061268944266881877]
	TIME [epoch: 14.9 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05877702066864692		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.05877702066864692 | validation: 0.083385583133842]
	TIME [epoch: 14.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0911610378663028		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.0911610378663028 | validation: 0.06306936239280381]
	TIME [epoch: 14.9 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05451000969858059		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.05451000969858059 | validation: 0.046481159656367045]
	TIME [epoch: 14.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056691816922564586		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.056691816922564586 | validation: 0.10067605959162312]
	TIME [epoch: 14.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06980648306450855		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.06980648306450855 | validation: 0.047035663267012286]
	TIME [epoch: 14.9 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10450390701259121		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.10450390701259121 | validation: 0.1016513010603024]
	TIME [epoch: 14.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05706868788816792		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.05706868788816792 | validation: 0.05094883537050902]
	TIME [epoch: 14.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045864509446062234		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.045864509446062234 | validation: 0.04552681174744169]
	TIME [epoch: 14.9 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051738725933034135		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.051738725933034135 | validation: 0.10177877143925204]
	TIME [epoch: 14.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0675195215685853		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.0675195215685853 | validation: 0.0647411905482644]
	TIME [epoch: 14.9 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08357009184746168		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.08357009184746168 | validation: 0.06067355486900003]
	TIME [epoch: 14.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04845532212272424		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.04845532212272424 | validation: 0.04873672758554164]
	TIME [epoch: 14.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07472261823264847		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.07472261823264847 | validation: 0.07321821725711675]
	TIME [epoch: 14.9 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05737262090346257		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.05737262090346257 | validation: 0.06791391516217155]
	TIME [epoch: 14.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053951456332116156		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.053951456332116156 | validation: 0.13218638015012965]
	TIME [epoch: 14.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09809016185204486		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.09809016185204486 | validation: 0.07471950366378519]
	TIME [epoch: 14.9 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051271808098456634		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.051271808098456634 | validation: 0.03862265796109762]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05614437636389594		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.05614437636389594 | validation: 0.0477330471528947]
	TIME [epoch: 14.9 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04087886126537462		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.04087886126537462 | validation: 0.5697918958945629]
	TIME [epoch: 14.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2855342808889377		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.2855342808889377 | validation: 0.2066019635660759]
	TIME [epoch: 14.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10672953585898379		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.10672953585898379 | validation: 0.0528443403557741]
	TIME [epoch: 14.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04251388703716155		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.04251388703716155 | validation: 0.041350927312377606]
	TIME [epoch: 14.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03687718847563815		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.03687718847563815 | validation: 0.03753015522736154]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04094122169380039		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.04094122169380039 | validation: 0.3371048353680112]
	TIME [epoch: 14.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24039652421125574		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.24039652421125574 | validation: 0.08157150307067618]
	TIME [epoch: 14.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05762442003774758		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.05762442003774758 | validation: 0.042414125777388095]
	TIME [epoch: 14.9 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03407334215771629		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.03407334215771629 | validation: 0.05644053114265328]
	TIME [epoch: 14.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04547324002306396		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.04547324002306396 | validation: 0.0425772300766295]
	TIME [epoch: 14.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07168450706459752		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.07168450706459752 | validation: 0.10542093022325359]
	TIME [epoch: 14.9 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0633404067374777		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.0633404067374777 | validation: 0.04741539122162544]
	TIME [epoch: 14.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03780912076647226		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.03780912076647226 | validation: 0.0996616231070778]
	TIME [epoch: 14.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06798484651894637		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.06798484651894637 | validation: 0.31191478303333087]
	TIME [epoch: 14.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17786347796284657		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.17786347796284657 | validation: 0.10384904982963997]
	TIME [epoch: 14.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06751729313525273		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.06751729313525273 | validation: 0.05692854064464459]
	TIME [epoch: 14.9 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03781615375099642		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.03781615375099642 | validation: 0.04030397357215042]
	TIME [epoch: 14.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1206124676307886		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.1206124676307886 | validation: 0.194955605561219]
	TIME [epoch: 14.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1682098620295981		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.1682098620295981 | validation: 0.06537602393702208]
	TIME [epoch: 14.9 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0669630882822559		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.0669630882822559 | validation: 0.06570198432380923]
	TIME [epoch: 14.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038704193169019865		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.038704193169019865 | validation: 0.03676399233762462]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03515208404453608		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.03515208404453608 | validation: 0.03925072077157326]
	TIME [epoch: 14.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03178716561216937		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.03178716561216937 | validation: 0.03232937795605431]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04497417292569799		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.04497417292569799 | validation: 0.05104790585732281]
	TIME [epoch: 14.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03517790408762949		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.03517790408762949 | validation: 0.0419394852014862]
	TIME [epoch: 14.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041950684836202776		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.041950684836202776 | validation: 0.041658777365422686]
	TIME [epoch: 14.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04573307272183469		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.04573307272183469 | validation: 0.19400025713852637]
	TIME [epoch: 14.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11840090283668883		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.11840090283668883 | validation: 0.052314014733716344]
	TIME [epoch: 14.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04528117805006981		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.04528117805006981 | validation: 0.04507799054082162]
	TIME [epoch: 14.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03554201535226009		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.03554201535226009 | validation: 0.028734380311361673]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02647679748847436		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.02647679748847436 | validation: 0.04656512253066261]
	TIME [epoch: 14.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060536133023572955		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.060536133023572955 | validation: 0.04129837547915027]
	TIME [epoch: 14.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030942366360062208		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.030942366360062208 | validation: 0.044393573427075575]
	TIME [epoch: 14.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04512133736349218		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.04512133736349218 | validation: 0.04352919114851239]
	TIME [epoch: 14.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04233562282688387		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.04233562282688387 | validation: 0.030280653484248278]
	TIME [epoch: 14.9 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027499732961567016		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.027499732961567016 | validation: 0.02295572841768271]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03972849485095237		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.03972849485095237 | validation: 0.048708303842650014]
	TIME [epoch: 14.9 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06531724740003235		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.06531724740003235 | validation: 0.03800048673878856]
	TIME [epoch: 14.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047754892695660585		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.047754892695660585 | validation: 0.036116289075352775]
	TIME [epoch: 14.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027323764816279055		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.027323764816279055 | validation: 0.02632910264450311]
	TIME [epoch: 14.9 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031451680786963095		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.031451680786963095 | validation: 0.030913182097678973]
	TIME [epoch: 14.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04839506737458818		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.04839506737458818 | validation: 0.08468798622282563]
	TIME [epoch: 14.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05760559738287485		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.05760559738287485 | validation: 0.05326766514201832]
	TIME [epoch: 14.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03258504774627691		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.03258504774627691 | validation: 0.02875955696297338]
	TIME [epoch: 14.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024838449988748044		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.024838449988748044 | validation: 0.03416979172084136]
	TIME [epoch: 14.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036008615074000765		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.036008615074000765 | validation: 0.04535897989570321]
	TIME [epoch: 14.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02792531113109825		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.02792531113109825 | validation: 0.026350417008733227]
	TIME [epoch: 14.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09883867004059396		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.09883867004059396 | validation: 0.09000255169059643]
	TIME [epoch: 14.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07178671906491509		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.07178671906491509 | validation: 0.06701021426926415]
	TIME [epoch: 14.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04430600870876206		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.04430600870876206 | validation: 0.029009060571075333]
	TIME [epoch: 14.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02676088714491459		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.02676088714491459 | validation: 0.03683302231599624]
	TIME [epoch: 14.9 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02920352766050049		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.02920352766050049 | validation: 0.029600889499599853]
	TIME [epoch: 14.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03180218337355613		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.03180218337355613 | validation: 0.030088156076445327]
	TIME [epoch: 14.9 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03011152748448398		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.03011152748448398 | validation: 0.03915675338750546]
	TIME [epoch: 14.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029629330275866086		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.029629330275866086 | validation: 0.2754907611979669]
	TIME [epoch: 14.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1633952020764769		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.1633952020764769 | validation: 0.08680305971995525]
	TIME [epoch: 14.9 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04286591662473952		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.04286591662473952 | validation: 0.030784596390819534]
	TIME [epoch: 14.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021492360112483712		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.021492360112483712 | validation: 0.02666511482783661]
	TIME [epoch: 14.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022663851458442463		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.022663851458442463 | validation: 0.01905526420990452]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02041770639388251		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.02041770639388251 | validation: 0.021564483082289275]
	TIME [epoch: 14.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021276805265051137		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.021276805265051137 | validation: 0.04417904671396928]
	TIME [epoch: 14.9 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04395997001899565		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.04395997001899565 | validation: 0.027163901158048124]
	TIME [epoch: 14.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030011175179030822		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.030011175179030822 | validation: 0.07290385352918087]
	TIME [epoch: 14.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03624820001976289		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.03624820001976289 | validation: 0.0335692597734099]
	TIME [epoch: 14.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023956423675215915		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.023956423675215915 | validation: 0.020990892321730528]
	TIME [epoch: 14.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0443313741872904		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.0443313741872904 | validation: 0.07457222171346461]
	TIME [epoch: 14.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053253466353471016		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.053253466353471016 | validation: 0.028246174741036183]
	TIME [epoch: 14.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029976464148880916		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.029976464148880916 | validation: 0.042705236514385805]
	TIME [epoch: 14.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029847005211266216		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.029847005211266216 | validation: 0.018757926150820108]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03783081769662378		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.03783081769662378 | validation: 0.02536446196736855]
	TIME [epoch: 14.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033311505675335765		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.033311505675335765 | validation: 0.030167051010655255]
	TIME [epoch: 14.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024666846725413322		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.024666846725413322 | validation: 0.03356731255746584]
	TIME [epoch: 14.9 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08457363766027581		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.08457363766027581 | validation: 0.07882459922870236]
	TIME [epoch: 14.8 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039732950985091		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.039732950985091 | validation: 0.025954074923517558]
	TIME [epoch: 14.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020622598554591476		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.020622598554591476 | validation: 0.018356115611021403]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01898986681367013		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.01898986681367013 | validation: 0.022221806348006787]
	TIME [epoch: 14.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01867344123051072		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.01867344123051072 | validation: 0.031040839391299514]
	TIME [epoch: 14.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02408429860021		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.02408429860021 | validation: 0.020896311579316144]
	TIME [epoch: 14.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02668612035678286		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.02668612035678286 | validation: 0.03416117453760222]
	TIME [epoch: 14.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020476901151409117		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.020476901151409117 | validation: 0.04816978135885498]
	TIME [epoch: 14.9 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036186572560531004		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.036186572560531004 | validation: 0.04766104542955241]
	TIME [epoch: 14.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329439453173407		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.0329439453173407 | validation: 0.020910063253743602]
	TIME [epoch: 14.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02313847347702048		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.02313847347702048 | validation: 0.01922916461245279]
	TIME [epoch: 14.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06776808511534047		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.06776808511534047 | validation: 0.047645703014375825]
	TIME [epoch: 14.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028465271390233953		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.028465271390233953 | validation: 0.03951696190800309]
	TIME [epoch: 14.9 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026500636039053897		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.026500636039053897 | validation: 0.017892649682543976]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01646213845684493		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01646213845684493 | validation: 0.019398584419018348]
	TIME [epoch: 14.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021887040538161262		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.021887040538161262 | validation: 0.02202253082596514]
	TIME [epoch: 14.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024209006145622294		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.024209006145622294 | validation: 0.02056469425550906]
	TIME [epoch: 14.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018878374590596936		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.018878374590596936 | validation: 0.01455157140423867]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02453040732801006		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.02453040732801006 | validation: 0.057085130752342284]
	TIME [epoch: 14.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028937536633847794		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.028937536633847794 | validation: 0.03007663481279456]
	TIME [epoch: 14.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046742783476456204		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.046742783476456204 | validation: 0.023128617755506297]
	TIME [epoch: 14.9 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014731642481558805		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.014731642481558805 | validation: 0.013615028477689056]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014785869245387584		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.014785869245387584 | validation: 0.015248546388131523]
	TIME [epoch: 14.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02242728480641816		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.02242728480641816 | validation: 0.04507899837255376]
	TIME [epoch: 14.9 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03864137184693954		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.03864137184693954 | validation: 0.013701063143644686]
	TIME [epoch: 14.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013747424233417415		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.013747424233417415 | validation: 0.015488451633791808]
	TIME [epoch: 14.9 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019567974432270666		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.019567974432270666 | validation: 0.056209529723937314]
	TIME [epoch: 14.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024022323866556783		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.024022323866556783 | validation: 0.0177614120231731]
	TIME [epoch: 14.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014317755282941044		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.014317755282941044 | validation: 0.036462096368592226]
	TIME [epoch: 14.9 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03247111688731706		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.03247111688731706 | validation: 0.02746943138739607]
	TIME [epoch: 14.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022059685347194047		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.022059685347194047 | validation: 0.02580656219970469]
	TIME [epoch: 14.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02724901826581468		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.02724901826581468 | validation: 0.027149702139193217]
	TIME [epoch: 14.9 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017434033116446066		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.017434033116446066 | validation: 0.017512307091987846]
	TIME [epoch: 14.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017175786251397766		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.017175786251397766 | validation: 0.05305364992107571]
	TIME [epoch: 14.9 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03149582071056865		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.03149582071056865 | validation: 0.013038173549583011]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01494059608788752		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.01494059608788752 | validation: 0.026371860978137134]
	TIME [epoch: 14.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02181215974382747		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.02181215974382747 | validation: 0.015319065828214708]
	TIME [epoch: 14.9 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0335227893423139		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.0335227893423139 | validation: 0.03884981430626925]
	TIME [epoch: 14.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016360395515691874		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.016360395515691874 | validation: 0.016938119974671866]
	TIME [epoch: 14.9 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015572646931131414		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.015572646931131414 | validation: 0.02010441128339345]
	TIME [epoch: 14.9 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023061817906245115		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.023061817906245115 | validation: 0.024462485837203816]
	TIME [epoch: 14.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02189584331395926		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.02189584331395926 | validation: 0.014054213241618412]
	TIME [epoch: 14.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014738910198346684		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.014738910198346684 | validation: 0.03246312421057242]
	TIME [epoch: 14.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019981929030097705		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.019981929030097705 | validation: 0.02401967119880785]
	TIME [epoch: 14.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024981589638174165		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.024981589638174165 | validation: 0.03565328516274004]
	TIME [epoch: 14.9 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02579930513681599		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.02579930513681599 | validation: 0.04177805413213677]
	TIME [epoch: 14.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036736502160285296		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.036736502160285296 | validation: 0.01624076018412678]
	TIME [epoch: 14.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012918198430415568		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.012918198430415568 | validation: 0.01857576762919573]
	TIME [epoch: 14.9 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014074060271169857		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.014074060271169857 | validation: 0.014310510166751493]
	TIME [epoch: 14.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01716125759828871		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.01716125759828871 | validation: 0.021377538570997057]
	TIME [epoch: 14.9 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024970895151203876		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.024970895151203876 | validation: 0.02134077836254678]
	TIME [epoch: 14.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015023051513599155		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.015023051513599155 | validation: 0.013409867753129696]
	TIME [epoch: 14.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014113218117962154		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.014113218117962154 | validation: 0.022887596133808807]
	TIME [epoch: 14.9 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018022807124045238		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.018022807124045238 | validation: 0.03532845594394628]
	TIME [epoch: 14.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05033893991248643		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.05033893991248643 | validation: 0.05083663656784534]
	TIME [epoch: 14.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035842677119026264		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.035842677119026264 | validation: 0.0161260856609432]
	TIME [epoch: 14.9 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012989541065756158		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.012989541065756158 | validation: 0.014578694910865027]
	TIME [epoch: 14.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01614686256033082		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.01614686256033082 | validation: 0.020143863304994857]
	TIME [epoch: 14.9 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015086204803613158		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.015086204803613158 | validation: 0.013322395522779073]
	TIME [epoch: 14.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014251965155087264		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.014251965155087264 | validation: 0.014397988600100952]
	TIME [epoch: 14.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012042449537072493		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.012042449537072493 | validation: 0.016539494115666332]
	TIME [epoch: 14.9 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020455410869789782		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.020455410869789782 | validation: 0.019920241260782234]
	TIME [epoch: 14.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027409132913685928		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.027409132913685928 | validation: 0.029251412028183554]
	TIME [epoch: 14.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017018244747637662		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.017018244747637662 | validation: 0.027785063063235003]
	TIME [epoch: 14.9 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016011541964981776		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.016011541964981776 | validation: 0.02215882394393235]
	TIME [epoch: 14.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10759365053510161		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.10759365053510161 | validation: 0.06479615197110204]
	TIME [epoch: 14.9 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054982155543982086		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.054982155543982086 | validation: 0.011584749752024254]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01323564278120757		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.01323564278120757 | validation: 0.012675514796312428]
	TIME [epoch: 14.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010755686157531		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.010755686157531 | validation: 0.012089709754233978]
	TIME [epoch: 14.9 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01077778327152528		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.01077778327152528 | validation: 0.01178353626049427]
	TIME [epoch: 14.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013038336214148804		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.013038336214148804 | validation: 0.013037887494022214]
	TIME [epoch: 14.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012888224840048578		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.012888224840048578 | validation: 0.012826456014561328]
	TIME [epoch: 14.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025223447415507187		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.025223447415507187 | validation: 0.013399895413336567]
	TIME [epoch: 14.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013845291138850482		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.013845291138850482 | validation: 0.024689212985461916]
	TIME [epoch: 14.9 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01963432411970447		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.01963432411970447 | validation: 0.017389014339198934]
	TIME [epoch: 14.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012643468191992094		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.012643468191992094 | validation: 0.01732870146092627]
	TIME [epoch: 14.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02184378308840164		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.02184378308840164 | validation: 0.01335142993774964]
	TIME [epoch: 14.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012610953780630462		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.012610953780630462 | validation: 0.030723833592973475]
	TIME [epoch: 14.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04638877442100046		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.04638877442100046 | validation: 0.04217631171075279]
	TIME [epoch: 14.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02191738547612581		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.02191738547612581 | validation: 0.019423660802892928]
	TIME [epoch: 14.9 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011441007095576964		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.011441007095576964 | validation: 0.01151128416208079]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008974222956774672		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.008974222956774672 | validation: 0.014625712991646618]
	TIME [epoch: 14.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014922337868727145		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.014922337868727145 | validation: 0.015451336909011858]
	TIME [epoch: 14.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014247744664920974		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.014247744664920974 | validation: 0.009971156000126285]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604496205218884		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.01604496205218884 | validation: 0.018493100939346176]
	TIME [epoch: 14.9 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018117976801153648		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.018117976801153648 | validation: 0.02674072798440652]
	TIME [epoch: 14.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01480069575615606		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.01480069575615606 | validation: 0.015475664066876517]
	TIME [epoch: 140 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020047073508661244		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.020047073508661244 | validation: 0.04398340913860293]
	TIME [epoch: 32.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027116490381383362		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.027116490381383362 | validation: 0.018424851285434325]
	TIME [epoch: 32.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014941090164606296		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.014941090164606296 | validation: 0.020610847746493098]
	TIME [epoch: 32.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013656706704310576		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.013656706704310576 | validation: 0.020815376463970105]
	TIME [epoch: 32.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026027595161789915		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.026027595161789915 | validation: 0.016198782898858535]
	TIME [epoch: 32.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015298662906439421		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.015298662906439421 | validation: 0.010375007212666836]
	TIME [epoch: 32.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01377545579443621		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.01377545579443621 | validation: 0.018365967914058506]
	TIME [epoch: 32.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048037056822539406		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.048037056822539406 | validation: 0.031523470637868235]
	TIME [epoch: 32.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021676532477901085		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.021676532477901085 | validation: 0.009989212540110681]
	TIME [epoch: 32.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009076026071158849		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.009076026071158849 | validation: 0.008532404047815693]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009423534536571208		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.009423534536571208 | validation: 0.009184159129406861]
	TIME [epoch: 32.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015362091307784043		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.015362091307784043 | validation: 0.009189739468774787]
	TIME [epoch: 32.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009716987603026016		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.009716987603026016 | validation: 0.01450662331007425]
	TIME [epoch: 32.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012866825276205569		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.012866825276205569 | validation: 0.015738285354582913]
	TIME [epoch: 32.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01580861302210592		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.01580861302210592 | validation: 0.021676219370869165]
	TIME [epoch: 32.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014235905186900558		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.014235905186900558 | validation: 0.010378819910944779]
	TIME [epoch: 32.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01619300619675892		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.01619300619675892 | validation: 0.012440659871844124]
	TIME [epoch: 32.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01202193120452151		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.01202193120452151 | validation: 0.009341431643724729]
	TIME [epoch: 32.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010024440065272951		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.010024440065272951 | validation: 0.012585284080863617]
	TIME [epoch: 32.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010204732163130528		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.010204732163130528 | validation: 0.013425622814702293]
	TIME [epoch: 32.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014544846443287405		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.014544846443287405 | validation: 0.017441918359449346]
	TIME [epoch: 32.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016016980846910507		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.016016980846910507 | validation: 0.01801611183381062]
	TIME [epoch: 32.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018269125233754815		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.018269125233754815 | validation: 0.013087431044792267]
	TIME [epoch: 32.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010291486241621944		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.010291486241621944 | validation: 0.009102313916257397]
	TIME [epoch: 32.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013123458070348513		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.013123458070348513 | validation: 0.014312786428462242]
	TIME [epoch: 32.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015351104002100324		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.015351104002100324 | validation: 0.02034104209172509]
	TIME [epoch: 32.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013513849178183334		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.013513849178183334 | validation: 0.017271113568055385]
	TIME [epoch: 32.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011561990715598438		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.011561990715598438 | validation: 0.0193446934050372]
	TIME [epoch: 32.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012209280567373717		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.012209280567373717 | validation: 0.02164529071134316]
	TIME [epoch: 32.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01731642485736967		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.01731642485736967 | validation: 0.01575623990906985]
	TIME [epoch: 32.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011202433830889197		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.011202433830889197 | validation: 0.010723413217208072]
	TIME [epoch: 32.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015544979206373183		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.015544979206373183 | validation: 0.029111648014383838]
	TIME [epoch: 32.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01785346253711776		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.01785346253711776 | validation: 0.010787925258253384]
	TIME [epoch: 32.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008998427464383271		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.008998427464383271 | validation: 0.01165574780230615]
	TIME [epoch: 32.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009976933009761461		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.009976933009761461 | validation: 0.019026650613655348]
	TIME [epoch: 32.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009920453360196875		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.009920453360196875 | validation: 0.013430053738117792]
	TIME [epoch: 32.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025326572768292782		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.025326572768292782 | validation: 0.021421393830447347]
	TIME [epoch: 32.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02209477344681797		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.02209477344681797 | validation: 0.02469091702050697]
	TIME [epoch: 32.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01462005961293665		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.01462005961293665 | validation: 0.01010015048448114]
	TIME [epoch: 32.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011182887747392206		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.011182887747392206 | validation: 0.008423796160750775]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067145028695024265		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.0067145028695024265 | validation: 0.015543624422870845]
	TIME [epoch: 32.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010890414502345014		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.010890414502345014 | validation: 0.020987269120961237]
	TIME [epoch: 32.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020132962210217764		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.020132962210217764 | validation: 0.01253888471454529]
	TIME [epoch: 32.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008501592033951948		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.008501592033951948 | validation: 0.008905161621168368]
	TIME [epoch: 32.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013194346316631897		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.013194346316631897 | validation: 0.012019041051316244]
	TIME [epoch: 32.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013790963966470216		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.013790963966470216 | validation: 0.008893976338959435]
	TIME [epoch: 32.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010417819143131647		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.010417819143131647 | validation: 0.019687630039952237]
	TIME [epoch: 32.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013139305345845956		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.013139305345845956 | validation: 0.013305092676940878]
	TIME [epoch: 32.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010848566566574873		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.010848566566574873 | validation: 0.02454319559973878]
	TIME [epoch: 32.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014167315592963893		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.014167315592963893 | validation: 0.011712098475545576]
	TIME [epoch: 32.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009119668701580019		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.009119668701580019 | validation: 0.008162722111289756]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03118742914113956		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.03118742914113956 | validation: 0.029159669215688667]
	TIME [epoch: 32.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020858108753946305		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.020858108753946305 | validation: 0.01473206058168618]
	TIME [epoch: 32.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011980109617787713		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.011980109617787713 | validation: 0.009695798907492139]
	TIME [epoch: 32.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00767088015553921		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.00767088015553921 | validation: 0.013241693867240454]
	TIME [epoch: 32.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008799429822588654		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.008799429822588654 | validation: 0.013461347246475292]
	TIME [epoch: 32.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03940839686323116		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.03940839686323116 | validation: 0.05249768841100848]
	TIME [epoch: 32.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029922715741119627		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.029922715741119627 | validation: 0.02582732761861166]
	TIME [epoch: 32.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011436134952236165		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.011436134952236165 | validation: 0.01180558574571328]
	TIME [epoch: 32.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007947828350521435		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.007947828350521435 | validation: 0.009555994170054233]
	TIME [epoch: 32.2 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007080321909916466		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.007080321909916466 | validation: 0.010127842548293332]
	TIME [epoch: 32.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007554745919582038		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.007554745919582038 | validation: 0.01149803492241531]
	TIME [epoch: 32.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007851524521460884		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.007851524521460884 | validation: 0.011597107931866054]
	TIME [epoch: 32.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007711344668716438		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.007711344668716438 | validation: 0.009394712376126007]
	TIME [epoch: 32.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007383788585237806		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.007383788585237806 | validation: 0.010194237025980005]
	TIME [epoch: 32.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01384722880123147		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.01384722880123147 | validation: 0.010856484338259902]
	TIME [epoch: 32.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009277412553037814		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.009277412553037814 | validation: 0.02757427713955618]
	TIME [epoch: 32.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012792579343261959		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.012792579343261959 | validation: 0.010227655738207362]
	TIME [epoch: 32.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00812830297382879		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.00812830297382879 | validation: 0.010065554739915083]
	TIME [epoch: 32.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012633043270547293		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.012633043270547293 | validation: 0.021417208875313752]
	TIME [epoch: 32.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013021541161274823		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.013021541161274823 | validation: 0.015606369708595837]
	TIME [epoch: 32.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007161073532898171		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.007161073532898171 | validation: 0.015393077742142654]
	TIME [epoch: 32.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012268049254910617		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.012268049254910617 | validation: 0.015586879745490879]
	TIME [epoch: 32.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014420030240771837		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.014420030240771837 | validation: 0.010575330749012046]
	TIME [epoch: 32.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007145857768852824		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.007145857768852824 | validation: 0.00950347095679127]
	TIME [epoch: 32.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009292024760493135		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.009292024760493135 | validation: 0.014084365107368735]
	TIME [epoch: 32.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010947780671022773		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.010947780671022773 | validation: 0.011833826368056344]
	TIME [epoch: 32.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01108725830020055		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.01108725830020055 | validation: 0.006983407914311946]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009617863389687207		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.009617863389687207 | validation: 0.009167935193821058]
	TIME [epoch: 32.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0170119442836268		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.0170119442836268 | validation: 0.010159861658719503]
	TIME [epoch: 32.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010878123749816523		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.010878123749816523 | validation: 0.009441045701563592]
	TIME [epoch: 32.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007366211503241013		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.007366211503241013 | validation: 0.007280953804004979]
	TIME [epoch: 32.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009295031931816751		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.009295031931816751 | validation: 0.009573118437374633]
	TIME [epoch: 32.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010702647109920332		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.010702647109920332 | validation: 0.02482949034083928]
	TIME [epoch: 32.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012867621924839704		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.012867621924839704 | validation: 0.010236533767169344]
	TIME [epoch: 32.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006798995654432184		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.006798995654432184 | validation: 0.009159000204196648]
	TIME [epoch: 32.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007334877429968714		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.007334877429968714 | validation: 0.00842028405947543]
	TIME [epoch: 32.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010524632708780539		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.010524632708780539 | validation: 0.0227825914404522]
	TIME [epoch: 32.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01190635160508199		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.01190635160508199 | validation: 0.009747212414123048]
	TIME [epoch: 32.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00705992020225275		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.00705992020225275 | validation: 0.010815181650141753]
	TIME [epoch: 32.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009378983909231673		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.009378983909231673 | validation: 0.016805983019387218]
	TIME [epoch: 32.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010662205141858354		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.010662205141858354 | validation: 0.009823869056260086]
	TIME [epoch: 32.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009950094025481719		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.009950094025481719 | validation: 0.009171875098524334]
	TIME [epoch: 32.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010437119949727193		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.010437119949727193 | validation: 0.020544141794782814]
	TIME [epoch: 32.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009851954248066662		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.009851954248066662 | validation: 0.012158196858161602]
	TIME [epoch: 32.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006869313100591986		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.006869313100591986 | validation: 0.010404923624213566]
	TIME [epoch: 32.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008505519215375679		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.008505519215375679 | validation: 0.008335895923892176]
	TIME [epoch: 32.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009152132743416988		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.009152132743416988 | validation: 0.01165553439979388]
	TIME [epoch: 32.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010006610874373361		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.010006610874373361 | validation: 0.012244143483841856]
	TIME [epoch: 32.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009065499640806152		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.009065499640806152 | validation: 0.013342513843701581]
	TIME [epoch: 32.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01206210644871804		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.01206210644871804 | validation: 0.008237438912796335]
	TIME [epoch: 32.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006504151014606073		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.006504151014606073 | validation: 0.01561127773088305]
	TIME [epoch: 32.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008320310419392362		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.008320310419392362 | validation: 0.007661137033801281]
	TIME [epoch: 32.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008708144811588821		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.008708144811588821 | validation: 0.00849558224141655]
	TIME [epoch: 32.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01124653104978951		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.01124653104978951 | validation: 0.007488159466191109]
	TIME [epoch: 32.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008035362031930726		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.008035362031930726 | validation: 0.013332542029623539]
	TIME [epoch: 32.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007167127143370539		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.007167127143370539 | validation: 0.0072475590639838995]
	TIME [epoch: 32.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006138750518252682		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.006138750518252682 | validation: 0.008480638504862158]
	TIME [epoch: 32.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013285004904824302		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.013285004904824302 | validation: 0.009467768685360051]
	TIME [epoch: 32.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009220179199064989		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.009220179199064989 | validation: 0.011198385868495139]
	TIME [epoch: 32.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011638636392929818		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.011638636392929818 | validation: 0.009478083134098259]
	TIME [epoch: 32.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070746535247635505		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.0070746535247635505 | validation: 0.007991561044356913]
	TIME [epoch: 32.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006875160302515055		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.006875160302515055 | validation: 0.012942011241844943]
	TIME [epoch: 32.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006424690553015062		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.006424690553015062 | validation: 0.009749981496148499]
	TIME [epoch: 32.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008940111427097209		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.008940111427097209 | validation: 0.008175802602014406]
	TIME [epoch: 32.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00874354557545609		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.00874354557545609 | validation: 0.030409659404391998]
	TIME [epoch: 32.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01353040343511091		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.01353040343511091 | validation: 0.011787770979982487]
	TIME [epoch: 32.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009326404623837478		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.009326404623837478 | validation: 0.008095667323774505]
	TIME [epoch: 32.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00680477280391381		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.00680477280391381 | validation: 0.011109294011306411]
	TIME [epoch: 32.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010697241326828767		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.010697241326828767 | validation: 0.011590267377308058]
	TIME [epoch: 32.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006763402247130472		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.006763402247130472 | validation: 0.007317190424277021]
	TIME [epoch: 32.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006088027520266055		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.006088027520266055 | validation: 0.016715633407276238]
	TIME [epoch: 32.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008656053046549374		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.008656053046549374 | validation: 0.007476938412404689]
	TIME [epoch: 32.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00769646226433669		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.00769646226433669 | validation: 0.007254039558857744]
	TIME [epoch: 32.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007346791975023611		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.007346791975023611 | validation: 0.010249312393860868]
	TIME [epoch: 32.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008145078787332182		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.008145078787332182 | validation: 0.011970416907345708]
	TIME [epoch: 32.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009666167488029467		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.009666167488029467 | validation: 0.008365303431834257]
	TIME [epoch: 32.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010488434813271517		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.010488434813271517 | validation: 0.011989061728908157]
	TIME [epoch: 32.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009039461965413881		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.009039461965413881 | validation: 0.010256986614364434]
	TIME [epoch: 32.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008422389464728243		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.008422389464728243 | validation: 0.006577402719108148]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007838705886141991		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.007838705886141991 | validation: 0.01776499486260257]
	TIME [epoch: 32.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008804290378885199		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.008804290378885199 | validation: 0.0069872576027893134]
	TIME [epoch: 32.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006406936357860135		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.006406936357860135 | validation: 0.024026670635686653]
	TIME [epoch: 32.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011130572897760389		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.011130572897760389 | validation: 0.007086194786714331]
	TIME [epoch: 32.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070532416516333315		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.0070532416516333315 | validation: 0.009162077246511656]
	TIME [epoch: 32.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006989561498348348		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.006989561498348348 | validation: 0.009368947369533663]
	TIME [epoch: 32.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006794079779608965		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.006794079779608965 | validation: 0.007333534197488107]
	TIME [epoch: 32.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006575642827361657		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.006575642827361657 | validation: 0.006493705467411553]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006658154037796648		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.006658154037796648 | validation: 0.015216081303553203]
	TIME [epoch: 32.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013874944058163228		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.013874944058163228 | validation: 0.011781576243106544]
	TIME [epoch: 32.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007688103443898369		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.007688103443898369 | validation: 0.006933663460701165]
	TIME [epoch: 32.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005170481510844456		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.005170481510844456 | validation: 0.009159867002756311]
	TIME [epoch: 32.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006577519868401891		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.006577519868401891 | validation: 0.008120848699944642]
	TIME [epoch: 32.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005692906711644972		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.005692906711644972 | validation: 0.022137434715422413]
	TIME [epoch: 32.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012368739313286137		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.012368739313286137 | validation: 0.0071261548656637265]
	TIME [epoch: 32.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009890284708674976		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.009890284708674976 | validation: 0.008836967500954203]
	TIME [epoch: 32.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006362789008204694		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.006362789008204694 | validation: 0.010600126723721709]
	TIME [epoch: 32.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007259706458547618		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.007259706458547618 | validation: 0.006304573512374031]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006291935554713104		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.006291935554713104 | validation: 0.008574938045909879]
	TIME [epoch: 32.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00516987241142222		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.00516987241142222 | validation: 0.008433697265119725]
	TIME [epoch: 32.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006769351059218063		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.006769351059218063 | validation: 0.008755324921895884]
	TIME [epoch: 32.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006046781013626391		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.006046781013626391 | validation: 0.008458426526149202]
	TIME [epoch: 32.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007494788550684157		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.007494788550684157 | validation: 0.015117844513303138]
	TIME [epoch: 32.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00942582232642386		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.00942582232642386 | validation: 0.00788774489227796]
	TIME [epoch: 32.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005563761695842905		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.005563761695842905 | validation: 0.013013673888584692]
	TIME [epoch: 32.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007651684901472144		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.007651684901472144 | validation: 0.008225528849388607]
	TIME [epoch: 32.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006942666890784894		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.006942666890784894 | validation: 0.013537190698015185]
	TIME [epoch: 32.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00737505033173347		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.00737505033173347 | validation: 0.013635253646137688]
	TIME [epoch: 32.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007879864577558375		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.007879864577558375 | validation: 0.006703173138343975]
	TIME [epoch: 32.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067700212234627286		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0067700212234627286 | validation: 0.01991001427489838]
	TIME [epoch: 32.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009837756294786051		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.009837756294786051 | validation: 0.0090574554579434]
	TIME [epoch: 32.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005345275152891439		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.005345275152891439 | validation: 0.008732040778673218]
	TIME [epoch: 32.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006640583074301977		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.006640583074301977 | validation: 0.008291828694545991]
	TIME [epoch: 32.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006502456363756994		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.006502456363756994 | validation: 0.005896977863945069]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01370752591970088		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.01370752591970088 | validation: 0.010588603497473712]
	TIME [epoch: 32.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008164030197990833		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.008164030197990833 | validation: 0.00663040883835324]
	TIME [epoch: 32.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005041072672004492		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.005041072672004492 | validation: 0.009334099203946043]
	TIME [epoch: 32.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00578252340415482		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.00578252340415482 | validation: 0.006984133172737517]
	TIME [epoch: 32.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006356738368339857		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.006356738368339857 | validation: 0.005352623040740249]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005444975707353522		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.005444975707353522 | validation: 0.008578219907518246]
	TIME [epoch: 32.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008295257259191993		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.008295257259191993 | validation: 0.009070145327145817]
	TIME [epoch: 32.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007180991072624782		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.007180991072624782 | validation: 0.009804333667954024]
	TIME [epoch: 32.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007309043250876767		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.007309043250876767 | validation: 0.009377664280668021]
	TIME [epoch: 32.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006321336259304646		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.006321336259304646 | validation: 0.006724469839520604]
	TIME [epoch: 32.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00607888683741975		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.00607888683741975 | validation: 0.010761955622499118]
	TIME [epoch: 32.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006218087070426259		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.006218087070426259 | validation: 0.009719464961352786]
	TIME [epoch: 32.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007318460446834594		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.007318460446834594 | validation: 0.034715547426031654]
	TIME [epoch: 32.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017571073112699665		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.017571073112699665 | validation: 0.007954670485939622]
	TIME [epoch: 32.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006128634518180694		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.006128634518180694 | validation: 0.006394936814508463]
	TIME [epoch: 32.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005214585508603554		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.005214585508603554 | validation: 0.007064902802506381]
	TIME [epoch: 32.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005993261010739824		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.005993261010739824 | validation: 0.008090484039615122]
	TIME [epoch: 32.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004674052844646152		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.004674052844646152 | validation: 0.005966020958211856]
	TIME [epoch: 32.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057314777141807376		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0057314777141807376 | validation: 0.0144740112756392]
	TIME [epoch: 32.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007285068761981241		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.007285068761981241 | validation: 0.008090208589707213]
	TIME [epoch: 32.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005505425846391856		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.005505425846391856 | validation: 0.007349379465957784]
	TIME [epoch: 32.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006207128116984709		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.006207128116984709 | validation: 0.0073789598692510765]
	TIME [epoch: 32.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006191253789676941		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.006191253789676941 | validation: 0.006409897659309984]
	TIME [epoch: 32.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00618894726122604		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.00618894726122604 | validation: 0.006778517069934228]
	TIME [epoch: 32.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014380670453755094		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.014380670453755094 | validation: 0.021667615609577077]
	TIME [epoch: 32.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008947516806256842		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.008947516806256842 | validation: 0.006941849709944738]
	TIME [epoch: 32.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050075381353829485		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.0050075381353829485 | validation: 0.005470450607308484]
	TIME [epoch: 32.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004470092976384618		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.004470092976384618 | validation: 0.007228650694298216]
	TIME [epoch: 32.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005976543960295464		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.005976543960295464 | validation: 0.006025276278328424]
	TIME [epoch: 32.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023239724112160774		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.023239724112160774 | validation: 0.01776258136904291]
	TIME [epoch: 32.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012944645603958312		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.012944645603958312 | validation: 0.009542714375316109]
	TIME [epoch: 32.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005574904220263844		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.005574904220263844 | validation: 0.006788260984969145]
	TIME [epoch: 32.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004712583143549043		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004712583143549043 | validation: 0.006046698862028683]
	TIME [epoch: 32.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005219478588177332		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.005219478588177332 | validation: 0.0065986910266014895]
	TIME [epoch: 32.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004794957725100618		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.004794957725100618 | validation: 0.0056138793257617305]
	TIME [epoch: 32.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005379064868079282		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.005379064868079282 | validation: 0.005498075688813829]
	TIME [epoch: 32.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006476058286542906		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.006476058286542906 | validation: 0.005487273452139833]
	TIME [epoch: 32.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047095855366027905		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.0047095855366027905 | validation: 0.008662864302698683]
	TIME [epoch: 32.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006771888551981273		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.006771888551981273 | validation: 0.0076269436144512145]
	TIME [epoch: 32.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060483980362644055		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.0060483980362644055 | validation: 0.00642798727634444]
	TIME [epoch: 32.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064905641130579535		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0064905641130579535 | validation: 0.008913662631965026]
	TIME [epoch: 32.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005226428088875001		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.005226428088875001 | validation: 0.008466756227230608]
	TIME [epoch: 32.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006819578461184641		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.006819578461184641 | validation: 0.007220648405830565]
	TIME [epoch: 32.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061100432608228936		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0061100432608228936 | validation: 0.010867288353339964]
	TIME [epoch: 32.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005356624995076613		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.005356624995076613 | validation: 0.008056122532701842]
	TIME [epoch: 32.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004956044141004918		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.004956044141004918 | validation: 0.006794442231348587]
	TIME [epoch: 32.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006570897299503254		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.006570897299503254 | validation: 0.006779688053211311]
	TIME [epoch: 32.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004848043888007122		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.004848043888007122 | validation: 0.006205041751134414]
	TIME [epoch: 32.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049225547132891855		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0049225547132891855 | validation: 0.038379992927554646]
	TIME [epoch: 32.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016543549466762633		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.016543549466762633 | validation: 0.005199249903400527]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004813244882492164		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.004813244882492164 | validation: 0.009624727813070166]
	TIME [epoch: 32.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006833906717182915		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.006833906717182915 | validation: 0.0075752949386869856]
	TIME [epoch: 32.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00490981976083245		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.00490981976083245 | validation: 0.006551931762602511]
	TIME [epoch: 32.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005158193979374006		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.005158193979374006 | validation: 0.005498582600484674]
	TIME [epoch: 32.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004189410475446004		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.004189410475446004 | validation: 0.006054408167518009]
	TIME [epoch: 32.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055207935861043035		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0055207935861043035 | validation: 0.008344429322653724]
	TIME [epoch: 32.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012652719288544606		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.012652719288544606 | validation: 0.019579719724263853]
	TIME [epoch: 32.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010481326439943582		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.010481326439943582 | validation: 0.008981051553250479]
	TIME [epoch: 32.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004982596302662564		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.004982596302662564 | validation: 0.006275489183685406]
	TIME [epoch: 32.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003935188094966393		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.003935188094966393 | validation: 0.005688398834757945]
	TIME [epoch: 32.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041945117853878135		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.0041945117853878135 | validation: 0.00687425464583199]
	TIME [epoch: 32.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004669592854715613		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.004669592854715613 | validation: 0.0054755249075479715]
	TIME [epoch: 32.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004487141068972076		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.004487141068972076 | validation: 0.009575783321926884]
	TIME [epoch: 32.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007768766324683832		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.007768766324683832 | validation: 0.0073178096978534606]
	TIME [epoch: 32.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004976957513566322		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.004976957513566322 | validation: 0.005242147532838391]
	TIME [epoch: 32.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005740912988117875		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.005740912988117875 | validation: 0.006168537815303604]
	TIME [epoch: 32.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004882553939090746		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.004882553939090746 | validation: 0.018158597288104043]
	TIME [epoch: 32.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00826246023781145		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.00826246023781145 | validation: 0.006954259967243271]
	TIME [epoch: 32.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047289591149803105		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.0047289591149803105 | validation: 0.009826117655662402]
	TIME [epoch: 32.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006331930074928845		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.006331930074928845 | validation: 0.006909263059134209]
	TIME [epoch: 32.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005180517018113043		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.005180517018113043 | validation: 0.010207870724553422]
	TIME [epoch: 32.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00500665553598773		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.00500665553598773 | validation: 0.006439187484327721]
	TIME [epoch: 32.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005423977872911036		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.005423977872911036 | validation: 0.008050608464321374]
	TIME [epoch: 32.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004931072961864685		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.004931072961864685 | validation: 0.005559431994162336]
	TIME [epoch: 32.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004211527703626134		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.004211527703626134 | validation: 0.007069972764372549]
	TIME [epoch: 32.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050772523158611055		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0050772523158611055 | validation: 0.006174257184977482]
	TIME [epoch: 32.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005059457800892986		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.005059457800892986 | validation: 0.011596066755695618]
	TIME [epoch: 32.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005848722417870108		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.005848722417870108 | validation: 0.006315281930056552]
	TIME [epoch: 32.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004266851660139594		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.004266851660139594 | validation: 0.006050398395404431]
	TIME [epoch: 32.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070561819109184135		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.0070561819109184135 | validation: 0.006252862385809665]
	TIME [epoch: 32.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005256927213234416		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.005256927213234416 | validation: 0.0063150536562796376]
	TIME [epoch: 32.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005309002629568474		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.005309002629568474 | validation: 0.007168487464885716]
	TIME [epoch: 32.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004341005485477249		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.004341005485477249 | validation: 0.006329058935075709]
	TIME [epoch: 32.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005776233603310648		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.005776233603310648 | validation: 0.005824743316876554]
	TIME [epoch: 32.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004525612367826151		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.004525612367826151 | validation: 0.004490577956751125]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004735126568382254		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.004735126568382254 | validation: 0.008461967283119225]
	TIME [epoch: 32.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005907078082905522		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.005907078082905522 | validation: 0.006251844449749567]
	TIME [epoch: 32.3 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003969680889031258		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.003969680889031258 | validation: 0.008654483819191195]
	TIME [epoch: 32.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004791807243429608		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.004791807243429608 | validation: 0.006811342636981939]
	TIME [epoch: 32.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005859580565617908		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.005859580565617908 | validation: 0.007253524324828132]
	TIME [epoch: 32.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00476981936521459		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.00476981936521459 | validation: 0.007787429381555754]
	TIME [epoch: 32.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004959779500599507		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.004959779500599507 | validation: 0.004843441744956806]
	TIME [epoch: 32.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00399693470273674		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.00399693470273674 | validation: 0.006379763486104199]
	TIME [epoch: 32.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004053178205788124		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.004053178205788124 | validation: 0.008437775151818016]
	TIME [epoch: 32.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006275803037816812		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.006275803037816812 | validation: 0.006480340404928737]
	TIME [epoch: 32.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004544680187734189		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.004544680187734189 | validation: 0.006950773715515232]
	TIME [epoch: 32.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004400345374910669		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.004400345374910669 | validation: 0.006855350898500343]
	TIME [epoch: 32.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004250076154863925		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.004250076154863925 | validation: 0.0059921869004365864]
	TIME [epoch: 32.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00644263692908271		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.00644263692908271 | validation: 0.006248864123973964]
	TIME [epoch: 32.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004165767825939		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.004165767825939 | validation: 0.004676241132442337]
	TIME [epoch: 32.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003915311075431012		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.003915311075431012 | validation: 0.006087790869136606]
	TIME [epoch: 32.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005193694654665062		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.005193694654665062 | validation: 0.005944257491615873]
	TIME [epoch: 32.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004642984151099259		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.004642984151099259 | validation: 0.005987745518871618]
	TIME [epoch: 32.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004957458254947856		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.004957458254947856 | validation: 0.005570175219372969]
	TIME [epoch: 32.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004192499893212595		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.004192499893212595 | validation: 0.005806671506945956]
	TIME [epoch: 32.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004848951968802511		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.004848951968802511 | validation: 0.004991305154505285]
	TIME [epoch: 32.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004898217968351696		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.004898217968351696 | validation: 0.0061827642055259325]
	TIME [epoch: 32.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004667576422042154		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.004667576422042154 | validation: 0.005894779330490837]
	TIME [epoch: 32.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003945455008110527		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.003945455008110527 | validation: 0.005011472248368825]
	TIME [epoch: 32.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004306774383894235		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.004306774383894235 | validation: 0.005367035249029192]
	TIME [epoch: 32.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057191360349578575		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0057191360349578575 | validation: 0.006328285768621964]
	TIME [epoch: 32.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004203751809483306		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.004203751809483306 | validation: 0.006214613193970547]
	TIME [epoch: 32.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051129046347392695		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0051129046347392695 | validation: 0.005351653422572391]
	TIME [epoch: 32.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00454416234713915		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.00454416234713915 | validation: 0.006494034225562828]
	TIME [epoch: 32.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004779870000079224		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.004779870000079224 | validation: 0.005553622619617131]
	TIME [epoch: 32.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00414837742785479		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.00414837742785479 | validation: 0.004776198278786387]
	TIME [epoch: 32.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038999513690616025		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0038999513690616025 | validation: 0.004803280220965345]
	TIME [epoch: 32.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00427316039041067		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.00427316039041067 | validation: 0.007143354205839312]
	TIME [epoch: 32.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045381083842690115		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0045381083842690115 | validation: 0.00719755865440927]
	TIME [epoch: 32.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056944145371699475		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.0056944145371699475 | validation: 0.004680385090235747]
	TIME [epoch: 32.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004387619337909873		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.004387619337909873 | validation: 0.006373247420810895]
	TIME [epoch: 32.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004309652050017001		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.004309652050017001 | validation: 0.0069506679576085]
	TIME [epoch: 32.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005387739702167847		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.005387739702167847 | validation: 0.004763153070382827]
	TIME [epoch: 32.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003963943651602618		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.003963943651602618 | validation: 0.00580972285459657]
	TIME [epoch: 32.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004773238198407186		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.004773238198407186 | validation: 0.004804719565607191]
	TIME [epoch: 32.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038550624244318493		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0038550624244318493 | validation: 0.004643816650624039]
	TIME [epoch: 32.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033429959806345107		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0033429959806345107 | validation: 0.004536748602757418]
	TIME [epoch: 32.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005003905694242788		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.005003905694242788 | validation: 0.0064780189933545725]
	TIME [epoch: 32.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004530891628764178		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.004530891628764178 | validation: 0.005271648660394041]
	TIME [epoch: 32.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004269581977652166		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.004269581977652166 | validation: 0.0048718080097402885]
	TIME [epoch: 32.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005083217616424281		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.005083217616424281 | validation: 0.005934643711869874]
	TIME [epoch: 32.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004173764711165327		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.004173764711165327 | validation: 0.004961968220288123]
	TIME [epoch: 32.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046429150223304535		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0046429150223304535 | validation: 0.005218157148420463]
	TIME [epoch: 32.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003930120433596529		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.003930120433596529 | validation: 0.005816133024148375]
	TIME [epoch: 32.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036313414998826675		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0036313414998826675 | validation: 0.00645341222033761]
	TIME [epoch: 32.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038949521448842133		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.0038949521448842133 | validation: 0.0054924154910767214]
	TIME [epoch: 32.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00404987482660672		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.00404987482660672 | validation: 0.007577340602528574]
	TIME [epoch: 32.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004011924908924306		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.004011924908924306 | validation: 0.006303787213633307]
	TIME [epoch: 32.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004255931939655486		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.004255931939655486 | validation: 0.0044281931746852285]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00392702147700299		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.00392702147700299 | validation: 0.005771112373853666]
	TIME [epoch: 32.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004490778466205637		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.004490778466205637 | validation: 0.004639545651111026]
	TIME [epoch: 32.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003764622282137442		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.003764622282137442 | validation: 0.0060923910725225495]
	TIME [epoch: 32.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00450748635274202		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.00450748635274202 | validation: 0.004909750445414613]
	TIME [epoch: 32.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034686863869865295		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0034686863869865295 | validation: 0.004357099752171646]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004399451488534796		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.004399451488534796 | validation: 0.005866705963294647]
	TIME [epoch: 32.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004266970199441186		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.004266970199441186 | validation: 0.004433156988215644]
	TIME [epoch: 32.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003779774289006735		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.003779774289006735 | validation: 0.005538773004303499]
	TIME [epoch: 32.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006111797170143864		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.006111797170143864 | validation: 0.005166830144443513]
	TIME [epoch: 32.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038017379885782773		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.0038017379885782773 | validation: 0.0049676752236466355]
	TIME [epoch: 32.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032186126301774537		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.0032186126301774537 | validation: 0.00509404472184782]
	TIME [epoch: 32.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035617108124893463		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.0035617108124893463 | validation: 0.004802793643749774]
	TIME [epoch: 32.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034110795606591754		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.0034110795606591754 | validation: 0.004990047347005337]
	TIME [epoch: 32.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004265496308407531		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.004265496308407531 | validation: 0.0043806050412683204]
	TIME [epoch: 32.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004092668055033115		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.004092668055033115 | validation: 0.006117955372037409]
	TIME [epoch: 32.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003970985953574654		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.003970985953574654 | validation: 0.004162089796909784]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032593302095723187		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.0032593302095723187 | validation: 0.005413964877408098]
	TIME [epoch: 32.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003357074758931814		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.003357074758931814 | validation: 0.008491135871396382]
	TIME [epoch: 32.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005065926893561971		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.005065926893561971 | validation: 0.0061057660157803235]
	TIME [epoch: 32.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038371795081082504		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.0038371795081082504 | validation: 0.005016151621909354]
	TIME [epoch: 32.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004598124227394143		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.004598124227394143 | validation: 0.007295045636716718]
	TIME [epoch: 32.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004036407413990072		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.004036407413990072 | validation: 0.004387409092425287]
	TIME [epoch: 32.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035862397261700154		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0035862397261700154 | validation: 0.00581371046557602]
	TIME [epoch: 32.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004410430965075822		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.004410430965075822 | validation: 0.004545024492227279]
	TIME [epoch: 32.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036532574912467282		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.0036532574912467282 | validation: 0.005163095753339675]
	TIME [epoch: 32.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034042496255067877		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.0034042496255067877 | validation: 0.005267314170774926]
	TIME [epoch: 32.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003930287399180254		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.003930287399180254 | validation: 0.004526665436098641]
	TIME [epoch: 32.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037511805885067943		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0037511805885067943 | validation: 0.0060375763177796665]
	TIME [epoch: 32.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003699170421056639		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.003699170421056639 | validation: 0.005474266269878493]
	TIME [epoch: 32.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003528508832190756		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.003528508832190756 | validation: 0.004450131825917745]
	TIME [epoch: 32.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037953114873462194		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0037953114873462194 | validation: 0.003755516684339769]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003710216051167055		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.003710216051167055 | validation: 0.0044047303875339]
	TIME [epoch: 32.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003524981526652136		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.003524981526652136 | validation: 0.004427024113740847]
	TIME [epoch: 32.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038753450863799202		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0038753450863799202 | validation: 0.004803845697612419]
	TIME [epoch: 32.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003821524227923394		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.003821524227923394 | validation: 0.004100966916602215]
	TIME [epoch: 32.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034258045050882266		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0034258045050882266 | validation: 0.0058381165101928974]
	TIME [epoch: 32.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040559668288209115		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.0040559668288209115 | validation: 0.007502070936207689]
	TIME [epoch: 32.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059627969695992475		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0059627969695992475 | validation: 0.00671274713065902]
	TIME [epoch: 32.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005899216979706978		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.005899216979706978 | validation: 0.008410179198210448]
	TIME [epoch: 32.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004794383760343095		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.004794383760343095 | validation: 0.004241458934603143]
	TIME [epoch: 32.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031990686745554083		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0031990686745554083 | validation: 0.004088980794910513]
	TIME [epoch: 32.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030834309769085778		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0030834309769085778 | validation: 0.0036751658473533674]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003095844663897857		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.003095844663897857 | validation: 0.00406251494815287]
	TIME [epoch: 32.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032035814486638223		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0032035814486638223 | validation: 0.0043144901175854854]
	TIME [epoch: 32.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003590801743127958		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.003590801743127958 | validation: 0.003893399859900149]
	TIME [epoch: 32.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033571688166802974		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0033571688166802974 | validation: 0.005558461121099608]
	TIME [epoch: 32.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003732387776360383		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.003732387776360383 | validation: 0.005504881963200696]
	TIME [epoch: 32.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003528861696714706		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.003528861696714706 | validation: 0.0044531398784752865]
	TIME [epoch: 32.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003744456513967323		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.003744456513967323 | validation: 0.004627840591904544]
	TIME [epoch: 32.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003333568619821058		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.003333568619821058 | validation: 0.005534260366506386]
	TIME [epoch: 32.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031874767568940126		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0031874767568940126 | validation: 0.004365711229893701]
	TIME [epoch: 32.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039970259006456635		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0039970259006456635 | validation: 0.003922085042320797]
	TIME [epoch: 32.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003169532738079135		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.003169532738079135 | validation: 0.00628514622750803]
	TIME [epoch: 32.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004319013085333534		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.004319013085333534 | validation: 0.0035982573694279233]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003074268099455977		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.003074268099455977 | validation: 0.004596863309202904]
	TIME [epoch: 32.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028885860967127162		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.0028885860967127162 | validation: 0.004956803261236382]
	TIME [epoch: 32.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032273423505068547		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0032273423505068547 | validation: 0.0055929980034336154]
	TIME [epoch: 32.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040255859823961		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0040255859823961 | validation: 0.005714970025445435]
	TIME [epoch: 32.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003849100588074332		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.003849100588074332 | validation: 0.0038538061655826895]
	TIME [epoch: 32.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004053349746942409		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.004053349746942409 | validation: 0.0039047924836088143]
	TIME [epoch: 32.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003330932604832248		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.003330932604832248 | validation: 0.003772883210010885]
	TIME [epoch: 32.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002981863441955238		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.002981863441955238 | validation: 0.00415122351036285]
	TIME [epoch: 32.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029765590420229747		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0029765590420229747 | validation: 0.0036204131046066543]
	TIME [epoch: 32.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031021574028088847		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.0031021574028088847 | validation: 0.006030709672927752]
	TIME [epoch: 32.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003762116521901896		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.003762116521901896 | validation: 0.005014479420854379]
	TIME [epoch: 32.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003276625520582554		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.003276625520582554 | validation: 0.004538633696021997]
	TIME [epoch: 32.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002875226950396813		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.002875226950396813 | validation: 0.004535423854480537]
	TIME [epoch: 32.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005242900414075721		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.005242900414075721 | validation: 0.005619553117094222]
	TIME [epoch: 32.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033384511652937354		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0033384511652937354 | validation: 0.004032830520370049]
	TIME [epoch: 32.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002785901991313456		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.002785901991313456 | validation: 0.0035977729961365135]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_874.pth
	Model improved!!!
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029067896627787363		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0029067896627787363 | validation: 0.0048040121528651475]
	TIME [epoch: 32.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028422654127614817		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0028422654127614817 | validation: 0.004038470350071929]
	TIME [epoch: 32.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033571367075335393		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0033571367075335393 | validation: 0.004659842120638366]
	TIME [epoch: 32.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030666775413415864		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0030666775413415864 | validation: 0.004241256271532883]
	TIME [epoch: 32.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00312210024434363		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.00312210024434363 | validation: 0.0032112199135027257]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003158818094827983		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.003158818094827983 | validation: 0.008304242360571017]
	TIME [epoch: 32.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037640489496222674		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.0037640489496222674 | validation: 0.0037066746995868816]
	TIME [epoch: 32.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003072099910838127		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.003072099910838127 | validation: 0.0046900051987246215]
	TIME [epoch: 32.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00325325432865013		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.00325325432865013 | validation: 0.003824069741088189]
	TIME [epoch: 32.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004480160422275963		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.004480160422275963 | validation: 0.004069908717071549]
	TIME [epoch: 32.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002933179839884622		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.002933179839884622 | validation: 0.005443789985085266]
	TIME [epoch: 32.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030259437646460987		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0030259437646460987 | validation: 0.0033255893045116584]
	TIME [epoch: 32.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002943667644357803		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.002943667644357803 | validation: 0.0055943599265347735]
	TIME [epoch: 32.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003570124944135332		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.003570124944135332 | validation: 0.0033140064441862377]
	TIME [epoch: 32.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029788236101301923		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.0029788236101301923 | validation: 0.004688155217505682]
	TIME [epoch: 32.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028426670220246934		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.0028426670220246934 | validation: 0.0049651373463509355]
	TIME [epoch: 32.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003214469392491403		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.003214469392491403 | validation: 0.004133891591185237]
	TIME [epoch: 32.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034360967029799278		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0034360967029799278 | validation: 0.0038691891410490087]
	TIME [epoch: 32.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003767550191712264		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.003767550191712264 | validation: 0.004493709637399939]
	TIME [epoch: 32.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031477845660352836		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.0031477845660352836 | validation: 0.0034737835831561894]
	TIME [epoch: 32.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027824297451385623		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0027824297451385623 | validation: 0.005893766503581688]
	TIME [epoch: 32.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003628152138719632		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.003628152138719632 | validation: 0.005709852499239062]
	TIME [epoch: 32.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004318974689843311		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.004318974689843311 | validation: 0.0034372545513486983]
	TIME [epoch: 32.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025467804515786397		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.0025467804515786397 | validation: 0.0033775591388687707]
	TIME [epoch: 32.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029423855875643744		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0029423855875643744 | validation: 0.003831202714296103]
	TIME [epoch: 32.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003201667987622928		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.003201667987622928 | validation: 0.004548837243051938]
	TIME [epoch: 32.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003537133766936861		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.003537133766936861 | validation: 0.0039810798512089985]
	TIME [epoch: 32.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031667258624208244		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0031667258624208244 | validation: 0.00353875591616145]
	TIME [epoch: 32.3 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026450108026619254		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0026450108026619254 | validation: 0.005102723299592889]
	TIME [epoch: 32.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004079046583689461		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.004079046583689461 | validation: 0.004090574116675471]
	TIME [epoch: 32.3 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003073149166423979		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.003073149166423979 | validation: 0.004324129826151968]
	TIME [epoch: 32.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003720645668852815		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.003720645668852815 | validation: 0.004792494830526997]
	TIME [epoch: 32.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034905547989746597		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0034905547989746597 | validation: 0.003230299403757175]
	TIME [epoch: 32.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028085922536347373		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.0028085922536347373 | validation: 0.004244147230455226]
	TIME [epoch: 32.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030619305379141855		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0030619305379141855 | validation: 0.0033231567265383267]
	TIME [epoch: 32.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002772263542001194		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.002772263542001194 | validation: 0.003958464839370772]
	TIME [epoch: 32.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030229279514144072		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0030229279514144072 | validation: 0.004574201746264512]
	TIME [epoch: 32.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032906066403477464		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.0032906066403477464 | validation: 0.0037918634067692844]
	TIME [epoch: 32.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030464663282219664		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0030464663282219664 | validation: 0.0032413900437239444]
	TIME [epoch: 32.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028519287299335868		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0028519287299335868 | validation: 0.004066700557572793]
	TIME [epoch: 32.3 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033276152260055406		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0033276152260055406 | validation: 0.004664907163683468]
	TIME [epoch: 32.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003956164790799605		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.003956164790799605 | validation: 0.004688568976426516]
	TIME [epoch: 32.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003262973997406248		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.003262973997406248 | validation: 0.003235813185963009]
	TIME [epoch: 32.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030562888442135106		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0030562888442135106 | validation: 0.0041960092790472876]
	TIME [epoch: 32.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024572372687617634		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0024572372687617634 | validation: 0.0031488755295196336]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032762636434411816		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0032762636434411816 | validation: 0.003471362550588018]
	TIME [epoch: 32.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002591412741019303		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.002591412741019303 | validation: 0.00356921605281403]
	TIME [epoch: 32.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028583201651576086		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0028583201651576086 | validation: 0.004229260552814904]
	TIME [epoch: 32.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002908538406960201		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.002908538406960201 | validation: 0.004044093813959296]
	TIME [epoch: 32.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033363039016762524		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0033363039016762524 | validation: 0.00412466621479393]
	TIME [epoch: 32.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00328010955420452		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.00328010955420452 | validation: 0.0034350666625136885]
	TIME [epoch: 32.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002954786300592256		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.002954786300592256 | validation: 0.003283658776358146]
	TIME [epoch: 32.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032147135518161064		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0032147135518161064 | validation: 0.0028815047157303387]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030496271721405094		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0030496271721405094 | validation: 0.003578265868909882]
	TIME [epoch: 32.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024708022414266064		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0024708022414266064 | validation: 0.004025303140735205]
	TIME [epoch: 32.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027743030739268678		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0027743030739268678 | validation: 0.0042589444503091396]
	TIME [epoch: 32.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002781790943812492		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.002781790943812492 | validation: 0.004357162304000593]
	TIME [epoch: 32.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003908354197468068		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.003908354197468068 | validation: 0.0032333779972029087]
	TIME [epoch: 32.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026634664752575097		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0026634664752575097 | validation: 0.003918848101938918]
	TIME [epoch: 32.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00299319005573495		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.00299319005573495 | validation: 0.004195432767834529]
	TIME [epoch: 32.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028404356539769126		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0028404356539769126 | validation: 0.0035505366134602596]
	TIME [epoch: 32.3 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035167127164881544		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0035167127164881544 | validation: 0.003747863096461507]
	TIME [epoch: 32.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003393768801532312		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.003393768801532312 | validation: 0.003651788940584721]
	TIME [epoch: 32.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002958422106226974		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.002958422106226974 | validation: 0.00335575178625869]
	TIME [epoch: 32.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002461969210129577		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.002461969210129577 | validation: 0.0031659737283597604]
	TIME [epoch: 32.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004278535552214586		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.004278535552214586 | validation: 0.005192780975496638]
	TIME [epoch: 32.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035909431443784754		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0035909431443784754 | validation: 0.0036618758635233867]
	TIME [epoch: 32.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027308412000358138		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0027308412000358138 | validation: 0.002773079697740031]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002523471303815158		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.002523471303815158 | validation: 0.002905404469834709]
	TIME [epoch: 32.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023711302038222117		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0023711302038222117 | validation: 0.003535658611734429]
	TIME [epoch: 32.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003395339449093555		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.003395339449093555 | validation: 0.004469325733042157]
	TIME [epoch: 32.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002830728563084972		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.002830728563084972 | validation: 0.0038885464490802527]
	TIME [epoch: 32.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028053217635327005		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.0028053217635327005 | validation: 0.0035259830319661666]
	TIME [epoch: 32.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00276858125361114		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.00276858125361114 | validation: 0.002838181459638935]
	TIME [epoch: 32.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002554694977669544		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.002554694977669544 | validation: 0.0029220786604419284]
	TIME [epoch: 32.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002518727370011748		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.002518727370011748 | validation: 0.0036968173006458214]
	TIME [epoch: 32.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024989012612648257		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0024989012612648257 | validation: 0.003788690480427828]
	TIME [epoch: 32.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003025985029408814		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.003025985029408814 | validation: 0.004170202345401452]
	TIME [epoch: 32.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023475439611672505		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0023475439611672505 | validation: 0.0028917355181694504]
	TIME [epoch: 32.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003299317037093736		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.003299317037093736 | validation: 0.004114312478347468]
	TIME [epoch: 32.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003067062747951075		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.003067062747951075 | validation: 0.0035628087645763217]
	TIME [epoch: 32.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00267382778032587		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.00267382778032587 | validation: 0.003316652376967607]
	TIME [epoch: 32.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002605359128184197		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.002605359128184197 | validation: 0.003205142033059748]
	TIME [epoch: 32.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455339492983594		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.002455339492983594 | validation: 0.003007818907832302]
	TIME [epoch: 32.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025723273281790986		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0025723273281790986 | validation: 0.002794516849763097]
	TIME [epoch: 32.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025483046992450277		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.0025483046992450277 | validation: 0.0033389076253441665]
	TIME [epoch: 32.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023365873733263536		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.0023365873733263536 | validation: 0.0028436219588852145]
	TIME [epoch: 32.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002742398944696389		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.002742398944696389 | validation: 0.0034582256251675975]
	TIME [epoch: 32.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025023517266171864		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.0025023517266171864 | validation: 0.0032104415428829292]
	TIME [epoch: 32.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003396775021310455		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.003396775021310455 | validation: 0.004476139088612574]
	TIME [epoch: 32.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022497653396527117		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0022497653396527117 | validation: 0.0032885451920471164]
	TIME [epoch: 32.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024113382338821193		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0024113382338821193 | validation: 0.003601105871793695]
	TIME [epoch: 32.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029289547751492535		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0029289547751492535 | validation: 0.0035462405991441683]
	TIME [epoch: 32.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026510119995517794		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0026510119995517794 | validation: 0.003261352318932182]
	TIME [epoch: 32.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00228263045179849		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.00228263045179849 | validation: 0.0030957042394588693]
	TIME [epoch: 32.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022915695925070224		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0022915695925070224 | validation: 0.0034252075515980914]
	TIME [epoch: 32.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002719640000191589		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.002719640000191589 | validation: 0.003933919586354987]
	TIME [epoch: 32.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003015167250366292		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.003015167250366292 | validation: 0.002932646682716179]
	TIME [epoch: 32.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025274985806222564		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0025274985806222564 | validation: 0.0034058721889840475]
	TIME [epoch: 32.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002700744694570708		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.002700744694570708 | validation: 0.0037364623718954635]
	TIME [epoch: 32.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026588416576869256		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0026588416576869256 | validation: 0.002964169119684395]
	TIME [epoch: 32.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002500334723216511		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.002500334723216511 | validation: 0.003960978488444915]
	TIME [epoch: 32.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022425561356625963		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0022425561356625963 | validation: 0.0038903288332393154]
	TIME [epoch: 32.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027756004802494756		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0027756004802494756 | validation: 0.0028795550380508123]
	TIME [epoch: 32.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029763446527871964		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0029763446527871964 | validation: 0.004533986577562414]
	TIME [epoch: 32.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003017175690069693		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.003017175690069693 | validation: 0.003021317033195153]
	TIME [epoch: 32.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024854497587683414		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0024854497587683414 | validation: 0.0025465344948712304]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_981.pth
	Model improved!!!
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025913466805713185		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0025913466805713185 | validation: 0.003430679105588086]
	TIME [epoch: 32.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021902376966713172		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0021902376966713172 | validation: 0.0026636230634448713]
	TIME [epoch: 32.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027512152146888236		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0027512152146888236 | validation: 0.0037680801625433373]
	TIME [epoch: 32.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023519197495946932		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0023519197495946932 | validation: 0.0028960819227958554]
	TIME [epoch: 32.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021971312461682947		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.0021971312461682947 | validation: 0.003490622236909018]
	TIME [epoch: 32.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025005264606486467		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.0025005264606486467 | validation: 0.003712884644285483]
	TIME [epoch: 32.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025571144782502784		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0025571144782502784 | validation: 0.003875076436669021]
	TIME [epoch: 32.3 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025386171266775443		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0025386171266775443 | validation: 0.003543273815799006]
	TIME [epoch: 32.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023451004022904294		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0023451004022904294 | validation: 0.0029145350744671852]
	TIME [epoch: 32.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020871741604498923		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0020871741604498923 | validation: 0.002682549135865071]
	TIME [epoch: 32.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002213795150475977		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.002213795150475977 | validation: 0.0031524436028026685]
	TIME [epoch: 32.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002319852103535955		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.002319852103535955 | validation: 0.003369621481404392]
	TIME [epoch: 32.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026826379306670142		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.0026826379306670142 | validation: 0.0051823195482521185]
	TIME [epoch: 32.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002576816923233967		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.002576816923233967 | validation: 0.0026947660062250243]
	TIME [epoch: 32.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023831728050869314		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0023831728050869314 | validation: 0.0037473841286441794]
	TIME [epoch: 32.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024377672320211974		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0024377672320211974 | validation: 0.003705668882725737]
	TIME [epoch: 32.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002519517383133823		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.002519517383133823 | validation: 0.0028415620548777796]
	TIME [epoch: 32.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002436505081378001		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.002436505081378001 | validation: 0.0030705075704496937]
	TIME [epoch: 32.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002601261749533948		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.002601261749533948 | validation: 0.0035488027113217222]
	TIME [epoch: 32.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002522378814444809		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.002522378814444809 | validation: 0.0036535017433166443]
	TIME [epoch: 173 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029226646409670773		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0029226646409670773 | validation: 0.0035200566979373127]
	TIME [epoch: 69 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027411611076028908		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0027411611076028908 | validation: 0.0028520687257308852]
	TIME [epoch: 69.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022393408851265027		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0022393408851265027 | validation: 0.003845976834384219]
	TIME [epoch: 69.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020952456774049983		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0020952456774049983 | validation: 0.0032912202253545146]
	TIME [epoch: 69.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022821793905068934		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0022821793905068934 | validation: 0.0050098860054055075]
	TIME [epoch: 69.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033809588061792335		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0033809588061792335 | validation: 0.002856905418572861]
	TIME [epoch: 69.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002747901304214746		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.002747901304214746 | validation: 0.0032150926692647783]
	TIME [epoch: 69.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002521779611976114		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.002521779611976114 | validation: 0.0026964776185654494]
	TIME [epoch: 69.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021828371803228807		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0021828371803228807 | validation: 0.002784616623419088]
	TIME [epoch: 69.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002440473197332744		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.002440473197332744 | validation: 0.002825045411747302]
	TIME [epoch: 69.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020773964778837504		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0020773964778837504 | validation: 0.0027335700660096112]
	TIME [epoch: 69.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021420539780218385		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0021420539780218385 | validation: 0.0032011860474726772]
	TIME [epoch: 69.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021440990642097708		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0021440990642097708 | validation: 0.002918898810895665]
	TIME [epoch: 69.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002377157540192204		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.002377157540192204 | validation: 0.004395370663996006]
	TIME [epoch: 69 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027575041303553515		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0027575041303553515 | validation: 0.0027689463495212323]
	TIME [epoch: 69.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021199706124268095		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0021199706124268095 | validation: 0.003436560971485707]
	TIME [epoch: 69.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002490934119323514		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.002490934119323514 | validation: 0.0032284549253427117]
	TIME [epoch: 69.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002482784245640975		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.002482784245640975 | validation: 0.0027154864506463774]
	TIME [epoch: 69.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002397888299609424		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.002397888299609424 | validation: 0.0036389796658860395]
	TIME [epoch: 69.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023524845552678514		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0023524845552678514 | validation: 0.0048420105454847675]
	TIME [epoch: 69.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025138868248119235		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.0025138868248119235 | validation: 0.0035016186171351153]
	TIME [epoch: 69 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021998411421912598		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0021998411421912598 | validation: 0.00287798189950439]
	TIME [epoch: 69 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020685596002750226		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0020685596002750226 | validation: 0.003116942533047962]
	TIME [epoch: 69 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002261169527957709		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.002261169527957709 | validation: 0.002773915415057206]
	TIME [epoch: 69.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023578514846739193		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.0023578514846739193 | validation: 0.00272621834770939]
	TIME [epoch: 69 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024051286583828876		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0024051286583828876 | validation: 0.0028529637977114213]
	TIME [epoch: 69.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019994503270710264		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0019994503270710264 | validation: 0.0024906663601131796]
	TIME [epoch: 69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020202435005269833		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0020202435005269833 | validation: 0.0028882991576127913]
	TIME [epoch: 69 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021403132666200078		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0021403132666200078 | validation: 0.0028083843163517193]
	TIME [epoch: 69 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022808863737445876		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0022808863737445876 | validation: 0.0026046803040664754]
	TIME [epoch: 69 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002135562547715089		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.002135562547715089 | validation: 0.0027285008898173214]
	TIME [epoch: 69 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020994136896794495		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0020994136896794495 | validation: 0.0028677407694672934]
	TIME [epoch: 69 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002453092856202088		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.002453092856202088 | validation: 0.0032202511539650776]
	TIME [epoch: 69 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025735834197781558		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0025735834197781558 | validation: 0.0037595443021498243]
	TIME [epoch: 69.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022512753928741348		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0022512753928741348 | validation: 0.0030587580638495393]
	TIME [epoch: 69 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022379040990368106		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0022379040990368106 | validation: 0.002161850496143739]
	TIME [epoch: 69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1037.pth
	Model improved!!!
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023052897293340253		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0023052897293340253 | validation: 0.0026749432559546875]
	TIME [epoch: 69.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021586997208472777		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0021586997208472777 | validation: 0.003127745417670456]
	TIME [epoch: 69.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026974918812525915		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0026974918812525915 | validation: 0.003958330517455044]
	TIME [epoch: 69.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002379712619154108		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.002379712619154108 | validation: 0.0033511294529678147]
	TIME [epoch: 69.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023472996531473814		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0023472996531473814 | validation: 0.00265187436699188]
	TIME [epoch: 69.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001918651809318235		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.001918651809318235 | validation: 0.002288308896456588]
	TIME [epoch: 69.1 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022298525829111196		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0022298525829111196 | validation: 0.004305841710470708]
	TIME [epoch: 69.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002431893481810023		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.002431893481810023 | validation: 0.002572784184371673]
	TIME [epoch: 69.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022046516800102558		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0022046516800102558 | validation: 0.0028235646548286095]
	TIME [epoch: 69.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026009183023648114		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.0026009183023648114 | validation: 0.0028490345827527908]
	TIME [epoch: 69.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021917940179791732		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0021917940179791732 | validation: 0.002920185909680223]
	TIME [epoch: 69.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002296479592591883		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.002296479592591883 | validation: 0.002813914877824999]
	TIME [epoch: 69.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002106220125275388		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.002106220125275388 | validation: 0.0031579106011098233]
	TIME [epoch: 69.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025494129405470465		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0025494129405470465 | validation: 0.0026216601937268068]
	TIME [epoch: 69.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002189698986818957		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.002189698986818957 | validation: 0.002397150715077659]
	TIME [epoch: 69.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021956850385760622		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0021956850385760622 | validation: 0.0026468809583352427]
	TIME [epoch: 69.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002064719748223575		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.002064719748223575 | validation: 0.0026551415075570387]
	TIME [epoch: 69.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023041857402399278		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0023041857402399278 | validation: 0.0028151995285389643]
	TIME [epoch: 69.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026278807369098677		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0026278807369098677 | validation: 0.002613385943441326]
	TIME [epoch: 69.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002239023412473597		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.002239023412473597 | validation: 0.0022571552273024775]
	TIME [epoch: 69.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020344184881822946		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0020344184881822946 | validation: 0.002502607187229293]
	TIME [epoch: 69.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019643320161988184		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0019643320161988184 | validation: 0.0022972907847691425]
	TIME [epoch: 69.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001892239463056851		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.001892239463056851 | validation: 0.003175453978572968]
	TIME [epoch: 69.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020684153279161466		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.0020684153279161466 | validation: 0.003757004712985631]
	TIME [epoch: 69.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002189606194816842		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.002189606194816842 | validation: 0.003194699809428294]
	TIME [epoch: 69.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024122267598670876		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0024122267598670876 | validation: 0.0031918307532561156]
	TIME [epoch: 69.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001934546719169547		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.001934546719169547 | validation: 0.0032350692350588666]
	TIME [epoch: 69.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005491504012208348		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.005491504012208348 | validation: 0.006147118968546311]
	TIME [epoch: 69.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003543713646384888		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.003543713646384888 | validation: 0.0029745233914579014]
	TIME [epoch: 69.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002110195934600318		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.002110195934600318 | validation: 0.002722665746307384]
	TIME [epoch: 69.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018762573868287824		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0018762573868287824 | validation: 0.0023439796932408514]
	TIME [epoch: 69.1 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018285202891487914		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.0018285202891487914 | validation: 0.003478132965900483]
	TIME [epoch: 69.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020653029895762133		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0020653029895762133 | validation: 0.002502006394269176]
	TIME [epoch: 69.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020418388630593846		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0020418388630593846 | validation: 0.002424618579721206]
	TIME [epoch: 69.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020750137597718606		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0020750137597718606 | validation: 0.0028616457101415856]
	TIME [epoch: 69.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019519933119922402		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0019519933119922402 | validation: 0.0030531810067855655]
	TIME [epoch: 69.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022106481576782797		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0022106481576782797 | validation: 0.002966482547406284]
	TIME [epoch: 69.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002246446793518109		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.002246446793518109 | validation: 0.002667402353788824]
	TIME [epoch: 69.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002168983267981072		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.002168983267981072 | validation: 0.0031871110786528484]
	TIME [epoch: 69.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021033565243539195		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0021033565243539195 | validation: 0.0033148912766726053]
	TIME [epoch: 69.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002009076151801492		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.002009076151801492 | validation: 0.0028117118245782457]
	TIME [epoch: 69.2 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001745090307785118		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.001745090307785118 | validation: 0.0024081459084895566]
	TIME [epoch: 69.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002006289577658774		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.002006289577658774 | validation: 0.0030639834434645617]
	TIME [epoch: 69.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002313118959063042		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.002313118959063042 | validation: 0.0026270667338677564]
	TIME [epoch: 69.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024067164662353565		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.0024067164662353565 | validation: 0.0030530834894081363]
	TIME [epoch: 69.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020041210758324417		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0020041210758324417 | validation: 0.0034552480046799866]
	TIME [epoch: 69.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025791845920201113		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0025791845920201113 | validation: 0.0024571548989314397]
	TIME [epoch: 69.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019968408972372556		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.0019968408972372556 | validation: 0.002615651102900931]
	TIME [epoch: 69.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026813833684124224		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0026813833684124224 | validation: 0.003877687710119443]
	TIME [epoch: 69.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024119542720267527		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0024119542720267527 | validation: 0.0034024916837924815]
	TIME [epoch: 69.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002179509850724945		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.002179509850724945 | validation: 0.003308840961336215]
	TIME [epoch: 69.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001775943530697164		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.001775943530697164 | validation: 0.0030516651909253]
	TIME [epoch: 69.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001766769112193844		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.001766769112193844 | validation: 0.003414835930419703]
	TIME [epoch: 69.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002003084505154559		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.002003084505154559 | validation: 0.002735578933476577]
	TIME [epoch: 69.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020097657730591854		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0020097657730591854 | validation: 0.0024424816079953266]
	TIME [epoch: 69.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023261938472447915		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0023261938472447915 | validation: 0.0021044573497909173]
	TIME [epoch: 69.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1093.pth
	Model improved!!!
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020043787232552532		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0020043787232552532 | validation: 0.003064715324287924]
	TIME [epoch: 69.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018592578025054345		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.0018592578025054345 | validation: 0.002965161001299587]
	TIME [epoch: 69.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020655964769063593		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.0020655964769063593 | validation: 0.0033861424449915228]
	TIME [epoch: 69.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001966279672735365		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.001966279672735365 | validation: 0.0026003475298232315]
	TIME [epoch: 69.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018892668208395098		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0018892668208395098 | validation: 0.00269089272399722]
	TIME [epoch: 69.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002261646557169954		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.002261646557169954 | validation: 0.0034300884853154384]
	TIME [epoch: 69.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022253309661207123		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0022253309661207123 | validation: 0.0030073299333117877]
	TIME [epoch: 69.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020557769242296508		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0020557769242296508 | validation: 0.0028147775859012347]
	TIME [epoch: 69.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020737723072759124		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.0020737723072759124 | validation: 0.002700449162308619]
	TIME [epoch: 69.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018961287196998498		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0018961287196998498 | validation: 0.003214179813560213]
	TIME [epoch: 69.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021048736210831573		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0021048736210831573 | validation: 0.0029889945924147397]
	TIME [epoch: 69.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019154313153146127		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.0019154313153146127 | validation: 0.0029816373000549623]
	TIME [epoch: 69.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002107472448186219		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.002107472448186219 | validation: 0.002608192569380609]
	TIME [epoch: 69.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001832760878098382		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.001832760878098382 | validation: 0.0025763954979156284]
	TIME [epoch: 69.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019787813238789213		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0019787813238789213 | validation: 0.002667286502313025]
	TIME [epoch: 69.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027038563929851697		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0027038563929851697 | validation: 0.0024931482323167437]
	TIME [epoch: 69.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002152689815672914		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.002152689815672914 | validation: 0.002294990613020538]
	TIME [epoch: 69.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002027000084898428		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.002027000084898428 | validation: 0.002733918783029095]
	TIME [epoch: 69.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017765683898002254		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0017765683898002254 | validation: 0.002196289290981925]
	TIME [epoch: 69.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002005990053669749		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.002005990053669749 | validation: 0.0027237210383541013]
	TIME [epoch: 69.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002019641981044102		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.002019641981044102 | validation: 0.002827990604816229]
	TIME [epoch: 69.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019299996948161386		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0019299996948161386 | validation: 0.0029119702309255337]
	TIME [epoch: 69.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019846860105609595		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0019846860105609595 | validation: 0.002768762707739154]
	TIME [epoch: 69.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00201015462419868		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.00201015462419868 | validation: 0.0023741874024279033]
	TIME [epoch: 69.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00195820956333894		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.00195820956333894 | validation: 0.002346898046577453]
	TIME [epoch: 69.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018804896023648199		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0018804896023648199 | validation: 0.003012985723653428]
	TIME [epoch: 69.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018835876655222625		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0018835876655222625 | validation: 0.003334450768910541]
	TIME [epoch: 69.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020044467753179366		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0020044467753179366 | validation: 0.0027438555583958334]
	TIME [epoch: 69.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017942007495533332		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0017942007495533332 | validation: 0.004325944568103132]
	TIME [epoch: 69.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002291609922265207		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.002291609922265207 | validation: 0.0023561186476744583]
	TIME [epoch: 69.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016169671276872097		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0016169671276872097 | validation: 0.003262800228953465]
	TIME [epoch: 69.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018654514228285013		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.0018654514228285013 | validation: 0.0027221893635890016]
	TIME [epoch: 69.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001989850496561755		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.001989850496561755 | validation: 0.0024517240380832733]
	TIME [epoch: 69.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019296570340963835		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0019296570340963835 | validation: 0.0023825998288097127]
	TIME [epoch: 69.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001945080836877354		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.001945080836877354 | validation: 0.0023361705927464867]
	TIME [epoch: 69.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018079226949872873		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0018079226949872873 | validation: 0.0019852231564355706]
	TIME [epoch: 69.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1129.pth
	Model improved!!!
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001947963108750791		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.001947963108750791 | validation: 0.0026421735588435277]
	TIME [epoch: 69.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019526224257394931		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0019526224257394931 | validation: 0.0022612674194218813]
	TIME [epoch: 69.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019332403521608805		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0019332403521608805 | validation: 0.004147823775925484]
	TIME [epoch: 69.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023247266605877057		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0023247266605877057 | validation: 0.002549754114720469]
	TIME [epoch: 69.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017407814371439034		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0017407814371439034 | validation: 0.002841000112556653]
	TIME [epoch: 69.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016757895562264035		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0016757895562264035 | validation: 0.002558306844328043]
	TIME [epoch: 69.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001914941284180121		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.001914941284180121 | validation: 0.002557189498067004]
	TIME [epoch: 69.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021524538292327453		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0021524538292327453 | validation: 0.002899925120085042]
	TIME [epoch: 69.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001826000736610487		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.001826000736610487 | validation: 0.0022774213053577378]
	TIME [epoch: 69.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001905412874750139		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.001905412874750139 | validation: 0.0030460797843191428]
	TIME [epoch: 69.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020324928590564266		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0020324928590564266 | validation: 0.0025792796596739516]
	TIME [epoch: 69.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002821005484542385		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.002821005484542385 | validation: 0.005514006372166554]
	TIME [epoch: 69.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035709478698939734		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.0035709478698939734 | validation: 0.0025438339420606393]
	TIME [epoch: 69.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019289784798388217		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0019289784798388217 | validation: 0.0025721861531583807]
	TIME [epoch: 69.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017720882828652592		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0017720882828652592 | validation: 0.002327531888015205]
	TIME [epoch: 69.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002011609844245684		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.002011609844245684 | validation: 0.002504291434729471]
	TIME [epoch: 69.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019219405682710641		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0019219405682710641 | validation: 0.002307317826440646]
	TIME [epoch: 69.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017495447097493231		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.0017495447097493231 | validation: 0.002263287406439318]
	TIME [epoch: 69.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017980981905751172		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0017980981905751172 | validation: 0.001951136957785894]
	TIME [epoch: 69.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1148.pth
	Model improved!!!
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001867935694093208		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.001867935694093208 | validation: 0.002714918287833188]
	TIME [epoch: 69.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001736435969987384		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.001736435969987384 | validation: 0.002324051187382663]
	TIME [epoch: 69.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017371770241433847		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.0017371770241433847 | validation: 0.0025059440365938386]
	TIME [epoch: 69.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001875308504290788		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.001875308504290788 | validation: 0.00250857640260107]
	TIME [epoch: 69.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017431269784740655		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0017431269784740655 | validation: 0.002274313161113659]
	TIME [epoch: 69.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018427229488873745		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0018427229488873745 | validation: 0.0022422406748500707]
	TIME [epoch: 69.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016531060535483074		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.0016531060535483074 | validation: 0.002636306552812682]
	TIME [epoch: 69.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001957348177658476		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.001957348177658476 | validation: 0.0024263471730321963]
	TIME [epoch: 69.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001891452099291726		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.001891452099291726 | validation: 0.0023476251487143953]
	TIME [epoch: 69.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019452173368897517		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0019452173368897517 | validation: 0.002372045421663527]
	TIME [epoch: 69.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016609326896751117		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0016609326896751117 | validation: 0.002318743848123712]
	TIME [epoch: 69.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002075843339359821		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.002075843339359821 | validation: 0.0026496776179951906]
	TIME [epoch: 69.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018872981792417542		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0018872981792417542 | validation: 0.002210104350913107]
	TIME [epoch: 69.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017319070071057721		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0017319070071057721 | validation: 0.0023403952423096132]
	TIME [epoch: 69.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017516836534903902		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0017516836534903902 | validation: 0.0031637104099006066]
	TIME [epoch: 69.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002110006325664983		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.002110006325664983 | validation: 0.0023089337402329886]
	TIME [epoch: 69.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016414155148948657		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0016414155148948657 | validation: 0.0033724879972799418]
	TIME [epoch: 69.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001900954343287035		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.001900954343287035 | validation: 0.002852513657133758]
	TIME [epoch: 69.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017596868199299751		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0017596868199299751 | validation: 0.0021135459300212645]
	TIME [epoch: 69.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002044854821585562		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.002044854821585562 | validation: 0.002552354882516006]
	TIME [epoch: 69.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020881628682511595		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.0020881628682511595 | validation: 0.002524740355585032]
	TIME [epoch: 69.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017974024573229848		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0017974024573229848 | validation: 0.0024272640508034814]
	TIME [epoch: 69.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016852598276716662		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.0016852598276716662 | validation: 0.0026486728737575745]
	TIME [epoch: 69.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017571072151622627		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.0017571072151622627 | validation: 0.0027732462661208056]
	TIME [epoch: 69.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018178148148730471		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0018178148148730471 | validation: 0.00253470783027762]
	TIME [epoch: 69.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017829862693016646		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0017829862693016646 | validation: 0.0026529129431479568]
	TIME [epoch: 69.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019169068181857816		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0019169068181857816 | validation: 0.003222030889166236]
	TIME [epoch: 69.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001742276064933659		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.001742276064933659 | validation: 0.002226330069184705]
	TIME [epoch: 69.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001857931148815262		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.001857931148815262 | validation: 0.0022806996998396513]
	TIME [epoch: 69.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019542393670936528		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0019542393670936528 | validation: 0.0023487017707616085]
	TIME [epoch: 69.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018225168801009775		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.0018225168801009775 | validation: 0.0025504313295220725]
	TIME [epoch: 69.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019219411751134922		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.0019219411751134922 | validation: 0.00210683277971211]
	TIME [epoch: 69.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019217891789003265		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.0019217891789003265 | validation: 0.0022754541000646967]
	TIME [epoch: 69.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001628248374836932		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.001628248374836932 | validation: 0.0019031935055692727]
	TIME [epoch: 69.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001550318032371949		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.001550318032371949 | validation: 0.002795568905880927]
	TIME [epoch: 69.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002081649025250823		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.002081649025250823 | validation: 0.002801840823814325]
	TIME [epoch: 69.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018478602002739413		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.0018478602002739413 | validation: 0.0030348409980602096]
	TIME [epoch: 69.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015479674294505183		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.0015479674294505183 | validation: 0.0025311059926093043]
	TIME [epoch: 69.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019126160610120628		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0019126160610120628 | validation: 0.002354727199658656]
	TIME [epoch: 69.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001709855581553195		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.001709855581553195 | validation: 0.002591870467451539]
	TIME [epoch: 69.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017553585219258544		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.0017553585219258544 | validation: 0.0019817914557093905]
	TIME [epoch: 69.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018081855558932322		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.0018081855558932322 | validation: 0.002244237313929015]
	TIME [epoch: 69.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001864310148728751		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.001864310148728751 | validation: 0.0021978762254937978]
	TIME [epoch: 69.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018732625097584946		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0018732625097584946 | validation: 0.0022344485108873923]
	TIME [epoch: 69.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018089885741326144		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0018089885741326144 | validation: 0.002197466568518127]
	TIME [epoch: 69.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017397450703981736		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0017397450703981736 | validation: 0.0024364649597116003]
	TIME [epoch: 69.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001674285791856587		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.001674285791856587 | validation: 0.002370483128872213]
	TIME [epoch: 69.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018933011829042289		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0018933011829042289 | validation: 0.0023527065049984523]
	TIME [epoch: 69.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001748713366626888		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.001748713366626888 | validation: 0.0024612714835820972]
	TIME [epoch: 69.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017976459044073414		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.0017976459044073414 | validation: 0.0021897356137867076]
	TIME [epoch: 69.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017477402687559574		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.0017477402687559574 | validation: 0.003092147924537286]
	TIME [epoch: 69.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002074804896736865		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.002074804896736865 | validation: 0.002535487467498526]
	TIME [epoch: 69.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017424425787832764		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.0017424425787832764 | validation: 0.0029467065499991767]
	TIME [epoch: 69.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016539565874327876		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0016539565874327876 | validation: 0.002370490116990187]
	TIME [epoch: 69.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001718157839095304		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.001718157839095304 | validation: 0.002248776464820138]
	TIME [epoch: 69.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017849181284433058		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0017849181284433058 | validation: 0.002397639639230988]
	TIME [epoch: 69.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018408589055931635		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0018408589055931635 | validation: 0.0019944991569249065]
	TIME [epoch: 69.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001669585953742969		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.001669585953742969 | validation: 0.0031969002497608443]
	TIME [epoch: 69.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017912034629393363		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.0017912034629393363 | validation: 0.0024171034706050787]
	TIME [epoch: 69.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015661779671398693		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.0015661779671398693 | validation: 0.0021421739806254045]
	TIME [epoch: 69.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017077513922251046		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.0017077513922251046 | validation: 0.0022795751151357155]
	TIME [epoch: 69.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015924387535070274		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0015924387535070274 | validation: 0.002546800334905569]
	TIME [epoch: 69.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001588270026907892		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.001588270026907892 | validation: 0.002960387048178548]
	TIME [epoch: 69.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019097406470850223		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.0019097406470850223 | validation: 0.0030603311929114875]
	TIME [epoch: 69.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001948268062881562		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.001948268062881562 | validation: 0.0025821789718228675]
	TIME [epoch: 69.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020428467205958404		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0020428467205958404 | validation: 0.002296178813768399]
	TIME [epoch: 69.4 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015762565369332833		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.0015762565369332833 | validation: 0.002200433994996246]
	TIME [epoch: 69.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016092106044798822		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.0016092106044798822 | validation: 0.002024721616321756]
	TIME [epoch: 69.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015960189435908704		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.0015960189435908704 | validation: 0.0025166873840137705]
	TIME [epoch: 69.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018625817573238858		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.0018625817573238858 | validation: 0.0023881646307446188]
	TIME [epoch: 69.4 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017382899431973142		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.0017382899431973142 | validation: 0.0030620039189823986]
	TIME [epoch: 69.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015605142560419561		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.0015605142560419561 | validation: 0.001525916397037861]
	TIME [epoch: 69.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240704_134102/states/model_phi1_1a_v_mmd1_1220.pth
	Model improved!!!
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018256064774118531		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.0018256064774118531 | validation: 0.002054223846568892]
	TIME [epoch: 69.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00162769739860697		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.00162769739860697 | validation: 0.002341412967403873]
	TIME [epoch: 69.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015446179648949793		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0015446179648949793 | validation: 0.002365760076897459]
	TIME [epoch: 69.2 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017553596056042002		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.0017553596056042002 | validation: 0.002159224309597012]
	TIME [epoch: 69.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001598988442618785		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.001598988442618785 | validation: 0.0021969956320715242]
	TIME [epoch: 69.2 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015757268686265365		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0015757268686265365 | validation: 0.002754646257023776]
	TIME [epoch: 69.2 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017238360340363702		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.0017238360340363702 | validation: 0.002559295624761826]
	TIME [epoch: 69.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016533602608234084		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0016533602608234084 | validation: 0.002098466493408361]
	TIME [epoch: 69.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015530582542592343		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.0015530582542592343 | validation: 0.0024630784533417847]
	TIME [epoch: 69.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018577595592357652		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.0018577595592357652 | validation: 0.0017497087555187285]
	TIME [epoch: 69.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016139404130125943		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.0016139404130125943 | validation: 0.0027980537004863642]
	TIME [epoch: 69.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019142758434581425		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.0019142758434581425 | validation: 0.0025064685912832047]
	TIME [epoch: 69.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020780322277700733		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0020780322277700733 | validation: 0.002017941427063124]
	TIME [epoch: 69.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015158217263878321		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.0015158217263878321 | validation: 0.001915230939840213]
	TIME [epoch: 69.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001526469365632315		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.001526469365632315 | validation: 0.0016393229434646058]
	TIME [epoch: 69.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015376198592948738		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.0015376198592948738 | validation: 0.0018603178806216665]
	TIME [epoch: 69.2 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016176459922410925		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.0016176459922410925 | validation: 0.002726651901076906]
	TIME [epoch: 69.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017196126227021634		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.0017196126227021634 | validation: 0.0021514383652651093]
	TIME [epoch: 69.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017036677275364415		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.0017036677275364415 | validation: 0.002693453383683652]
	TIME [epoch: 69.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015662964261475639		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0015662964261475639 | validation: 0.0018577030148764972]
	TIME [epoch: 69.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015525224557357445		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.0015525224557357445 | validation: 0.0020152342663621293]
	TIME [epoch: 69.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015681788872950906		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0015681788872950906 | validation: 0.0018355767858548714]
	TIME [epoch: 69.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014411380177519082		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.0014411380177519082 | validation: 0.00240061698561606]
	TIME [epoch: 69.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001622165789294332		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.001622165789294332 | validation: 0.0023209529228064756]
	TIME [epoch: 69.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016837542348114637		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.0016837542348114637 | validation: 0.001882571615785987]
	TIME [epoch: 69.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017611613740974683		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0017611613740974683 | validation: 0.002760797794499492]
	TIME [epoch: 69.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019126398074520426		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.0019126398074520426 | validation: 0.002455334936136641]
	TIME [epoch: 69.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001421772220632733		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.001421772220632733 | validation: 0.002377788832616723]
	TIME [epoch: 69.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001980397538993803		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.001980397538993803 | validation: 0.002490114522963243]
	TIME [epoch: 69.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015424375721257164		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.0015424375721257164 | validation: 0.002143854286885189]
	TIME [epoch: 69.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014427962624539335		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.0014427962624539335 | validation: 0.002042872518815809]
	TIME [epoch: 69.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017529499941725175		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.0017529499941725175 | validation: 0.0021269475665502283]
	TIME [epoch: 69.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016316911562901087		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0016316911562901087 | validation: 0.0023673562619569866]
	TIME [epoch: 69.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016269766450984287		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.0016269766450984287 | validation: 0.0021435998355067857]
	TIME [epoch: 69 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017945466042039212		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.0017945466042039212 | validation: 0.0022466033712480204]
	TIME [epoch: 69.1 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017691813585114372		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.0017691813585114372 | validation: 0.0021436074721001893]
	TIME [epoch: 69 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016428210650062582		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.0016428210650062582 | validation: 0.0018078547695275509]
	TIME [epoch: 69.1 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002108321491183886		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.002108321491183886 | validation: 0.002391718160478847]
	TIME [epoch: 69 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001587029037161629		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.001587029037161629 | validation: 0.0020722183600318297]
	TIME [epoch: 69.1 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015986776743296613		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0015986776743296613 | validation: 0.002100662881488977]
	TIME [epoch: 69.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001610110399951643		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.001610110399951643 | validation: 0.0023747875096783817]
	TIME [epoch: 69.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017203299223624172		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.0017203299223624172 | validation: 0.002254261741227454]
	TIME [epoch: 69.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015497131250406361		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.0015497131250406361 | validation: 0.0018888128176926316]
	TIME [epoch: 69.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015935645704961213		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.0015935645704961213 | validation: 0.002096000595059478]
	TIME [epoch: 69.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016787087734522986		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0016787087734522986 | validation: 0.0021041083974358485]
	TIME [epoch: 69.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017181909162815134		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.0017181909162815134 | validation: 0.0018899702403626373]
	TIME [epoch: 69.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014529344393748793		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.0014529344393748793 | validation: 0.002158447757565253]
	TIME [epoch: 69.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016942124766149928		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.0016942124766149928 | validation: 0.00197699764598201]
	TIME [epoch: 69.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002022682354291491		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.002022682354291491 | validation: 0.0017436387726890005]
	TIME [epoch: 69.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017170425450982428		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.0017170425450982428 | validation: 0.0018758806935673827]
	TIME [epoch: 69.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015174014333639743		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.0015174014333639743 | validation: 0.001670828637209783]
	TIME [epoch: 69.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015087717920054947		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.0015087717920054947 | validation: 0.0029976607027174234]
	TIME [epoch: 69.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016696079077015572		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.0016696079077015572 | validation: 0.001800390255424122]
	TIME [epoch: 69.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001338704847664385		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.001338704847664385 | validation: 0.002233652974298923]
	TIME [epoch: 69.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014002897512882702		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.0014002897512882702 | validation: 0.0017229634177001359]
	TIME [epoch: 69.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012904154565475659		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.0012904154565475659 | validation: 0.0029342212214412055]
	TIME [epoch: 69.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021274545532889906		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.0021274545532889906 | validation: 0.002219568993625691]
	TIME [epoch: 69.4 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015232748509277775		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.0015232748509277775 | validation: 0.002056235505700489]
	TIME [epoch: 69.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014998536115427312		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.0014998536115427312 | validation: 0.001860047442156505]
	TIME [epoch: 69.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014981399643683732		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0014981399643683732 | validation: 0.002161253304932356]
	TIME [epoch: 69.4 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016010212833491985		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.0016010212833491985 | validation: 0.00220769180458135]
	TIME [epoch: 69.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016552739626400285		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.0016552739626400285 | validation: 0.002104795950244209]
	TIME [epoch: 69.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015261562968385113		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.0015261562968385113 | validation: 0.0019098428810083589]
	TIME [epoch: 69.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014137157881882808		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.0014137157881882808 | validation: 0.0018047194774940697]
	TIME [epoch: 69.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016310972641665963		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.0016310972641665963 | validation: 0.00254829804826877]
	TIME [epoch: 69.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019042845494869466		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.0019042845494869466 | validation: 0.0023868446103630053]
	TIME [epoch: 69.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016105499495896718		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.0016105499495896718 | validation: 0.0020820653212661197]
	TIME [epoch: 69.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015117109430606967		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.0015117109430606967 | validation: 0.0019824518114047035]
	TIME [epoch: 69.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016402756931535982		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.0016402756931535982 | validation: 0.0019461532233041154]
	TIME [epoch: 69.4 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001412346523960065		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.001412346523960065 | validation: 0.0018301085973937417]
	TIME [epoch: 69.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017221087987953337		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.0017221087987953337 | validation: 0.0017215461444096318]
	TIME [epoch: 69.4 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016712115194878268		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.0016712115194878268 | validation: 0.0020355225480276477]
	TIME [epoch: 69.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014535578968141614		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.0014535578968141614 | validation: 0.001833967390438194]
	TIME [epoch: 69.4 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014497255433909618		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.0014497255433909618 | validation: 0.0020362228938459676]
	TIME [epoch: 69.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014403598923609954		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.0014403598923609954 | validation: 0.0019365794142064468]
	TIME [epoch: 69.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015724292933693234		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.0015724292933693234 | validation: 0.001723077910278824]
	TIME [epoch: 69.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013682859559804167		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.0013682859559804167 | validation: 0.002226914609821213]
	TIME [epoch: 69.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015111448687859803		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.0015111448687859803 | validation: 0.0019125573418382986]
	TIME [epoch: 69.3 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019404040135628969		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.0019404040135628969 | validation: 0.00192514831330182]
	TIME [epoch: 69.4 sec]
EPOCH 1300/2000:
	Training over batches...
