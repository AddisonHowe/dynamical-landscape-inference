Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2223949938

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.370099515373994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.370099515373994 | validation: 5.170707724361477]
	TIME [epoch: 183 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.70965112630938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.70965112630938 | validation: 4.502767692478931]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.610361498338824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.610361498338824 | validation: 3.994046458204835]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.277984314464577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.277984314464577 | validation: 3.916374664332233]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.200178664026224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.200178664026224 | validation: 4.031714344672653]
	TIME [epoch: 1.4 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.154107961551611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.154107961551611 | validation: 3.949065896794882]
	TIME [epoch: 1.4 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.324303962191333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.324303962191333 | validation: 3.757074575920576]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.965482701237493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.965482701237493 | validation: 3.840428455203445]
	TIME [epoch: 1.4 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9932499920398183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9932499920398183 | validation: 3.6801619083752577]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9945743632930353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9945743632930353 | validation: 3.691535310088742]
	TIME [epoch: 1.4 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8955375093843623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8955375093843623 | validation: 3.660576784039174]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8680670073700836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8680670073700836 | validation: 3.594009803685917]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8342969384787002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8342969384787002 | validation: 3.5958088772223764]
	TIME [epoch: 1.4 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8080973474600843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8080973474600843 | validation: 3.545231114503899]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7923612678170286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7923612678170286 | validation: 3.587706750354744]
	TIME [epoch: 1.39 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.790345569103136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.790345569103136 | validation: 3.532316935575341]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8235020093386836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8235020093386836 | validation: 3.600226433685114]
	TIME [epoch: 1.39 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.787296962252615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.787296962252615 | validation: 3.4887508556413573]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.782900632944248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.782900632944248 | validation: 3.5085234186113414]
	TIME [epoch: 1.4 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7032310401206874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7032310401206874 | validation: 3.415412559138348]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6698602483426566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6698602483426566 | validation: 3.4382676377889396]
	TIME [epoch: 1.4 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.648115346361142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.648115346361142 | validation: 3.38463018387791]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.650735253367524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.650735253367524 | validation: 3.441946490679584]
	TIME [epoch: 1.39 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6569935347917624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6569935347917624 | validation: 3.3792769273035193]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.666530581164858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.666530581164858 | validation: 3.382685810734351]
	TIME [epoch: 1.39 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5967285933422297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5967285933422297 | validation: 3.309604112428106]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5743703712994854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5743703712994854 | validation: 3.349474172279181]
	TIME [epoch: 1.39 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.563249777583593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.563249777583593 | validation: 3.3142317389806055]
	TIME [epoch: 1.39 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.59131599978625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.59131599978625 | validation: 3.3223852875227693]
	TIME [epoch: 1.39 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5437344796328647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5437344796328647 | validation: 3.251234289852192]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5147556830176714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5147556830176714 | validation: 3.249011714597975]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.477534816793443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.477534816793443 | validation: 3.2059740490736512]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.459775526056686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.459775526056686 | validation: 3.221179204919965]
	TIME [epoch: 1.39 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4513706507530233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4513706507530233 | validation: 3.195482049459056]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.455326391171965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.455326391171965 | validation: 3.2278785368757292]
	TIME [epoch: 1.39 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4634669378578935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4634669378578935 | validation: 3.21772849727285]
	TIME [epoch: 1.39 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4754696979726885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4754696979726885 | validation: 3.1649187651019015]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.401054361865795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.401054361865795 | validation: 3.1228363901172793]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3656356451393727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3656356451393727 | validation: 3.1125644431172654]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3494448511141948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3494448511141948 | validation: 3.1036865596511003]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3380236703209825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3380236703209825 | validation: 3.0879854861777054]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3192706216821444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3192706216821444 | validation: 3.073848711144331]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.300150278083819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.300150278083819 | validation: 3.0581881126354045]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.278620292987113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.278620292987113 | validation: 3.0541095972568986]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.251116485785602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.251116485785602 | validation: 3.0339112901039145]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2321097684079265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2321097684079265 | validation: 3.0205169090615644]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2068746341285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2068746341285 | validation: 2.9949133100683114]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1913891000700825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1913891000700825 | validation: 2.9820670298308545]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.187457721708436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.187457721708436 | validation: 3.071795224847051]
	TIME [epoch: 1.39 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2710117689034384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2710117689034384 | validation: 3.094930966986389]
	TIME [epoch: 1.39 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.389773469053574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.389773469053574 | validation: 2.9497665462662748]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1562627028222265		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.1562627028222265 | validation: 3.0175476581157]
	TIME [epoch: 1.39 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2192761385428157		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.2192761385428157 | validation: 2.9146083072643147]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1707003912356186		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.1707003912356186 | validation: 2.88582417368785]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0819822070402485		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.0819822070402485 | validation: 2.8837035838982628]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.079376160591918		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.079376160591918 | validation: 2.832788549619072]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0525757952692465		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.0525757952692465 | validation: 2.8357064307143247]
	TIME [epoch: 1.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.023729363470426		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.023729363470426 | validation: 2.8132046185728785]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0102753568776324		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0102753568776324 | validation: 2.8079706730491187]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.998344130535212		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.998344130535212 | validation: 2.789768003029274]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9815236620338887		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.9815236620338887 | validation: 2.7774184515762403]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.966413484207684		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.966413484207684 | validation: 2.767430750637739]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9414918398587924		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.9414918398587924 | validation: 2.7465190890593]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8906610062702067		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8906610062702067 | validation: 2.7134165003134774]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.757014187927911		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.757014187927911 | validation: 2.6422650210182184]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4570104417525163		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.4570104417525163 | validation: 2.6989307180250264]
	TIME [epoch: 1.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2211975543517686		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.2211975543517686 | validation: 2.2125838277128147]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0036097423049055		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.0036097423049055 | validation: 1.6487953017466788]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3829359913196921		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3829359913196921 | validation: 1.621049996358046]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2657828830778415		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.2657828830778415 | validation: 1.602782963398691]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5656610181603015		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.5656610181603015 | validation: 2.109336062723027]
	TIME [epoch: 1.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9455340307598534		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.9455340307598534 | validation: 1.3153613630228227]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0127330214851586		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.0127330214851586 | validation: 1.2456191128563852]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1113614881977842		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.1113614881977842 | validation: 1.1965308548333269]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9623169435756788		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9623169435756788 | validation: 1.1500812555804873]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191959793347837		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9191959793347837 | validation: 0.9871766377455189]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873432016202371		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.873432016202371 | validation: 0.9642715020229766]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.875132577783332		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.875132577783332 | validation: 1.11059710458682]
	TIME [epoch: 1.39 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608078461139325		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.8608078461139325 | validation: 0.9619652136834032]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364356879590286		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8364356879590286 | validation: 0.9714654072629548]
	TIME [epoch: 1.4 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276658352381481		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8276658352381481 | validation: 1.037254709892399]
	TIME [epoch: 1.4 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281708712506728		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.8281708712506728 | validation: 0.9627781443258439]
	TIME [epoch: 1.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822967603686287		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.822967603686287 | validation: 0.9847180807135948]
	TIME [epoch: 1.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132794468201717		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8132794468201717 | validation: 0.9271754835785391]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8118835266060623		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8118835266060623 | validation: 1.038860100438777]
	TIME [epoch: 1.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105536961211331		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8105536961211331 | validation: 0.9263812316552937]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052589393406566		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8052589393406566 | validation: 0.9700819949707498]
	TIME [epoch: 1.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989049970286982		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7989049970286982 | validation: 0.9943268212702835]
	TIME [epoch: 1.39 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794414348313729		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.794414348313729 | validation: 0.8683476094446432]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108225429622372		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8108225429622372 | validation: 1.1382273510961498]
	TIME [epoch: 1.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306194154601519		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8306194154601519 | validation: 0.9190497801011408]
	TIME [epoch: 1.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834007768400787		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8834007768400787 | validation: 1.2022120785843828]
	TIME [epoch: 1.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438610735767869		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8438610735767869 | validation: 0.9231279148411139]
	TIME [epoch: 1.39 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804678927933264		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.7804678927933264 | validation: 0.9141037420382347]
	TIME [epoch: 1.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843723101080181		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7843723101080181 | validation: 0.9880505608074078]
	TIME [epoch: 1.39 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790091798301318		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.7790091798301318 | validation: 0.8969423098164381]
	TIME [epoch: 1.39 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658321843880732		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.7658321843880732 | validation: 0.9087516130155315]
	TIME [epoch: 1.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807241187238152		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.7807241187238152 | validation: 1.0171556180064876]
	TIME [epoch: 1.4 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823625337186582		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.823625337186582 | validation: 0.9745003843766948]
	TIME [epoch: 1.4 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9125297364904688		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9125297364904688 | validation: 1.0717222446217487]
	TIME [epoch: 1.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711469165271203		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8711469165271203 | validation: 0.9370837896231934]
	TIME [epoch: 1.39 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719953591107487		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7719953591107487 | validation: 0.8629466445683033]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158627025831038		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8158627025831038 | validation: 1.0490845710939747]
	TIME [epoch: 1.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195339084620981		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8195339084620981 | validation: 0.8832606350102854]
	TIME [epoch: 1.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746811932481997		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7746811932481997 | validation: 0.9079339791428105]
	TIME [epoch: 1.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874124621859298		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7874124621859298 | validation: 0.9202829639073017]
	TIME [epoch: 1.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960032629733662		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7960032629733662 | validation: 0.9897302336582071]
	TIME [epoch: 1.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941707274023068		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.7941707274023068 | validation: 0.9045515898885352]
	TIME [epoch: 1.4 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384615492727423		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8384615492727423 | validation: 1.1235591136964775]
	TIME [epoch: 1.4 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8191146263834819		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8191146263834819 | validation: 0.8941896817383491]
	TIME [epoch: 1.4 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573685004572814		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7573685004572814 | validation: 0.872230656460007]
	TIME [epoch: 1.4 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765517114445708		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.765517114445708 | validation: 0.9320183916626098]
	TIME [epoch: 1.4 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576523033232904		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7576523033232904 | validation: 0.8727709826677427]
	TIME [epoch: 1.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673329210897744		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7673329210897744 | validation: 0.9101730571505773]
	TIME [epoch: 1.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598690919498244		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7598690919498244 | validation: 0.8859617021151739]
	TIME [epoch: 1.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7647649274447181		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7647649274447181 | validation: 0.8909029541265483]
	TIME [epoch: 1.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572772520328337		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7572772520328337 | validation: 0.9158630318836333]
	TIME [epoch: 1.4 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770857779482837		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.770857779482837 | validation: 0.9322609558617048]
	TIME [epoch: 1.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165406383280711		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8165406383280711 | validation: 1.064721566872353]
	TIME [epoch: 1.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723169118127789		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8723169118127789 | validation: 0.9426006636288338]
	TIME [epoch: 1.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260630810100184		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8260630810100184 | validation: 0.9506082551784711]
	TIME [epoch: 1.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596627832230283		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7596627832230283 | validation: 0.8936294501354681]
	TIME [epoch: 1.4 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645140961472043		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7645140961472043 | validation: 0.9117500902023935]
	TIME [epoch: 1.4 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734820607625856		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.7734820607625856 | validation: 0.8820627464311834]
	TIME [epoch: 1.39 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773019771532994		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.773019771532994 | validation: 0.9011823200650142]
	TIME [epoch: 1.39 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566788581480088		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7566788581480088 | validation: 0.9173393666863338]
	TIME [epoch: 1.39 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509680768912915		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7509680768912915 | validation: 0.8818857762173513]
	TIME [epoch: 1.39 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490809509454345		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7490809509454345 | validation: 0.9326692520809895]
	TIME [epoch: 1.39 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748904690624874		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.748904690624874 | validation: 0.850649053496948]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615242131700108		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7615242131700108 | validation: 1.1067676381100686]
	TIME [epoch: 1.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055653622556909		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8055653622556909 | validation: 0.9380949225691261]
	TIME [epoch: 1.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9140386948988166		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9140386948988166 | validation: 1.1068976153114627]
	TIME [epoch: 1.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132656682388216		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8132656682388216 | validation: 1.0375413115363894]
	TIME [epoch: 1.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898118481652988		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7898118481652988 | validation: 0.908954614757271]
	TIME [epoch: 1.4 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289201154115889		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8289201154115889 | validation: 0.8856419030457459]
	TIME [epoch: 1.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439828663664217		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7439828663664217 | validation: 1.0165211549275888]
	TIME [epoch: 1.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832411496307989		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7832411496307989 | validation: 0.8462053516253785]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722879386301861		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7722879386301861 | validation: 0.8666456760871348]
	TIME [epoch: 1.39 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419962595186097		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7419962595186097 | validation: 0.9301912651685242]
	TIME [epoch: 1.39 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480512652111782		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.7480512652111782 | validation: 0.8259898250941202]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577420311727581		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7577420311727581 | validation: 0.9242958364049015]
	TIME [epoch: 1.39 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722385585705348		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7722385585705348 | validation: 0.9349408437882684]
	TIME [epoch: 1.39 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914493005871859		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.7914493005871859 | validation: 0.9364104742418592]
	TIME [epoch: 1.39 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987349511620323		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7987349511620323 | validation: 0.9045526520417173]
	TIME [epoch: 1.39 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530912204970203		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7530912204970203 | validation: 0.8700403431814158]
	TIME [epoch: 1.39 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392480831659813		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7392480831659813 | validation: 0.8492580081247798]
	TIME [epoch: 1.39 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338684370122378		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7338684370122378 | validation: 0.9304839576862638]
	TIME [epoch: 1.39 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740285842225251		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.740285842225251 | validation: 0.8410377944937004]
	TIME [epoch: 1.39 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524032952643493		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7524032952643493 | validation: 0.940958930139477]
	TIME [epoch: 1.39 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410271399334485		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7410271399334485 | validation: 0.8482207332931527]
	TIME [epoch: 1.39 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350323969649005		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7350323969649005 | validation: 0.875402939488576]
	TIME [epoch: 1.39 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321798724760188		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7321798724760188 | validation: 0.8899194432114884]
	TIME [epoch: 1.39 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590327860978701		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7590327860978701 | validation: 0.9638938778026841]
	TIME [epoch: 1.39 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761047588773433		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8761047588773433 | validation: 0.9959032439361195]
	TIME [epoch: 1.39 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310145523606559		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8310145523606559 | validation: 0.9206409173730841]
	TIME [epoch: 1.39 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291639837240596		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7291639837240596 | validation: 0.8433947916909574]
	TIME [epoch: 1.39 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747740257072166		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.747740257072166 | validation: 0.9143304683826253]
	TIME [epoch: 1.39 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7462402751085563		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7462402751085563 | validation: 0.8868672215352117]
	TIME [epoch: 1.39 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316541382057667		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7316541382057667 | validation: 0.8298146386812593]
	TIME [epoch: 1.39 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336334344046559		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7336334344046559 | validation: 1.0078020414727646]
	TIME [epoch: 1.39 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486555324747317		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7486555324747317 | validation: 0.8270631576915453]
	TIME [epoch: 1.39 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522397377084314		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7522397377084314 | validation: 0.9736416824915036]
	TIME [epoch: 1.39 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431922515895826		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7431922515895826 | validation: 0.8359941914505038]
	TIME [epoch: 1.39 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423421603169241		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7423421603169241 | validation: 0.8857862619145739]
	TIME [epoch: 1.39 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.723542890559487		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.723542890559487 | validation: 0.9287421582343381]
	TIME [epoch: 1.39 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303192260209346		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7303192260209346 | validation: 0.8322486987334803]
	TIME [epoch: 1.39 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773172240848232		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.773172240848232 | validation: 0.9942936806740965]
	TIME [epoch: 1.39 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769148389250762		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7769148389250762 | validation: 0.8855691162802803]
	TIME [epoch: 1.39 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382355949183317		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.7382355949183317 | validation: 0.8512866064159408]
	TIME [epoch: 1.39 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768091455713646		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7768091455713646 | validation: 1.0354026881784026]
	TIME [epoch: 1.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765010366664452		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.765010366664452 | validation: 0.814810084997017]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267689885553966		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7267689885553966 | validation: 0.8380289121931451]
	TIME [epoch: 1.39 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145138702359886		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7145138702359886 | validation: 0.8733917819636562]
	TIME [epoch: 1.39 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107655898421913		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7107655898421913 | validation: 0.803151134132367]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120030208152422		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7120030208152422 | validation: 0.9457035309293063]
	TIME [epoch: 1.39 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156878202964714		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7156878202964714 | validation: 0.8014225775541073]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246243227252501		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7246243227252501 | validation: 0.9129752282515025]
	TIME [epoch: 1.39 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728152735225838		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.728152735225838 | validation: 0.8858351123101305]
	TIME [epoch: 1.39 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485153355804309		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7485153355804309 | validation: 0.9639667070180526]
	TIME [epoch: 1.39 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541028592240236		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8541028592240236 | validation: 0.9734047730263802]
	TIME [epoch: 1.39 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801160204331896		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.801160204331896 | validation: 0.9251460449539194]
	TIME [epoch: 1.39 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170654616442331		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7170654616442331 | validation: 0.8443109941415626]
	TIME [epoch: 1.39 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722002675693461		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.722002675693461 | validation: 0.8912822074052038]
	TIME [epoch: 1.39 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149862936188722		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7149862936188722 | validation: 0.9102713451919908]
	TIME [epoch: 1.39 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061648486749136		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7061648486749136 | validation: 0.7959713103024593]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201824026705379		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7201824026705379 | validation: 0.9164910023357493]
	TIME [epoch: 1.39 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078769244323667		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7078769244323667 | validation: 0.802594818902845]
	TIME [epoch: 1.39 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959665816174117		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6959665816174117 | validation: 0.8015504235439329]
	TIME [epoch: 1.39 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927916751019342		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6927916751019342 | validation: 0.8400444877284831]
	TIME [epoch: 1.39 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908651212086493		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6908651212086493 | validation: 0.7822175101780817]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.702374911796951		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.702374911796951 | validation: 0.9261259526975455]
	TIME [epoch: 1.39 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075608463660722		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7075608463660722 | validation: 0.8012298062338297]
	TIME [epoch: 1.39 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782171382973762		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7782171382973762 | validation: 1.2568463324870367]
	TIME [epoch: 1.39 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261200288162082		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.8261200288162082 | validation: 0.8308368401697325]
	TIME [epoch: 1.39 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712148483913081		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.712148483913081 | validation: 0.794724592675766]
	TIME [epoch: 1.39 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317656675920808		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7317656675920808 | validation: 0.9906051252522519]
	TIME [epoch: 1.39 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703869481762187		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7703869481762187 | validation: 0.9008941571415918]
	TIME [epoch: 1.39 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339821106604856		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7339821106604856 | validation: 0.8091797458852819]
	TIME [epoch: 1.39 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716282318934306		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.716282318934306 | validation: 0.8528494691644646]
	TIME [epoch: 1.39 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855400734728353		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6855400734728353 | validation: 0.8347353596817624]
	TIME [epoch: 1.39 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690068987612838		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.690068987612838 | validation: 0.8182773994918655]
	TIME [epoch: 189 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898889020678981		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6898889020678981 | validation: 0.8218314422139605]
	TIME [epoch: 2.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859807339054261		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6859807339054261 | validation: 0.8390097477578626]
	TIME [epoch: 2.77 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073438219704207		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7073438219704207 | validation: 0.9122912173351048]
	TIME [epoch: 2.88 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7381654286151508		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7381654286151508 | validation: 0.9413600849269401]
	TIME [epoch: 2.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703340483021498		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7703340483021498 | validation: 0.9048090183942862]
	TIME [epoch: 2.76 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719374992269831		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.719374992269831 | validation: 0.8571222699554796]
	TIME [epoch: 2.76 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869646532823128		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6869646532823128 | validation: 0.8418553802608113]
	TIME [epoch: 2.77 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763966039412114		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6763966039412114 | validation: 0.8153801015953784]
	TIME [epoch: 2.77 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743112084965117		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6743112084965117 | validation: 0.8729959271430829]
	TIME [epoch: 2.77 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873205360263165		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6873205360263165 | validation: 0.8431724596021832]
	TIME [epoch: 2.77 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967962404758644		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6967962404758644 | validation: 0.8484390067321421]
	TIME [epoch: 2.77 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016366160414178		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7016366160414178 | validation: 0.8981757245484078]
	TIME [epoch: 2.77 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267322152641308		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7267322152641308 | validation: 0.8748170733156457]
	TIME [epoch: 2.77 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600893968796624		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7600893968796624 | validation: 0.9091116646988571]
	TIME [epoch: 2.77 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188730584472481		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7188730584472481 | validation: 0.8486341285282653]
	TIME [epoch: 2.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795460735845364		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6795460735845364 | validation: 0.8508664092574296]
	TIME [epoch: 2.77 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615585216847666		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6615585216847666 | validation: 0.830537228395672]
	TIME [epoch: 2.77 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707534451954129		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6707534451954129 | validation: 0.8113780103904077]
	TIME [epoch: 2.77 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6696663969553827		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6696663969553827 | validation: 0.7689798040785126]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785880755227092		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6785880755227092 | validation: 0.9144735521143379]
	TIME [epoch: 2.77 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159109102095306		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7159109102095306 | validation: 0.8016402171142971]
	TIME [epoch: 2.77 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722373552769742		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.722373552769742 | validation: 0.8619383516947118]
	TIME [epoch: 2.76 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077529211228877		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7077529211228877 | validation: 1.0407357797799912]
	TIME [epoch: 2.76 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111662014974897		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7111662014974897 | validation: 0.7780030842630993]
	TIME [epoch: 2.76 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945058234389967		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6945058234389967 | validation: 0.8786658122705774]
	TIME [epoch: 2.77 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675978686739132		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6675978686739132 | validation: 0.855267835854599]
	TIME [epoch: 2.76 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602089970039208		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6602089970039208 | validation: 0.8313411221415432]
	TIME [epoch: 2.76 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689579463748527		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6689579463748527 | validation: 0.8269001359775301]
	TIME [epoch: 2.77 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723088606160218		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6723088606160218 | validation: 0.8357524291699072]
	TIME [epoch: 2.77 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864730177031457		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6864730177031457 | validation: 0.8273996362203443]
	TIME [epoch: 2.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155192028061429		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7155192028061429 | validation: 0.9410578512261639]
	TIME [epoch: 2.76 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284041184866956		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7284041184866956 | validation: 0.9411115895396563]
	TIME [epoch: 2.77 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779114364821791		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6779114364821791 | validation: 0.7892647401721327]
	TIME [epoch: 2.77 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700483751265438		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6700483751265438 | validation: 0.9013929091527416]
	TIME [epoch: 2.77 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664606234051116		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.664606234051116 | validation: 0.7965750929529014]
	TIME [epoch: 2.77 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6523395472339629		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6523395472339629 | validation: 0.7644320386139762]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504481364710754		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6504481364710754 | validation: 0.8468389901709404]
	TIME [epoch: 2.77 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561412726162859		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6561412726162859 | validation: 0.7552035937319241]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977390796412855		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6977390796412855 | validation: 1.1123591194047735]
	TIME [epoch: 2.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7387480407823358		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7387480407823358 | validation: 0.8548084962776623]
	TIME [epoch: 2.77 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659090807172399		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.659090807172399 | validation: 0.7756224079170404]
	TIME [epoch: 2.76 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744873004454942		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6744873004454942 | validation: 0.9736554711112015]
	TIME [epoch: 2.77 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757252123265778		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6757252123265778 | validation: 0.8040842790990962]
	TIME [epoch: 2.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551806797660576		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6551806797660576 | validation: 0.8256499637574477]
	TIME [epoch: 2.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582089700267324		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6582089700267324 | validation: 0.851025791582785]
	TIME [epoch: 2.76 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601835969591877		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6601835969591877 | validation: 0.8342343063465938]
	TIME [epoch: 2.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6645940283108149		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6645940283108149 | validation: 0.8723467015076607]
	TIME [epoch: 2.77 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628958858068483		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6628958858068483 | validation: 0.8282894498792137]
	TIME [epoch: 2.77 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656894406439046		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6656894406439046 | validation: 0.8404904216596862]
	TIME [epoch: 2.77 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679649900005984		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6679649900005984 | validation: 0.8363323464188159]
	TIME [epoch: 2.77 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658165207312439		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6658165207312439 | validation: 0.8107610424409813]
	TIME [epoch: 2.77 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541290372118919		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6541290372118919 | validation: 0.8577993024089619]
	TIME [epoch: 2.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419553016182639		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6419553016182639 | validation: 0.7589790808082459]
	TIME [epoch: 2.77 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434424169751927		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6434424169751927 | validation: 0.863894023765233]
	TIME [epoch: 2.77 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424021297598771		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6424021297598771 | validation: 0.7516572454197472]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397487420654722		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6397487420654722 | validation: 0.8297370598490897]
	TIME [epoch: 2.77 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330778429473578		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6330778429473578 | validation: 0.7717886242970278]
	TIME [epoch: 2.77 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232169823877404		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6232169823877404 | validation: 0.7596779187100307]
	TIME [epoch: 2.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293846022564715		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6293846022564715 | validation: 0.9315249842616091]
	TIME [epoch: 2.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672559259469799		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6672559259469799 | validation: 0.8673587570674814]
	TIME [epoch: 2.76 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794123973154798		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7794123973154798 | validation: 0.9082909435120681]
	TIME [epoch: 2.76 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840510400012351		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6840510400012351 | validation: 0.9597851570569482]
	TIME [epoch: 2.76 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451530223934351		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6451530223934351 | validation: 0.8065219028713022]
	TIME [epoch: 2.76 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630806399225377		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.630806399225377 | validation: 0.825192005224052]
	TIME [epoch: 2.76 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258005518655813		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6258005518655813 | validation: 0.8968656585966134]
	TIME [epoch: 2.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341066785619084		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6341066785619084 | validation: 0.746609421859167]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637470517725116		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.637470517725116 | validation: 0.8670169806619699]
	TIME [epoch: 2.76 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63810063228265		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.63810063228265 | validation: 0.7863361535784603]
	TIME [epoch: 2.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387309509371696		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6387309509371696 | validation: 0.7848448297338624]
	TIME [epoch: 2.76 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324104906106446		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6324104906106446 | validation: 0.8835071974981091]
	TIME [epoch: 2.76 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279613190132023		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6279613190132023 | validation: 0.7840294340680279]
	TIME [epoch: 2.76 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6101379890829363		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.6101379890829363 | validation: 0.8154213318378638]
	TIME [epoch: 2.76 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059823025298353		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6059823025298353 | validation: 0.7666864899744419]
	TIME [epoch: 2.86 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021072669748273		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6021072669748273 | validation: 0.7377244978826698]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.610507002599803		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.610507002599803 | validation: 0.8147407246108221]
	TIME [epoch: 2.77 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380024310418958		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6380024310418958 | validation: 0.8878176659198767]
	TIME [epoch: 2.76 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772987333601406		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6772987333601406 | validation: 0.8377616476653984]
	TIME [epoch: 2.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186632498796069		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6186632498796069 | validation: 0.8366723483690364]
	TIME [epoch: 2.76 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899645527126153		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5899645527126153 | validation: 0.7385722376126476]
	TIME [epoch: 2.76 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895733141048769		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5895733141048769 | validation: 0.74818795303666]
	TIME [epoch: 2.76 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961348964837664		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5961348964837664 | validation: 0.751998435594691]
	TIME [epoch: 2.76 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5922038566380222		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5922038566380222 | validation: 0.7473624173705882]
	TIME [epoch: 2.76 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.601650548609717		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.601650548609717 | validation: 0.9459071749692414]
	TIME [epoch: 2.76 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360789060730981		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6360789060730981 | validation: 0.7267509574123494]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5937670801896691		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.5937670801896691 | validation: 0.7566140166054032]
	TIME [epoch: 2.76 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5688319025352494		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5688319025352494 | validation: 0.70269211803827]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562890056970752		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5562890056970752 | validation: 0.7290431032982414]
	TIME [epoch: 2.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5691394229314337		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5691394229314337 | validation: 0.7436938530543274]
	TIME [epoch: 2.76 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606236827370144		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5606236827370144 | validation: 0.7464785146178303]
	TIME [epoch: 2.76 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5896494674930557		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5896494674930557 | validation: 0.7871532496740914]
	TIME [epoch: 2.76 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6192837573964571		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6192837573964571 | validation: 0.8104394412445164]
	TIME [epoch: 2.76 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5630905179307896		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5630905179307896 | validation: 0.6714733640245074]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.53740626783404		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.53740626783404 | validation: 0.7358215076309421]
	TIME [epoch: 2.76 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5325973576829648		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5325973576829648 | validation: 0.636024659488138]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5565322028800184		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5565322028800184 | validation: 0.9450010049647827]
	TIME [epoch: 2.76 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5811537140114098		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5811537140114098 | validation: 0.6396895850455819]
	TIME [epoch: 2.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5317916305474906		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.5317916305474906 | validation: 0.7136152756805704]
	TIME [epoch: 2.76 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504536216100584		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.504536216100584 | validation: 0.6487616919356323]
	TIME [epoch: 2.76 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027374955173817		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.5027374955173817 | validation: 0.6960267405832776]
	TIME [epoch: 2.76 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5023776822200152		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5023776822200152 | validation: 0.6695058484533768]
	TIME [epoch: 2.76 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5330558133614381		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5330558133614381 | validation: 0.7607617851647215]
	TIME [epoch: 2.76 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069299804977567		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5069299804977567 | validation: 0.595232677245284]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728198678001631		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.4728198678001631 | validation: 0.7377369229508113]
	TIME [epoch: 2.76 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4791817549598714		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.4791817549598714 | validation: 0.5663445339971889]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5052039074243062		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5052039074243062 | validation: 0.9557888111518674]
	TIME [epoch: 2.75 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567692727550624		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.5567692727550624 | validation: 0.5851451769096759]
	TIME [epoch: 2.76 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47898465146214847		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.47898465146214847 | validation: 0.6201212687251583]
	TIME [epoch: 2.76 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4306689454320666		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.4306689454320666 | validation: 0.6686963334369063]
	TIME [epoch: 2.76 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42792546283712185		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.42792546283712185 | validation: 0.5662303823909072]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4233829681715949		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.4233829681715949 | validation: 0.6973464866637408]
	TIME [epoch: 2.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4272869334094321		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.4272869334094321 | validation: 0.5233118969665185]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258633499275632		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.4258633499275632 | validation: 0.7827074543081405]
	TIME [epoch: 2.76 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4613897159516623		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.4613897159516623 | validation: 0.4941154220873125]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.454658152345737		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.454658152345737 | validation: 0.7567619373086947]
	TIME [epoch: 2.76 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255081887695233		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.4255081887695233 | validation: 0.5464406916189182]
	TIME [epoch: 2.76 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3788776817162831		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.3788776817162831 | validation: 0.5451254381817415]
	TIME [epoch: 2.76 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3650797159398975		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3650797159398975 | validation: 0.6446645949743548]
	TIME [epoch: 2.76 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3808500533379919		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.3808500533379919 | validation: 0.5229243277826704]
	TIME [epoch: 2.76 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4008316005261595		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.4008316005261595 | validation: 0.7842905546481804]
	TIME [epoch: 2.76 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41298456176821463		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.41298456176821463 | validation: 0.48089265700897016]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3776201321799452		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3776201321799452 | validation: 0.7032068959811517]
	TIME [epoch: 2.76 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4152791315052825		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4152791315052825 | validation: 0.5037567984597991]
	TIME [epoch: 2.76 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850212030621418		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.3850212030621418 | validation: 0.6224263716796481]
	TIME [epoch: 2.75 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33927283737034153		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.33927283737034153 | validation: 0.47306128524360286]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3394826463900175		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.3394826463900175 | validation: 0.7338738781751005]
	TIME [epoch: 2.76 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37491293125373604		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.37491293125373604 | validation: 0.439294391351805]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659430310735937		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3659430310735937 | validation: 0.7089451754088185]
	TIME [epoch: 2.75 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36858191013444325		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.36858191013444325 | validation: 0.4625551691312243]
	TIME [epoch: 2.76 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173171716620671		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.3173171716620671 | validation: 0.5849317098961615]
	TIME [epoch: 2.76 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30309962241152566		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.30309962241152566 | validation: 0.47147675605471684]
	TIME [epoch: 2.76 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937157986896446		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.2937157986896446 | validation: 0.5912627770938261]
	TIME [epoch: 2.76 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2971901274574122		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.2971901274574122 | validation: 0.42696631445927197]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238513745279829		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.3238513745279829 | validation: 0.8194213965115003]
	TIME [epoch: 2.76 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42721503880764355		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.42721503880764355 | validation: 0.4202394640589555]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228926968049386		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.3228926968049386 | validation: 0.5492831494973303]
	TIME [epoch: 2.75 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26962387318156306		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.26962387318156306 | validation: 0.47665521205855826]
	TIME [epoch: 2.75 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25508759630545874		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.25508759630545874 | validation: 0.49309817832655317]
	TIME [epoch: 2.76 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250924701510553		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.250924701510553 | validation: 0.4702912369923281]
	TIME [epoch: 2.75 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24169752903445726		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.24169752903445726 | validation: 0.5207410821037819]
	TIME [epoch: 2.76 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23896810981031308		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.23896810981031308 | validation: 0.4167400031553862]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3050593606655155		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.3050593606655155 | validation: 0.9782376216323891]
	TIME [epoch: 2.76 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445020238647342		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5445020238647342 | validation: 0.42245964460043517]
	TIME [epoch: 2.76 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25138888903216666		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.25138888903216666 | validation: 0.44548999305980086]
	TIME [epoch: 2.76 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25892090482405494		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.25892090482405494 | validation: 0.6885631322657105]
	TIME [epoch: 2.76 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30134300871860936		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.30134300871860936 | validation: 0.41636002250606574]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2606164545794244		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.2606164545794244 | validation: 0.5966894733719506]
	TIME [epoch: 2.76 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30472579947620654		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.30472579947620654 | validation: 0.4027980835769738]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24680525009452187		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.24680525009452187 | validation: 0.6220412464915525]
	TIME [epoch: 2.76 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591321859692125		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2591321859692125 | validation: 0.3905821538627008]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25686185215798685		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.25686185215798685 | validation: 0.6454638070817024]
	TIME [epoch: 2.76 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2759573122570839		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2759573122570839 | validation: 0.39758028913502896]
	TIME [epoch: 2.75 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2621932092695707		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.2621932092695707 | validation: 0.6108275354783868]
	TIME [epoch: 2.75 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26086381824090965		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.26086381824090965 | validation: 0.3965376286105561]
	TIME [epoch: 2.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23312686755204692		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.23312686755204692 | validation: 0.580490927918912]
	TIME [epoch: 2.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23332259087018556		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.23332259087018556 | validation: 0.3847000154568226]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2373449581264681		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.2373449581264681 | validation: 0.599102464402724]
	TIME [epoch: 2.76 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2398232574846947		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.2398232574846947 | validation: 0.3780031618705619]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320462517821685		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.2320462517821685 | validation: 0.5976894506148401]
	TIME [epoch: 2.76 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2471918951614338		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.2471918951614338 | validation: 0.37701618927723757]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23769712717401867		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.23769712717401867 | validation: 0.608086394379461]
	TIME [epoch: 2.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423747745268289		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.2423747745268289 | validation: 0.3833309365444683]
	TIME [epoch: 2.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22773302981107646		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.22773302981107646 | validation: 0.5646846895365294]
	TIME [epoch: 2.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21914962278627603		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.21914962278627603 | validation: 0.37195116430777175]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21869635395174342		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.21869635395174342 | validation: 0.5473359604751654]
	TIME [epoch: 2.76 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.214773868621214		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.214773868621214 | validation: 0.3860702584024561]
	TIME [epoch: 2.75 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2107121403132645		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.2107121403132645 | validation: 0.6072480922131777]
	TIME [epoch: 2.76 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2408519270314138		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2408519270314138 | validation: 0.38053455277009074]
	TIME [epoch: 2.75 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22816751260816565		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.22816751260816565 | validation: 0.5659065712034176]
	TIME [epoch: 2.76 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21509997703980144		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.21509997703980144 | validation: 0.388346494847458]
	TIME [epoch: 2.75 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1979785604577291		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.1979785604577291 | validation: 0.5313475364530255]
	TIME [epoch: 2.75 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19486866118626708		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.19486866118626708 | validation: 0.40189392158535303]
	TIME [epoch: 2.75 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16688647397705048		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.16688647397705048 | validation: 0.46696781175734436]
	TIME [epoch: 2.75 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1664939388428934		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.1664939388428934 | validation: 0.3764592366660363]
	TIME [epoch: 2.75 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22257902347060743		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.22257902347060743 | validation: 0.7036061645790335]
	TIME [epoch: 2.76 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33303895006767986		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.33303895006767986 | validation: 0.3884241907035526]
	TIME [epoch: 2.75 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22481070097169364		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.22481070097169364 | validation: 0.594222946620722]
	TIME [epoch: 2.75 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22306279192446282		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.22306279192446282 | validation: 0.39434154553468037]
	TIME [epoch: 2.75 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153287038775853		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.153287038775853 | validation: 0.41463460205512387]
	TIME [epoch: 2.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16257518324430018		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.16257518324430018 | validation: 0.4434032779721223]
	TIME [epoch: 2.75 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15890538801147053		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.15890538801147053 | validation: 0.3869884049621676]
	TIME [epoch: 2.76 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1505850738742111		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.1505850738742111 | validation: 0.4938498979050424]
	TIME [epoch: 2.75 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600401027051494		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.1600401027051494 | validation: 0.36264848041366876]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19829979867133354		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.19829979867133354 | validation: 0.7775102822246528]
	TIME [epoch: 2.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35970950472777874		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.35970950472777874 | validation: 0.37358305482282406]
	TIME [epoch: 2.76 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24680295263674995		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.24680295263674995 | validation: 0.514254331212536]
	TIME [epoch: 2.76 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17220696456226342		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.17220696456226342 | validation: 0.38709445730467795]
	TIME [epoch: 2.76 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14392417601039256		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.14392417601039256 | validation: 0.42907638692164074]
	TIME [epoch: 2.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14125874915065725		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.14125874915065725 | validation: 0.43174649562113654]
	TIME [epoch: 2.75 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13850971414993826		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.13850971414993826 | validation: 0.3780451000306084]
	TIME [epoch: 2.76 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15470853744443952		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.15470853744443952 | validation: 0.6235898834543684]
	TIME [epoch: 2.75 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2355159166569408		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2355159166569408 | validation: 0.35916040765076934]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33109927135563916		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.33109927135563916 | validation: 0.583305304652769]
	TIME [epoch: 2.75 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20868454658215832		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.20868454658215832 | validation: 0.38594685619612373]
	TIME [epoch: 2.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14109158749780745		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.14109158749780745 | validation: 0.40138182448154086]
	TIME [epoch: 2.75 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13530267313089936		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.13530267313089936 | validation: 0.44883456383245923]
	TIME [epoch: 2.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14230456672159558		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.14230456672159558 | validation: 0.3782718724366443]
	TIME [epoch: 2.76 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655186280398822		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.1655186280398822 | validation: 0.5261748358555105]
	TIME [epoch: 2.75 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18920009944137534		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.18920009944137534 | validation: 0.3396117858039053]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22955744499821282		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.22955744499821282 | validation: 0.6922830345223007]
	TIME [epoch: 2.75 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25362074869466456		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.25362074869466456 | validation: 0.37935467116757365]
	TIME [epoch: 2.75 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21106334334963373		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.21106334334963373 | validation: 0.5436762764885916]
	TIME [epoch: 2.76 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17427697145251206		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.17427697145251206 | validation: 0.39046353987634536]
	TIME [epoch: 2.76 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13261994976242603		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.13261994976242603 | validation: 0.3787730162425844]
	TIME [epoch: 2.76 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14263625225242377		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.14263625225242377 | validation: 0.4552912701817568]
	TIME [epoch: 2.76 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13940201785031026		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.13940201785031026 | validation: 0.36842435875782753]
	TIME [epoch: 2.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12967849011466778		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.12967849011466778 | validation: 0.42878444665294896]
	TIME [epoch: 2.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12472938873276769		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.12472938873276769 | validation: 0.38419753079132707]
	TIME [epoch: 2.76 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14472807176576225		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.14472807176576225 | validation: 0.6081981976301123]
	TIME [epoch: 2.76 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23430873243706046		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.23430873243706046 | validation: 0.3556813211706948]
	TIME [epoch: 2.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253795088798055		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.253795088798055 | validation: 0.6420878284134157]
	TIME [epoch: 2.76 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22009352686609535		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.22009352686609535 | validation: 0.35197002251312615]
	TIME [epoch: 2.75 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14376223178922168		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.14376223178922168 | validation: 0.4668149620190774]
	TIME [epoch: 2.75 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376137625228899		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1376137625228899 | validation: 0.3531409412212402]
	TIME [epoch: 2.75 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13242896504728885		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.13242896504728885 | validation: 0.46474680172232485]
	TIME [epoch: 2.75 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122634089070144		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.14122634089070144 | validation: 0.33839005321242605]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1653392392757079		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1653392392757079 | validation: 0.6219077205571952]
	TIME [epoch: 2.75 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21128784018434169		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.21128784018434169 | validation: 0.3466351614823724]
	TIME [epoch: 2.75 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17779260340365186		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.17779260340365186 | validation: 0.5659114855837365]
	TIME [epoch: 2.76 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1701767485628293		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.1701767485628293 | validation: 0.3262474210687037]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14143863753954578		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.14143863753954578 | validation: 0.4959766115069162]
	TIME [epoch: 2.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14621822911186833		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.14621822911186833 | validation: 0.34590275125371545]
	TIME [epoch: 2.75 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15544339661763737		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.15544339661763737 | validation: 0.5336022617216076]
	TIME [epoch: 2.75 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17651016260360677		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.17651016260360677 | validation: 0.348918983484431]
	TIME [epoch: 2.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16680216048010998		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.16680216048010998 | validation: 0.5571840297618766]
	TIME [epoch: 2.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19968243324908128		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.19968243324908128 | validation: 0.368086250244953]
	TIME [epoch: 2.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13517519415657697		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.13517519415657697 | validation: 0.3798103511074793]
	TIME [epoch: 2.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094352325678895		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.12094352325678895 | validation: 0.41483065838265354]
	TIME [epoch: 2.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12416279159158798		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.12416279159158798 | validation: 0.3715879049627313]
	TIME [epoch: 2.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356056313087927		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.1356056313087927 | validation: 0.477708434191233]
	TIME [epoch: 2.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441285278051154		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.1441285278051154 | validation: 0.3609060344067572]
	TIME [epoch: 2.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12702400446399895		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.12702400446399895 | validation: 0.5177115262517633]
	TIME [epoch: 2.75 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14629191956399487		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.14629191956399487 | validation: 0.3305552631385068]
	TIME [epoch: 2.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111989748845923		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.2111989748845923 | validation: 0.7119576902855027]
	TIME [epoch: 2.75 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2851669986135302		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2851669986135302 | validation: 0.34362861736945766]
	TIME [epoch: 2.75 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14852266052353702		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.14852266052353702 | validation: 0.4546748196853947]
	TIME [epoch: 2.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13865152927032362		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.13865152927032362 | validation: 0.38679816789094174]
	TIME [epoch: 2.74 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1165185873871806		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.1165185873871806 | validation: 0.3513068015986871]
	TIME [epoch: 2.75 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627339811726836		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.11627339811726836 | validation: 0.44974379094870703]
	TIME [epoch: 2.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12335359086539885		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.12335359086539885 | validation: 0.3351099527155311]
	TIME [epoch: 2.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293151898534423		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.1293151898534423 | validation: 0.4934622336265383]
	TIME [epoch: 2.75 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14654130687953337		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.14654130687953337 | validation: 0.33523815829689446]
	TIME [epoch: 2.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734179097701555		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.1734179097701555 | validation: 0.597558897141896]
	TIME [epoch: 2.75 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2086874981155828		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.2086874981155828 | validation: 0.33404960192757716]
	TIME [epoch: 2.75 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17372405612307418		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.17372405612307418 | validation: 0.5429839458415614]
	TIME [epoch: 2.75 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14826140983705055		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.14826140983705055 | validation: 0.34620450624739174]
	TIME [epoch: 2.75 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11335817008440882		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.11335817008440882 | validation: 0.36795632923803606]
	TIME [epoch: 2.75 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022604324347196		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.11022604324347196 | validation: 0.39491452361450613]
	TIME [epoch: 2.75 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11379589161736864		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.11379589161736864 | validation: 0.3476362066446424]
	TIME [epoch: 2.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12179862337794417		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.12179862337794417 | validation: 0.41683571318639046]
	TIME [epoch: 2.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336542970734212		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1336542970734212 | validation: 0.3237673163595841]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13579362915534668		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.13579362915534668 | validation: 0.5458866603358978]
	TIME [epoch: 2.75 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16555018519827117		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.16555018519827117 | validation: 0.34469774884295923]
	TIME [epoch: 2.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20956086936310145		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.20956086936310145 | validation: 0.5793971005069685]
	TIME [epoch: 2.75 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17069419560384616		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.17069419560384616 | validation: 0.33014396669570006]
	TIME [epoch: 2.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12338040551435674		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12338040551435674 | validation: 0.37569507100791744]
	TIME [epoch: 2.75 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655832755021098		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.10655832755021098 | validation: 0.3814282283521109]
	TIME [epoch: 2.75 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10972950693749167		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.10972950693749167 | validation: 0.37268892096967665]
	TIME [epoch: 2.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11411817626156275		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.11411817626156275 | validation: 0.3902655135696688]
	TIME [epoch: 2.75 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11541190117611101		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.11541190117611101 | validation: 0.37754684698944224]
	TIME [epoch: 2.75 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10836001346100488		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.10836001346100488 | validation: 0.35016675546921694]
	TIME [epoch: 2.75 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10928550136029458		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.10928550136029458 | validation: 0.48746366514063677]
	TIME [epoch: 2.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380367017310438		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1380367017310438 | validation: 0.3179574817137607]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2044829209005483		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2044829209005483 | validation: 0.6703322149733485]
	TIME [epoch: 2.75 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.240399651299024		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.240399651299024 | validation: 0.3328073275602601]
	TIME [epoch: 2.75 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15475018854942826		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.15475018854942826 | validation: 0.45946694689002454]
	TIME [epoch: 2.75 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204483115455281		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.1204483115455281 | validation: 0.33824670823087233]
	TIME [epoch: 2.75 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039309585065259		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.1039309585065259 | validation: 0.36471848419750297]
	TIME [epoch: 2.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09814786926218298		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.09814786926218298 | validation: 0.36405443827432427]
	TIME [epoch: 2.75 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09899263129264373		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.09899263129264373 | validation: 0.3372962593963249]
	TIME [epoch: 2.75 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10513211199733548		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.10513211199733548 | validation: 0.3903615200476214]
	TIME [epoch: 2.75 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1196245185162266		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.1196245185162266 | validation: 0.3507198791492379]
	TIME [epoch: 2.75 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12850082901065238		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.12850082901065238 | validation: 0.4258293986580272]
	TIME [epoch: 2.75 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15077112130606543		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.15077112130606543 | validation: 0.33513665560632494]
	TIME [epoch: 2.75 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11386180161889277		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.11386180161889277 | validation: 0.5022938077295785]
	TIME [epoch: 2.75 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13098781015823086		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.13098781015823086 | validation: 0.3908176472205389]
	TIME [epoch: 2.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757279976114484		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.2757279976114484 | validation: 0.6065940632692906]
	TIME [epoch: 2.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854621621534151		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1854621621534151 | validation: 0.347879536670673]
	TIME [epoch: 2.75 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757316757550157		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.10757316757550157 | validation: 0.3544524108669727]
	TIME [epoch: 2.75 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489283996079824		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.11489283996079824 | validation: 0.4171191523214877]
	TIME [epoch: 2.75 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551880076976283		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.10551880076976283 | validation: 0.34091880667430974]
	TIME [epoch: 2.75 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996833358286729		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.09996833358286729 | validation: 0.3651755560468706]
	TIME [epoch: 2.76 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10095317678821612		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.10095317678821612 | validation: 0.33679572609265024]
	TIME [epoch: 2.75 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373012580481585		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.11373012580481585 | validation: 0.4655912863250574]
	TIME [epoch: 2.76 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14009777784719776		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.14009777784719776 | validation: 0.30969764723153603]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11934980787539484		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.11934980787539484 | validation: 0.47585295840189057]
	TIME [epoch: 2.77 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489424489169414		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.11489424489169414 | validation: 0.3397283088441337]
	TIME [epoch: 2.76 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10842831588440863		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.10842831588440863 | validation: 0.42165279301906167]
	TIME [epoch: 2.76 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10793533935023435		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.10793533935023435 | validation: 0.33507198148630074]
	TIME [epoch: 2.76 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189886556407121		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.10189886556407121 | validation: 0.40770402134166145]
	TIME [epoch: 2.76 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10733499910636402		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.10733499910636402 | validation: 0.31987276021322675]
	TIME [epoch: 2.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11981820257807513		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.11981820257807513 | validation: 0.5501112147150911]
	TIME [epoch: 2.76 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16245582184727794		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.16245582184727794 | validation: 0.2992059888051697]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564086473626793		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.2564086473626793 | validation: 0.4701361901362797]
	TIME [epoch: 2.76 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14990117520368282		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.14990117520368282 | validation: 0.3378960021753321]
	TIME [epoch: 2.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09451143414126069		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.09451143414126069 | validation: 0.328885121398081]
	TIME [epoch: 2.76 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11738219497108883		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.11738219497108883 | validation: 0.4054417217564565]
	TIME [epoch: 2.75 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11367538709953989		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.11367538709953989 | validation: 0.3114832908502048]
	TIME [epoch: 2.76 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986075292307687		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.0986075292307687 | validation: 0.41805900193474727]
	TIME [epoch: 2.76 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10397772817026944		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.10397772817026944 | validation: 0.32296997555747725]
	TIME [epoch: 2.76 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09672550547489818		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.09672550547489818 | validation: 0.3754728351547896]
	TIME [epoch: 190 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09331012406470107		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.09331012406470107 | validation: 0.3164765248372084]
	TIME [epoch: 5.95 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09376171793685895		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.09376171793685895 | validation: 0.3772585951847428]
	TIME [epoch: 5.92 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787529231271949		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.09787529231271949 | validation: 0.3194351190534101]
	TIME [epoch: 5.93 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11598304782354234		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.11598304782354234 | validation: 0.4901486070400216]
	TIME [epoch: 5.92 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14581692203062954		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.14581692203062954 | validation: 0.3128663090277896]
	TIME [epoch: 5.92 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17028226681850078		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.17028226681850078 | validation: 0.609328834566327]
	TIME [epoch: 5.92 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17401533514102982		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.17401533514102982 | validation: 0.3119219480551342]
	TIME [epoch: 5.92 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13290876670275995		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.13290876670275995 | validation: 0.41055483364749396]
	TIME [epoch: 5.92 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843785710239292		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.09843785710239292 | validation: 0.3719535470456869]
	TIME [epoch: 5.94 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961571344631928		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.0961571344631928 | validation: 0.32589526983706923]
	TIME [epoch: 5.94 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10422250504503086		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.10422250504503086 | validation: 0.40058478282347726]
	TIME [epoch: 5.94 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0967542453269684		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.0967542453269684 | validation: 0.3166788054738305]
	TIME [epoch: 5.93 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104621913522067		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09104621913522067 | validation: 0.37987470392934597]
	TIME [epoch: 5.93 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09604321797773788		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.09604321797773788 | validation: 0.31218116091188636]
	TIME [epoch: 5.92 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160978263015455		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.09160978263015455 | validation: 0.4102153914486338]
	TIME [epoch: 5.92 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09700924644120729		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.09700924644120729 | validation: 0.3267010939835153]
	TIME [epoch: 5.92 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10289582040494004		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10289582040494004 | validation: 0.4488137240540912]
	TIME [epoch: 5.93 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13319367889106495		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.13319367889106495 | validation: 0.3385298573148337]
	TIME [epoch: 5.93 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021394715305284		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.1021394715305284 | validation: 0.34612463943790595]
	TIME [epoch: 5.93 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08522885087408703		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.08522885087408703 | validation: 0.3214307913331029]
	TIME [epoch: 5.92 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08647342579779663		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.08647342579779663 | validation: 0.3629178919931592]
	TIME [epoch: 5.93 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08916235647298747		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.08916235647298747 | validation: 0.30744548431072916]
	TIME [epoch: 5.93 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11233904006858537		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.11233904006858537 | validation: 0.622693602012917]
	TIME [epoch: 5.92 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2073205256531513		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.2073205256531513 | validation: 0.29288633271616965]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23453442812034803		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.23453442812034803 | validation: 0.4153319954543925]
	TIME [epoch: 5.91 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507739619602669		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.09507739619602669 | validation: 0.38227202683719325]
	TIME [epoch: 5.91 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032152246354656		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.10032152246354656 | validation: 0.2807686593387777]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10316273723791845		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.10316273723791845 | validation: 0.41867165200188955]
	TIME [epoch: 5.93 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09959869600794863		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.09959869600794863 | validation: 0.31760408442522864]
	TIME [epoch: 5.91 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08843682992715927		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.08843682992715927 | validation: 0.33600813181894634]
	TIME [epoch: 5.92 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848558044218191		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.0848558044218191 | validation: 0.3578619742231681]
	TIME [epoch: 5.92 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541628232594041		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.08541628232594041 | validation: 0.3145913356907406]
	TIME [epoch: 5.92 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09686207490079272		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.09686207490079272 | validation: 0.41818089356794597]
	TIME [epoch: 5.91 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10386867221263575		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.10386867221263575 | validation: 0.2651387081726709]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23969054685643998		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.23969054685643998 | validation: 0.412626190451911]
	TIME [epoch: 5.91 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902249493169964		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.09902249493169964 | validation: 0.3480850512401876]
	TIME [epoch: 5.91 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11143070661797552		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.11143070661797552 | validation: 0.39662501453712024]
	TIME [epoch: 5.91 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509737254161803		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.09509737254161803 | validation: 0.31260766891813824]
	TIME [epoch: 5.91 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08886001603645786		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.08886001603645786 | validation: 0.3973814534078585]
	TIME [epoch: 5.92 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09973683145516972		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.09973683145516972 | validation: 0.30388247284283465]
	TIME [epoch: 5.91 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08811517895893181		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.08811517895893181 | validation: 0.4146829585543743]
	TIME [epoch: 5.91 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09359155867529109		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.09359155867529109 | validation: 0.2907740125050502]
	TIME [epoch: 5.91 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10498073859719648		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.10498073859719648 | validation: 0.589179035544079]
	TIME [epoch: 5.92 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15670455454784255		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.15670455454784255 | validation: 0.3022800225163107]
	TIME [epoch: 5.93 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10812911234859225		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.10812911234859225 | validation: 0.40426193669369276]
	TIME [epoch: 5.92 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542206992901775		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.09542206992901775 | validation: 0.3381124560130171]
	TIME [epoch: 5.92 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144981850078713		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.08144981850078713 | validation: 0.32008454848985074]
	TIME [epoch: 5.93 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829717781899637		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.0829717781899637 | validation: 0.4189799836222264]
	TIME [epoch: 5.92 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219231518495401		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.09219231518495401 | validation: 0.2996912858868771]
	TIME [epoch: 5.93 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11099273329671688		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.11099273329671688 | validation: 0.5740828545277762]
	TIME [epoch: 5.92 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1350300982452997		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1350300982452997 | validation: 0.28325971054431104]
	TIME [epoch: 5.92 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12133525024916246		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.12133525024916246 | validation: 0.3949849705158189]
	TIME [epoch: 5.91 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09010682977101897		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.09010682977101897 | validation: 0.3315108498217776]
	TIME [epoch: 5.92 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08520199160530431		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.08520199160530431 | validation: 0.31245270742804915]
	TIME [epoch: 5.91 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090465960989767		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.090465960989767 | validation: 0.36336449018895206]
	TIME [epoch: 5.92 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08474441581945684		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.08474441581945684 | validation: 0.3173484741025901]
	TIME [epoch: 5.92 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951583696949183		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.07951583696949183 | validation: 0.33939443652869083]
	TIME [epoch: 5.92 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07868398563238523		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.07868398563238523 | validation: 0.2892983503844479]
	TIME [epoch: 5.92 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382708723852489		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.08382708723852489 | validation: 0.4433942260219655]
	TIME [epoch: 5.92 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994930540173276		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.0994930540173276 | validation: 0.283932626044773]
	TIME [epoch: 5.92 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.110172906034695		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.110172906034695 | validation: 0.5240929624621852]
	TIME [epoch: 5.92 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13924558540251641		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.13924558540251641 | validation: 0.2588603182709079]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14194059108514506		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.14194059108514506 | validation: 0.3513998081722669]
	TIME [epoch: 5.92 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10530620156384184		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.10530620156384184 | validation: 0.4168791063068043]
	TIME [epoch: 5.93 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09307931077172803		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.09307931077172803 | validation: 0.3000653091412498]
	TIME [epoch: 5.91 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577344685121617		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.09577344685121617 | validation: 0.3878049872968472]
	TIME [epoch: 5.92 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08523478757843758		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.08523478757843758 | validation: 0.32224615105859056]
	TIME [epoch: 5.92 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761955485272558		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.0761955485272558 | validation: 0.28713806586004864]
	TIME [epoch: 5.91 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023144215502491		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.08023144215502491 | validation: 0.38796507324636464]
	TIME [epoch: 5.92 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257868395339017		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.08257868395339017 | validation: 0.29741703003514586]
	TIME [epoch: 5.91 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08787781729856656		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.08787781729856656 | validation: 0.44636448412637036]
	TIME [epoch: 5.92 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945726452785156		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.09945726452785156 | validation: 0.2905858763401827]
	TIME [epoch: 5.92 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10316147484244508		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.10316147484244508 | validation: 0.4830655512965053]
	TIME [epoch: 5.93 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687001871053536		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10687001871053536 | validation: 0.30371659647935956]
	TIME [epoch: 5.92 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905153445141509		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.08905153445141509 | validation: 0.34470868866721704]
	TIME [epoch: 5.92 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09093943370353005		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.09093943370353005 | validation: 0.3717467696869623]
	TIME [epoch: 5.92 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029262001526033		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1029262001526033 | validation: 0.28432542402243527]
	TIME [epoch: 5.92 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572854310257885		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.09572854310257885 | validation: 0.44041419727390085]
	TIME [epoch: 5.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1066321226779691		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1066321226779691 | validation: 0.2852837770205838]
	TIME [epoch: 5.92 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10858546719414651		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.10858546719414651 | validation: 0.5029017985057251]
	TIME [epoch: 5.91 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10799291174225983		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.10799291174225983 | validation: 0.2835335696387693]
	TIME [epoch: 5.92 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665308696400469		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.07665308696400469 | validation: 0.3162748521279366]
	TIME [epoch: 5.91 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07454332249241265		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.07454332249241265 | validation: 0.3343542738186628]
	TIME [epoch: 5.92 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447144253275305		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.07447144253275305 | validation: 0.29430578997856544]
	TIME [epoch: 5.92 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365235136660914		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.07365235136660914 | validation: 0.36931971089055643]
	TIME [epoch: 5.91 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0836611279469602		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.0836611279469602 | validation: 0.2514540529308997]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12821883992087119		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.12821883992087119 | validation: 0.4078195168446878]
	TIME [epoch: 5.91 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.116932419426478		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.116932419426478 | validation: 0.30535684974445126]
	TIME [epoch: 5.91 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175155314931196		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.07175155314931196 | validation: 0.3223058496871843]
	TIME [epoch: 5.92 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821637777802501		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.07821637777802501 | validation: 0.3387894464420744]
	TIME [epoch: 5.92 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777434165276725		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.07777434165276725 | validation: 0.28682412314213884]
	TIME [epoch: 5.92 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08200253838767102		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.08200253838767102 | validation: 0.4518487841224298]
	TIME [epoch: 5.92 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11102476259369973		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.11102476259369973 | validation: 0.32823908340158836]
	TIME [epoch: 5.92 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17013508845201497		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.17013508845201497 | validation: 0.48008900300547525]
	TIME [epoch: 5.92 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12274326782624201		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.12274326782624201 | validation: 0.3996438512156926]
	TIME [epoch: 5.92 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09346873353731339		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.09346873353731339 | validation: 0.2809037517447669]
	TIME [epoch: 5.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10658834711203378		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.10658834711203378 | validation: 0.411596388731536]
	TIME [epoch: 5.92 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08581224446949959		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.08581224446949959 | validation: 0.31903969814293004]
	TIME [epoch: 5.91 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263302846628063		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.07263302846628063 | validation: 0.28601335249350585]
	TIME [epoch: 5.92 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07011155179827486		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.07011155179827486 | validation: 0.3495351178316268]
	TIME [epoch: 5.92 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700732052672956		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.0700732052672956 | validation: 0.3090529543784524]
	TIME [epoch: 5.91 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920260271341498		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.06920260271341498 | validation: 0.31567310358418776]
	TIME [epoch: 5.91 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07150187334520428		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.07150187334520428 | validation: 0.3094616589820241]
	TIME [epoch: 5.91 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155450527489836		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07155450527489836 | validation: 0.303479582349865]
	TIME [epoch: 5.91 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07502020782084216		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07502020782084216 | validation: 0.29234514345595203]
	TIME [epoch: 5.91 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.085328687596664		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.085328687596664 | validation: 0.3831323580015578]
	TIME [epoch: 5.92 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1054824343436487		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1054824343436487 | validation: 0.27586548613813]
	TIME [epoch: 5.91 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08911770882477534		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.08911770882477534 | validation: 0.45808292015567925]
	TIME [epoch: 5.93 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09836738050047496		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.09836738050047496 | validation: 0.29018970501191316]
	TIME [epoch: 5.91 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819291258172904		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.11819291258172904 | validation: 0.39417139259969136]
	TIME [epoch: 5.93 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869697213905632		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.0869697213905632 | validation: 0.29019115671357215]
	TIME [epoch: 5.91 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08231423279519996		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.08231423279519996 | validation: 0.29615409059305625]
	TIME [epoch: 5.92 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08263266748715546		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.08263266748715546 | validation: 0.31375795792493766]
	TIME [epoch: 5.91 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06725360683777874		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.06725360683777874 | validation: 0.3268450938681932]
	TIME [epoch: 5.92 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06637817831125135		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.06637817831125135 | validation: 0.29505109973372273]
	TIME [epoch: 5.91 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339579411079356		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07339579411079356 | validation: 0.37217863067179746]
	TIME [epoch: 5.92 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08198194236995647		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.08198194236995647 | validation: 0.2641842562259707]
	TIME [epoch: 5.92 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09034201928984459		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.09034201928984459 | validation: 0.4139456694026855]
	TIME [epoch: 5.92 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09075579461354036		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.09075579461354036 | validation: 0.24812874349701272]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555094716221155		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.08555094716221155 | validation: 0.3821659170143032]
	TIME [epoch: 5.92 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08683554723731392		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.08683554723731392 | validation: 0.27147253760527784]
	TIME [epoch: 5.92 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959462576065981		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.08959462576065981 | validation: 0.3122211643597735]
	TIME [epoch: 5.92 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07700660327951582		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07700660327951582 | validation: 0.30436530204431494]
	TIME [epoch: 5.91 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06808235533922152		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.06808235533922152 | validation: 0.31377832715640924]
	TIME [epoch: 5.92 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408535313173681		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.06408535313173681 | validation: 0.2829428005173948]
	TIME [epoch: 5.91 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06459830897799902		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.06459830897799902 | validation: 0.31975215427526876]
	TIME [epoch: 5.92 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859241983848607		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.06859241983848607 | validation: 0.27892409838189003]
	TIME [epoch: 5.93 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06724013464138233		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.06724013464138233 | validation: 0.3539355759489215]
	TIME [epoch: 5.92 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08062164898719941		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.08062164898719941 | validation: 0.25383328337160954]
	TIME [epoch: 5.92 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09194704733256277		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.09194704733256277 | validation: 0.49728450179782374]
	TIME [epoch: 5.92 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11939089485277198		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.11939089485277198 | validation: 0.26080303573138947]
	TIME [epoch: 5.92 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734433466215783		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.09734433466215783 | validation: 0.36211273705256586]
	TIME [epoch: 5.92 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07386563497828753		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07386563497828753 | validation: 0.2778245559345673]
	TIME [epoch: 5.92 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06977299681575895		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.06977299681575895 | validation: 0.27472326796831936]
	TIME [epoch: 5.92 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103886549393504		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.07103886549393504 | validation: 0.32466981819942675]
	TIME [epoch: 5.92 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076259163230304		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.076259163230304 | validation: 0.28198284628628006]
	TIME [epoch: 5.92 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612560528904696		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06612560528904696 | validation: 0.30183160611893683]
	TIME [epoch: 5.92 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575707677594858		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.07575707677594858 | validation: 0.3106902983318805]
	TIME [epoch: 5.92 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09241480790116807		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.09241480790116807 | validation: 0.2910751148654602]
	TIME [epoch: 5.91 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10176860025988332		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.10176860025988332 | validation: 0.44884339073205126]
	TIME [epoch: 5.92 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09808969255607838		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.09808969255607838 | validation: 0.24916565618247155]
	TIME [epoch: 5.91 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10735047530769415		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.10735047530769415 | validation: 0.3984831274813829]
	TIME [epoch: 5.91 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08885175959618884		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.08885175959618884 | validation: 0.297134319580702]
	TIME [epoch: 5.91 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576921665795947		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.06576921665795947 | validation: 0.24702222266532306]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11721400992044233		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.11721400992044233 | validation: 0.4477115953673252]
	TIME [epoch: 5.92 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09638471211717028		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.09638471211717028 | validation: 0.28687203879170675]
	TIME [epoch: 5.92 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06490142019119378		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06490142019119378 | validation: 0.2603166776788918]
	TIME [epoch: 5.92 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242580927636402		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.07242580927636402 | validation: 0.3382617950391734]
	TIME [epoch: 5.92 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714655415937003		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.0714655415937003 | validation: 0.2760671999938629]
	TIME [epoch: 5.92 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575567898703952		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.06575567898703952 | validation: 0.2699910087819012]
	TIME [epoch: 5.92 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508855807085037		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06508855807085037 | validation: 0.3347945505451844]
	TIME [epoch: 5.92 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992411350083401		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06992411350083401 | validation: 0.25342856938061536]
	TIME [epoch: 5.92 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907645193320198		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.06907645193320198 | validation: 0.33726052728691397]
	TIME [epoch: 5.92 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662163172691568		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.06662163172691568 | validation: 0.2756466593450833]
	TIME [epoch: 5.92 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07062822078295668		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.07062822078295668 | validation: 0.3081771824470023]
	TIME [epoch: 5.91 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06952785993636748		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.06952785993636748 | validation: 0.27035433505291506]
	TIME [epoch: 5.92 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06915969336178002		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.06915969336178002 | validation: 0.34547186723669415]
	TIME [epoch: 5.92 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06502859555956439		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.06502859555956439 | validation: 0.25937649974754473]
	TIME [epoch: 5.91 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07146119005220707		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.07146119005220707 | validation: 0.37890932983186676]
	TIME [epoch: 5.92 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861494035652293		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0861494035652293 | validation: 0.23395157676092604]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976930981924808		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.08976930981924808 | validation: 0.3620476526559191]
	TIME [epoch: 5.91 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07612364899238305		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.07612364899238305 | validation: 0.2662435797057114]
	TIME [epoch: 5.91 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514655132355725		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06514655132355725 | validation: 0.3369530254092439]
	TIME [epoch: 5.92 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661443447990402		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.0661443447990402 | validation: 0.26224006407818035]
	TIME [epoch: 5.92 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0679442958361032		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.0679442958361032 | validation: 0.3924423355448236]
	TIME [epoch: 5.91 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0803885946059873		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.0803885946059873 | validation: 0.2748542249327972]
	TIME [epoch: 5.91 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08814351445387926		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08814351445387926 | validation: 0.4027116983843173]
	TIME [epoch: 5.92 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329968926542819		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08329968926542819 | validation: 0.25997464322877273]
	TIME [epoch: 5.92 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06845946764976396		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06845946764976396 | validation: 0.31132020857497417]
	TIME [epoch: 5.93 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805365103767289		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06805365103767289 | validation: 0.26895481940019156]
	TIME [epoch: 5.92 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426424390850263		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06426424390850263 | validation: 0.27441001613290705]
	TIME [epoch: 5.92 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090136143416599		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07090136143416599 | validation: 0.2583061230183093]
	TIME [epoch: 5.93 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07220714100266575		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07220714100266575 | validation: 0.2914327831374818]
	TIME [epoch: 5.93 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856106451519317		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.06856106451519317 | validation: 0.2685801365376172]
	TIME [epoch: 5.93 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06045578231069107		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.06045578231069107 | validation: 0.3126777180896988]
	TIME [epoch: 5.92 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078334374514755		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06078334374514755 | validation: 0.24146298273467448]
	TIME [epoch: 5.93 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373776196356401		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.06373776196356401 | validation: 0.3510983621087709]
	TIME [epoch: 5.92 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07743389578791007		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.07743389578791007 | validation: 0.23040542841711997]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787964619019102		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.09787964619019102 | validation: 0.38632606861023955]
	TIME [epoch: 5.92 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302378831901571		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.09302378831901571 | validation: 0.26569441274274336]
	TIME [epoch: 5.93 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061923001196017236		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.061923001196017236 | validation: 0.2784093260570519]
	TIME [epoch: 5.94 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103267871816986		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.06103267871816986 | validation: 0.3090747743092311]
	TIME [epoch: 5.92 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335472742749676		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06335472742749676 | validation: 0.25443671874768636]
	TIME [epoch: 5.93 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05889310049904596		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.05889310049904596 | validation: 0.32010303767593157]
	TIME [epoch: 5.93 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07319788031355168		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07319788031355168 | validation: 0.30083055263878533]
	TIME [epoch: 5.94 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908419797607142		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.09908419797607142 | validation: 0.4478044135184627]
	TIME [epoch: 5.95 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133665759118763		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.09133665759118763 | validation: 0.23570048872100122]
	TIME [epoch: 5.94 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07472033760952501		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07472033760952501 | validation: 0.28745164717967775]
	TIME [epoch: 5.93 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07743036333499342		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.07743036333499342 | validation: 0.28091734978232413]
	TIME [epoch: 5.94 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056933889811789984		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.056933889811789984 | validation: 0.26278690129346755]
	TIME [epoch: 5.93 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662412569890726		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.0662412569890726 | validation: 0.3071781339628163]
	TIME [epoch: 5.94 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689077049604043		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06689077049604043 | validation: 0.24930560265867127]
	TIME [epoch: 5.94 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06297088957957542		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06297088957957542 | validation: 0.34295642957680966]
	TIME [epoch: 5.94 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06590534459419242		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.06590534459419242 | validation: 0.24614258503955166]
	TIME [epoch: 5.93 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576590568158584		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06576590568158584 | validation: 0.34911951074952485]
	TIME [epoch: 5.93 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06904848919296355		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.06904848919296355 | validation: 0.25108547712088597]
	TIME [epoch: 5.93 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314487105147165		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06314487105147165 | validation: 0.3049643108254257]
	TIME [epoch: 5.94 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483310896816737		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.06483310896816737 | validation: 0.2498497749713791]
	TIME [epoch: 5.94 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062463972528613784		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.062463972528613784 | validation: 0.2910482737843431]
	TIME [epoch: 5.94 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06536538693045116		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.06536538693045116 | validation: 0.25473176312404083]
	TIME [epoch: 5.93 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062348618643776874		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.062348618643776874 | validation: 0.312365004335896]
	TIME [epoch: 5.93 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255124427805609		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.07255124427805609 | validation: 0.23555341647202202]
	TIME [epoch: 5.92 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09713237379846071		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09713237379846071 | validation: 0.33224756559287866]
	TIME [epoch: 5.93 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07806620417105255		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.07806620417105255 | validation: 0.23409066145040366]
	TIME [epoch: 5.93 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945839776117967		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05945839776117967 | validation: 0.3077710776381518]
	TIME [epoch: 5.93 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119781531538494		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.07119781531538494 | validation: 0.28036977105930266]
	TIME [epoch: 5.93 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07684535763506638		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.07684535763506638 | validation: 0.31173239328311636]
	TIME [epoch: 5.93 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011668804815786		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.06011668804815786 | validation: 0.22608276817230932]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06986303936117204		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06986303936117204 | validation: 0.36037290704580366]
	TIME [epoch: 5.88 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316655685455141		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.07316655685455141 | validation: 0.255440984664728]
	TIME [epoch: 5.87 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060938972277442896		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.060938972277442896 | validation: 0.29452552062935466]
	TIME [epoch: 5.88 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06726253445008942		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06726253445008942 | validation: 0.26483051497904114]
	TIME [epoch: 5.88 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475503546344505		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.06475503546344505 | validation: 0.27767675069478803]
	TIME [epoch: 5.89 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056538261664644804		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.056538261664644804 | validation: 0.27206241999749947]
	TIME [epoch: 5.88 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534376379490556		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05534376379490556 | validation: 0.2778161100637039]
	TIME [epoch: 5.88 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05863982610911654		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05863982610911654 | validation: 0.2425290836619862]
	TIME [epoch: 5.88 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153592088044233		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.06153592088044233 | validation: 0.4489122181043101]
	TIME [epoch: 5.89 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097278265062762		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.10097278265062762 | validation: 0.22377932280539503]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09090449014630114		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.09090449014630114 | validation: 0.35427529034590827]
	TIME [epoch: 5.92 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07146625528801176		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.07146625528801176 | validation: 0.25375532637867776]
	TIME [epoch: 5.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057093895950280034		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.057093895950280034 | validation: 0.2436802393247541]
	TIME [epoch: 5.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928350190874209		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.05928350190874209 | validation: 0.29556767397746725]
	TIME [epoch: 5.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06001597635717721		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06001597635717721 | validation: 0.2334200038660205]
	TIME [epoch: 5.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663951041705895		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.05663951041705895 | validation: 0.2634489304206623]
	TIME [epoch: 5.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05620291249401582		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.05620291249401582 | validation: 0.2818402795268014]
	TIME [epoch: 5.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664416380613526		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.06664416380613526 | validation: 0.2628570791002245]
	TIME [epoch: 5.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07035811726019818		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07035811726019818 | validation: 0.3033328493031202]
	TIME [epoch: 5.91 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06026467534557134		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.06026467534557134 | validation: 0.24187527859694635]
	TIME [epoch: 5.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05690784188833673		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.05690784188833673 | validation: 0.3198605191943309]
	TIME [epoch: 5.87 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310162319258403		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.06310162319258403 | validation: 0.21754576118972763]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0814588414515135		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.0814588414515135 | validation: 0.35175992047827775]
	TIME [epoch: 5.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07239036385170972		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.07239036385170972 | validation: 0.25185119456300564]
	TIME [epoch: 5.91 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056130008118907296		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.056130008118907296 | validation: 0.26937359323395543]
	TIME [epoch: 5.91 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060814849644639346		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.060814849644639346 | validation: 0.2631735127053211]
	TIME [epoch: 5.92 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0547719959326045		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.0547719959326045 | validation: 0.2548376570507127]
	TIME [epoch: 5.92 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05325727060324704		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.05325727060324704 | validation: 0.3011602993533301]
	TIME [epoch: 5.92 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229444194633482		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.06229444194633482 | validation: 0.25039849981804024]
	TIME [epoch: 5.92 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06966949132694812		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.06966949132694812 | validation: 0.2526464432949988]
	TIME [epoch: 5.91 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05870398484272377		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05870398484272377 | validation: 0.257439732079953]
	TIME [epoch: 5.91 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056035729889439445		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.056035729889439445 | validation: 0.3109845101714895]
	TIME [epoch: 5.91 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804946565080666		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05804946565080666 | validation: 0.21674807385621092]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810640540984512		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.06810640540984512 | validation: 0.35658115702317156]
	TIME [epoch: 5.89 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916889681379036		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.06916889681379036 | validation: 0.24520573101646664]
	TIME [epoch: 5.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831958372388219		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05831958372388219 | validation: 0.28882107271586793]
	TIME [epoch: 5.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660758486794091		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05660758486794091 | validation: 0.22539295185710004]
	TIME [epoch: 5.92 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05471269452362354		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.05471269452362354 | validation: 0.2949014708537007]
	TIME [epoch: 5.92 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05647734562403883		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05647734562403883 | validation: 0.21972825295862375]
	TIME [epoch: 5.92 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537763687430517		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.06537763687430517 | validation: 0.3580018193557348]
	TIME [epoch: 5.91 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226679405923603		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07226679405923603 | validation: 0.2665865838182822]
	TIME [epoch: 5.92 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231802266341216		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05231802266341216 | validation: 0.23822726751150985]
	TIME [epoch: 5.92 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057180766669920315		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.057180766669920315 | validation: 0.3350009605099553]
	TIME [epoch: 5.92 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630456761875219		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.0630456761875219 | validation: 0.2316207864060021]
	TIME [epoch: 5.91 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455556233487726		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05455556233487726 | validation: 0.2632770601476951]
	TIME [epoch: 5.91 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618961513820531		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.05618961513820531 | validation: 0.35283459911573267]
	TIME [epoch: 5.92 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154129463882834		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.07154129463882834 | validation: 0.24591615130033184]
	TIME [epoch: 5.91 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409111311783075		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.06409111311783075 | validation: 0.27305193328800154]
	TIME [epoch: 5.92 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503193721764278		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.05503193721764278 | validation: 0.24815468604702307]
	TIME [epoch: 5.91 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07274174640548432		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07274174640548432 | validation: 0.31497123447232794]
	TIME [epoch: 5.91 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024511303118015		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.07024511303118015 | validation: 0.22022881813192763]
	TIME [epoch: 5.91 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06018949721815055		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.06018949721815055 | validation: 0.37462496138577756]
	TIME [epoch: 5.91 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07293663487555216		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.07293663487555216 | validation: 0.2432582264123712]
	TIME [epoch: 5.91 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056366810305664254		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.056366810305664254 | validation: 0.22215476117512215]
	TIME [epoch: 5.92 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052466432001809976		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.052466432001809976 | validation: 0.30540398393277396]
	TIME [epoch: 5.91 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410975283989906		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05410975283989906 | validation: 0.23205752238541769]
	TIME [epoch: 5.92 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05497981111561969		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.05497981111561969 | validation: 0.2726710928252103]
	TIME [epoch: 5.91 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209835440870813		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.05209835440870813 | validation: 0.23066188656222822]
	TIME [epoch: 5.92 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049378806772945955		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.049378806772945955 | validation: 0.24272888211768998]
	TIME [epoch: 5.91 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189032447559086		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05189032447559086 | validation: 0.26402951366118627]
	TIME [epoch: 5.92 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05450807992829502		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05450807992829502 | validation: 0.2890653898163295]
	TIME [epoch: 5.91 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06381725058472262		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.06381725058472262 | validation: 0.21513409139672698]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08174001735327789		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.08174001735327789 | validation: 0.28775461068143865]
	TIME [epoch: 5.87 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522273943444076		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.07522273943444076 | validation: 0.2794671718039548]
	TIME [epoch: 5.87 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563424251823791		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.0563424251823791 | validation: 0.21235865515723767]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748414353547165		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.0748414353547165 | validation: 0.3541847161128171]
	TIME [epoch: 5.92 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866190128523637		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.06866190128523637 | validation: 0.2481763543828877]
	TIME [epoch: 5.92 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05033888957330641		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.05033888957330641 | validation: 0.22926918973751143]
	TIME [epoch: 5.92 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397723317565006		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.05397723317565006 | validation: 0.26963574519502]
	TIME [epoch: 5.86 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05398821066033854		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.05398821066033854 | validation: 0.23945062282901458]
	TIME [epoch: 5.86 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06313121999471658		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06313121999471658 | validation: 0.35282528518421596]
	TIME [epoch: 5.91 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06815048991671964		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.06815048991671964 | validation: 0.25009969041625113]
	TIME [epoch: 5.91 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049605228624496046		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.049605228624496046 | validation: 0.2331033074811152]
	TIME [epoch: 5.92 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917752210759136		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05917752210759136 | validation: 0.27417797181643894]
	TIME [epoch: 5.91 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05638837274736558		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.05638837274736558 | validation: 0.2539115303209526]
	TIME [epoch: 5.92 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048351701863108644		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.048351701863108644 | validation: 0.22634004962538504]
	TIME [epoch: 5.91 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051504617990191764		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.051504617990191764 | validation: 0.24904870506500887]
	TIME [epoch: 5.91 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050991806262530914		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.050991806262530914 | validation: 0.2278367440130957]
	TIME [epoch: 5.89 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885874928909534		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.04885874928909534 | validation: 0.2519039787165614]
	TIME [epoch: 5.91 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05053208523357089		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.05053208523357089 | validation: 0.2356435157767731]
	TIME [epoch: 5.9 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050923218107623146		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.050923218107623146 | validation: 0.26016793256125503]
	TIME [epoch: 5.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05156470392996413		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.05156470392996413 | validation: 0.21259438583241907]
	TIME [epoch: 5.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051649449839654034		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.051649449839654034 | validation: 0.2588107091236174]
	TIME [epoch: 5.9 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05047741256528425		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05047741256528425 | validation: 0.2306548422271215]
	TIME [epoch: 5.89 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05049785162018942		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05049785162018942 | validation: 0.26176295252115445]
	TIME [epoch: 5.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895757309684649		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04895757309684649 | validation: 0.2202944568847056]
	TIME [epoch: 5.9 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05439147409118961		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.05439147409118961 | validation: 0.3697253667055307]
	TIME [epoch: 5.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086189214444		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.07086189214444 | validation: 0.21462102663290078]
	TIME [epoch: 5.9 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07120759459080132		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.07120759459080132 | validation: 0.3041970877408182]
	TIME [epoch: 5.91 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702734769827782		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.05702734769827782 | validation: 0.23212942803875156]
	TIME [epoch: 5.91 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050648049858386925		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.050648049858386925 | validation: 0.22216609898456038]
	TIME [epoch: 5.91 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059697030570038744		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.059697030570038744 | validation: 0.252743254021571]
	TIME [epoch: 5.87 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06952551110976654		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.06952551110976654 | validation: 0.23506139299770368]
	TIME [epoch: 5.88 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049935302530077656		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.049935302530077656 | validation: 0.23087084164139637]
	TIME [epoch: 5.88 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05376402871787665		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.05376402871787665 | validation: 0.24496378727338702]
	TIME [epoch: 5.87 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522082658584854		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.05522082658584854 | validation: 0.2241620492068438]
	TIME [epoch: 5.87 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04839528499038834		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.04839528499038834 | validation: 0.20588700568297105]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05116474570637675		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.05116474570637675 | validation: 0.28677932860380045]
	TIME [epoch: 5.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054015929628204125		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.054015929628204125 | validation: 0.209150070433151]
	TIME [epoch: 5.92 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04854302110852032		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.04854302110852032 | validation: 0.25598089132316065]
	TIME [epoch: 5.92 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047272257856177745		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.047272257856177745 | validation: 0.22723926739306144]
	TIME [epoch: 5.92 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04942194966501573		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.04942194966501573 | validation: 0.2633255542171489]
	TIME [epoch: 5.91 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570069594760411		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.05570069594760411 | validation: 0.20677964581031985]
	TIME [epoch: 5.92 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05160737933048609		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05160737933048609 | validation: 0.24421939516185312]
	TIME [epoch: 5.92 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050574054303055		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.050574054303055 | validation: 0.26361256260150573]
	TIME [epoch: 5.91 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046283636412855635		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.046283636412855635 | validation: 0.2083493643629964]
	TIME [epoch: 5.92 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056419607142534		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05056419607142534 | validation: 0.3560585180373959]
	TIME [epoch: 5.91 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827984156962134		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.06827984156962134 | validation: 0.21748552074719987]
	TIME [epoch: 5.91 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979109693849787		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.06979109693849787 | validation: 0.2997989426486174]
	TIME [epoch: 5.92 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056315327305018544		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.056315327305018544 | validation: 0.27149569560981385]
	TIME [epoch: 5.92 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185840723498659		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.05185840723498659 | validation: 0.2026398442017445]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061946835000975316		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.061946835000975316 | validation: 0.24301246723577696]
	TIME [epoch: 5.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054422560947051606		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.054422560947051606 | validation: 0.264584621466394]
	TIME [epoch: 5.89 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052629447301101195		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.052629447301101195 | validation: 0.21199179946082058]
	TIME [epoch: 5.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05081578721768297		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.05081578721768297 | validation: 0.2114233694024054]
	TIME [epoch: 5.89 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556697218688309		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.04556697218688309 | validation: 0.22613943623619656]
	TIME [epoch: 5.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04420446214371996		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.04420446214371996 | validation: 0.21649949046959255]
	TIME [epoch: 5.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049053759228607476		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.049053759228607476 | validation: 0.21569361482975596]
	TIME [epoch: 5.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04494767470202645		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04494767470202645 | validation: 0.21845331526981707]
	TIME [epoch: 5.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045454978507702165		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.045454978507702165 | validation: 0.2342585388240758]
	TIME [epoch: 5.89 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047524444111818054		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.047524444111818054 | validation: 0.24022873077843437]
	TIME [epoch: 5.89 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05334850782757167		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.05334850782757167 | validation: 0.2079150866840721]
	TIME [epoch: 5.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051561762222537465		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.051561762222537465 | validation: 0.2399315651669911]
	TIME [epoch: 5.89 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046028533859737444		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.046028533859737444 | validation: 0.22773041126279595]
	TIME [epoch: 5.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732544990339904		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04732544990339904 | validation: 0.23027431855615796]
	TIME [epoch: 5.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04906767091390308		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04906767091390308 | validation: 0.22776186116871389]
	TIME [epoch: 5.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487009120380662		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.0487009120380662 | validation: 0.22462844943349927]
	TIME [epoch: 5.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044318067979125464		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.044318067979125464 | validation: 0.21595910262224713]
	TIME [epoch: 5.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873063851556241		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.04873063851556241 | validation: 0.209756858875522]
	TIME [epoch: 5.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048362223916082935		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.048362223916082935 | validation: 0.26045180792152756]
	TIME [epoch: 5.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0476620833669605		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.0476620833669605 | validation: 0.18782576897304792]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04737109614242965		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.04737109614242965 | validation: 0.26039779468465385]
	TIME [epoch: 5.91 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05427667094974327		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.05427667094974327 | validation: 0.18567733057439847]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115366475434215		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.05115366475434215 | validation: 0.24670132707787956]
	TIME [epoch: 5.89 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107650286550769		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.05107650286550769 | validation: 0.18971889589227758]
	TIME [epoch: 5.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0477777209637466		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.0477777209637466 | validation: 0.22658051305520674]
	TIME [epoch: 5.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750268574378838		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04750268574378838 | validation: 0.17698987346122974]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04536396870793169		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.04536396870793169 | validation: 0.2362295902945226]
	TIME [epoch: 5.89 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050892322830358924		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.050892322830358924 | validation: 0.2216986010736034]
	TIME [epoch: 5.89 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803766667660691		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.05803766667660691 | validation: 0.20249828853342566]
	TIME [epoch: 5.89 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599042154206945		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.04599042154206945 | validation: 0.24769808592029557]
	TIME [epoch: 5.89 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287405564235341		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.05287405564235341 | validation: 0.21020617974963135]
	TIME [epoch: 5.89 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042437147583501054		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.042437147583501054 | validation: 0.22285401577793304]
	TIME [epoch: 5.89 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04457249153879514		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.04457249153879514 | validation: 0.20514550834383588]
	TIME [epoch: 5.89 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045062859194499344		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.045062859194499344 | validation: 0.262366850701603]
	TIME [epoch: 5.89 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06114797968024595		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.06114797968024595 | validation: 0.18960947995183106]
	TIME [epoch: 5.89 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06194326937230171		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.06194326937230171 | validation: 0.2541941992698856]
	TIME [epoch: 5.9 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04478666015548722		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04478666015548722 | validation: 0.20082111200750916]
	TIME [epoch: 5.9 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731184162928915		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.04731184162928915 | validation: 0.21457991835452433]
	TIME [epoch: 5.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045617564730957794		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.045617564730957794 | validation: 0.21259799948497024]
	TIME [epoch: 5.89 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04296085410223492		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.04296085410223492 | validation: 0.223179439732507]
	TIME [epoch: 5.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044821537250979836		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.044821537250979836 | validation: 0.22506549444342816]
	TIME [epoch: 5.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043137186053077994		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.043137186053077994 | validation: 0.21984119128885626]
	TIME [epoch: 5.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042746849451810266		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.042746849451810266 | validation: 0.1931267032434515]
	TIME [epoch: 5.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04711311784849773		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.04711311784849773 | validation: 0.2534825143969767]
	TIME [epoch: 5.89 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142134933457074		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.05142134933457074 | validation: 0.1908853582575339]
	TIME [epoch: 5.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638605019978964		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.04638605019978964 | validation: 0.2457985390439036]
	TIME [epoch: 5.89 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059108185634517384		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.059108185634517384 | validation: 0.19683289818277885]
	TIME [epoch: 5.89 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327348883535235		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.04327348883535235 | validation: 0.2199882241232068]
	TIME [epoch: 5.89 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147190911474686		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.04147190911474686 | validation: 0.22036907591362453]
	TIME [epoch: 5.89 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04337918132989648		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.04337918132989648 | validation: 0.19171242937724475]
	TIME [epoch: 5.89 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04808258631067361		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04808258631067361 | validation: 0.2137110896979614]
	TIME [epoch: 5.89 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05469308817628716		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.05469308817628716 | validation: 0.21281027508784175]
	TIME [epoch: 5.89 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678236305622608		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04678236305622608 | validation: 0.2298012006084288]
	TIME [epoch: 5.86 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04371362498358752		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.04371362498358752 | validation: 0.19560994645010873]
	TIME [epoch: 5.86 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0417968523336911		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.0417968523336911 | validation: 0.22465675639426488]
	TIME [epoch: 5.86 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346104851482324		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.04346104851482324 | validation: 0.1969331195649022]
	TIME [epoch: 5.87 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052811038349162856		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.052811038349162856 | validation: 0.24124799085084733]
	TIME [epoch: 5.86 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783325530904151		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.05783325530904151 | validation: 0.1964347276637715]
	TIME [epoch: 5.86 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696688580277792		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04696688580277792 | validation: 0.21019312463664025]
	TIME [epoch: 5.87 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042620980038870736		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.042620980038870736 | validation: 0.21470377230581464]
	TIME [epoch: 5.86 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04310258419604088		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.04310258419604088 | validation: 0.20150760713893803]
	TIME [epoch: 5.86 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859619172110535		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.04859619172110535 | validation: 0.29488040878483773]
	TIME [epoch: 5.86 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208415800454358		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.05208415800454358 | validation: 0.1966133938429608]
	TIME [epoch: 5.87 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04101784375230898		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04101784375230898 | validation: 0.18001031942528395]
	TIME [epoch: 5.86 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800515982046727		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.04800515982046727 | validation: 0.29166517960012056]
	TIME [epoch: 5.86 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04987907458249691		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.04987907458249691 | validation: 0.1872374229701368]
	TIME [epoch: 5.86 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616959989108457		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.04616959989108457 | validation: 0.20110657742698265]
	TIME [epoch: 5.86 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046145419856328		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.04046145419856328 | validation: 0.22594250249192105]
	TIME [epoch: 5.85 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434197803624724		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.0434197803624724 | validation: 0.19312127299743365]
	TIME [epoch: 5.86 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04012550618933886		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04012550618933886 | validation: 0.18923039784852413]
	TIME [epoch: 5.86 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556055615065347		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04556055615065347 | validation: 0.2097325638697917]
	TIME [epoch: 5.87 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04252120161312199		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.04252120161312199 | validation: 0.21280529837426362]
	TIME [epoch: 5.86 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042605632893913725		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.042605632893913725 | validation: 0.19203897857151722]
	TIME [epoch: 5.87 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631318558521985		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.04631318558521985 | validation: 0.2212504189648847]
	TIME [epoch: 5.87 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344165447386764		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.04344165447386764 | validation: 0.18622368483748886]
	TIME [epoch: 5.87 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043047069268575794		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.043047069268575794 | validation: 0.24699438276519858]
	TIME [epoch: 5.87 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04806740624045712		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.04806740624045712 | validation: 0.16971498641206986]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372288249590991		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.04372288249590991 | validation: 0.2393179650919134]
	TIME [epoch: 5.87 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04295602583746854		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.04295602583746854 | validation: 0.19780437218361466]
	TIME [epoch: 5.87 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041290679861681875		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.041290679861681875 | validation: 0.2113446553640467]
	TIME [epoch: 5.88 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04385200505690651		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.04385200505690651 | validation: 0.18354070937919573]
	TIME [epoch: 5.87 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04415477614911791		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.04415477614911791 | validation: 0.25452577954779615]
	TIME [epoch: 5.87 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04749940895589155		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.04749940895589155 | validation: 0.1759656134284464]
	TIME [epoch: 5.86 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043060348867563915		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.043060348867563915 | validation: 0.22151433498917028]
	TIME [epoch: 5.87 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04902037212691287		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04902037212691287 | validation: 0.2017611298252674]
	TIME [epoch: 5.87 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04070613899727963		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04070613899727963 | validation: 0.19788993139949135]
	TIME [epoch: 5.87 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04071894590510363		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04071894590510363 | validation: 0.20536061081039592]
	TIME [epoch: 5.87 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04317734914952788		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04317734914952788 | validation: 0.1913261194677952]
	TIME [epoch: 5.88 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04515383962635609		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.04515383962635609 | validation: 0.2137615790619643]
	TIME [epoch: 5.87 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115817302484302		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05115817302484302 | validation: 0.2041004221147964]
	TIME [epoch: 5.88 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395329870720016		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0395329870720016 | validation: 0.19567833193577383]
	TIME [epoch: 5.88 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084870798233973		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.04084870798233973 | validation: 0.20799694847926237]
	TIME [epoch: 5.88 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043525272622800336		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.043525272622800336 | validation: 0.19750219211971878]
	TIME [epoch: 5.87 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945044477285218		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03945044477285218 | validation: 0.17785070465722463]
	TIME [epoch: 5.87 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045408386932994496		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.045408386932994496 | validation: 0.25760978761338577]
	TIME [epoch: 5.86 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04939853932010649		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04939853932010649 | validation: 0.18780914808835633]
	TIME [epoch: 5.87 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530068359870662		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.04530068359870662 | validation: 0.21843621179296835]
	TIME [epoch: 5.88 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04207695917114637		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.04207695917114637 | validation: 0.19211123451712317]
	TIME [epoch: 5.88 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039618349521336325		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.039618349521336325 | validation: 0.18235658930152657]
	TIME [epoch: 5.87 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043426658191659195		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.043426658191659195 | validation: 0.24423896092790376]
	TIME [epoch: 5.87 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04495125913234636		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.04495125913234636 | validation: 0.1949039225246945]
	TIME [epoch: 5.87 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881010281807984		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03881010281807984 | validation: 0.215902345063544]
	TIME [epoch: 5.87 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559634736660084		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.04559634736660084 | validation: 0.18894207355706405]
	TIME [epoch: 5.87 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040010932043858416		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.040010932043858416 | validation: 0.1826089410947259]
	TIME [epoch: 5.87 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859425562751581		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03859425562751581 | validation: 0.18683849232366523]
	TIME [epoch: 5.87 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498402034444388		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03498402034444388 | validation: 0.18660700721542733]
	TIME [epoch: 5.87 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099620725955986		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.05099620725955986 | validation: 0.22302895280215762]
	TIME [epoch: 5.87 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554441863339645		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0554441863339645 | validation: 0.19994193983895903]
	TIME [epoch: 5.87 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04011519614634405		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04011519614634405 | validation: 0.16207651035341938]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04632680276584827		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.04632680276584827 | validation: 0.2251227400839203]
	TIME [epoch: 5.86 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042656018123837756		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.042656018123837756 | validation: 0.19615054388776487]
	TIME [epoch: 5.87 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888484829954059		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03888484829954059 | validation: 0.19163141194170386]
	TIME [epoch: 5.86 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038036165589169495		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.038036165589169495 | validation: 0.18714723484765575]
	TIME [epoch: 5.87 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04025249951351001		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.04025249951351001 | validation: 0.22327445179000552]
	TIME [epoch: 5.86 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040799473820822		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.040799473820822 | validation: 0.16730171702290192]
	TIME [epoch: 5.87 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421226047581569		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.04421226047581569 | validation: 0.1970350696786971]
	TIME [epoch: 5.86 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03976690184479283		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03976690184479283 | validation: 0.2511797724803881]
	TIME [epoch: 5.87 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421371761686212		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.04421371761686212 | validation: 0.16811029178049944]
	TIME [epoch: 5.87 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04121110949296356		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.04121110949296356 | validation: 0.1991129059013984]
	TIME [epoch: 5.87 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04053840414285477		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.04053840414285477 | validation: 0.19221428030003182]
	TIME [epoch: 5.86 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04108535953284477		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.04108535953284477 | validation: 0.17828558604043662]
	TIME [epoch: 5.87 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035861789949838574		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.035861789949838574 | validation: 0.1868533618639423]
	TIME [epoch: 5.86 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03903786969740465		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03903786969740465 | validation: 0.19918806351099835]
	TIME [epoch: 5.87 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04601407053592559		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.04601407053592559 | validation: 0.19141785536632946]
	TIME [epoch: 5.86 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039660147036599896		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.039660147036599896 | validation: 0.18811345086966363]
	TIME [epoch: 5.87 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037226403435714046		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.037226403435714046 | validation: 0.21109395559938787]
	TIME [epoch: 5.86 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042969618822979706		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.042969618822979706 | validation: 0.18630046915317436]
	TIME [epoch: 5.87 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03849139071806001		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03849139071806001 | validation: 0.17030631603145663]
	TIME [epoch: 5.86 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038859052164351995		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.038859052164351995 | validation: 0.21730719117042288]
	TIME [epoch: 5.87 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691841561185574		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.04691841561185574 | validation: 0.19322263704478612]
	TIME [epoch: 5.86 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037423531819982564		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.037423531819982564 | validation: 0.18264506288007404]
	TIME [epoch: 5.86 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03908309738820776		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.03908309738820776 | validation: 0.21706137748527976]
	TIME [epoch: 5.85 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864634030699211		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.03864634030699211 | validation: 0.16012022906138262]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03924340542387189		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03924340542387189 | validation: 0.20531625615089188]
	TIME [epoch: 5.86 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037195519038285954		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.037195519038285954 | validation: 0.17544069595142653]
	TIME [epoch: 5.86 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03922859843839642		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03922859843839642 | validation: 0.19800391277899598]
	TIME [epoch: 5.86 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04081340577737119		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.04081340577737119 | validation: 0.2033664681019187]
	TIME [epoch: 5.86 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04489093084167041		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.04489093084167041 | validation: 0.17772569407617575]
	TIME [epoch: 5.86 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036969563686881805		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.036969563686881805 | validation: 0.15900725089434964]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_959.pth
	Model improved!!!
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03948338848673062		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.03948338848673062 | validation: 0.19494419249912529]
	TIME [epoch: 5.86 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038006489462165664		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.038006489462165664 | validation: 0.17814535431260675]
	TIME [epoch: 5.85 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035505837844326156		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.035505837844326156 | validation: 0.22597450518249365]
	TIME [epoch: 5.86 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039673876443939146		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.039673876443939146 | validation: 0.1739479063084025]
	TIME [epoch: 5.87 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039212183248812586		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.039212183248812586 | validation: 0.19447618861161156]
	TIME [epoch: 5.86 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037206410966168744		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.037206410966168744 | validation: 0.20436368573351948]
	TIME [epoch: 5.86 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039461090536733266		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.039461090536733266 | validation: 0.18002342465450763]
	TIME [epoch: 5.86 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040835091511953456		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.040835091511953456 | validation: 0.24470679148659702]
	TIME [epoch: 5.86 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043556240067032674		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.043556240067032674 | validation: 0.17122915631883895]
	TIME [epoch: 5.86 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0385909266560574		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.0385909266560574 | validation: 0.17120049400137283]
	TIME [epoch: 5.86 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692793941401258		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.03692793941401258 | validation: 0.17547182046434942]
	TIME [epoch: 5.85 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043031762094761845		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.043031762094761845 | validation: 0.2157654276434478]
	TIME [epoch: 5.86 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040586485865977906		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.040586485865977906 | validation: 0.1863774693664827]
	TIME [epoch: 5.86 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04100757338948321		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.04100757338948321 | validation: 0.18293434110256254]
	TIME [epoch: 5.86 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044091644396135735		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.044091644396135735 | validation: 0.20636519366209927]
	TIME [epoch: 5.86 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038210136875807786		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.038210136875807786 | validation: 0.18826814370228312]
	TIME [epoch: 5.86 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03543281650048977		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03543281650048977 | validation: 0.15377558037079153]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653024964603311		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.03653024964603311 | validation: 0.1796275531401269]
	TIME [epoch: 5.86 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03691815132531917		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03691815132531917 | validation: 0.18533548845014547]
	TIME [epoch: 5.86 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362828648304175		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0362828648304175 | validation: 0.2111625078962466]
	TIME [epoch: 5.86 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03838780345683284		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.03838780345683284 | validation: 0.1674858719595283]
	TIME [epoch: 5.87 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03986258276297517		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.03986258276297517 | validation: 0.18288495028664864]
	TIME [epoch: 5.86 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03752730004162733		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03752730004162733 | validation: 0.16317567550912424]
	TIME [epoch: 5.87 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03835550354797554		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03835550354797554 | validation: 0.17022548464660703]
	TIME [epoch: 5.86 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623027390818552		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03623027390818552 | validation: 0.1656223102024073]
	TIME [epoch: 5.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037303220063184137		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.037303220063184137 | validation: 0.18443497931253747]
	TIME [epoch: 5.86 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03467139836649377		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03467139836649377 | validation: 0.17266956459201177]
	TIME [epoch: 5.86 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039341054136025275		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.039341054136025275 | validation: 0.18897535693823464]
	TIME [epoch: 5.86 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04161345211329085		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.04161345211329085 | validation: 0.17311284328869908]
	TIME [epoch: 5.86 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996013723666277		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.03996013723666277 | validation: 0.19113152859665095]
	TIME [epoch: 5.86 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03534847217857344		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03534847217857344 | validation: 0.17200930399201747]
	TIME [epoch: 5.87 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037411077759503274		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.037411077759503274 | validation: 0.19552611853664525]
	TIME [epoch: 5.87 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850256121321693		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03850256121321693 | validation: 0.16723546330726202]
	TIME [epoch: 5.87 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040234914238649305		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.040234914238649305 | validation: 0.21470008222828574]
	TIME [epoch: 5.86 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03718254262088583		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.03718254262088583 | validation: 0.15634671887833065]
	TIME [epoch: 5.87 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034710536515051696		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.034710536515051696 | validation: 0.15368791625760841]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914989125013512		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.03914989125013512 | validation: 0.25238320899073996]
	TIME [epoch: 5.86 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042203474171197		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.04042203474171197 | validation: 0.2000995494545987]
	TIME [epoch: 5.86 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09477115892301413		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.09477115892301413 | validation: 0.14650863444544812]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_998.pth
	Model improved!!!
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049314799160258015		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.049314799160258015 | validation: 0.17877946845879686]
	TIME [epoch: 5.86 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0433701472085696		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0433701472085696 | validation: 0.19854353702091806]
	TIME [epoch: 5.87 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06253181205753564		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.06253181205753564 | validation: 0.2020160251671036]
	TIME [epoch: 195 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037666014444765986		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.037666014444765986 | validation: 0.16611781943789106]
	TIME [epoch: 12.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04215338889566916		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.04215338889566916 | validation: 0.16262339336178108]
	TIME [epoch: 12.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574347868312514		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03574347868312514 | validation: 0.1868449735980475]
	TIME [epoch: 12.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693755214784202		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.03693755214784202 | validation: 0.1942351395935377]
	TIME [epoch: 12.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668794605499578		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.03668794605499578 | validation: 0.1663875656999216]
	TIME [epoch: 12.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03496660052492567		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.03496660052492567 | validation: 0.16913751018142908]
	TIME [epoch: 12.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03598055833949057		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.03598055833949057 | validation: 0.19096338033167745]
	TIME [epoch: 12.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743937963044883		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.03743937963044883 | validation: 0.18181210372989054]
	TIME [epoch: 12.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035015177494090256		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.035015177494090256 | validation: 0.18466980913637898]
	TIME [epoch: 12.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0359364672178404		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0359364672178404 | validation: 0.18844026704445827]
	TIME [epoch: 12.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03654470633929627		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03654470633929627 | validation: 0.1519407618165065]
	TIME [epoch: 12.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037808275676273595		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.037808275676273595 | validation: 0.18404920777597775]
	TIME [epoch: 12.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554252396982954		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03554252396982954 | validation: 0.17707970166785497]
	TIME [epoch: 12.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03713781764096571		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.03713781764096571 | validation: 0.22495668869087898]
	TIME [epoch: 12.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04022544447211354		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.04022544447211354 | validation: 0.17483088321545515]
	TIME [epoch: 12.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03453263603946003		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03453263603946003 | validation: 0.15434463841612323]
	TIME [epoch: 12.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037776666315341316		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.037776666315341316 | validation: 0.21756592579644718]
	TIME [epoch: 12.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057456335209228		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.04057456335209228 | validation: 0.17769116886044542]
	TIME [epoch: 12.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035325334295791454		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.035325334295791454 | validation: 0.16913328562806107]
	TIME [epoch: 12.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743639546101301		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.03743639546101301 | validation: 0.20092949342534344]
	TIME [epoch: 12.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558186715281693		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03558186715281693 | validation: 0.20418182425268167]
	TIME [epoch: 12.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036719004997295725		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.036719004997295725 | validation: 0.16680720877581484]
	TIME [epoch: 12.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03659766046969294		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.03659766046969294 | validation: 0.17691122501208761]
	TIME [epoch: 12.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360254144932745		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.0360254144932745 | validation: 0.16824050231800305]
	TIME [epoch: 12.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035550824661797015		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.035550824661797015 | validation: 0.18117176218255926]
	TIME [epoch: 12.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03371456516728882		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.03371456516728882 | validation: 0.15860224947943885]
	TIME [epoch: 12.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03524366695096965		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.03524366695096965 | validation: 0.1767745412130279]
	TIME [epoch: 12.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04012185064839865		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.04012185064839865 | validation: 0.16657975625218724]
	TIME [epoch: 12.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03661427432276994		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03661427432276994 | validation: 0.17013563264398723]
	TIME [epoch: 12.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033761267114422955		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.033761267114422955 | validation: 0.17806558552634622]
	TIME [epoch: 12.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038263803550642676		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.038263803550642676 | validation: 0.17854404858652495]
	TIME [epoch: 12.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03452783452392934		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.03452783452392934 | validation: 0.16531395624456854]
	TIME [epoch: 12.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621190685138784		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03621190685138784 | validation: 0.17938877183744759]
	TIME [epoch: 12.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692316341377297		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.03692316341377297 | validation: 0.17127699767566842]
	TIME [epoch: 12.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670496049165346		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.04670496049165346 | validation: 0.18133692384899558]
	TIME [epoch: 12.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576614472385237		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.03576614472385237 | validation: 0.1894202615981742]
	TIME [epoch: 12.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602837962633519		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03602837962633519 | validation: 0.16859254964009485]
	TIME [epoch: 12.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0328532961751724		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.0328532961751724 | validation: 0.16502992932211047]
	TIME [epoch: 12.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032672984340728996		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.032672984340728996 | validation: 0.18268884330590987]
	TIME [epoch: 12.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034809783211230794		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.034809783211230794 | validation: 0.18837558873071744]
	TIME [epoch: 12.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416918767007933		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.03416918767007933 | validation: 0.16547192044470727]
	TIME [epoch: 12.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036398102857715636		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.036398102857715636 | validation: 0.16830396544381318]
	TIME [epoch: 12.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602096971199945		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.03602096971199945 | validation: 0.17939307924043524]
	TIME [epoch: 12.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03362139122583301		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.03362139122583301 | validation: 0.15452629261948833]
	TIME [epoch: 12.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03493868278344388		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03493868278344388 | validation: 0.19643481978250038]
	TIME [epoch: 12.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873665622322304		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.03873665622322304 | validation: 0.17585047631697878]
	TIME [epoch: 12.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03439437931607804		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.03439437931607804 | validation: 0.14458627229121565]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03423442288171485		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.03423442288171485 | validation: 0.1668163457260936]
	TIME [epoch: 12.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03354927034923193		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.03354927034923193 | validation: 0.1890061740495473]
	TIME [epoch: 12.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03295836104069265		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.03295836104069265 | validation: 0.1676639992984907]
	TIME [epoch: 12.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03530747885090502		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.03530747885090502 | validation: 0.1788313799753348]
	TIME [epoch: 12.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033985988845695154		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.033985988845695154 | validation: 0.19477677869502974]
	TIME [epoch: 12.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032294595241739096		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.032294595241739096 | validation: 0.16596135161082454]
	TIME [epoch: 12.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034075854184171		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.034075854184171 | validation: 0.1559379330887392]
	TIME [epoch: 12.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035853413899501745		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.035853413899501745 | validation: 0.174309232433295]
	TIME [epoch: 12.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342746243621924		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.0342746243621924 | validation: 0.18329126226482095]
	TIME [epoch: 12.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253978983064786		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03253978983064786 | validation: 0.14246402165083363]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03286047158445443		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.03286047158445443 | validation: 0.16694532518787253]
	TIME [epoch: 12.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031344076202332344		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.031344076202332344 | validation: 0.1514871688551679]
	TIME [epoch: 13.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033131018622357414		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.033131018622357414 | validation: 0.1549516216379132]
	TIME [epoch: 12.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033099543444992004		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.033099543444992004 | validation: 0.16115798307348625]
	TIME [epoch: 12.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03305347982317461		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.03305347982317461 | validation: 0.1848184227713411]
	TIME [epoch: 12.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214036707552517		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.03214036707552517 | validation: 0.18050601695139748]
	TIME [epoch: 12.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031005170011840662		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.031005170011840662 | validation: 0.16968042362790584]
	TIME [epoch: 12.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03269327479952942		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.03269327479952942 | validation: 0.16030332881770773]
	TIME [epoch: 12.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364538910283528		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.03364538910283528 | validation: 0.17033526992801842]
	TIME [epoch: 12.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03137695761693923		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.03137695761693923 | validation: 0.1618517479666448]
	TIME [epoch: 12.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032553216659180276		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.032553216659180276 | validation: 0.19483163531403955]
	TIME [epoch: 12.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399069934086037		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.03399069934086037 | validation: 0.2143412373378927]
	TIME [epoch: 12.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497355830487641		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.03497355830487641 | validation: 0.16550069240059614]
	TIME [epoch: 12.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04038690026124723		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.04038690026124723 | validation: 0.193752327321715]
	TIME [epoch: 12.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350525616580509		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.0350525616580509 | validation: 0.15749852559270444]
	TIME [epoch: 12.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326799724067192		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.0326799724067192 | validation: 0.15751815247249895]
	TIME [epoch: 12.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307029439397146		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.03307029439397146 | validation: 0.16247315114027527]
	TIME [epoch: 12.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04165421532413562		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.04165421532413562 | validation: 0.17402255662997224]
	TIME [epoch: 12.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037432609116996175		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.037432609116996175 | validation: 0.18900401099228747]
	TIME [epoch: 12.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03103786610485657		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03103786610485657 | validation: 0.1517430997585165]
	TIME [epoch: 12.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03326519583506636		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.03326519583506636 | validation: 0.1790000553343067]
	TIME [epoch: 12.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331250223456439		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.03331250223456439 | validation: 0.16605794831130283]
	TIME [epoch: 12.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0337632497467205		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.0337632497467205 | validation: 0.1467671380165275]
	TIME [epoch: 12.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03390585774394219		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.03390585774394219 | validation: 0.1596796006641238]
	TIME [epoch: 12.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03522806019636018		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.03522806019636018 | validation: 0.1437625447764066]
	TIME [epoch: 12.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031857664052060086		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.031857664052060086 | validation: 0.15648536247239847]
	TIME [epoch: 12.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359688927244219		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.03359688927244219 | validation: 0.17768706730764836]
	TIME [epoch: 12.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603050239511945		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.03603050239511945 | validation: 0.17104809769141316]
	TIME [epoch: 12.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032551150569605704		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.032551150569605704 | validation: 0.15307963054687532]
	TIME [epoch: 12.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03444404277644345		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.03444404277644345 | validation: 0.1525117965625559]
	TIME [epoch: 12.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030507262365372618		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.030507262365372618 | validation: 0.1739289476738766]
	TIME [epoch: 12.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347821903415998		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04347821903415998 | validation: 0.15596582080433408]
	TIME [epoch: 12.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593509146982808		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.03593509146982808 | validation: 0.15939454334597147]
	TIME [epoch: 12.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032014411143504756		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.032014411143504756 | validation: 0.17201630076038968]
	TIME [epoch: 12.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03354946341500792		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.03354946341500792 | validation: 0.16471373012667215]
	TIME [epoch: 12.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034453976047440375		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.034453976047440375 | validation: 0.16770935478315924]
	TIME [epoch: 12.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220288232311218		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.03220288232311218 | validation: 0.16157604002458237]
	TIME [epoch: 12.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03481078965885396		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.03481078965885396 | validation: 0.15310579330601692]
	TIME [epoch: 12.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509959462263102		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.03509959462263102 | validation: 0.1731826736420904]
	TIME [epoch: 12.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03281252246461854		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.03281252246461854 | validation: 0.16384778539442724]
	TIME [epoch: 12.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032685170867685936		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.032685170867685936 | validation: 0.15220208678859662]
	TIME [epoch: 12.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031308300484446334		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.031308300484446334 | validation: 0.17817357597339664]
	TIME [epoch: 12.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031162147394697266		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.031162147394697266 | validation: 0.1782033974865976]
	TIME [epoch: 12.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032712275033331205		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.032712275033331205 | validation: 0.15577171854231603]
	TIME [epoch: 12.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02989381099907927		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.02989381099907927 | validation: 0.14840947851091993]
	TIME [epoch: 12.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0325032456700399		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.0325032456700399 | validation: 0.1658307981183708]
	TIME [epoch: 12.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584242788580514		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.03584242788580514 | validation: 0.14980239372644394]
	TIME [epoch: 12.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241881880150991		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.03241881880150991 | validation: 0.16757256497274114]
	TIME [epoch: 12.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191788928025649		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.03191788928025649 | validation: 0.15801000685151131]
	TIME [epoch: 12.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031114612753543422		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.031114612753543422 | validation: 0.14667731153873817]
	TIME [epoch: 12.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03233309527970642		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.03233309527970642 | validation: 0.1914822367053838]
	TIME [epoch: 12.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033410774460362974		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.033410774460362974 | validation: 0.15430579444386683]
	TIME [epoch: 12.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030899704291406725		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.030899704291406725 | validation: 0.15374129096979927]
	TIME [epoch: 12.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326677158362914		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.0326677158362914 | validation: 0.18715389784391306]
	TIME [epoch: 12.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509591648290391		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.03509591648290391 | validation: 0.14146603356374646]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1113.pth
	Model improved!!!
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031248800324214933		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.031248800324214933 | validation: 0.15641540962721345]
	TIME [epoch: 12.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029666979944254886		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.029666979944254886 | validation: 0.16010145809568593]
	TIME [epoch: 12.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030057986621765745		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.030057986621765745 | validation: 0.17219516944173544]
	TIME [epoch: 12.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460784153242843		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.03460784153242843 | validation: 0.1578571339188842]
	TIME [epoch: 12.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03100389137830331		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.03100389137830331 | validation: 0.15892517590212807]
	TIME [epoch: 12.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334821807967312		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.0334821807967312 | validation: 0.16041954997083022]
	TIME [epoch: 12.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03423085412166981		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.03423085412166981 | validation: 0.15990410796340765]
	TIME [epoch: 12.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030177254903698478		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.030177254903698478 | validation: 0.15651773312418632]
	TIME [epoch: 12.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448188828439652		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.03448188828439652 | validation: 0.16016665432242827]
	TIME [epoch: 12.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032313591099292605		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.032313591099292605 | validation: 0.164239403962973]
	TIME [epoch: 12.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031913424745380364		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.031913424745380364 | validation: 0.159310885195662]
	TIME [epoch: 12.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030989416026072237		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.030989416026072237 | validation: 0.14887283185282052]
	TIME [epoch: 12.4 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03092354563126397		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.03092354563126397 | validation: 0.1829878735278706]
	TIME [epoch: 12.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030590474470124383		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.030590474470124383 | validation: 0.15461970699219477]
	TIME [epoch: 12.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03097146445673862		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.03097146445673862 | validation: 0.1680937509490289]
	TIME [epoch: 12.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193152291227502		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.03193152291227502 | validation: 0.16025982844314074]
	TIME [epoch: 12.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03188296802205512		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.03188296802205512 | validation: 0.15836418645819808]
	TIME [epoch: 12.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030624865412112293		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.030624865412112293 | validation: 0.1485639744991197]
	TIME [epoch: 12.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254572513270921		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.03254572513270921 | validation: 0.1654708961788605]
	TIME [epoch: 12.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032170326299334084		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.032170326299334084 | validation: 0.16117935953759716]
	TIME [epoch: 12.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030510671539778624		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.030510671539778624 | validation: 0.1633205413278368]
	TIME [epoch: 12.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230490431097531		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.03230490431097531 | validation: 0.2003489241195875]
	TIME [epoch: 12.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429624054564339		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.03429624054564339 | validation: 0.1629122748680819]
	TIME [epoch: 12.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02872229969608129		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.02872229969608129 | validation: 0.13635560165915903]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1137.pth
	Model improved!!!
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032213639583401246		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.032213639583401246 | validation: 0.14571355658847587]
	TIME [epoch: 12.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029358403942198446		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.029358403942198446 | validation: 0.15226196817173385]
	TIME [epoch: 12.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032541685269312406		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.032541685269312406 | validation: 0.17283489392336293]
	TIME [epoch: 12.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007478700822265		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.04007478700822265 | validation: 0.16504010606207006]
	TIME [epoch: 12.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03058795686690777		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.03058795686690777 | validation: 0.14293744648032156]
	TIME [epoch: 12.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400381651229409		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.0400381651229409 | validation: 0.16123004977076413]
	TIME [epoch: 12.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029202398156262736		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.029202398156262736 | validation: 0.16013146921644594]
	TIME [epoch: 12.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03182824315613457		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.03182824315613457 | validation: 0.14838351619178883]
	TIME [epoch: 12.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029964958564856065		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.029964958564856065 | validation: 0.15358132596135227]
	TIME [epoch: 12.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03153969904882431		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.03153969904882431 | validation: 0.14809312503473282]
	TIME [epoch: 12.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029328114072883098		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.029328114072883098 | validation: 0.15430190763456084]
	TIME [epoch: 12.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030343674909938407		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.030343674909938407 | validation: 0.1541927842590122]
	TIME [epoch: 12.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03510738801552386		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.03510738801552386 | validation: 0.1356261411775743]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1150.pth
	Model improved!!!
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0323891448018415		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.0323891448018415 | validation: 0.14913418469358075]
	TIME [epoch: 12.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532818138328882		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.03532818138328882 | validation: 0.15823210152028486]
	TIME [epoch: 12.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029049750699639106		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.029049750699639106 | validation: 0.15954779763489788]
	TIME [epoch: 12.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0289266925056858		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.0289266925056858 | validation: 0.13928230640395875]
	TIME [epoch: 12.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02954101233682894		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.02954101233682894 | validation: 0.17712468646735782]
	TIME [epoch: 12.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151577248242834		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.03151577248242834 | validation: 0.17334544167086258]
	TIME [epoch: 12.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239295638711691		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.05239295638711691 | validation: 0.16826582856915134]
	TIME [epoch: 12.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04308103665957309		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.04308103665957309 | validation: 0.1483353149387554]
	TIME [epoch: 12.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030616739509305508		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.030616739509305508 | validation: 0.1472482692301063]
	TIME [epoch: 12.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352670451380556		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.03352670451380556 | validation: 0.14732199949048455]
	TIME [epoch: 12.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03104424504537868		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.03104424504537868 | validation: 0.156987438339244]
	TIME [epoch: 12.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031083764010010652		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.031083764010010652 | validation: 0.14530171300455272]
	TIME [epoch: 12.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032792117251417424		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.032792117251417424 | validation: 0.1422859135268541]
	TIME [epoch: 12.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0303428655523035		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.0303428655523035 | validation: 0.1580367815832925]
	TIME [epoch: 12.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029605733498577404		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.029605733498577404 | validation: 0.1605256682217593]
	TIME [epoch: 12.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031200260546984257		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.031200260546984257 | validation: 0.15305939953004433]
	TIME [epoch: 12.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03269275104588053		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.03269275104588053 | validation: 0.14665621668630166]
	TIME [epoch: 12.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142915815628431		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.03142915815628431 | validation: 0.15847769641776682]
	TIME [epoch: 12.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02969241398780951		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.02969241398780951 | validation: 0.1570428430235778]
	TIME [epoch: 12.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162888389233122		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.03162888389233122 | validation: 0.13999555087013224]
	TIME [epoch: 12.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028499700468527116		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.028499700468527116 | validation: 0.1507051214326248]
	TIME [epoch: 12.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030460353497903502		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.030460353497903502 | validation: 0.14539532645736722]
	TIME [epoch: 12.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162756403085551		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.03162756403085551 | validation: 0.14533865864216977]
	TIME [epoch: 12.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030637310783590693		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.030637310783590693 | validation: 0.1573594349403853]
	TIME [epoch: 12.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03092595596348123		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.03092595596348123 | validation: 0.1560914843393998]
	TIME [epoch: 12.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02987302329072018		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02987302329072018 | validation: 0.1266525900312909]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1176.pth
	Model improved!!!
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03148728896422179		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.03148728896422179 | validation: 0.1553868747176349]
	TIME [epoch: 12.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030748984765894525		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.030748984765894525 | validation: 0.1567784329014264]
	TIME [epoch: 12.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030781641876562245		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.030781641876562245 | validation: 0.1483625633431957]
	TIME [epoch: 12.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033605945746290604		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.033605945746290604 | validation: 0.13650400373763824]
	TIME [epoch: 12.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02906897988772055		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.02906897988772055 | validation: 0.14606172039027052]
	TIME [epoch: 12.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03262867788703652		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.03262867788703652 | validation: 0.15763818454244516]
	TIME [epoch: 12.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02997819050188614		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.02997819050188614 | validation: 0.149606745482366]
	TIME [epoch: 12.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030458623959316176		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.030458623959316176 | validation: 0.15883056233127657]
	TIME [epoch: 12.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029817298010156326		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.029817298010156326 | validation: 0.14311845459370173]
	TIME [epoch: 12.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029513721422534243		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.029513721422534243 | validation: 0.15175633654270304]
	TIME [epoch: 12.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028509616168523367		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.028509616168523367 | validation: 0.13697767226969645]
	TIME [epoch: 12.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050837984126516605		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.050837984126516605 | validation: 0.1507607522356671]
	TIME [epoch: 12.4 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03940704458606361		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.03940704458606361 | validation: 0.13496729761631807]
	TIME [epoch: 12.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028298682224479094		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.028298682224479094 | validation: 0.14825524259981954]
	TIME [epoch: 12.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03355570352962341		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.03355570352962341 | validation: 0.13565234343446816]
	TIME [epoch: 12.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03026500515993448		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.03026500515993448 | validation: 0.14261607995468176]
	TIME [epoch: 12.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331896085379191		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.03331896085379191 | validation: 0.14315667960073628]
	TIME [epoch: 12.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03255725748040398		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.03255725748040398 | validation: 0.15294008944349186]
	TIME [epoch: 12.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030111380704465122		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.030111380704465122 | validation: 0.13660561148467043]
	TIME [epoch: 12.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031313306227863866		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.031313306227863866 | validation: 0.14102556660528484]
	TIME [epoch: 12.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166925782023784		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.03166925782023784 | validation: 0.15270088427286047]
	TIME [epoch: 12.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03071069219117396		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.03071069219117396 | validation: 0.13973186368733478]
	TIME [epoch: 12.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027895953525987904		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.027895953525987904 | validation: 0.14086398895561156]
	TIME [epoch: 12.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02993881948712941		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.02993881948712941 | validation: 0.1326553015586811]
	TIME [epoch: 12.4 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029338705389441343		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.029338705389441343 | validation: 0.1510385024432805]
	TIME [epoch: 12.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02777480837065106		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02777480837065106 | validation: 0.15202303671090883]
	TIME [epoch: 12.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028670218231590845		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.028670218231590845 | validation: 0.14154602742537467]
	TIME [epoch: 12.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030011487100967866		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.030011487100967866 | validation: 0.14610451796077495]
	TIME [epoch: 12.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03082869969583354		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.03082869969583354 | validation: 0.1388485255011004]
	TIME [epoch: 12.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028778085401121267		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.028778085401121267 | validation: 0.14605394756472687]
	TIME [epoch: 12.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030540005605898115		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.030540005605898115 | validation: 0.14909761632360827]
	TIME [epoch: 12.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03021543601190305		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.03021543601190305 | validation: 0.1416624626677206]
	TIME [epoch: 12.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02941743800451702		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02941743800451702 | validation: 0.1424694594485345]
	TIME [epoch: 12.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028245472842927204		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.028245472842927204 | validation: 0.14149474558180805]
	TIME [epoch: 12.4 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027833721561740712		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.027833721561740712 | validation: 0.1327992175962215]
	TIME [epoch: 12.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322088843182945		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.0322088843182945 | validation: 0.1574654279979656]
	TIME [epoch: 12.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028146396309210883		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.028146396309210883 | validation: 0.15235610493720253]
	TIME [epoch: 12.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029504958822007068		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.029504958822007068 | validation: 0.12770608005527495]
	TIME [epoch: 12.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02874583066824731		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.02874583066824731 | validation: 0.16084386312949583]
	TIME [epoch: 12.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02975250360991737		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.02975250360991737 | validation: 0.1549820556580569]
	TIME [epoch: 12.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027822174310543665		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.027822174310543665 | validation: 0.13782806216759688]
	TIME [epoch: 12.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031022153410734173		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.031022153410734173 | validation: 0.13660180390919244]
	TIME [epoch: 12.4 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026731209453054183		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.026731209453054183 | validation: 0.1390789367564322]
	TIME [epoch: 12.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028782734356444385		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.028782734356444385 | validation: 0.12413791657096672]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1220.pth
	Model improved!!!
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017239670069734		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.03017239670069734 | validation: 0.14726839045620457]
	TIME [epoch: 12.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03014649800466998		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.03014649800466998 | validation: 0.1357502440255763]
	TIME [epoch: 12.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029179742410381442		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.029179742410381442 | validation: 0.14053982968256995]
	TIME [epoch: 12.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028915538491634817		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.028915538491634817 | validation: 0.1470462984566699]
	TIME [epoch: 12.4 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028368064558394402		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.028368064558394402 | validation: 0.14295664759136958]
	TIME [epoch: 12.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028412211766986068		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.028412211766986068 | validation: 0.14676306829052574]
	TIME [epoch: 12.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029066969059344253		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.029066969059344253 | validation: 0.13256784545942515]
	TIME [epoch: 12.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03269413231698387		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.03269413231698387 | validation: 0.19782747087932226]
	TIME [epoch: 12.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035407423230169995		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.035407423230169995 | validation: 0.13710530005419078]
	TIME [epoch: 12.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311215789172417		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.0311215789172417 | validation: 0.13807431581551755]
	TIME [epoch: 12.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027393466269046308		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.027393466269046308 | validation: 0.14379853902722903]
	TIME [epoch: 12.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032530672098898705		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.032530672098898705 | validation: 0.14249854215818403]
	TIME [epoch: 12.4 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028492446916484813		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.028492446916484813 | validation: 0.14121244481318676]
	TIME [epoch: 12.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032638166952300554		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.032638166952300554 | validation: 0.1489466278867153]
	TIME [epoch: 12.4 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027248271817094452		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.027248271817094452 | validation: 0.1362673531945763]
	TIME [epoch: 12.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028654683335453113		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.028654683335453113 | validation: 0.13793210939936226]
	TIME [epoch: 12.4 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030248353185267522		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.030248353185267522 | validation: 0.1429432875090755]
	TIME [epoch: 12.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028383343012821464		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.028383343012821464 | validation: 0.1370723863812946]
	TIME [epoch: 12.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03018968797840039		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.03018968797840039 | validation: 0.1255133368652706]
	TIME [epoch: 12.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030984402447755886		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.030984402447755886 | validation: 0.14075398539963266]
	TIME [epoch: 12.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029569784104336742		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.029569784104336742 | validation: 0.13407411056414306]
	TIME [epoch: 12.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030616315151166005		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.030616315151166005 | validation: 0.13319940726889362]
	TIME [epoch: 12.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028556826861722232		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.028556826861722232 | validation: 0.2345786761329075]
	TIME [epoch: 12.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008180040537565		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.04008180040537565 | validation: 0.1961463995692141]
	TIME [epoch: 12.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03730705144472116		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.03730705144472116 | validation: 0.14632905034350238]
	TIME [epoch: 12.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027206506716448475		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.027206506716448475 | validation: 0.13239357816409397]
	TIME [epoch: 12.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191782303558403		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.03191782303558403 | validation: 0.1377660841157863]
	TIME [epoch: 12.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02966248920680931		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.02966248920680931 | validation: 0.15163995626364937]
	TIME [epoch: 12.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029173728349899352		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.029173728349899352 | validation: 0.15135932734031937]
	TIME [epoch: 12.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028429768869651803		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.028429768869651803 | validation: 0.1438444288124958]
	TIME [epoch: 12.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027107950596263298		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.027107950596263298 | validation: 0.1347716657534757]
	TIME [epoch: 12.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029062462650232327		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.029062462650232327 | validation: 0.1319608885130647]
	TIME [epoch: 12.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02786623057281277		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.02786623057281277 | validation: 0.14624711039125451]
	TIME [epoch: 12.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029370606891916753		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.029370606891916753 | validation: 0.14601762215522657]
	TIME [epoch: 12.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02678240135028304		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.02678240135028304 | validation: 0.146318011549499]
	TIME [epoch: 12.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031122181273582933		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.031122181273582933 | validation: 0.14458149716967794]
	TIME [epoch: 12.4 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02932278094634028		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.02932278094634028 | validation: 0.1352135393799687]
	TIME [epoch: 12.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029698715624531566		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.029698715624531566 | validation: 0.1399004857733643]
	TIME [epoch: 12.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029197752947125274		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.029197752947125274 | validation: 0.1461112349110589]
	TIME [epoch: 12.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026870645549541522		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.026870645549541522 | validation: 0.1414601919980203]
	TIME [epoch: 12.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028330002100504032		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.028330002100504032 | validation: 0.13190528795231635]
	TIME [epoch: 12.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02876914357456249		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02876914357456249 | validation: 0.15272403349654465]
	TIME [epoch: 12.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028639008169828638		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.028639008169828638 | validation: 0.141910854799661]
	TIME [epoch: 12.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03589397968021922		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.03589397968021922 | validation: 0.13772596591567357]
	TIME [epoch: 12.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026628127796032484		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.026628127796032484 | validation: 0.13675036461449977]
	TIME [epoch: 12.4 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02943022297734166		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.02943022297734166 | validation: 0.13995789271045475]
	TIME [epoch: 12.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03138882983715077		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.03138882983715077 | validation: 0.13272443658381625]
	TIME [epoch: 12.4 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027293638472621486		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.027293638472621486 | validation: 0.1472813681317443]
	TIME [epoch: 12.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029028903961851912		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.029028903961851912 | validation: 0.1468431072544117]
	TIME [epoch: 12.4 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027601122764339826		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.027601122764339826 | validation: 0.1402441568137756]
	TIME [epoch: 12.4 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031085243583708844		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.031085243583708844 | validation: 0.13323539971491447]
	TIME [epoch: 12.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024989894100990867		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.024989894100990867 | validation: 0.13899485842933076]
	TIME [epoch: 12.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028597031363193894		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.028597031363193894 | validation: 0.1488345982912342]
	TIME [epoch: 12.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028580826026375298		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.028580826026375298 | validation: 0.13465583187912472]
	TIME [epoch: 12.4 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03101642274101983		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.03101642274101983 | validation: 0.1416087384408621]
	TIME [epoch: 12.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028017625399889216		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.028017625399889216 | validation: 0.14362533656912801]
	TIME [epoch: 12.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026817112304342798		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.026817112304342798 | validation: 0.14811178427561209]
	TIME [epoch: 12.4 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027711056062775283		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.027711056062775283 | validation: 0.1516168726158905]
	TIME [epoch: 12.4 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032685091283707494		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.032685091283707494 | validation: 0.1520675064701929]
	TIME [epoch: 12.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027020076574099564		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.027020076574099564 | validation: 0.137848421489864]
	TIME [epoch: 12.4 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032588560675567785		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.032588560675567785 | validation: 0.1392430074557612]
	TIME [epoch: 12.4 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02877626128729192		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.02877626128729192 | validation: 0.13326161939431327]
	TIME [epoch: 12.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026438828377684784		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.026438828377684784 | validation: 0.14058929819054203]
	TIME [epoch: 12.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027584560495174073		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.027584560495174073 | validation: 0.15129289808639337]
	TIME [epoch: 12.4 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028403738483066116		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.028403738483066116 | validation: 0.14618204999157708]
	TIME [epoch: 12.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027642542931748164		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.027642542931748164 | validation: 0.13886724040296392]
	TIME [epoch: 12.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026964488674019158		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.026964488674019158 | validation: 0.14544399780087644]
	TIME [epoch: 12.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027794951218776856		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.027794951218776856 | validation: 0.13803370644248797]
	TIME [epoch: 12.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02816484125394246		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02816484125394246 | validation: 0.12748860574303028]
	TIME [epoch: 12.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027937954143072003		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.027937954143072003 | validation: 0.1362017612551402]
	TIME [epoch: 12.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02843882499154149		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02843882499154149 | validation: 0.14353187075136728]
	TIME [epoch: 12.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027629312386264458		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.027629312386264458 | validation: 0.1431069663972991]
	TIME [epoch: 12.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027501400679934476		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.027501400679934476 | validation: 0.15112730960339849]
	TIME [epoch: 12.4 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028871514400544324		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.028871514400544324 | validation: 0.14067193241704126]
	TIME [epoch: 12.4 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027245116712878943		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.027245116712878943 | validation: 0.1375597741735434]
	TIME [epoch: 12.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027206232664568595		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.027206232664568595 | validation: 0.12739568428116432]
	TIME [epoch: 12.4 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027048390756827356		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.027048390756827356 | validation: 0.13510972687962602]
	TIME [epoch: 12.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748849986704692		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.02748849986704692 | validation: 0.13018948361092983]
	TIME [epoch: 12.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026977495566888762		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.026977495566888762 | validation: 0.13657705380909166]
	TIME [epoch: 12.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026707366118274515		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.026707366118274515 | validation: 0.14308040220869458]
	TIME [epoch: 12.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026595585272913502		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.026595585272913502 | validation: 0.135187675588009]
	TIME [epoch: 12.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02668852879762743		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.02668852879762743 | validation: 0.13789231802996968]
	TIME [epoch: 12.4 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02639530736013093		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.02639530736013093 | validation: 0.13421042784432433]
	TIME [epoch: 12.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028112039121601932		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.028112039121601932 | validation: 0.12834600572738597]
	TIME [epoch: 12.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02790230517243627		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.02790230517243627 | validation: 0.14954587960633814]
	TIME [epoch: 12.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031446017970222596		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.031446017970222596 | validation: 0.139773794586173]
	TIME [epoch: 12.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02871232688951069		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.02871232688951069 | validation: 0.11796123365209683]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1307.pth
	Model improved!!!
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029386506421429426		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.029386506421429426 | validation: 0.12429892433441934]
	TIME [epoch: 12.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02698620675615386		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.02698620675615386 | validation: 0.1509802675755442]
	TIME [epoch: 12.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028228449826482628		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.028228449826482628 | validation: 0.15108408560451608]
	TIME [epoch: 12.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028116010133321927		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.028116010133321927 | validation: 0.1434013150304295]
	TIME [epoch: 12.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028456829114137577		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.028456829114137577 | validation: 0.1374473730409075]
	TIME [epoch: 12.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02706607304179209		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02706607304179209 | validation: 0.12233003602207965]
	TIME [epoch: 12.4 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029416754927837657		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.029416754927837657 | validation: 0.1410340128529235]
	TIME [epoch: 12.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025591290765121146		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.025591290765121146 | validation: 0.13596617394591376]
	TIME [epoch: 12.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025806769494502085		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.025806769494502085 | validation: 0.1510112013102395]
	TIME [epoch: 12.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509485345654703		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.03509485345654703 | validation: 0.13768961919614395]
	TIME [epoch: 12.4 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03351582224440229		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.03351582224440229 | validation: 0.13259091923079946]
	TIME [epoch: 12.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027546336183053636		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.027546336183053636 | validation: 0.1326709176218874]
	TIME [epoch: 12.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02808860023605825		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02808860023605825 | validation: 0.13460759242146808]
	TIME [epoch: 12.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0278785938025017		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.0278785938025017 | validation: 0.12230194236199049]
	TIME [epoch: 12.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02836945083850222		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.02836945083850222 | validation: 0.12939684891718645]
	TIME [epoch: 12.4 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025913962560227714		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.025913962560227714 | validation: 0.1318994404753355]
	TIME [epoch: 12.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02939771495101301		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.02939771495101301 | validation: 0.15322740616922126]
	TIME [epoch: 12.4 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635548381899418		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.02635548381899418 | validation: 0.13608457608922137]
	TIME [epoch: 12.4 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027905974096768392		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.027905974096768392 | validation: 0.12467172552435374]
	TIME [epoch: 12.4 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026653698127126094		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.026653698127126094 | validation: 0.14060277712030364]
	TIME [epoch: 12.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026843924299281997		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.026843924299281997 | validation: 0.15031490445116483]
	TIME [epoch: 12.4 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0270252594743044		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.0270252594743044 | validation: 0.1426728873894439]
	TIME [epoch: 12.4 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026869046306872212		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.026869046306872212 | validation: 0.13964080074047827]
	TIME [epoch: 12.4 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027981477299359014		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.027981477299359014 | validation: 0.14065234332727017]
	TIME [epoch: 12.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026980896497901183		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.026980896497901183 | validation: 0.14509881451447101]
	TIME [epoch: 12.4 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027466910913233226		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.027466910913233226 | validation: 0.13306482745140508]
	TIME [epoch: 12.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750207493804648		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02750207493804648 | validation: 0.13686757727209575]
	TIME [epoch: 12.4 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028567707736795952		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.028567707736795952 | validation: 0.1498206922484225]
	TIME [epoch: 12.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026748247998854995		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.026748247998854995 | validation: 0.14303818119870945]
	TIME [epoch: 12.4 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026024603502198932		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.026024603502198932 | validation: 0.12655009499787837]
	TIME [epoch: 12.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028476585733934637		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.028476585733934637 | validation: 0.124534444679714]
	TIME [epoch: 12.4 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02796622072876453		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.02796622072876453 | validation: 0.14079747242051785]
	TIME [epoch: 12.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024725616842325736		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.024725616842325736 | validation: 0.1260031124252833]
	TIME [epoch: 12.4 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026279451455172055		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.026279451455172055 | validation: 0.1358761250854436]
	TIME [epoch: 12.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027447977475754292		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.027447977475754292 | validation: 0.14666337419095157]
	TIME [epoch: 12.4 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028230413430154307		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.028230413430154307 | validation: 0.1288797218164689]
	TIME [epoch: 12.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028187205555855157		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.028187205555855157 | validation: 0.12714400113848687]
	TIME [epoch: 12.4 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026984391824055837		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.026984391824055837 | validation: 0.14347344993246816]
	TIME [epoch: 12.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026270554517060722		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.026270554517060722 | validation: 0.1275475887123668]
	TIME [epoch: 12.4 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0279132099741578		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.0279132099741578 | validation: 0.13889722225343978]
	TIME [epoch: 12.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02730594056392193		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02730594056392193 | validation: 0.12841139105817717]
	TIME [epoch: 12.4 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02795518115804861		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.02795518115804861 | validation: 0.13340566155691116]
	TIME [epoch: 12.4 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031794343219195303		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.031794343219195303 | validation: 0.135228154555197]
	TIME [epoch: 12.4 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02844792276364478		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.02844792276364478 | validation: 0.1267815288624545]
	TIME [epoch: 12.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026462752959869384		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.026462752959869384 | validation: 0.1278591882999908]
	TIME [epoch: 12.4 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02990102321338867		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.02990102321338867 | validation: 0.1445155980428157]
	TIME [epoch: 12.4 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750460299615118		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.02750460299615118 | validation: 0.1347528316151373]
	TIME [epoch: 12.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02847912719060652		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.02847912719060652 | validation: 0.13389894708257968]
	TIME [epoch: 12.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025259276712322615		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.025259276712322615 | validation: 0.1371032564495806]
	TIME [epoch: 12.4 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539202778215832		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.02539202778215832 | validation: 0.12569644784123213]
	TIME [epoch: 12.4 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028219898041277003		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.028219898041277003 | validation: 0.1317685964836365]
	TIME [epoch: 12.4 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0258608763139139		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.0258608763139139 | validation: 0.13057699581550447]
	TIME [epoch: 12.4 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027534532675523006		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.027534532675523006 | validation: 0.1402428560841499]
	TIME [epoch: 12.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029999044468726668		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.029999044468726668 | validation: 0.1423280117926397]
	TIME [epoch: 12.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027127313634637575		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.027127313634637575 | validation: 0.13633616121177802]
	TIME [epoch: 12.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024961158924471365		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.024961158924471365 | validation: 0.1288046168608617]
	TIME [epoch: 12.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02775978023178329		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.02775978023178329 | validation: 0.12911083378668847]
	TIME [epoch: 12.4 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027797878813480628		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.027797878813480628 | validation: 0.12783725301092352]
	TIME [epoch: 12.4 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02616528222418487		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02616528222418487 | validation: 0.13741936303581023]
	TIME [epoch: 12.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026373857592724734		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.026373857592724734 | validation: 0.12772557147656216]
	TIME [epoch: 12.4 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026606821640931255		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.026606821640931255 | validation: 0.1387037991480772]
	TIME [epoch: 12.4 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027027097727436118		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.027027097727436118 | validation: 0.1350916015116673]
	TIME [epoch: 12.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02608082736329513		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.02608082736329513 | validation: 0.12749464133039914]
	TIME [epoch: 12.4 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027343797844127743		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.027343797844127743 | validation: 0.12041352872879625]
	TIME [epoch: 12.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026504015105684477		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.026504015105684477 | validation: 0.1331479727976089]
	TIME [epoch: 12.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026854978295530848		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.026854978295530848 | validation: 0.14131265104016674]
	TIME [epoch: 12.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026680511644515573		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.026680511644515573 | validation: 0.1221610102404712]
	TIME [epoch: 12.4 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02434136164313797		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.02434136164313797 | validation: 0.13596183124813208]
	TIME [epoch: 12.4 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026674966074697153		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.026674966074697153 | validation: 0.1430088841514944]
	TIME [epoch: 12.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02678844493142618		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.02678844493142618 | validation: 0.1452468941295071]
	TIME [epoch: 12.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02569862977268727		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.02569862977268727 | validation: 0.12818569365084517]
	TIME [epoch: 12.4 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027978879904315513		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.027978879904315513 | validation: 0.1184286523478249]
	TIME [epoch: 12.4 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026414121009090144		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.026414121009090144 | validation: 0.12568964268423807]
	TIME [epoch: 12.4 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02709415512824087		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.02709415512824087 | validation: 0.13294844628336158]
	TIME [epoch: 12.4 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02555858046989118		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.02555858046989118 | validation: 0.11949066185125333]
	TIME [epoch: 12.4 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025714285986708215		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.025714285986708215 | validation: 0.13881381077120636]
	TIME [epoch: 12.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026456039541116878		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.026456039541116878 | validation: 0.12786912371741999]
	TIME [epoch: 12.4 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02606362385939431		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.02606362385939431 | validation: 0.13122437993828331]
	TIME [epoch: 12.4 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024718970476836077		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.024718970476836077 | validation: 0.12078300837221954]
	TIME [epoch: 12.4 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028910224436352775		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.028910224436352775 | validation: 0.14041403652507234]
	TIME [epoch: 12.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026331340625301163		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.026331340625301163 | validation: 0.15604639597446002]
	TIME [epoch: 12.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02655653021298487		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.02655653021298487 | validation: 0.13372664514265611]
	TIME [epoch: 12.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02611040710781932		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.02611040710781932 | validation: 0.13046313214593316]
	TIME [epoch: 12.4 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02577135824605083		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.02577135824605083 | validation: 0.13321659497792027]
	TIME [epoch: 12.4 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025656179952266517		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.025656179952266517 | validation: 0.1263445508126879]
	TIME [epoch: 12.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026168616348551943		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.026168616348551943 | validation: 0.14241790078507918]
	TIME [epoch: 12.4 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025687016811862785		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.025687016811862785 | validation: 0.1199311587723828]
	TIME [epoch: 12.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028193943144074845		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.028193943144074845 | validation: 0.11964198221944455]
	TIME [epoch: 12.4 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028482447221725115		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.028482447221725115 | validation: 0.12275258232858932]
	TIME [epoch: 12.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026390537779157754		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.026390537779157754 | validation: 0.13309235913337011]
	TIME [epoch: 12.4 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02657979344932304		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.02657979344932304 | validation: 0.12893700161418728]
	TIME [epoch: 12.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024838589169558483		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.024838589169558483 | validation: 0.12668933693299755]
	TIME [epoch: 12.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02444351163126086		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.02444351163126086 | validation: 0.13992846005257564]
	TIME [epoch: 12.4 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025142393213601757		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.025142393213601757 | validation: 0.12267952500737596]
	TIME [epoch: 12.4 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02545352980376036		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02545352980376036 | validation: 0.12572587204152605]
	TIME [epoch: 12.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875332063311843		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.02875332063311843 | validation: 0.12022689456549732]
	TIME [epoch: 12.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02533124125048082		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.02533124125048082 | validation: 0.13251922041267067]
	TIME [epoch: 12.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02666673574205526		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.02666673574205526 | validation: 0.13984813532535675]
	TIME [epoch: 12.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02694063560027372		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.02694063560027372 | validation: 0.14618541651253023]
	TIME [epoch: 12.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030940717842732477		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.030940717842732477 | validation: 0.14079424510087846]
	TIME [epoch: 12.4 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028957888734409593		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.028957888734409593 | validation: 0.12180784514434256]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_150748/states/model_phi1_4b_v_mmd1_1408.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10025.769 seconds.
