Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3071412737

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.568275904126742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.568275904126742 | validation: 5.1200178667376]
	TIME [epoch: 177 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.116879620000116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.116879620000116 | validation: 4.760049418797695]
	TIME [epoch: 1.52 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.72397611432497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.72397611432497 | validation: 6.7841056627250715]
	TIME [epoch: 1.45 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.792800107420325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.792800107420325 | validation: 5.316387540052476]
	TIME [epoch: 1.45 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.286904150260178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.286904150260178 | validation: 4.656204659282584]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.602730076348385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.602730076348385 | validation: 4.654825977689967]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.602602520019284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.602602520019284 | validation: 4.469198319949192]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.452691325861298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.452691325861298 | validation: 4.31597993501453]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3666206114982575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3666206114982575 | validation: 4.257411045356348]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.356952732548039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.356952732548039 | validation: 4.168002517271682]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.268471214599344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.268471214599344 | validation: 4.0251902918014695]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.135786245958175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.135786245958175 | validation: 3.830733103345814]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.062126184304012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.062126184304012 | validation: 3.8990029011694647]
	TIME [epoch: 1.45 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9213258271878035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9213258271878035 | validation: 3.737478309706244]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7852540775489287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7852540775489287 | validation: 3.594968130336834]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.684538737503966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.684538737503966 | validation: 3.395473707637619]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6804066918352762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6804066918352762 | validation: 4.126001353332213]
	TIME [epoch: 1.45 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.195089331138169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.195089331138169 | validation: 3.1956856453044704]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.143586887526623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.143586887526623 | validation: 3.449150263581666]
	TIME [epoch: 1.45 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.516123990861457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.516123990861457 | validation: 2.6794216692227626]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3913932477831112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3913932477831112 | validation: 2.9516715374284512]
	TIME [epoch: 1.45 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2297553998448376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2297553998448376 | validation: 1.894010144279006]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1156378491883525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1156378491883525 | validation: 2.0017470289850983]
	TIME [epoch: 1.45 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4224352630260648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4224352630260648 | validation: 1.4988840399386563]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5572424754687173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5572424754687173 | validation: 1.8470577737551837]
	TIME [epoch: 1.45 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283560713718712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.283560713718712 | validation: 1.3227430152322048]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1044448388305805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1044448388305805 | validation: 1.8057722798991644]
	TIME [epoch: 1.45 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1510583909361232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1510583909361232 | validation: 1.1254724336916844]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835208763700922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9835208763700922 | validation: 1.129751772628115]
	TIME [epoch: 1.45 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016275965062014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016275965062014 | validation: 1.844606935445983]
	TIME [epoch: 1.45 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0875454746061126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0875454746061126 | validation: 1.4300927569389343]
	TIME [epoch: 1.45 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9371882790473995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9371882790473995 | validation: 0.9832793639165095]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9878767671203588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9878767671203588 | validation: 1.5040693179696623]
	TIME [epoch: 1.45 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9235492899016114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9235492899016114 | validation: 1.4854565085620905]
	TIME [epoch: 1.45 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8802466198244416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8802466198244416 | validation: 0.9452989992105835]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509229040257155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8509229040257155 | validation: 1.136496363575802]
	TIME [epoch: 1.45 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278123432481147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8278123432481147 | validation: 1.1084636069283444]
	TIME [epoch: 1.45 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837065978705286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.837065978705286 | validation: 1.2213896056567253]
	TIME [epoch: 1.45 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848607867874843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848607867874843 | validation: 1.045549486749752]
	TIME [epoch: 1.45 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9073064717328256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9073064717328256 | validation: 1.3525116240984243]
	TIME [epoch: 1.45 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012486418415995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012486418415995 | validation: 1.0087259810178204]
	TIME [epoch: 1.45 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721279915213224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721279915213224 | validation: 1.1606306280258827]
	TIME [epoch: 1.45 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963061052496591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963061052496591 | validation: 1.0656412909810318]
	TIME [epoch: 1.45 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899235007418689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7899235007418689 | validation: 1.0841702549235994]
	TIME [epoch: 1.45 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832362707196466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7832362707196466 | validation: 1.1208020852768539]
	TIME [epoch: 1.45 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841023450383999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841023450383999 | validation: 1.0692760720219165]
	TIME [epoch: 1.45 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962511127937961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962511127937961 | validation: 1.2403201181246413]
	TIME [epoch: 1.45 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8116175714643701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8116175714643701 | validation: 1.056699910312014]
	TIME [epoch: 1.45 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661128160222391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661128160222391 | validation: 1.2716552178118912]
	TIME [epoch: 1.45 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360269841862034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8360269841862034 | validation: 0.9784223689225844]
	TIME [epoch: 1.45 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.88742339937111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.88742339937111 | validation: 1.513745761326048]
	TIME [epoch: 1.45 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926123324954412		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.926123324954412 | validation: 0.9319377567880255]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8534282685581431		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.8534282685581431 | validation: 1.12932703109012]
	TIME [epoch: 1.45 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869498938382878		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.7869498938382878 | validation: 1.0124907511664791]
	TIME [epoch: 1.45 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697267816917895		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.7697267816917895 | validation: 0.9954628646938218]
	TIME [epoch: 1.45 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698681618818429		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.7698681618818429 | validation: 0.9706674310472689]
	TIME [epoch: 1.45 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681380487845156		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.7681380487845156 | validation: 1.0918840555412368]
	TIME [epoch: 1.45 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793234646470151		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.7793234646470151 | validation: 0.9087640239235246]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777971856379789		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.777971856379789 | validation: 1.1055695717999807]
	TIME [epoch: 1.45 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966750726354843		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.7966750726354843 | validation: 0.9582433577736801]
	TIME [epoch: 1.43 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752656230799971		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.7752656230799971 | validation: 0.9656715274369637]
	TIME [epoch: 1.44 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694325720361473		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.7694325720361473 | validation: 1.0167014580515044]
	TIME [epoch: 1.44 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752490362442638		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.7752490362442638 | validation: 0.9643767750655116]
	TIME [epoch: 1.44 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865862007167435		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.7865862007167435 | validation: 1.029446157597224]
	TIME [epoch: 1.44 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786128046850729		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.786128046850729 | validation: 0.9827469065054063]
	TIME [epoch: 1.43 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804836409881978		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.7804836409881978 | validation: 0.9119255429000148]
	TIME [epoch: 1.44 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781618393430154		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.7781618393430154 | validation: 1.200061660847035]
	TIME [epoch: 1.44 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293252950675085		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.8293252950675085 | validation: 0.8615552113698683]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770511194860674		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7770511194860674 | validation: 1.0635044208520321]
	TIME [epoch: 1.44 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694105237324209		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.7694105237324209 | validation: 0.8764905518588613]
	TIME [epoch: 1.44 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841176243307737		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.7841176243307737 | validation: 1.1237388189137765]
	TIME [epoch: 1.44 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045427272375758		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8045427272375758 | validation: 0.9298292186802044]
	TIME [epoch: 1.44 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023637869370808		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.8023637869370808 | validation: 0.9636646271325002]
	TIME [epoch: 1.43 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748035034300973		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.748035034300973 | validation: 0.8931437341691411]
	TIME [epoch: 1.44 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351482241120926		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.7351482241120926 | validation: 0.8918971196789184]
	TIME [epoch: 1.44 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465829486108067		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.7465829486108067 | validation: 0.9753492816825876]
	TIME [epoch: 1.44 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486574999726971		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.7486574999726971 | validation: 0.8709595813403983]
	TIME [epoch: 1.44 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542503873644187		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.7542503873644187 | validation: 1.0660446024120371]
	TIME [epoch: 1.44 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708694436006978		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.7708694436006978 | validation: 0.8454437321648922]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159648410341115		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8159648410341115 | validation: 1.2949179379881592]
	TIME [epoch: 1.44 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985235737047114		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8985235737047114 | validation: 0.9076722540687489]
	TIME [epoch: 1.44 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7453817613830939		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7453817613830939 | validation: 0.855768652850804]
	TIME [epoch: 1.44 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815665952744942		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.7815665952744942 | validation: 1.1380417384221502]
	TIME [epoch: 1.44 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099223508422574		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8099223508422574 | validation: 0.8851600116864007]
	TIME [epoch: 1.44 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415837099610505		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7415837099610505 | validation: 0.8341006061194721]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780793113954326		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.780793113954326 | validation: 1.0234870092856088]
	TIME [epoch: 1.45 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031112333055606		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8031112333055606 | validation: 0.9527130909987677]
	TIME [epoch: 1.44 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658976363100898		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7658976363100898 | validation: 0.8835081277488517]
	TIME [epoch: 1.44 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793389839344096		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.7793389839344096 | validation: 0.9723300140971631]
	TIME [epoch: 1.44 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7555393079086454		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7555393079086454 | validation: 0.8693610033350203]
	TIME [epoch: 1.44 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761870639631383		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.761870639631383 | validation: 0.8863698080394228]
	TIME [epoch: 1.44 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403544611021251		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.7403544611021251 | validation: 0.9440156045157642]
	TIME [epoch: 1.43 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.74228541070181		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.74228541070181 | validation: 0.8407534048616494]
	TIME [epoch: 1.44 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747773085513715		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.747773085513715 | validation: 0.9358437665518409]
	TIME [epoch: 1.44 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514996429230837		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7514996429230837 | validation: 0.8742267440702314]
	TIME [epoch: 1.44 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461230798748792		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.7461230798748792 | validation: 0.9036663789569698]
	TIME [epoch: 1.43 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697267123645289		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.7697267123645289 | validation: 1.0029132755146521]
	TIME [epoch: 1.43 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840999579116493		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.7840999579116493 | validation: 0.9401138764971982]
	TIME [epoch: 1.44 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822138414522855		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.7822138414522855 | validation: 0.8822957739018611]
	TIME [epoch: 1.44 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7397492643504257		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.7397492643504257 | validation: 0.9487053009789492]
	TIME [epoch: 1.44 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347141189251064		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7347141189251064 | validation: 0.8356097046984353]
	TIME [epoch: 1.44 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731403917861745		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.731403917861745 | validation: 0.9997844003421797]
	TIME [epoch: 1.44 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504030754058766		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7504030754058766 | validation: 0.8323624363993724]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781006953495403		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7781006953495403 | validation: 1.1829829512646521]
	TIME [epoch: 1.44 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306884411109914		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8306884411109914 | validation: 0.8571858537301154]
	TIME [epoch: 1.44 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733608522285783		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.733608522285783 | validation: 0.8671262292345614]
	TIME [epoch: 1.44 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377302720284287		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7377302720284287 | validation: 0.9239951305814245]
	TIME [epoch: 1.44 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712785974446039		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.7712785974446039 | validation: 0.9523385889879331]
	TIME [epoch: 1.43 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807112566531496		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.7807112566531496 | validation: 0.9131021959861618]
	TIME [epoch: 1.44 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889507033832398		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.7889507033832398 | validation: 0.9128068727701706]
	TIME [epoch: 1.44 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292051764296525		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7292051764296525 | validation: 0.8746641545437568]
	TIME [epoch: 1.44 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145159159205992		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.7145159159205992 | validation: 0.838781192357336]
	TIME [epoch: 1.44 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247456432569033		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7247456432569033 | validation: 0.976369657299713]
	TIME [epoch: 1.44 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744614098930663		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.744614098930663 | validation: 0.8532881833082716]
	TIME [epoch: 1.44 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7441841094811269		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7441841094811269 | validation: 0.955258981211403]
	TIME [epoch: 1.44 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298655677365536		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7298655677365536 | validation: 0.8218738483909114]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316367230987999		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7316367230987999 | validation: 1.0226558152520544]
	TIME [epoch: 1.45 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751745918100354		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.751745918100354 | validation: 0.8373333468885094]
	TIME [epoch: 1.45 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696121973662517		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7696121973662517 | validation: 1.0521064222600887]
	TIME [epoch: 1.46 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8123858595406662		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8123858595406662 | validation: 0.9706917491439522]
	TIME [epoch: 1.45 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710809760456635		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.7710809760456635 | validation: 0.878566088205507]
	TIME [epoch: 1.45 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717726846399853		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7717726846399853 | validation: 0.9725786297467924]
	TIME [epoch: 1.47 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268258208449403		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7268258208449403 | validation: 0.8451648390426594]
	TIME [epoch: 1.45 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086416439308896		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.7086416439308896 | validation: 0.8518161909499637]
	TIME [epoch: 1.45 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080084198196877		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7080084198196877 | validation: 0.8929587295054457]
	TIME [epoch: 1.45 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152031419529361		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7152031419529361 | validation: 0.8400977709233122]
	TIME [epoch: 1.46 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165242521159327		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7165242521159327 | validation: 0.9412318675148342]
	TIME [epoch: 1.45 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233532478822713		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7233532478822713 | validation: 0.9202830143922335]
	TIME [epoch: 1.45 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750782704284475		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.750782704284475 | validation: 0.8877387016705545]
	TIME [epoch: 1.45 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8397644670878609		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8397644670878609 | validation: 1.0639905177848794]
	TIME [epoch: 1.45 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231075551523372		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8231075551523372 | validation: 0.8781426504613953]
	TIME [epoch: 1.45 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035339426327452		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7035339426327452 | validation: 0.7973244328230384]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365577078556551		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7365577078556551 | validation: 0.9169846214118176]
	TIME [epoch: 1.44 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292121291340934		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7292121291340934 | validation: 0.8505183877625573]
	TIME [epoch: 1.45 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011559688773923		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.7011559688773923 | validation: 0.8031974207183075]
	TIME [epoch: 1.45 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7355427931170015		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7355427931170015 | validation: 1.073178390538575]
	TIME [epoch: 1.45 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613097877450146		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7613097877450146 | validation: 0.8263046763025305]
	TIME [epoch: 1.45 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430115606034116		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7430115606034116 | validation: 0.8168413928381284]
	TIME [epoch: 1.45 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793117798882528		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.6793117798882528 | validation: 0.9331292351791145]
	TIME [epoch: 1.45 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969650889635156		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6969650889635156 | validation: 0.7788118344353029]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909786442250925		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6909786442250925 | validation: 0.8245115611925984]
	TIME [epoch: 1.44 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713689719830329		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6713689719830329 | validation: 0.7320289212811057]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739618886754718		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6739618886754718 | validation: 0.8617006495789554]
	TIME [epoch: 1.44 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170040892687476		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7170040892687476 | validation: 0.9613641683498777]
	TIME [epoch: 1.44 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8978581738645004		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8978581738645004 | validation: 0.9306897181159739]
	TIME [epoch: 1.44 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008038741794665		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8008038741794665 | validation: 0.8692645090596491]
	TIME [epoch: 1.44 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521233406380011		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.6521233406380011 | validation: 0.7601263226192886]
	TIME [epoch: 1.44 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611919844724465		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6611919844724465 | validation: 0.783191990381257]
	TIME [epoch: 1.44 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649444807350304		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.649444807350304 | validation: 0.7374731861242597]
	TIME [epoch: 1.44 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.620917559649844		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.620917559649844 | validation: 0.6940449096135324]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6010468132648668		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6010468132648668 | validation: 0.7385029134708723]
	TIME [epoch: 1.45 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6184229736626498		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6184229736626498 | validation: 0.9777432215874274]
	TIME [epoch: 1.45 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022405997410239		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.022405997410239 | validation: 0.9932897694774877]
	TIME [epoch: 1.45 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9545987292761191		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9545987292761191 | validation: 0.8998385856269113]
	TIME [epoch: 1.45 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470548132913787		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6470548132913787 | validation: 0.9088030713167902]
	TIME [epoch: 1.45 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148463969560044		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7148463969560044 | validation: 0.8005460303499284]
	TIME [epoch: 1.45 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405602945939417		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6405602945939417 | validation: 0.8229810820389192]
	TIME [epoch: 1.45 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468651313351947		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6468651313351947 | validation: 0.8256244063600461]
	TIME [epoch: 1.45 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224780030602909		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6224780030602909 | validation: 0.7822312530902377]
	TIME [epoch: 1.45 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6135382565705387		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6135382565705387 | validation: 0.7384586323333662]
	TIME [epoch: 1.45 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58173083536582		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.58173083536582 | validation: 0.7269620934552085]
	TIME [epoch: 1.45 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5623185326067088		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5623185326067088 | validation: 0.6778112597118794]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.549262389246275		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.549262389246275 | validation: 0.6694424727093984]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5425190488959878		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5425190488959878 | validation: 0.6299698909432436]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5365403420375431		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5365403420375431 | validation: 0.6467994151202642]
	TIME [epoch: 1.45 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248632935979001		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.5248632935979001 | validation: 0.621150358104798]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5321076367814177		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.5321076367814177 | validation: 0.796309641209225]
	TIME [epoch: 1.45 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5746349182311932		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.5746349182311932 | validation: 0.6659084165558746]
	TIME [epoch: 1.45 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515033611844209		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6515033611844209 | validation: 0.811418838696734]
	TIME [epoch: 1.45 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5590233584103589		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5590233584103589 | validation: 0.7247444550450015]
	TIME [epoch: 1.45 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089226840507943		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.5089226840507943 | validation: 0.611245368768792]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727986742572267		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5727986742572267 | validation: 0.7984907860621222]
	TIME [epoch: 1.45 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962372839286135		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6962372839286135 | validation: 0.9295898479744095]
	TIME [epoch: 1.45 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882423612783183		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6882423612783183 | validation: 0.6884558061668233]
	TIME [epoch: 1.45 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5519047573484006		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.5519047573484006 | validation: 0.7432101254411276]
	TIME [epoch: 1.45 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5433217391584358		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.5433217391584358 | validation: 0.733245143385926]
	TIME [epoch: 1.45 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5057495845590665		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.5057495845590665 | validation: 0.5986511050314317]
	TIME [epoch: 1.46 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49303508294301324		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.49303508294301324 | validation: 0.678746412547517]
	TIME [epoch: 1.45 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137594865840455		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.5137594865840455 | validation: 0.6333477370900459]
	TIME [epoch: 1.45 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533379985179411		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.533379985179411 | validation: 0.7086920634893361]
	TIME [epoch: 1.45 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516261268493087		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.5516261268493087 | validation: 0.6577674576467147]
	TIME [epoch: 1.45 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47381995061536614		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.47381995061536614 | validation: 0.6302223798138233]
	TIME [epoch: 1.45 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45210291233771516		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.45210291233771516 | validation: 0.5635742634477477]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4455521411233766		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.4455521411233766 | validation: 0.564565873151868]
	TIME [epoch: 1.45 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4697612196310142		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.4697612196310142 | validation: 0.7905683353826012]
	TIME [epoch: 1.45 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6024779133384109		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6024779133384109 | validation: 0.7184970900175918]
	TIME [epoch: 1.45 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128382325512734		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.5128382325512734 | validation: 0.671678319006189]
	TIME [epoch: 1.45 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4315870861246768		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.4315870861246768 | validation: 0.5796405450907951]
	TIME [epoch: 1.45 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4267551133558629		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.4267551133558629 | validation: 0.5793863588593242]
	TIME [epoch: 1.45 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.494886454135782		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.494886454135782 | validation: 0.846648100366786]
	TIME [epoch: 1.45 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388597342329914		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.6388597342329914 | validation: 0.7855875877741331]
	TIME [epoch: 1.45 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47337368258305484		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.47337368258305484 | validation: 0.6880294453357317]
	TIME [epoch: 1.45 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46073215845982113		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.46073215845982113 | validation: 0.6559920793464777]
	TIME [epoch: 1.45 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44273805543625316		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.44273805543625316 | validation: 0.5495203520861017]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351254211693257		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.4351254211693257 | validation: 0.6122144846297711]
	TIME [epoch: 1.44 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47937002199066764		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.47937002199066764 | validation: 0.6081472283138519]
	TIME [epoch: 1.44 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4446200010727366		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.4446200010727366 | validation: 0.579886101501374]
	TIME [epoch: 1.44 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41240763312782025		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.41240763312782025 | validation: 0.5860496704620344]
	TIME [epoch: 1.44 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39025316734974924		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.39025316734974924 | validation: 0.5134659500680151]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3795939000437869		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.3795939000437869 | validation: 0.5709230831325346]
	TIME [epoch: 1.45 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37484817904568163		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.37484817904568163 | validation: 0.48035204662183983]
	TIME [epoch: 185 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872856477027131		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.3872856477027131 | validation: 0.7285822557195667]
	TIME [epoch: 2.88 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43967596846036017		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.43967596846036017 | validation: 0.5158596119406633]
	TIME [epoch: 2.86 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4678046028418406		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.4678046028418406 | validation: 0.7122220233526924]
	TIME [epoch: 2.86 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41445885736634125		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.41445885736634125 | validation: 0.5310172996478112]
	TIME [epoch: 2.86 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594576893628475		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.3594576893628475 | validation: 0.5159088347124231]
	TIME [epoch: 2.86 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.411652465880925		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.411652465880925 | validation: 0.5871339053358244]
	TIME [epoch: 2.87 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42707283114769257		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.42707283114769257 | validation: 0.6371010705497565]
	TIME [epoch: 2.86 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39941809614018425		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.39941809614018425 | validation: 0.5200471123572188]
	TIME [epoch: 2.86 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3509869708338935		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.3509869708338935 | validation: 0.525276690729941]
	TIME [epoch: 2.86 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3335620851782723		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.3335620851782723 | validation: 0.4566459838364154]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3320336501764198		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.3320336501764198 | validation: 0.5038875186594906]
	TIME [epoch: 2.86 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34699450861549525		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.34699450861549525 | validation: 0.6328574009175059]
	TIME [epoch: 2.86 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40709056590737547		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.40709056590737547 | validation: 0.5538561503750353]
	TIME [epoch: 2.86 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3498575225990368		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.3498575225990368 | validation: 0.5252282254814679]
	TIME [epoch: 2.86 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3179343784987436		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.3179343784987436 | validation: 0.4461038698548147]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32977596394320485		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.32977596394320485 | validation: 0.6826629236863044]
	TIME [epoch: 2.86 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40492536120612715		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.40492536120612715 | validation: 0.6291819235712439]
	TIME [epoch: 2.86 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37706332201961545		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.37706332201961545 | validation: 0.4161733675403377]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3230507786179641		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.3230507786179641 | validation: 0.5118440692344447]
	TIME [epoch: 2.86 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31187200668120707		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.31187200668120707 | validation: 0.4020499009010724]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2994934835864904		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.2994934835864904 | validation: 0.6476629321096609]
	TIME [epoch: 2.86 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.325915487028233		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.325915487028233 | validation: 0.4480930367331148]
	TIME [epoch: 2.86 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878800835919935		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.2878800835919935 | validation: 0.4779001133183]
	TIME [epoch: 2.86 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749787275911293		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.2749787275911293 | validation: 0.3814209303110963]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28990305068474376		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.28990305068474376 | validation: 0.5298665913177129]
	TIME [epoch: 2.86 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.357937627179111		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.357937627179111 | validation: 0.5248515142927884]
	TIME [epoch: 2.86 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37885556702450224		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.37885556702450224 | validation: 0.5244183560860225]
	TIME [epoch: 2.86 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29593339894096776		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.29593339894096776 | validation: 0.4293292708756328]
	TIME [epoch: 2.86 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24746748159968526		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.24746748159968526 | validation: 0.3746355718468239]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25306488638540514		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.25306488638540514 | validation: 0.4945787614084739]
	TIME [epoch: 2.86 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25235923190785337		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.25235923190785337 | validation: 0.38149295465519767]
	TIME [epoch: 2.86 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27945688795366735		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.27945688795366735 | validation: 0.6525273280109734]
	TIME [epoch: 2.87 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32382592056273524		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.32382592056273524 | validation: 0.3663952217020567]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24179490950314464		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.24179490950314464 | validation: 0.4790646029854978]
	TIME [epoch: 2.86 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743570293902276		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.2743570293902276 | validation: 0.5809749516053752]
	TIME [epoch: 2.86 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36659851328322246		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.36659851328322246 | validation: 0.4096170774113043]
	TIME [epoch: 2.86 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28625401147861995		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.28625401147861995 | validation: 0.37417284324507705]
	TIME [epoch: 2.86 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21236443051315582		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.21236443051315582 | validation: 0.46610237170063606]
	TIME [epoch: 2.86 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22321750583480568		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.22321750583480568 | validation: 0.3344326016985595]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2429793194883718		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.2429793194883718 | validation: 0.5138527641977227]
	TIME [epoch: 2.87 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24401465525489688		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.24401465525489688 | validation: 0.33114717825915563]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21396955558153125		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.21396955558153125 | validation: 0.47544131017715663]
	TIME [epoch: 2.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20963870438324736		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.20963870438324736 | validation: 0.33129980688386534]
	TIME [epoch: 2.86 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23063211247264992		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.23063211247264992 | validation: 0.6150411042394911]
	TIME [epoch: 2.87 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102409771537278		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.3102409771537278 | validation: 0.4560462514064394]
	TIME [epoch: 2.86 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21040310521048175		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.21040310521048175 | validation: 0.3138568871739306]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23385662909215016		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.23385662909215016 | validation: 0.5868409559814397]
	TIME [epoch: 2.86 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25259393270379277		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.25259393270379277 | validation: 0.3617779817936081]
	TIME [epoch: 2.86 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17109681940194407		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.17109681940194407 | validation: 0.3384618798034098]
	TIME [epoch: 2.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17992397444539737		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.17992397444539737 | validation: 0.44934832509129347]
	TIME [epoch: 2.87 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22366105702372746		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.22366105702372746 | validation: 0.3420499135148024]
	TIME [epoch: 2.87 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332116752970807		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2332116752970807 | validation: 0.40963214785640134]
	TIME [epoch: 2.87 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449858868533839		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.2449858868533839 | validation: 0.49080985541874134]
	TIME [epoch: 2.87 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25262831972944655		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.25262831972944655 | validation: 0.41944842061265125]
	TIME [epoch: 2.87 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21135453140322327		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.21135453140322327 | validation: 0.3183777489047928]
	TIME [epoch: 2.87 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15806242291473685		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.15806242291473685 | validation: 0.46173453321476476]
	TIME [epoch: 2.87 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19325942493975706		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.19325942493975706 | validation: 0.3207229288828266]
	TIME [epoch: 2.87 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18207920094116908		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.18207920094116908 | validation: 0.5307625031872883]
	TIME [epoch: 2.86 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2005895216368193		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.2005895216368193 | validation: 0.290911145037545]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19891297741834923		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.19891297741834923 | validation: 0.5132001955521969]
	TIME [epoch: 2.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21284610685352906		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.21284610685352906 | validation: 0.30290601834315045]
	TIME [epoch: 2.84 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14905368568285943		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.14905368568285943 | validation: 0.3213136532219319]
	TIME [epoch: 2.87 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341950571117612		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.1341950571117612 | validation: 0.37619215947538187]
	TIME [epoch: 2.87 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13694918038485412		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.13694918038485412 | validation: 0.2869006919431453]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15331570312087103		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.15331570312087103 | validation: 0.5514578419696308]
	TIME [epoch: 2.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20003951050147975		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.20003951050147975 | validation: 0.27516804541146045]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14696628062318862		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.14696628062318862 | validation: 0.4233068043137255]
	TIME [epoch: 2.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14264812197290624		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.14264812197290624 | validation: 0.2825844453496834]
	TIME [epoch: 2.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13959351518226645		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.13959351518226645 | validation: 0.48313000524614974]
	TIME [epoch: 2.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21429831193905893		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.21429831193905893 | validation: 0.310326061327984]
	TIME [epoch: 2.84 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3316280914634574		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3316280914634574 | validation: 0.42338871962196195]
	TIME [epoch: 2.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22646333496220258		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.22646333496220258 | validation: 0.4197681449601116]
	TIME [epoch: 2.85 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14964622910435407		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.14964622910435407 | validation: 0.27164259802448115]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16420497032088313		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.16420497032088313 | validation: 0.334583429599308]
	TIME [epoch: 2.86 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12054292436222769		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.12054292436222769 | validation: 0.31035007184624064]
	TIME [epoch: 2.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061038649790933		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.11061038649790933 | validation: 0.30011634741340504]
	TIME [epoch: 2.86 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11330810787951524		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.11330810787951524 | validation: 0.3389730582006494]
	TIME [epoch: 2.87 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181850503591744		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.1181850503591744 | validation: 0.3059415741298247]
	TIME [epoch: 2.86 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15015095856326943		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.15015095856326943 | validation: 0.6271215254780643]
	TIME [epoch: 2.86 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23291424092676566		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.23291424092676566 | validation: 0.2753379926712047]
	TIME [epoch: 2.87 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16318948630796234		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.16318948630796234 | validation: 0.4213796996133026]
	TIME [epoch: 2.86 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394355580577783		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.1394355580577783 | validation: 0.29818583805809223]
	TIME [epoch: 2.87 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12159588153075802		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.12159588153075802 | validation: 0.2886020499686252]
	TIME [epoch: 2.86 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12547784294030545		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.12547784294030545 | validation: 0.3334967443455103]
	TIME [epoch: 2.87 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12404162866368718		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.12404162866368718 | validation: 0.2968209063345277]
	TIME [epoch: 2.84 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913263309180616		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.11913263309180616 | validation: 0.32261694717522976]
	TIME [epoch: 2.84 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11331243602044815		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.11331243602044815 | validation: 0.28946364234330607]
	TIME [epoch: 2.84 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1028392323496432		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.1028392323496432 | validation: 0.38152396222965335]
	TIME [epoch: 2.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794926247632315		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.10794926247632315 | validation: 0.2608977872381432]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12000587570346646		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.12000587570346646 | validation: 0.6219639069219877]
	TIME [epoch: 2.87 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22122009673544207		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.22122009673544207 | validation: 0.23996425857338566]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23878716575734354		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.23878716575734354 | validation: 0.38379112227184076]
	TIME [epoch: 2.86 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21980829240210478		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.21980829240210478 | validation: 0.4777075111337894]
	TIME [epoch: 2.86 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14313012314549567		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.14313012314549567 | validation: 0.2523436025368822]
	TIME [epoch: 2.86 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11396035980196524		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.11396035980196524 | validation: 0.3181976619061626]
	TIME [epoch: 2.86 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14495466924895067		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.14495466924895067 | validation: 0.3912549859763961]
	TIME [epoch: 2.87 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11012752400738948		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.11012752400738948 | validation: 0.26956599719011254]
	TIME [epoch: 2.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10156902847720481		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.10156902847720481 | validation: 0.3122584456208703]
	TIME [epoch: 2.87 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10562080192858556		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.10562080192858556 | validation: 0.2768143884564403]
	TIME [epoch: 2.86 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08979421903399193		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.08979421903399193 | validation: 0.26775863662598576]
	TIME [epoch: 2.85 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039365540889877		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.08039365540889877 | validation: 0.29047125471081375]
	TIME [epoch: 2.84 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330806928140028		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.08330806928140028 | validation: 0.23193866811934552]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887045613616978		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.08887045613616978 | validation: 0.38690015073346873]
	TIME [epoch: 2.85 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12182797406459582		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.12182797406459582 | validation: 0.24396727639167787]
	TIME [epoch: 2.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18994774897960234		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.18994774897960234 | validation: 0.6974645400676256]
	TIME [epoch: 2.84 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26374745259790944		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.26374745259790944 | validation: 0.32224957809523763]
	TIME [epoch: 2.86 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18672166226742515		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.18672166226742515 | validation: 0.394881316722487]
	TIME [epoch: 2.86 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1811640254022148		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.1811640254022148 | validation: 0.3710768102053405]
	TIME [epoch: 2.86 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12215817803755821		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.12215817803755821 | validation: 0.26315904089693803]
	TIME [epoch: 2.86 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09592964014622837		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.09592964014622837 | validation: 0.3217605465038518]
	TIME [epoch: 2.86 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10914508899928287		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.10914508899928287 | validation: 0.27190741593729756]
	TIME [epoch: 2.86 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08532364440436167		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.08532364440436167 | validation: 0.249634240697932]
	TIME [epoch: 2.86 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435879272807363		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.08435879272807363 | validation: 0.3083570319055504]
	TIME [epoch: 2.86 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08628507667978955		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.08628507667978955 | validation: 0.2714139188864477]
	TIME [epoch: 2.87 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087999205913121		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.08087999205913121 | validation: 0.26123614295298164]
	TIME [epoch: 2.86 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0740103177210268		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.0740103177210268 | validation: 0.2727594102185932]
	TIME [epoch: 2.84 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0793520072401015		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.0793520072401015 | validation: 0.25284708329502453]
	TIME [epoch: 2.86 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468732891316283		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.08468732891316283 | validation: 0.2904571608778165]
	TIME [epoch: 2.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09762230336429284		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.09762230336429284 | validation: 0.22893256025181366]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738221192503042		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.1738221192503042 | validation: 0.4256468869788919]
	TIME [epoch: 2.85 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200477318538686		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.2200477318538686 | validation: 0.2533509029869049]
	TIME [epoch: 2.84 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748338256135442		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.0748338256135442 | validation: 0.20217348423070647]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13172903809102304		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.13172903809102304 | validation: 0.4728260287784933]
	TIME [epoch: 2.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473430984952854		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.1473430984952854 | validation: 0.23321908553014337]
	TIME [epoch: 2.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08485096256212678		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.08485096256212678 | validation: 0.41543600836186817]
	TIME [epoch: 2.84 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12483982286884385		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.12483982286884385 | validation: 0.20721939798896477]
	TIME [epoch: 2.84 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09064721852241871		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.09064721852241871 | validation: 0.4651015945463966]
	TIME [epoch: 2.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11443479292695495		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.11443479292695495 | validation: 0.2363186965091555]
	TIME [epoch: 2.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849761163850929		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.08849761163850929 | validation: 0.3219223142374893]
	TIME [epoch: 2.84 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08579546354989981		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.08579546354989981 | validation: 0.21380193873299974]
	TIME [epoch: 2.84 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500080571823475		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.07500080571823475 | validation: 0.30366136901740637]
	TIME [epoch: 2.84 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631502762787298		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.07631502762787298 | validation: 0.22764639309057355]
	TIME [epoch: 2.84 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08380326604469623		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.08380326604469623 | validation: 0.359638941586135]
	TIME [epoch: 2.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10792387193133135		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.10792387193133135 | validation: 0.2660307442978728]
	TIME [epoch: 2.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11724485712710436		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.11724485712710436 | validation: 0.28872734191733734]
	TIME [epoch: 2.84 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11753604996175444		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.11753604996175444 | validation: 0.31768317469519886]
	TIME [epoch: 2.84 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506212312073843		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.1506212312073843 | validation: 0.22936508005504158]
	TIME [epoch: 2.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537077882501144		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.08537077882501144 | validation: 0.42506459478295633]
	TIME [epoch: 2.84 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1048664640251602		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.1048664640251602 | validation: 0.23476551491829298]
	TIME [epoch: 2.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887073020762628		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.09887073020762628 | validation: 0.4852404002492876]
	TIME [epoch: 2.84 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11338164953710488		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.11338164953710488 | validation: 0.21012607508220194]
	TIME [epoch: 2.84 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07844062734131704		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.07844062734131704 | validation: 0.2561138716516343]
	TIME [epoch: 2.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838397658340526		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.0838397658340526 | validation: 0.24322792767167345]
	TIME [epoch: 2.84 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06315366049952878		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.06315366049952878 | validation: 0.21640602064949938]
	TIME [epoch: 2.84 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06003818688356215		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.06003818688356215 | validation: 0.24329061628310378]
	TIME [epoch: 2.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061874969359736395		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.061874969359736395 | validation: 0.18997762387184736]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752169732643645		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.06752169732643645 | validation: 0.3409727119734731]
	TIME [epoch: 2.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08715728852737371		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.08715728852737371 | validation: 0.20168452169203266]
	TIME [epoch: 2.84 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12512211518753433		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.12512211518753433 | validation: 0.6348028788439395]
	TIME [epoch: 2.84 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21712093445633485		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.21712093445633485 | validation: 0.2120712447262464]
	TIME [epoch: 2.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951972260717102		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.07951972260717102 | validation: 0.19783865500748643]
	TIME [epoch: 2.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11331533717742201		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.11331533717742201 | validation: 0.2561979011820735]
	TIME [epoch: 2.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754225968424517		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.11754225968424517 | validation: 0.27569516604013095]
	TIME [epoch: 2.84 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06156917850369237		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.06156917850369237 | validation: 0.18293400478741823]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468703575457233		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.07468703575457233 | validation: 0.22218207081100766]
	TIME [epoch: 2.85 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06572862674634145		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.06572862674634145 | validation: 0.2670701535899447]
	TIME [epoch: 2.85 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06878436494505671		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.06878436494505671 | validation: 0.21462393596278084]
	TIME [epoch: 2.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422822291939931		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.09422822291939931 | validation: 0.4745734081929506]
	TIME [epoch: 2.85 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11142728280371593		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.11142728280371593 | validation: 0.2028860745756203]
	TIME [epoch: 2.84 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06107653751390405		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.06107653751390405 | validation: 0.18367205681949483]
	TIME [epoch: 2.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06502737532599398		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.06502737532599398 | validation: 0.30291023004650885]
	TIME [epoch: 2.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936844621252716		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.06936844621252716 | validation: 0.1917119650733306]
	TIME [epoch: 2.85 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684569936600686		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.0684569936600686 | validation: 0.3616563688885248]
	TIME [epoch: 2.84 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08026462156827893		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.08026462156827893 | validation: 0.19038129051409194]
	TIME [epoch: 2.85 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0603795496697381		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.0603795496697381 | validation: 0.2005204516703107]
	TIME [epoch: 2.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737233424598309		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.0737233424598309 | validation: 0.42806850468346325]
	TIME [epoch: 2.85 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12599879654093113		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.12599879654093113 | validation: 0.2148463256523117]
	TIME [epoch: 2.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09454450791417582		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.09454450791417582 | validation: 0.1732218343318468]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06693949252568736		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.06693949252568736 | validation: 0.27635263311893093]
	TIME [epoch: 2.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487465829890008		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.10487465829890008 | validation: 0.1790035513497824]
	TIME [epoch: 2.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513692171320243		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.06513692171320243 | validation: 0.3949260931786641]
	TIME [epoch: 2.84 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08319820618265882		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.08319820618265882 | validation: 0.21412645255648177]
	TIME [epoch: 2.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122382235768384		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.08122382235768384 | validation: 0.39574564271090273]
	TIME [epoch: 2.84 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08870905565315657		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.08870905565315657 | validation: 0.30982503737097916]
	TIME [epoch: 2.84 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16869793783589504		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.16869793783589504 | validation: 0.19947925267746808]
	TIME [epoch: 2.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856904995676352		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.06856904995676352 | validation: 0.3539390034870145]
	TIME [epoch: 2.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10979631881637424		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.10979631881637424 | validation: 0.28347242346062296]
	TIME [epoch: 2.85 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10276444185383991		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.10276444185383991 | validation: 0.18776834859643216]
	TIME [epoch: 2.84 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386831330137416		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.06386831330137416 | validation: 0.3571597887268689]
	TIME [epoch: 2.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10023978836577899		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.10023978836577899 | validation: 0.20264009732742025]
	TIME [epoch: 2.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06722761548041921		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.06722761548041921 | validation: 0.1941859593254515]
	TIME [epoch: 2.84 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131253260855501		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.06131253260855501 | validation: 0.3913133047482735]
	TIME [epoch: 2.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792463780195844		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.07792463780195844 | validation: 0.18132837306006802]
	TIME [epoch: 2.85 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051033402298688		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.06051033402298688 | validation: 0.28241550989833475]
	TIME [epoch: 2.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798975652739369		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.05798975652739369 | validation: 0.17467781526958717]
	TIME [epoch: 2.84 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045183859669215315		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.045183859669215315 | validation: 0.2026746035575235]
	TIME [epoch: 2.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0498182648162956		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.0498182648162956 | validation: 0.16006000619706368]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504860842904874		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.05504860842904874 | validation: 0.37444108463602]
	TIME [epoch: 2.85 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725896684578994		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.0725896684578994 | validation: 0.17068971652028042]
	TIME [epoch: 2.85 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116548739932738		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.07116548739932738 | validation: 0.2860654810133476]
	TIME [epoch: 2.85 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756916113339337		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.1756916113339337 | validation: 0.2387295636777081]
	TIME [epoch: 2.85 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05474761966429235		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.05474761966429235 | validation: 0.1641979713675464]
	TIME [epoch: 2.84 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05861621954121338		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.05861621954121338 | validation: 0.23419667685346066]
	TIME [epoch: 2.85 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434989341684846		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.06434989341684846 | validation: 0.18398218844451836]
	TIME [epoch: 2.85 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04352262687841074		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.04352262687841074 | validation: 0.1579803905570174]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04468778578990632		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.04468778578990632 | validation: 0.16743533983208309]
	TIME [epoch: 2.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04212792796715764		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.04212792796715764 | validation: 0.1493384770020996]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04744434988983017		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.04744434988983017 | validation: 0.2758949479285365]
	TIME [epoch: 2.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534587680629035		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.05534587680629035 | validation: 0.16384532270357793]
	TIME [epoch: 2.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140052724504777		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.06140052724504777 | validation: 0.48807304938303475]
	TIME [epoch: 2.84 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11190085282008594		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.11190085282008594 | validation: 0.1721047356146772]
	TIME [epoch: 2.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690143192539161		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.06690143192539161 | validation: 0.22406369552820993]
	TIME [epoch: 2.85 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447269241131452		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.07447269241131452 | validation: 0.18322323110980987]
	TIME [epoch: 2.85 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348017193729128		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.07348017193729128 | validation: 0.13806879459818108]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504818168143716		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.05504818168143716 | validation: 0.25138977795312745]
	TIME [epoch: 2.87 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057858930962159494		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.057858930962159494 | validation: 0.1586815049951159]
	TIME [epoch: 2.88 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06854584204767898		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.06854584204767898 | validation: 0.40315860079333876]
	TIME [epoch: 2.87 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821630983444693		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.07821630983444693 | validation: 0.13115740914096882]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04797801595610712		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.04797801595610712 | validation: 0.1426532462472379]
	TIME [epoch: 2.85 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906180998287614		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.03906180998287614 | validation: 0.21160269645618107]
	TIME [epoch: 2.85 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534297798273156		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.04534297798273156 | validation: 0.19041057330675204]
	TIME [epoch: 2.87 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0802593148099332		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.0802593148099332 | validation: 0.29935453241272764]
	TIME [epoch: 2.87 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11702641287553317		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.11702641287553317 | validation: 0.20559580752842616]
	TIME [epoch: 2.88 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08703528725328237		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.08703528725328237 | validation: 0.14045553372229286]
	TIME [epoch: 2.87 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048077471409044094		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.048077471409044094 | validation: 0.3567293042000328]
	TIME [epoch: 2.87 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06944230097618216		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.06944230097618216 | validation: 0.1324844239201775]
	TIME [epoch: 3.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943528130471724		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.03943528130471724 | validation: 0.12582574753456388]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042509398215975144		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.042509398215975144 | validation: 0.21392133576148442]
	TIME [epoch: 2.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743909975222065		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.04743909975222065 | validation: 0.13562786214494402]
	TIME [epoch: 2.85 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041965521011081156		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.041965521011081156 | validation: 0.17462692250665768]
	TIME [epoch: 2.85 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03734155142034315		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.03734155142034315 | validation: 0.1376243310316538]
	TIME [epoch: 2.84 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03895575026574946		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.03895575026574946 | validation: 0.17402827238688867]
	TIME [epoch: 2.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434601747996251		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.04434601747996251 | validation: 0.1370432075850331]
	TIME [epoch: 2.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051203395639279954		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.051203395639279954 | validation: 0.14918984903496738]
	TIME [epoch: 2.84 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487843617117953		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.0487843617117953 | validation: 0.30510606011639124]
	TIME [epoch: 2.84 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25224128219800446		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.25224128219800446 | validation: 0.22592615753319759]
	TIME [epoch: 2.85 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06139062101258724		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.06139062101258724 | validation: 0.12697479085624602]
	TIME [epoch: 2.84 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16162398508206974		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.16162398508206974 | validation: 0.17815363314771337]
	TIME [epoch: 2.85 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04748783323389845		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.04748783323389845 | validation: 0.161707781787933]
	TIME [epoch: 2.84 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170716416800838		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.05170716416800838 | validation: 0.11524099399305054]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042934562675794756		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.042934562675794756 | validation: 0.14538697241911716]
	TIME [epoch: 2.87 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03361607203835239		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.03361607203835239 | validation: 0.12571812677949343]
	TIME [epoch: 2.87 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039585164635313495		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.039585164635313495 | validation: 0.2196502424884631]
	TIME [epoch: 2.87 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047202542042522395		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.047202542042522395 | validation: 0.15572670934883426]
	TIME [epoch: 2.87 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898159376990779		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.05898159376990779 | validation: 0.33785180486909927]
	TIME [epoch: 2.86 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07482293470339839		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.07482293470339839 | validation: 0.15912590157732198]
	TIME [epoch: 2.87 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891700685470499		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.04891700685470499 | validation: 0.20183014929435747]
	TIME [epoch: 2.86 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460359783828746		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.0460359783828746 | validation: 0.14454708096674793]
	TIME [epoch: 2.87 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039152002037719284		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.039152002037719284 | validation: 0.1289642430152556]
	TIME [epoch: 2.87 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0480575825981806		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.0480575825981806 | validation: 0.24000150969438008]
	TIME [epoch: 2.87 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013103754456214		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.05013103754456214 | validation: 0.12454149763642586]
	TIME [epoch: 2.86 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037864711630056315		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.037864711630056315 | validation: 0.1630710525530571]
	TIME [epoch: 2.87 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036556537517937006		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.036556537517937006 | validation: 0.107647239183183]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03343971787163704		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.03343971787163704 | validation: 0.13607290909466147]
	TIME [epoch: 2.86 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040476755831818145		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.040476755831818145 | validation: 0.11271503850159062]
	TIME [epoch: 2.86 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035493083075290206		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.035493083075290206 | validation: 0.1959547312077682]
	TIME [epoch: 2.86 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957902745521747		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.03957902745521747 | validation: 0.1324828735426197]
	TIME [epoch: 2.86 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03849281759552105		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.03849281759552105 | validation: 0.10641839924819836]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022451493860682		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.05022451493860682 | validation: 0.26083383669081706]
	TIME [epoch: 2.86 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08560943060492328		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.08560943060492328 | validation: 0.10813441138802289]
	TIME [epoch: 2.86 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03165138163597402		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.03165138163597402 | validation: 0.09576374739869198]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032433412530420376		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.032433412530420376 | validation: 0.11598789569173669]
	TIME [epoch: 2.86 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03304945748485301		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.03304945748485301 | validation: 0.244196013850235]
	TIME [epoch: 2.86 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05229095587691519		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.05229095587691519 | validation: 0.17903496165532062]
	TIME [epoch: 2.86 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06676040868984387		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.06676040868984387 | validation: 0.46945179113447283]
	TIME [epoch: 2.85 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11455727891402952		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.11455727891402952 | validation: 0.13803220265649557]
	TIME [epoch: 2.86 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679940247688194		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.06679940247688194 | validation: 0.13169307125113106]
	TIME [epoch: 2.85 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04836746619649958		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.04836746619649958 | validation: 0.21815985542347438]
	TIME [epoch: 2.86 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04294120978606879		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.04294120978606879 | validation: 0.1065535951508049]
	TIME [epoch: 2.86 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02959699744579818		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.02959699744579818 | validation: 0.10485088267915593]
	TIME [epoch: 2.86 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441493940574456		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.03441493940574456 | validation: 0.14242436384747068]
	TIME [epoch: 2.85 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032476435059019024		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.032476435059019024 | validation: 0.10585130775086836]
	TIME [epoch: 2.86 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029122374212488082		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.029122374212488082 | validation: 0.1374799715180571]
	TIME [epoch: 2.86 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510721132047		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.06510721132047 | validation: 0.11275128708366766]
	TIME [epoch: 2.86 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04109141900214903		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.04109141900214903 | validation: 0.12595705728389636]
	TIME [epoch: 2.86 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039327262329088335		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.039327262329088335 | validation: 0.0909909942649459]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03630812859352151		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.03630812859352151 | validation: 0.13586984310778694]
	TIME [epoch: 2.86 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04845736746049304		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.04845736746049304 | validation: 0.12688255294741127]
	TIME [epoch: 2.86 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046518432259213885		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.046518432259213885 | validation: 0.1581199446438022]
	TIME [epoch: 2.86 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08885280017512584		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.08885280017512584 | validation: 0.17049153652706395]
	TIME [epoch: 2.86 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04027496298976502		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.04027496298976502 | validation: 0.10981645296943379]
	TIME [epoch: 2.86 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009361713062521		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08009361713062521 | validation: 0.14718591509571746]
	TIME [epoch: 2.86 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694822253433338		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.0694822253433338 | validation: 0.15264701203631678]
	TIME [epoch: 2.86 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786308839722731		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.04786308839722731 | validation: 0.20535737553972983]
	TIME [epoch: 2.85 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056653395934027156		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.056653395934027156 | validation: 0.10847006035052509]
	TIME [epoch: 2.86 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418352116867105		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.03418352116867105 | validation: 0.10961997165100877]
	TIME [epoch: 2.86 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03326436427870293		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.03326436427870293 | validation: 0.14393044079232564]
	TIME [epoch: 2.86 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034511719501230116		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.034511719501230116 | validation: 0.1159182164597271]
	TIME [epoch: 2.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02784218995530333		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.02784218995530333 | validation: 0.12597496657293367]
	TIME [epoch: 2.85 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03034165612146026		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.03034165612146026 | validation: 0.12892460669761616]
	TIME [epoch: 2.86 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04156049758452122		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.04156049758452122 | validation: 0.30568259440172985]
	TIME [epoch: 2.86 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055371889657103004		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.055371889657103004 | validation: 0.09366679349420944]
	TIME [epoch: 2.86 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02773799348821		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.02773799348821 | validation: 0.10865679932077271]
	TIME [epoch: 2.86 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03314167653422397		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.03314167653422397 | validation: 0.28349145555241]
	TIME [epoch: 2.86 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0469825608392368		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.0469825608392368 | validation: 0.0960697315560267]
	TIME [epoch: 2.86 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031678236778131676		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.031678236778131676 | validation: 0.07922947051129281]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031394897650990926		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.031394897650990926 | validation: 0.1561728291799757]
	TIME [epoch: 2.86 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046565449007796834		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.046565449007796834 | validation: 0.09313813419790286]
	TIME [epoch: 2.86 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027915454391586315		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.027915454391586315 | validation: 0.13783156172963903]
	TIME [epoch: 2.86 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0302953203782602		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.0302953203782602 | validation: 0.10034380264450378]
	TIME [epoch: 2.85 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579512521885488		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.03579512521885488 | validation: 0.1820748016903458]
	TIME [epoch: 2.86 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057305055586773734		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.057305055586773734 | validation: 0.11727492922399133]
	TIME [epoch: 2.85 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06463535393761621		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.06463535393761621 | validation: 0.09126459114560077]
	TIME [epoch: 2.86 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032652530450697485		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.032652530450697485 | validation: 0.0919264117008412]
	TIME [epoch: 2.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026445424951193956		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.026445424951193956 | validation: 0.1440788931212971]
	TIME [epoch: 2.86 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029814538627006573		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.029814538627006573 | validation: 0.11131558904937802]
	TIME [epoch: 2.85 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037651069369296956		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.037651069369296956 | validation: 0.2005847876260127]
	TIME [epoch: 2.85 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04164294080329475		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.04164294080329475 | validation: 0.08959061964985304]
	TIME [epoch: 2.85 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030221854549828092		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.030221854549828092 | validation: 0.07920567198039113]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02342502482617413		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.02342502482617413 | validation: 0.11996847686014832]
	TIME [epoch: 191 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025318367183748327		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.025318367183748327 | validation: 0.0902179125896736]
	TIME [epoch: 6.18 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028288284948909795		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.028288284948909795 | validation: 0.11437037309858084]
	TIME [epoch: 6.17 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028428343564610794		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.028428343564610794 | validation: 0.0921545330238301]
	TIME [epoch: 6.16 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026915030308185168		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.026915030308185168 | validation: 0.17808514098399025]
	TIME [epoch: 6.16 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038191040753568474		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.038191040753568474 | validation: 0.12126365698317665]
	TIME [epoch: 6.16 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880777341492406		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.04880777341492406 | validation: 0.1416425664705759]
	TIME [epoch: 6.19 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0373905920059162		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.0373905920059162 | validation: 0.10756318847883846]
	TIME [epoch: 6.17 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056168956283357		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.056168956283357 | validation: 0.13358721989822245]
	TIME [epoch: 6.16 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028746108148575037		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.028746108148575037 | validation: 0.07816840039800067]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02883442398494432		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.02883442398494432 | validation: 0.16845881608800706]
	TIME [epoch: 6.16 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486463101967811		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.05486463101967811 | validation: 0.1242692808930995]
	TIME [epoch: 6.17 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060982787649956		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.04060982787649956 | validation: 0.1915457726797767]
	TIME [epoch: 6.16 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047144994049869715		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.047144994049869715 | validation: 0.1265683739878909]
	TIME [epoch: 6.16 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046086581539554895		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.046086581539554895 | validation: 0.09049083796169155]
	TIME [epoch: 6.16 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028611755149977738		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.028611755149977738 | validation: 0.08991922906656397]
	TIME [epoch: 6.16 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022818167078302795		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.022818167078302795 | validation: 0.09023480983090712]
	TIME [epoch: 6.16 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026925880493859564		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.026925880493859564 | validation: 0.10371640542802774]
	TIME [epoch: 6.16 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024153529204980907		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.024153529204980907 | validation: 0.07511264229578703]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02884332294617456		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.02884332294617456 | validation: 0.12050810779022872]
	TIME [epoch: 6.16 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042830384157671174		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.042830384157671174 | validation: 0.24663163621589615]
	TIME [epoch: 6.16 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049957590183749644		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.049957590183749644 | validation: 0.10069934530199515]
	TIME [epoch: 6.15 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03633346397644485		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.03633346397644485 | validation: 0.08837038539348058]
	TIME [epoch: 6.16 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025391917298358335		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.025391917298358335 | validation: 0.10012812169082888]
	TIME [epoch: 6.15 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034062326529711376		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.034062326529711376 | validation: 0.09330457745287579]
	TIME [epoch: 6.16 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036153547473231386		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.036153547473231386 | validation: 0.15370650891892673]
	TIME [epoch: 6.15 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321293548617612		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.03321293548617612 | validation: 0.12697695221525535]
	TIME [epoch: 6.15 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034010354303354494		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.034010354303354494 | validation: 0.12062367954567588]
	TIME [epoch: 6.16 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02845475161044613		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.02845475161044613 | validation: 0.09682945624240838]
	TIME [epoch: 6.16 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027779444969096786		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.027779444969096786 | validation: 0.1172829364974683]
	TIME [epoch: 6.16 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05246466090724608		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.05246466090724608 | validation: 0.1271636375104339]
	TIME [epoch: 6.16 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293150026928965		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.04293150026928965 | validation: 0.10394088326512413]
	TIME [epoch: 6.16 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022471549552730643		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.022471549552730643 | validation: 0.0790854145932067]
	TIME [epoch: 6.15 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027397929737541946		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.027397929737541946 | validation: 0.07923076930758827]
	TIME [epoch: 6.16 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02179267579088746		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.02179267579088746 | validation: 0.09502738491425151]
	TIME [epoch: 6.16 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02162885907317496		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.02162885907317496 | validation: 0.06886318125509171]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023038987354353672		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.023038987354353672 | validation: 0.07313016500368584]
	TIME [epoch: 6.17 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023255104733807687		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.023255104733807687 | validation: 0.11516458245661881]
	TIME [epoch: 6.16 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06854383159972283		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.06854383159972283 | validation: 0.20750497680822677]
	TIME [epoch: 6.16 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760561857635108		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.03760561857635108 | validation: 0.09259365894605007]
	TIME [epoch: 6.16 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254748379377419		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.03254748379377419 | validation: 0.081075663479654]
	TIME [epoch: 6.15 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029222490867489716		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.029222490867489716 | validation: 0.0722040561143588]
	TIME [epoch: 6.15 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022512280485658655		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.022512280485658655 | validation: 0.12315342027317411]
	TIME [epoch: 6.15 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989917303115176		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.03989917303115176 | validation: 0.20240057018651259]
	TIME [epoch: 6.16 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867254400844126		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.03867254400844126 | validation: 0.08712619110058445]
	TIME [epoch: 6.15 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03345089238539724		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.03345089238539724 | validation: 0.08886127220209145]
	TIME [epoch: 6.17 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026529104701242583		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.026529104701242583 | validation: 0.11954411902037979]
	TIME [epoch: 6.16 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593510759776582		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.03593510759776582 | validation: 0.1408179939811162]
	TIME [epoch: 6.16 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627660788620404		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.0627660788620404 | validation: 0.083898349237483]
	TIME [epoch: 6.16 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02346563498638878		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.02346563498638878 | validation: 0.07950099292820123]
	TIME [epoch: 6.16 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049511915743425956		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.049511915743425956 | validation: 0.10267315354334577]
	TIME [epoch: 6.16 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717279890887476		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.03717279890887476 | validation: 0.08825407244203062]
	TIME [epoch: 6.16 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020255518053562422		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.020255518053562422 | validation: 0.0655884760116924]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025185291693303028		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.025185291693303028 | validation: 0.07150121149062576]
	TIME [epoch: 6.16 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022842114575562324		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.022842114575562324 | validation: 0.06534681676950792]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020162752394651866		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.020162752394651866 | validation: 0.059505423764924015]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02219830973726106		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.02219830973726106 | validation: 0.06443401138834315]
	TIME [epoch: 6.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02008946643336779		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.02008946643336779 | validation: 0.07219219203447981]
	TIME [epoch: 6.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020287960975580034		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.020287960975580034 | validation: 0.0877745934580928]
	TIME [epoch: 6.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401630175866366		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.0401630175866366 | validation: 0.16440730820177596]
	TIME [epoch: 6.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039729957702612013		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.039729957702612013 | validation: 0.11669514101339806]
	TIME [epoch: 6.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03666183816369322		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.03666183816369322 | validation: 0.07089180459890461]
	TIME [epoch: 6.11 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02036113471797359		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.02036113471797359 | validation: 0.09865204519060936]
	TIME [epoch: 6.11 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03008824926348033		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.03008824926348033 | validation: 0.08037790905362176]
	TIME [epoch: 6.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024742659018125338		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.024742659018125338 | validation: 0.0938738097776801]
	TIME [epoch: 6.11 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022403281915574304		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.022403281915574304 | validation: 0.06579185860442598]
	TIME [epoch: 6.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0182142310115975		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.0182142310115975 | validation: 0.07831080096869358]
	TIME [epoch: 6.11 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020954715623368463		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.020954715623368463 | validation: 0.12145723274353176]
	TIME [epoch: 6.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028139422961568374		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.028139422961568374 | validation: 0.08960754395752635]
	TIME [epoch: 6.09 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053780969308471996		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.053780969308471996 | validation: 0.07308155754623728]
	TIME [epoch: 6.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020786910550028476		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.020786910550028476 | validation: 0.05641028619375957]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875727209432573		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.02875727209432573 | validation: 0.10019529160086993]
	TIME [epoch: 6.15 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027685437097043596		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.027685437097043596 | validation: 0.06385904193903949]
	TIME [epoch: 6.15 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021166644801753117		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.021166644801753117 | validation: 0.10314719333343564]
	TIME [epoch: 6.14 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03315498818075306		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.03315498818075306 | validation: 0.22904446891955965]
	TIME [epoch: 6.15 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400264462278605		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.03400264462278605 | validation: 0.1180195795443781]
	TIME [epoch: 6.14 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414671018682456		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.03414671018682456 | validation: 0.0903705302441855]
	TIME [epoch: 6.15 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01982885773743348		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.01982885773743348 | validation: 0.08692885989635749]
	TIME [epoch: 6.14 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017485606874159704		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.017485606874159704 | validation: 0.07067765117744583]
	TIME [epoch: 6.15 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019221712340905476		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.019221712340905476 | validation: 0.06069191209484476]
	TIME [epoch: 6.15 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01829154128789158		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.01829154128789158 | validation: 0.08875856957825728]
	TIME [epoch: 6.14 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03054001847244554		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.03054001847244554 | validation: 0.06193451677635758]
	TIME [epoch: 6.14 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024798123831423206		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.024798123831423206 | validation: 0.10395014285791078]
	TIME [epoch: 6.14 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06532789651399735		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.06532789651399735 | validation: 0.12738933810867745]
	TIME [epoch: 6.13 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023527264027076736		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.023527264027076736 | validation: 0.06727236935971266]
	TIME [epoch: 6.14 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474659655429145		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.03474659655429145 | validation: 0.08126980729525235]
	TIME [epoch: 6.13 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026646802714311718		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.026646802714311718 | validation: 0.08770006782265607]
	TIME [epoch: 6.14 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020569650923331278		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.020569650923331278 | validation: 0.0605900457577159]
	TIME [epoch: 6.14 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851267880782955		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.01851267880782955 | validation: 0.06670254562086698]
	TIME [epoch: 6.15 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021725222603424205		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.021725222603424205 | validation: 0.07247174372363123]
	TIME [epoch: 6.15 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019882133750303645		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.019882133750303645 | validation: 0.056822105877539913]
	TIME [epoch: 6.15 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01615139783741889		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.01615139783741889 | validation: 0.05166400712454439]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016262466779715302		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.016262466779715302 | validation: 0.055178594770777016]
	TIME [epoch: 6.15 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01787926106281308		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.01787926106281308 | validation: 0.0707348129815926]
	TIME [epoch: 6.14 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01843922698179193		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.01843922698179193 | validation: 0.0951785155709823]
	TIME [epoch: 6.14 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021411156749644552		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.021411156749644552 | validation: 0.11254365401816441]
	TIME [epoch: 6.15 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029157017959783423		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.029157017959783423 | validation: 0.17536095668124707]
	TIME [epoch: 6.15 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230091868209877		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.03230091868209877 | validation: 0.17881357858547667]
	TIME [epoch: 6.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057381317696880034		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.057381317696880034 | validation: 0.08371843756840994]
	TIME [epoch: 6.11 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028927881991576254		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.028927881991576254 | validation: 0.16569574665764417]
	TIME [epoch: 6.11 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404721151588806		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.03404721151588806 | validation: 0.07838245078361115]
	TIME [epoch: 6.17 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024393791700379377		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.024393791700379377 | validation: 0.10998335988007622]
	TIME [epoch: 6.18 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02526316226118297		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.02526316226118297 | validation: 0.06335159119585676]
	TIME [epoch: 6.17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016915179677097696		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.016915179677097696 | validation: 0.057991165929549164]
	TIME [epoch: 6.17 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01943873002323214		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.01943873002323214 | validation: 0.08480947363437939]
	TIME [epoch: 6.17 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030176616976743658		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.030176616976743658 | validation: 0.07390004135187356]
	TIME [epoch: 6.17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02827224454402605		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.02827224454402605 | validation: 0.05912331964378739]
	TIME [epoch: 6.17 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020575829186900273		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.020575829186900273 | validation: 0.06186499019790553]
	TIME [epoch: 6.17 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018807462541958395		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.018807462541958395 | validation: 0.0586221522709266]
	TIME [epoch: 6.16 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015444708883535326		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.015444708883535326 | validation: 0.05354791059903705]
	TIME [epoch: 6.17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015007655742255715		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.015007655742255715 | validation: 0.058451460629114285]
	TIME [epoch: 6.16 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016943195847583654		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.016943195847583654 | validation: 0.04611819915698876]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015374656882954432		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.015374656882954432 | validation: 0.05163325980033441]
	TIME [epoch: 6.11 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015512893430160444		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.015512893430160444 | validation: 0.060240134698928]
	TIME [epoch: 6.12 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01785516179110773		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.01785516179110773 | validation: 0.0597875552470477]
	TIME [epoch: 6.14 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020981566033926917		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.020981566033926917 | validation: 0.05721217356796389]
	TIME [epoch: 6.19 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019258796230879335		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.019258796230879335 | validation: 0.05430242652992876]
	TIME [epoch: 6.19 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0209707483495791		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.0209707483495791 | validation: 0.07938942287824338]
	TIME [epoch: 6.19 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02365708642082679		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.02365708642082679 | validation: 0.12050132531248253]
	TIME [epoch: 6.19 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036550589921899176		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.036550589921899176 | validation: 0.1743802384931078]
	TIME [epoch: 6.19 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031454433436949605		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.031454433436949605 | validation: 0.0583582983698713]
	TIME [epoch: 6.19 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016187171674003218		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.016187171674003218 | validation: 0.062227298628541955]
	TIME [epoch: 6.19 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018535916865365615		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.018535916865365615 | validation: 0.06635747979303724]
	TIME [epoch: 6.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021848424025973458		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.021848424025973458 | validation: 0.09530465615441659]
	TIME [epoch: 6.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06316806262546369		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.06316806262546369 | validation: 0.04950921225501135]
	TIME [epoch: 6.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022156424010926098		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.022156424010926098 | validation: 0.07700490595000983]
	TIME [epoch: 6.18 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0540594189780345		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.0540594189780345 | validation: 0.07077907397409684]
	TIME [epoch: 6.17 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0290172048448373		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.0290172048448373 | validation: 0.05078499510062706]
	TIME [epoch: 6.18 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016479850334572672		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.016479850334572672 | validation: 0.058278376152152436]
	TIME [epoch: 6.18 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018594307375407567		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.018594307375407567 | validation: 0.05684493285317208]
	TIME [epoch: 6.18 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014752939116858418		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.014752939116858418 | validation: 0.05844038394871634]
	TIME [epoch: 6.19 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01578548668003891		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.01578548668003891 | validation: 0.04939940374022797]
	TIME [epoch: 6.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01892596390832578		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.01892596390832578 | validation: 0.05909345828499283]
	TIME [epoch: 6.17 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015755140630232135		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.015755140630232135 | validation: 0.04725629172992688]
	TIME [epoch: 6.17 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015623069290766735		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.015623069290766735 | validation: 0.05628743564375813]
	TIME [epoch: 6.15 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015829811634811196		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.015829811634811196 | validation: 0.05357825472023839]
	TIME [epoch: 6.15 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01848860428759533		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.01848860428759533 | validation: 0.09436743564424606]
	TIME [epoch: 6.14 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018364379425491694		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.018364379425491694 | validation: 0.04677039377903761]
	TIME [epoch: 6.14 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01582137904801047		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.01582137904801047 | validation: 0.044871336068187606]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015865891605175826		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.015865891605175826 | validation: 0.053775587019608745]
	TIME [epoch: 6.14 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01919907019652964		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.01919907019652964 | validation: 0.06776575896737651]
	TIME [epoch: 6.13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01984790684966		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.01984790684966 | validation: 0.05746962280603392]
	TIME [epoch: 6.14 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02104784188294941		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.02104784188294941 | validation: 0.05724665111396672]
	TIME [epoch: 6.13 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022125126396290697		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.022125126396290697 | validation: 0.08889569173031087]
	TIME [epoch: 6.13 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019833684412254804		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.019833684412254804 | validation: 0.19252631247029384]
	TIME [epoch: 6.13 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07461418813753178		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.07461418813753178 | validation: 0.13119222224948798]
	TIME [epoch: 6.12 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030807722084986666		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.030807722084986666 | validation: 0.10994074587918734]
	TIME [epoch: 6.13 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083326653226461		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.04083326653226461 | validation: 0.06275492580329432]
	TIME [epoch: 6.13 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01765143993536057		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.01765143993536057 | validation: 0.07602763393041927]
	TIME [epoch: 6.14 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018655295109302392		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.018655295109302392 | validation: 0.05366075621883152]
	TIME [epoch: 6.14 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016024502003910982		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.016024502003910982 | validation: 0.05386688688563297]
	TIME [epoch: 6.14 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015389335283304757		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.015389335283304757 | validation: 0.05661841297974728]
	TIME [epoch: 6.13 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01674027910699707		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.01674027910699707 | validation: 0.058036131228402066]
	TIME [epoch: 6.12 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013563514521430394		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.013563514521430394 | validation: 0.06468793541537436]
	TIME [epoch: 6.12 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015202407690976494		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.015202407690976494 | validation: 0.0602175642943985]
	TIME [epoch: 6.13 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01656941740534117		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.01656941740534117 | validation: 0.053916949506387105]
	TIME [epoch: 6.13 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015040456941131337		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.015040456941131337 | validation: 0.10768111325068115]
	TIME [epoch: 6.14 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026020922560820323		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.026020922560820323 | validation: 0.07103885545145605]
	TIME [epoch: 6.13 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015702011751738087		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.015702011751738087 | validation: 0.08338189396614755]
	TIME [epoch: 6.14 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016794122249071668		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.016794122249071668 | validation: 0.073611712010854]
	TIME [epoch: 6.13 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017624354538370565		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.017624354538370565 | validation: 0.04864875503392413]
	TIME [epoch: 6.12 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014103453857380863		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.014103453857380863 | validation: 0.0797048612667412]
	TIME [epoch: 6.12 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0182881048687647		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.0182881048687647 | validation: 0.052887627057765486]
	TIME [epoch: 6.12 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01944524179272621		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.01944524179272621 | validation: 0.05544320454030536]
	TIME [epoch: 6.12 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021253162144205483		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.021253162144205483 | validation: 0.04604646907805293]
	TIME [epoch: 6.12 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017979154336831674		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.017979154336831674 | validation: 0.0412555879771718]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013439697650133127		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.013439697650133127 | validation: 0.04846179477515144]
	TIME [epoch: 6.15 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018891825137876832		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.018891825137876832 | validation: 0.05168655735997514]
	TIME [epoch: 6.13 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02490716486470032		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.02490716486470032 | validation: 0.07339241712012816]
	TIME [epoch: 6.14 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02941344691284417		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.02941344691284417 | validation: 0.11972459725440975]
	TIME [epoch: 6.12 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455898481307569		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06455898481307569 | validation: 0.18997291812478448]
	TIME [epoch: 6.13 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865311578662278		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06865311578662278 | validation: 0.13928810453271542]
	TIME [epoch: 6.13 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025143258497108995		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.025143258497108995 | validation: 0.05562349707862352]
	TIME [epoch: 6.13 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026483313261217934		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.026483313261217934 | validation: 0.08658726010464438]
	TIME [epoch: 6.14 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022942318227064887		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.022942318227064887 | validation: 0.04579628226046011]
	TIME [epoch: 6.15 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013920697078026942		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.013920697078026942 | validation: 0.05835359274044754]
	TIME [epoch: 6.13 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015414629458980788		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.015414629458980788 | validation: 0.04117511123671232]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01443590632669605		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.01443590632669605 | validation: 0.03967960484252725]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012703701173645318		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.012703701173645318 | validation: 0.044763883345905364]
	TIME [epoch: 6.12 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014096468923001313		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.014096468923001313 | validation: 0.05249530523576882]
	TIME [epoch: 6.12 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01781734165281776		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.01781734165281776 | validation: 0.05364593440369358]
	TIME [epoch: 6.13 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015050772950430263		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.015050772950430263 | validation: 0.03850623036376079]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01699579167397601		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.01699579167397601 | validation: 0.062150505977434184]
	TIME [epoch: 6.11 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015891280753714248		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.015891280753714248 | validation: 0.05264950434982412]
	TIME [epoch: 6.14 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014999885787454093		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.014999885787454093 | validation: 0.046798166556317605]
	TIME [epoch: 6.13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013547307069399581		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.013547307069399581 | validation: 0.044935293981799455]
	TIME [epoch: 6.14 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01405826219544085		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.01405826219544085 | validation: 0.050670892240432086]
	TIME [epoch: 6.13 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013359215122104102		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.013359215122104102 | validation: 0.5057015987188804]
	TIME [epoch: 6.13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794890639968761		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.10794890639968761 | validation: 0.4841992159598987]
	TIME [epoch: 6.14 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545482354445199		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09545482354445199 | validation: 0.1804210545600117]
	TIME [epoch: 6.14 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156363096148788		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.03156363096148788 | validation: 0.04422387664391625]
	TIME [epoch: 6.14 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018291284924523248		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.018291284924523248 | validation: 0.08920984972892625]
	TIME [epoch: 6.14 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024996521014909512		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.024996521014909512 | validation: 0.03236626945517836]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015877761975650325		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.015877761975650325 | validation: 0.044137721128353964]
	TIME [epoch: 6.14 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01493565511728347		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.01493565511728347 | validation: 0.047310312718863934]
	TIME [epoch: 6.13 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015753755610756016		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.015753755610756016 | validation: 0.037873453106430165]
	TIME [epoch: 6.14 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013762429869661369		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.013762429869661369 | validation: 0.03300009189142656]
	TIME [epoch: 6.14 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013925173382068747		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.013925173382068747 | validation: 0.038675828054278494]
	TIME [epoch: 6.14 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01650329829033273		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.01650329829033273 | validation: 0.035591175376322746]
	TIME [epoch: 6.13 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016359822840391242		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.016359822840391242 | validation: 0.03551341870668381]
	TIME [epoch: 6.14 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01435219470238403		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.01435219470238403 | validation: 0.03870023493690486]
	TIME [epoch: 6.14 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01729457550736529		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.01729457550736529 | validation: 0.03767684831919685]
	TIME [epoch: 6.14 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01741994514538115		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.01741994514538115 | validation: 0.04089438234478888]
	TIME [epoch: 6.14 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01543003924152043		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.01543003924152043 | validation: 0.040380887869983395]
	TIME [epoch: 6.14 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015658473975547652		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.015658473975547652 | validation: 0.05690938377620383]
	TIME [epoch: 6.15 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01654569486562486		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.01654569486562486 | validation: 0.03841426415370457]
	TIME [epoch: 6.14 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012253450384177987		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.012253450384177987 | validation: 0.03922896453300754]
	TIME [epoch: 6.14 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012005815487274094		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.012005815487274094 | validation: 0.04160247286270542]
	TIME [epoch: 6.15 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017387725488817275		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.017387725488817275 | validation: 0.050376048661140364]
	TIME [epoch: 6.14 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0174239046112034		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.0174239046112034 | validation: 0.03915228637591892]
	TIME [epoch: 6.14 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015512990477935355		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.015512990477935355 | validation: 0.05422203211337904]
	TIME [epoch: 6.15 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02311182604121688		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.02311182604121688 | validation: 0.05998186906036691]
	TIME [epoch: 6.16 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017288699056726455		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.017288699056726455 | validation: 0.03738801716088993]
	TIME [epoch: 6.15 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01565603373981096		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.01565603373981096 | validation: 0.05309655432215074]
	TIME [epoch: 6.15 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019954329766204666		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.019954329766204666 | validation: 0.05631666805307306]
	TIME [epoch: 6.15 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016366161261974356		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.016366161261974356 | validation: 0.03591060969406691]
	TIME [epoch: 6.15 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01186600003678963		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.01186600003678963 | validation: 0.08301368775644744]
	TIME [epoch: 6.16 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019335909749455264		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.019335909749455264 | validation: 0.04605329970526892]
	TIME [epoch: 6.15 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014708232122833863		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.014708232122833863 | validation: 0.05128053720234044]
	TIME [epoch: 6.15 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01600676126586479		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.01600676126586479 | validation: 0.035126486568721374]
	TIME [epoch: 6.15 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01348670946521768		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.01348670946521768 | validation: 0.04223067874083633]
	TIME [epoch: 6.14 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012687295418059203		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.012687295418059203 | validation: 0.037015938774445756]
	TIME [epoch: 6.15 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011639187555783432		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.011639187555783432 | validation: 0.03682956650805892]
	TIME [epoch: 6.15 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012907248881643405		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.012907248881643405 | validation: 0.058102165333278694]
	TIME [epoch: 6.15 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01449521097562137		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.01449521097562137 | validation: 0.034983331577519775]
	TIME [epoch: 6.15 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012964583517925071		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.012964583517925071 | validation: 0.055025204649802754]
	TIME [epoch: 6.16 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019328313906382583		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.019328313906382583 | validation: 0.05255808601417984]
	TIME [epoch: 6.15 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026212993808942736		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.026212993808942736 | validation: 0.03759265074896406]
	TIME [epoch: 6.16 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02375449196924028		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.02375449196924028 | validation: 0.041701846635900125]
	TIME [epoch: 6.15 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013640842775306717		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.013640842775306717 | validation: 0.04327827539624292]
	TIME [epoch: 6.15 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012947349343689296		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.012947349343689296 | validation: 0.04032412936010556]
	TIME [epoch: 6.15 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017641242395785336		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.017641242395785336 | validation: 0.044066672716235225]
	TIME [epoch: 6.15 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017332258697156724		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.017332258697156724 | validation: 0.04529868283710206]
	TIME [epoch: 6.15 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013160536073916936		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.013160536073916936 | validation: 0.04444866594370187]
	TIME [epoch: 6.15 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017168258047051518		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.017168258047051518 | validation: 0.04373603383626833]
	TIME [epoch: 6.15 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01474449992327719		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.01474449992327719 | validation: 0.05880379522921123]
	TIME [epoch: 6.15 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023499111634080326		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.023499111634080326 | validation: 0.05324923619688504]
	TIME [epoch: 6.15 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016392031740556895		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.016392031740556895 | validation: 0.03480331222459946]
	TIME [epoch: 6.15 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012727115911243314		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.012727115911243314 | validation: 0.07891299106932725]
	TIME [epoch: 6.15 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07664504398379035		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.07664504398379035 | validation: 0.06462776228537292]
	TIME [epoch: 6.15 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04976348438629142		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.04976348438629142 | validation: 0.03446464219992621]
	TIME [epoch: 6.15 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014281048540241573		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.014281048540241573 | validation: 0.04370788812456253]
	TIME [epoch: 6.15 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026328226497317368		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.026328226497317368 | validation: 0.042607892015285566]
	TIME [epoch: 6.15 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014457374675011		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.014457374675011 | validation: 0.042425351932538026]
	TIME [epoch: 6.16 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015363893341627228		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.015363893341627228 | validation: 0.03988492054859305]
	TIME [epoch: 6.14 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014023660175542484		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.014023660175542484 | validation: 0.04277901493587697]
	TIME [epoch: 6.15 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012906769626099839		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.012906769626099839 | validation: 0.02958854699232988]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012523484891028515		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.012523484891028515 | validation: 0.03263129213551167]
	TIME [epoch: 6.14 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01290204915409381		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.01290204915409381 | validation: 0.04288668161903306]
	TIME [epoch: 6.15 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011774120225772932		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.011774120225772932 | validation: 0.041222820157260566]
	TIME [epoch: 6.14 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011441371765855857		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.011441371765855857 | validation: 0.03203950006650915]
	TIME [epoch: 6.15 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012121669379454		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.012121669379454 | validation: 0.03490017287667736]
	TIME [epoch: 6.15 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011551670611090744		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.011551670611090744 | validation: 0.041584982227057754]
	TIME [epoch: 6.15 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012667458961854788		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.012667458961854788 | validation: 0.03730685561098327]
	TIME [epoch: 6.15 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011830148458358348		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.011830148458358348 | validation: 0.04036218077775229]
	TIME [epoch: 6.15 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012890471601814994		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.012890471601814994 | validation: 0.08600560622250603]
	TIME [epoch: 6.15 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016967567835356048		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.016967567835356048 | validation: 0.046681613616781026]
	TIME [epoch: 6.14 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01487549918290779		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.01487549918290779 | validation: 0.039696251234021776]
	TIME [epoch: 6.15 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015218825478313833		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.015218825478313833 | validation: 0.0337323482188169]
	TIME [epoch: 6.15 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012315492939208644		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.012315492939208644 | validation: 0.056548902805565515]
	TIME [epoch: 6.15 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014444547353271631		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.014444547353271631 | validation: 0.06429468630062689]
	TIME [epoch: 6.15 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015866477444765086		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.015866477444765086 | validation: 0.05426484804928582]
	TIME [epoch: 6.16 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017114041914983103		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.017114041914983103 | validation: 0.09136994886198391]
	TIME [epoch: 6.15 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020708014322052066		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.020708014322052066 | validation: 0.03884832686627939]
	TIME [epoch: 6.14 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018854255556030326		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.018854255556030326 | validation: 0.06621294898497385]
	TIME [epoch: 6.15 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017419858443699653		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.017419858443699653 | validation: 0.03217785374561717]
	TIME [epoch: 6.14 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011282103328096638		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.011282103328096638 | validation: 0.04143812520588147]
	TIME [epoch: 6.14 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011424398001873098		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.011424398001873098 | validation: 0.0419910391719094]
	TIME [epoch: 6.15 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01396599864191879		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.01396599864191879 | validation: 0.03448578918625583]
	TIME [epoch: 6.15 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013532518715608293		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.013532518715608293 | validation: 0.039462376840259675]
	TIME [epoch: 6.15 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012862475717625964		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.012862475717625964 | validation: 0.03430563729747096]
	TIME [epoch: 6.15 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013046538425242858		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.013046538425242858 | validation: 0.03100985588788374]
	TIME [epoch: 6.15 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013083020584947098		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.013083020584947098 | validation: 0.02996359852673497]
	TIME [epoch: 6.15 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011815611847844805		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.011815611847844805 | validation: 0.05208923508001609]
	TIME [epoch: 6.15 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019168352835196516		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.019168352835196516 | validation: 0.041461538355525976]
	TIME [epoch: 6.15 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015114900865146772		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.015114900865146772 | validation: 0.039140102974103745]
	TIME [epoch: 6.16 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012122419411831944		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.012122419411831944 | validation: 0.030977914335794377]
	TIME [epoch: 6.14 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012503127275460126		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.012503127275460126 | validation: 0.04230343953644636]
	TIME [epoch: 6.16 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01729791380170267		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.01729791380170267 | validation: 0.03742415897664616]
	TIME [epoch: 6.14 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011912620752932599		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.011912620752932599 | validation: 0.07472366083284648]
	TIME [epoch: 6.15 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019032511183514903		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.019032511183514903 | validation: 0.25675277832914606]
	TIME [epoch: 6.14 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15601415147113304		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.15601415147113304 | validation: 0.172327337231663]
	TIME [epoch: 6.15 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17104372006852558		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.17104372006852558 | validation: 0.10133602747534197]
	TIME [epoch: 6.14 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11695113052873549		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.11695113052873549 | validation: 0.11660467890522566]
	TIME [epoch: 6.15 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06437098868062406		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.06437098868062406 | validation: 0.07695246602897038]
	TIME [epoch: 6.15 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024695935459096235		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.024695935459096235 | validation: 0.04313520571353607]
	TIME [epoch: 6.17 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014242700661427634		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.014242700661427634 | validation: 0.046110495231875825]
	TIME [epoch: 6.16 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015655685302453277		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.015655685302453277 | validation: 0.041081447401894636]
	TIME [epoch: 6.17 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015065714140934898		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.015065714140934898 | validation: 0.04608228140880282]
	TIME [epoch: 6.15 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011870709272435201		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.011870709272435201 | validation: 0.04426822065844879]
	TIME [epoch: 6.16 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013561667909967042		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.013561667909967042 | validation: 0.038333936900776314]
	TIME [epoch: 6.15 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012032778241241245		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.012032778241241245 | validation: 0.040976432762283854]
	TIME [epoch: 6.16 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010991129253639079		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.010991129253639079 | validation: 0.04003118864769897]
	TIME [epoch: 6.14 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012575149626311264		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.012575149626311264 | validation: 0.03387928512176926]
	TIME [epoch: 6.16 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01115266291477023		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.01115266291477023 | validation: 0.033942601292652634]
	TIME [epoch: 6.15 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012115426740703459		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.012115426740703459 | validation: 0.03766169996956682]
	TIME [epoch: 6.16 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011028867435489158		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.011028867435489158 | validation: 0.043429662181461076]
	TIME [epoch: 6.15 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011847190161504538		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.011847190161504538 | validation: 0.034192463058973034]
	TIME [epoch: 6.15 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012507988350585801		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.012507988350585801 | validation: 0.037535717598962695]
	TIME [epoch: 6.14 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011362439023863245		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.011362439023863245 | validation: 0.04051718030782834]
	TIME [epoch: 6.14 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011445785219438865		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.011445785219438865 | validation: 0.03556906849742086]
	TIME [epoch: 6.15 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012295923167853105		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.012295923167853105 | validation: 0.038960735935849394]
	TIME [epoch: 6.16 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01410723227304725		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.01410723227304725 | validation: 0.034772504053153165]
	TIME [epoch: 6.16 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011812783948874212		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.011812783948874212 | validation: 0.03292600209970076]
	TIME [epoch: 6.17 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011941258675599295		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.011941258675599295 | validation: 0.031474402789496515]
	TIME [epoch: 6.16 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011554593893107274		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.011554593893107274 | validation: 0.04267450892295743]
	TIME [epoch: 6.16 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012951341461531225		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.012951341461531225 | validation: 0.040981220752017294]
	TIME [epoch: 6.15 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012701500805730744		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.012701500805730744 | validation: 0.040823825787490865]
	TIME [epoch: 9.06 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012681511676407155		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.012681511676407155 | validation: 0.03205653647819795]
	TIME [epoch: 6.14 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012316497326149483		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.012316497326149483 | validation: 0.03819243893392334]
	TIME [epoch: 6.15 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010868529284138869		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.010868529284138869 | validation: 0.036181495907440096]
	TIME [epoch: 6.15 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01174224962970807		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.01174224962970807 | validation: 0.03727441004789293]
	TIME [epoch: 6.16 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012532471572134524		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.012532471572134524 | validation: 0.04042090377745164]
	TIME [epoch: 6.15 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013089683753201218		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.013089683753201218 | validation: 0.03319042225221632]
	TIME [epoch: 6.16 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012872862735059902		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.012872862735059902 | validation: 0.039698885381345875]
	TIME [epoch: 6.15 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015523949577625103		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.015523949577625103 | validation: 0.032206493979827056]
	TIME [epoch: 6.16 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011551328367073302		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.011551328367073302 | validation: 0.029192541567009036]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012315204632187913		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.012315204632187913 | validation: 0.046026602967417535]
	TIME [epoch: 6.15 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02205330045358038		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.02205330045358038 | validation: 0.031737895683329485]
	TIME [epoch: 6.15 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010833453907391575		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.010833453907391575 | validation: 0.10918779614592916]
	TIME [epoch: 6.15 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02177567911445765		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.02177567911445765 | validation: 0.03556969908761994]
	TIME [epoch: 6.15 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011402534368603157		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.011402534368603157 | validation: 0.05547900142688092]
	TIME [epoch: 6.15 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015146690612445866		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.015146690612445866 | validation: 0.033727412327722694]
	TIME [epoch: 6.15 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012589518319793389		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.012589518319793389 | validation: 0.04771436585910265]
	TIME [epoch: 6.15 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014021265175835922		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.014021265175835922 | validation: 0.035096679126679554]
	TIME [epoch: 6.15 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011891165553458769		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.011891165553458769 | validation: 0.0367489275152]
	TIME [epoch: 6.15 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01304188782477437		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.01304188782477437 | validation: 0.03924861776868922]
	TIME [epoch: 6.15 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012687313333749808		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.012687313333749808 | validation: 0.03436801280702223]
	TIME [epoch: 6.15 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011287431657416486		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.011287431657416486 | validation: 0.034910530281398754]
	TIME [epoch: 6.16 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011518799766347737		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.011518799766347737 | validation: 0.06589412389748785]
	TIME [epoch: 6.15 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014747855301039406		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.014747855301039406 | validation: 0.050707063191254155]
	TIME [epoch: 6.15 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035085169188092506		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.035085169188092506 | validation: 0.08640230938265095]
	TIME [epoch: 6.14 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02465805535619297		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.02465805535619297 | validation: 0.05555906514910605]
	TIME [epoch: 6.15 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018033316643857364		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.018033316643857364 | validation: 0.08273523763169881]
	TIME [epoch: 6.14 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017911542275795667		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.017911542275795667 | validation: 0.06914460676634636]
	TIME [epoch: 6.15 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013946533116103036		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.013946533116103036 | validation: 0.050543685498553086]
	TIME [epoch: 6.14 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013563392437679893		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.013563392437679893 | validation: 0.06152021506670707]
	TIME [epoch: 6.14 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015977467261682747		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.015977467261682747 | validation: 0.0342687862426904]
	TIME [epoch: 6.14 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010879319954369927		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.010879319954369927 | validation: 0.030491908427888292]
	TIME [epoch: 6.14 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010280262568598644		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.010280262568598644 | validation: 0.03617615743724726]
	TIME [epoch: 6.14 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01053536294151232		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.01053536294151232 | validation: 0.028007725409762887]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010557720111100382		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.010557720111100382 | validation: 0.03507825967837346]
	TIME [epoch: 6.14 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012703262739694295		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.012703262739694295 | validation: 0.042040244125565576]
	TIME [epoch: 6.14 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011735207699295125		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.011735207699295125 | validation: 0.03242282997919045]
	TIME [epoch: 6.14 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012258310747346915		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.012258310747346915 | validation: 0.03426720246891764]
	TIME [epoch: 6.14 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010394368537559533		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.010394368537559533 | validation: 0.056593392057803484]
	TIME [epoch: 6.14 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018152412092068942		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.018152412092068942 | validation: 0.03715917915263032]
	TIME [epoch: 6.14 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01465196057602157		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.01465196057602157 | validation: 0.02723843365044322]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011367380165728825		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.011367380165728825 | validation: 0.04462157468049715]
	TIME [epoch: 6.13 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011861733968325478		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.011861733968325478 | validation: 0.037410930194638946]
	TIME [epoch: 6.13 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010927609436172546		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.010927609436172546 | validation: 0.029978606960573154]
	TIME [epoch: 6.13 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010279477672352363		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.010279477672352363 | validation: 0.03289815976785657]
	TIME [epoch: 6.13 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011324025409109813		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.011324025409109813 | validation: 0.038193175562899376]
	TIME [epoch: 6.13 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013099116460878205		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.013099116460878205 | validation: 0.031972320275191035]
	TIME [epoch: 6.14 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010784075579453309		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.010784075579453309 | validation: 0.031030269990451966]
	TIME [epoch: 6.15 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011884612646945597		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.011884612646945597 | validation: 0.05792715367914461]
	TIME [epoch: 6.15 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941469559661022		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04941469559661022 | validation: 0.04856173924665326]
	TIME [epoch: 6.15 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777435191229297		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03777435191229297 | validation: 0.04936348026924328]
	TIME [epoch: 6.14 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012747295965623358		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.012747295965623358 | validation: 0.032881772555786073]
	TIME [epoch: 6.14 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01722517333337388		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.01722517333337388 | validation: 0.03652436983764362]
	TIME [epoch: 6.15 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01167156196967332		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.01167156196967332 | validation: 0.027346525782102362]
	TIME [epoch: 6.15 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01091681662580605		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.01091681662580605 | validation: 0.03423987982516346]
	TIME [epoch: 6.15 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01278212504884227		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.01278212504884227 | validation: 0.038811771664250976]
	TIME [epoch: 6.15 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011585103443625056		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.011585103443625056 | validation: 0.03561556494256598]
	TIME [epoch: 6.14 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01103797729538625		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.01103797729538625 | validation: 0.0969223880045783]
	TIME [epoch: 6.14 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02010447073867236		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.02010447073867236 | validation: 0.027323162006124516]
	TIME [epoch: 6.13 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011976657536511282		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.011976657536511282 | validation: 0.03745547952875488]
	TIME [epoch: 6.13 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011939491754512033		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.011939491754512033 | validation: 0.036714575236041304]
	TIME [epoch: 6.13 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012086379349999305		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.012086379349999305 | validation: 0.06985207140127535]
	TIME [epoch: 6.13 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01900524313783927		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.01900524313783927 | validation: 0.03977246481129185]
	TIME [epoch: 6.14 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013290486873478946		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.013290486873478946 | validation: 0.03598723711743445]
	TIME [epoch: 6.13 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011610916691791885		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.011610916691791885 | validation: 0.031621181552468756]
	TIME [epoch: 6.14 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012879475430598634		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.012879475430598634 | validation: 0.03241054958264744]
	TIME [epoch: 6.14 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012673255815304508		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.012673255815304508 | validation: 0.03183726807824926]
	TIME [epoch: 6.13 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012533049300039085		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.012533049300039085 | validation: 0.033233566339050606]
	TIME [epoch: 6.13 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01150502891846705		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.01150502891846705 | validation: 0.03746434626314065]
	TIME [epoch: 6.13 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011635183532336637		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.011635183532336637 | validation: 0.03329261821579859]
	TIME [epoch: 6.13 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011093502187930912		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.011093502187930912 | validation: 0.033948590006767435]
	TIME [epoch: 6.13 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01063983275005231		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.01063983275005231 | validation: 0.03503084376580162]
	TIME [epoch: 6.13 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009902406307876048		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.009902406307876048 | validation: 0.036128288214607916]
	TIME [epoch: 6.13 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01007817612844435		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.01007817612844435 | validation: 0.03112974656179799]
	TIME [epoch: 6.13 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011427129867724078		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.011427129867724078 | validation: 0.03360273263991229]
	TIME [epoch: 6.13 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01185505219564439		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.01185505219564439 | validation: 0.0355318943077367]
	TIME [epoch: 6.14 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011101705869003465		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.011101705869003465 | validation: 0.034541477054854254]
	TIME [epoch: 6.14 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010184487702723476		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.010184487702723476 | validation: 0.04070247948547571]
	TIME [epoch: 6.14 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011956842373340692		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.011956842373340692 | validation: 0.0403269506028379]
	TIME [epoch: 6.14 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010452674017470609		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.010452674017470609 | validation: 0.033539461191511265]
	TIME [epoch: 6.14 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011786352925212025		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.011786352925212025 | validation: 0.031491873898040534]
	TIME [epoch: 6.14 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011292596949051145		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.011292596949051145 | validation: 0.03227805269238176]
	TIME [epoch: 6.14 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011070737938232988		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.011070737938232988 | validation: 0.039816013520846544]
	TIME [epoch: 6.13 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012730958206935426		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.012730958206935426 | validation: 0.043865797855957116]
	TIME [epoch: 6.13 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01043751731483338		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.01043751731483338 | validation: 0.03173149823311616]
	TIME [epoch: 6.13 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013537429997250026		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.013537429997250026 | validation: 0.035710449352027766]
	TIME [epoch: 6.13 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013147602601440017		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.013147602601440017 | validation: 0.03919798190527243]
	TIME [epoch: 6.13 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011211755157593846		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.011211755157593846 | validation: 0.036692545823560234]
	TIME [epoch: 6.13 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013027668572534509		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.013027668572534509 | validation: 0.03805484607375175]
	TIME [epoch: 6.14 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014836034308445599		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.014836034308445599 | validation: 0.04300642669807186]
	TIME [epoch: 6.14 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011599631865805109		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.011599631865805109 | validation: 0.030617149339871433]
	TIME [epoch: 6.13 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014498225997237428		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.014498225997237428 | validation: 0.03140772201300947]
	TIME [epoch: 6.13 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011705844300559263		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.011705844300559263 | validation: 0.03385038008173591]
	TIME [epoch: 6.13 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011217095068671561		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.011217095068671561 | validation: 0.028664824193263418]
	TIME [epoch: 6.12 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010823858733518366		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.010823858733518366 | validation: 0.03570322802772271]
	TIME [epoch: 6.13 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011824522837815925		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.011824522837815925 | validation: 0.03833960436307198]
	TIME [epoch: 6.13 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010019727115239019		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.010019727115239019 | validation: 0.04084667938748931]
	TIME [epoch: 6.13 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01100448724082516		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.01100448724082516 | validation: 0.03368796472462067]
	TIME [epoch: 6.14 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010386624264834048		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.010386624264834048 | validation: 0.028002265965502406]
	TIME [epoch: 6.14 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011790204064419154		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.011790204064419154 | validation: 0.025434791354555467]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010159319800213357		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.010159319800213357 | validation: 0.030710508981409992]
	TIME [epoch: 6.14 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010787791614108878		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.010787791614108878 | validation: 0.031823640527537035]
	TIME [epoch: 6.13 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009622338281381185		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.009622338281381185 | validation: 0.03434063152288619]
	TIME [epoch: 6.13 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010140152299066361		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.010140152299066361 | validation: 0.027816025498354414]
	TIME [epoch: 6.14 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01071074160133023		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.01071074160133023 | validation: 0.032668034985645536]
	TIME [epoch: 6.13 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010577700982065109		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.010577700982065109 | validation: 0.03763459916920669]
	TIME [epoch: 6.13 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010395876372743439		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.010395876372743439 | validation: 0.028941050820530424]
	TIME [epoch: 6.12 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012248893510953632		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.012248893510953632 | validation: 0.03499643632202683]
	TIME [epoch: 6.13 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012760281243543503		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.012760281243543503 | validation: 0.03298309747491656]
	TIME [epoch: 6.12 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010443249622728757		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.010443249622728757 | validation: 0.027804916363784116]
	TIME [epoch: 6.12 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012206078966095926		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.012206078966095926 | validation: 0.029681166172250986]
	TIME [epoch: 6.12 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009705056888107464		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.009705056888107464 | validation: 0.04465050170481655]
	TIME [epoch: 6.12 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011288727509779476		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.011288727509779476 | validation: 0.02944293175888293]
	TIME [epoch: 6.11 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01060057005294892		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.01060057005294892 | validation: 0.03174577864279458]
	TIME [epoch: 6.12 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010009914377588951		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.010009914377588951 | validation: 0.035278643378322684]
	TIME [epoch: 6.12 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0156333712336583		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.0156333712336583 | validation: 0.03631929447198956]
	TIME [epoch: 6.13 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010995069852343798		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.010995069852343798 | validation: 0.030832330251183506]
	TIME [epoch: 6.12 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010916465994803537		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.010916465994803537 | validation: 0.04363973571973742]
	TIME [epoch: 6.12 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012403432899015314		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.012403432899015314 | validation: 0.03761433799234806]
	TIME [epoch: 6.12 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011231149309173079		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.011231149309173079 | validation: 0.0312729411243902]
	TIME [epoch: 6.12 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010470504931380254		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.010470504931380254 | validation: 0.03342714698326004]
	TIME [epoch: 6.12 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01110122185618319		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.01110122185618319 | validation: 0.029109013058660083]
	TIME [epoch: 6.12 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010168439127072867		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.010168439127072867 | validation: 0.03131518984650864]
	TIME [epoch: 6.12 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010909239007911053		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.010909239007911053 | validation: 0.025557428273198835]
	TIME [epoch: 6.12 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011486946359734036		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.011486946359734036 | validation: 0.04435387353551345]
	TIME [epoch: 6.12 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01105850209881335		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.01105850209881335 | validation: 0.0482410970371818]
	TIME [epoch: 6.13 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01271195854948364		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.01271195854948364 | validation: 0.02702104300531052]
	TIME [epoch: 6.12 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011885466180329405		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.011885466180329405 | validation: 0.038390889953197616]
	TIME [epoch: 6.13 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01121871516002982		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.01121871516002982 | validation: 0.22388767147540944]
	TIME [epoch: 6.14 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955230885263875		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.05955230885263875 | validation: 0.16173063556241163]
	TIME [epoch: 6.13 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031700270000745004		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.031700270000745004 | validation: 0.03586492167675848]
	TIME [epoch: 6.12 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010669846741373796		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.010669846741373796 | validation: 0.048813016199882944]
	TIME [epoch: 6.13 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013179868061881775		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.013179868061881775 | validation: 0.05264908988644984]
	TIME [epoch: 6.12 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013393526496936242		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.013393526496936242 | validation: 0.03481381614654786]
	TIME [epoch: 6.13 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010925502871177512		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.010925502871177512 | validation: 0.030705353391730552]
	TIME [epoch: 6.12 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010608121814200374		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.010608121814200374 | validation: 0.04379786946447686]
	TIME [epoch: 6.12 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012327894012212164		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.012327894012212164 | validation: 0.030648581013988374]
	TIME [epoch: 6.12 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009526254546840904		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.009526254546840904 | validation: 0.02970043410801785]
	TIME [epoch: 6.12 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01213960738318548		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.01213960738318548 | validation: 0.031525711369061925]
	TIME [epoch: 6.12 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009394812616908136		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.009394812616908136 | validation: 0.029234466455773392]
	TIME [epoch: 6.13 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010423806698262313		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.010423806698262313 | validation: 0.03256411680735353]
	TIME [epoch: 6.12 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010024722044687177		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.010024722044687177 | validation: 0.03486014269809581]
	TIME [epoch: 6.12 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010235617019003178		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.010235617019003178 | validation: 0.03261534582459541]
	TIME [epoch: 6.12 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010781988166083513		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.010781988166083513 | validation: 0.023414089520603223]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010217688198189103		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.010217688198189103 | validation: 0.02782492597140204]
	TIME [epoch: 6.12 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010169101363272664		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.010169101363272664 | validation: 0.029054895427049556]
	TIME [epoch: 6.12 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009502340230105284		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.009502340230105284 | validation: 0.033300505148261696]
	TIME [epoch: 6.12 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011443407291214722		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.011443407291214722 | validation: 0.027243436214503328]
	TIME [epoch: 6.12 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012739581849781982		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.012739581849781982 | validation: 0.03318575467440448]
	TIME [epoch: 6.12 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0109571939412134		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.0109571939412134 | validation: 0.03313691255020135]
	TIME [epoch: 6.12 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011052461005183714		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.011052461005183714 | validation: 0.03100263007008839]
	TIME [epoch: 6.14 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015027793242995817		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.015027793242995817 | validation: 0.03214063979852766]
	TIME [epoch: 6.13 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01050172984640217		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.01050172984640217 | validation: 0.027514886958316498]
	TIME [epoch: 6.13 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012346289769996846		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.012346289769996846 | validation: 0.03414576297874226]
	TIME [epoch: 6.13 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01005754449727432		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.01005754449727432 | validation: 0.02955948672176565]
	TIME [epoch: 6.13 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009633876368447914		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.009633876368447914 | validation: 0.029745660942883025]
	TIME [epoch: 6.13 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010226899625687706		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.010226899625687706 | validation: 0.02822177742696698]
	TIME [epoch: 6.12 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00989436405891528		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.00989436405891528 | validation: 0.02610525313543808]
	TIME [epoch: 6.13 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010444017332073102		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.010444017332073102 | validation: 0.029420230683606274]
	TIME [epoch: 6.12 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009725125700599269		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.009725125700599269 | validation: 0.023495908192021964]
	TIME [epoch: 6.13 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010730525334091929		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.010730525334091929 | validation: 0.027104233114703582]
	TIME [epoch: 6.12 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013384369477646093		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.013384369477646093 | validation: 0.02607176642801159]
	TIME [epoch: 6.13 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01024364334652372		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.01024364334652372 | validation: 0.049924923644495016]
	TIME [epoch: 6.12 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015526962492538373		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.015526962492538373 | validation: 0.04458833949352251]
	TIME [epoch: 6.13 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019280150290466362		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.019280150290466362 | validation: 0.03071392469928538]
	TIME [epoch: 6.12 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014009429614485314		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.014009429614485314 | validation: 0.04000596871651594]
	TIME [epoch: 6.14 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010879225507608423		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.010879225507608423 | validation: 0.02946347101150334]
	TIME [epoch: 6.13 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013057458894902873		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.013057458894902873 | validation: 0.03266659262121324]
	TIME [epoch: 6.13 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009944753005067062		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.009944753005067062 | validation: 0.034997023226093785]
	TIME [epoch: 6.13 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009389418150402123		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.009389418150402123 | validation: 0.02841518552153599]
	TIME [epoch: 6.13 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009767602391091588		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.009767602391091588 | validation: 0.03002381774845314]
	TIME [epoch: 6.13 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010532029636225975		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.010532029636225975 | validation: 0.025631987823049797]
	TIME [epoch: 6.12 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009356753420560216		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.009356753420560216 | validation: 0.028294247155689057]
	TIME [epoch: 6.13 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010173499597348623		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.010173499597348623 | validation: 0.02859378102200817]
	TIME [epoch: 6.13 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011030130167583627		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.011030130167583627 | validation: 0.02857635351963215]
	TIME [epoch: 6.14 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009922239935805653		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.009922239935805653 | validation: 0.029044947808566448]
	TIME [epoch: 6.15 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010797317668394204		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.010797317668394204 | validation: 0.02471802030089726]
	TIME [epoch: 6.14 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010088372061543914		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.010088372061543914 | validation: 0.0244855745443761]
	TIME [epoch: 6.12 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009667314095313977		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.009667314095313977 | validation: 0.030038336059070938]
	TIME [epoch: 6.11 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010294830459893718		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.010294830459893718 | validation: 0.03369039959214997]
	TIME [epoch: 6.12 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01083240923253979		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.01083240923253979 | validation: 0.026792435269821793]
	TIME [epoch: 6.11 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009881698700779776		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.009881698700779776 | validation: 0.025808217464428598]
	TIME [epoch: 6.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009464074428618428		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.009464074428618428 | validation: 0.04444292782606431]
	TIME [epoch: 6.13 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011099448649145638		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.011099448649145638 | validation: 0.02929193157248171]
	TIME [epoch: 6.13 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011973800733086461		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.011973800733086461 | validation: 0.03812679256652421]
	TIME [epoch: 6.13 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011082049010510617		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.011082049010510617 | validation: 0.026824348960819436]
	TIME [epoch: 6.15 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0120594055270636		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.0120594055270636 | validation: 0.04156356582244422]
	TIME [epoch: 6.14 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010793842010441757		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.010793842010441757 | validation: 0.028461383534808073]
	TIME [epoch: 6.15 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010265880606506945		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.010265880606506945 | validation: 0.03955681014112205]
	TIME [epoch: 6.13 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011141719675023634		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.011141719675023634 | validation: 0.02824912831391109]
	TIME [epoch: 6.14 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011059956854247653		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.011059956854247653 | validation: 0.02584208476949843]
	TIME [epoch: 6.13 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009980114323037166		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.009980114323037166 | validation: 0.03133261403142392]
	TIME [epoch: 6.13 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0112275678282748		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0112275678282748 | validation: 0.02502456458949186]
	TIME [epoch: 6.13 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009689480946931837		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.009689480946931837 | validation: 0.027901024866820646]
	TIME [epoch: 203 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010213190845766966		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.010213190845766966 | validation: 0.03226598294077959]
	TIME [epoch: 13 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011771357242510872		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.011771357242510872 | validation: 0.025560517622981352]
	TIME [epoch: 13 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009741964802961323		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.009741964802961323 | validation: 0.027431649764364366]
	TIME [epoch: 13 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011632065379720327		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.011632065379720327 | validation: 0.047552277418978564]
	TIME [epoch: 13 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011221346730502527		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.011221346730502527 | validation: 0.024475357989131176]
	TIME [epoch: 13 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0104196880886764		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.0104196880886764 | validation: 0.029196742550961774]
	TIME [epoch: 13 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016036143249719415		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.016036143249719415 | validation: 0.028647709701880866]
	TIME [epoch: 13 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014307379773387143		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.014307379773387143 | validation: 0.028853342679727403]
	TIME [epoch: 13 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010079749155457533		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.010079749155457533 | validation: 0.026938794866807336]
	TIME [epoch: 13 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011938301351510472		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.011938301351510472 | validation: 0.0355612958764898]
	TIME [epoch: 13 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009186640269200852		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.009186640269200852 | validation: 0.027068449517741434]
	TIME [epoch: 13 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009555424789328769		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.009555424789328769 | validation: 0.03081190828024103]
	TIME [epoch: 13 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009158555045223784		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.009158555045223784 | validation: 0.030148269230950476]
	TIME [epoch: 13 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01002431734544251		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.01002431734544251 | validation: 0.029908915278660575]
	TIME [epoch: 13 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009683014358663833		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.009683014358663833 | validation: 0.03146073006497185]
	TIME [epoch: 13 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009199275570737495		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.009199275570737495 | validation: 0.033712800589632566]
	TIME [epoch: 13 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01017428003185683		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.01017428003185683 | validation: 0.02679985913237244]
	TIME [epoch: 13 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009835170533523247		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.009835170533523247 | validation: 0.02774632571893572]
	TIME [epoch: 13 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0099608520324442		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.0099608520324442 | validation: 0.035108672420967435]
	TIME [epoch: 13 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010307461791024018		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.010307461791024018 | validation: 0.028491949501964178]
	TIME [epoch: 13 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009037463837864837		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.009037463837864837 | validation: 0.030090719236427834]
	TIME [epoch: 13 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009257574807239077		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.009257574807239077 | validation: 0.03011375483429676]
	TIME [epoch: 13 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010304205859803614		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.010304205859803614 | validation: 0.027029759446538005]
	TIME [epoch: 13 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009586859375305384		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.009586859375305384 | validation: 0.02277232954242806]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01046522719786874		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.01046522719786874 | validation: 0.03478406278871867]
	TIME [epoch: 13 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012035071186978715		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.012035071186978715 | validation: 0.032454867211383444]
	TIME [epoch: 13 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009666989265032662		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.009666989265032662 | validation: 0.025648832521052156]
	TIME [epoch: 13 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009809221396095661		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.009809221396095661 | validation: 0.029720587158471458]
	TIME [epoch: 13 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010151516449362005		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.010151516449362005 | validation: 0.0866758255703535]
	TIME [epoch: 13 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967207724743629		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.01967207724743629 | validation: 0.03313902757185149]
	TIME [epoch: 13 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01054812081129596		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.01054812081129596 | validation: 0.03942575448060698]
	TIME [epoch: 13 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010500757001796727		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.010500757001796727 | validation: 0.03252859121233914]
	TIME [epoch: 13 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01065770698482557		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.01065770698482557 | validation: 0.030472605757535187]
	TIME [epoch: 13 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010539349995809811		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.010539349995809811 | validation: 0.02638291604325338]
	TIME [epoch: 13 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009985873421119404		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.009985873421119404 | validation: 0.029303340792594747]
	TIME [epoch: 13 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008967180284923292		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.008967180284923292 | validation: 0.030640807547413797]
	TIME [epoch: 13 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010825196348982392		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.010825196348982392 | validation: 0.032120172592076314]
	TIME [epoch: 13 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011022774066861986		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.011022774066861986 | validation: 0.026407439841857283]
	TIME [epoch: 13 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009178280393681421		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.009178280393681421 | validation: 0.031649620497908225]
	TIME [epoch: 13 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00973595539754354		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.00973595539754354 | validation: 0.03114304681946758]
	TIME [epoch: 13 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009733724222568576		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.009733724222568576 | validation: 0.025305431734239037]
	TIME [epoch: 13 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01124579326420313		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.01124579326420313 | validation: 0.03249436654477168]
	TIME [epoch: 13 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01136293749717149		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.01136293749717149 | validation: 0.02489014128346885]
	TIME [epoch: 13 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009650971245231317		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.009650971245231317 | validation: 0.028367465710831555]
	TIME [epoch: 13 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011789219403603226		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.011789219403603226 | validation: 0.02682456834710798]
	TIME [epoch: 13 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01051412846774955		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.01051412846774955 | validation: 0.023205129965165862]
	TIME [epoch: 13 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009700622980623581		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.009700622980623581 | validation: 0.032000828029923545]
	TIME [epoch: 13 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011835842362543297		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.011835842362543297 | validation: 0.025466714839948158]
	TIME [epoch: 13 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010548476655690045		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.010548476655690045 | validation: 0.049789990672443524]
	TIME [epoch: 13 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011162248679771438		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.011162248679771438 | validation: 0.05174396192623379]
	TIME [epoch: 13 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02275289187616936		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.02275289187616936 | validation: 0.028948121885322444]
	TIME [epoch: 13 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016287044005355748		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.016287044005355748 | validation: 0.03294680434477989]
	TIME [epoch: 13 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010325771517832648		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.010325771517832648 | validation: 0.03156542686922811]
	TIME [epoch: 13.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010170365771513172		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.010170365771513172 | validation: 0.030136730529942226]
	TIME [epoch: 13 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011573980168708909		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.011573980168708909 | validation: 0.033323386580157495]
	TIME [epoch: 13 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015970299522168975		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.015970299522168975 | validation: 0.03595731574717583]
	TIME [epoch: 13 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014541384341994446		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.014541384341994446 | validation: 0.044930755421326574]
	TIME [epoch: 13 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01221819822669623		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.01221819822669623 | validation: 0.03981840768019479]
	TIME [epoch: 13 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01320894618568572		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.01320894618568572 | validation: 0.030861304043910454]
	TIME [epoch: 13 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010707665567782581		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.010707665567782581 | validation: 0.024133371260261396]
	TIME [epoch: 13 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009780162433050679		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.009780162433050679 | validation: 0.02751523958954957]
	TIME [epoch: 13 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01125563564526906		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.01125563564526906 | validation: 0.03257465964395586]
	TIME [epoch: 13 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009532405424796067		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.009532405424796067 | validation: 0.03529885361655986]
	TIME [epoch: 13 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008567635955454738		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.008567635955454738 | validation: 0.026601427165582194]
	TIME [epoch: 13 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009669803241341266		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.009669803241341266 | validation: 0.03510190678806779]
	TIME [epoch: 13 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00924467127723612		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.00924467127723612 | validation: 0.03354504120879779]
	TIME [epoch: 13 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009683179389342642		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.009683179389342642 | validation: 0.03012649288257351]
	TIME [epoch: 13 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00954512028687105		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.00954512028687105 | validation: 0.02752276274176271]
	TIME [epoch: 13 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009548297055548983		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.009548297055548983 | validation: 0.02719285871139661]
	TIME [epoch: 13 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010846274929046417		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.010846274929046417 | validation: 0.02772853258915522]
	TIME [epoch: 13 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011257495475566709		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.011257495475566709 | validation: 0.026868985925312873]
	TIME [epoch: 13 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010629477177479496		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.010629477177479496 | validation: 0.03146075158374802]
	TIME [epoch: 13 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009006961116968427		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.009006961116968427 | validation: 0.033918874108540935]
	TIME [epoch: 13 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009762350780711766		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.009762350780711766 | validation: 0.024110703867755625]
	TIME [epoch: 13 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010939856527732024		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.010939856527732024 | validation: 0.031913589537759655]
	TIME [epoch: 13 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009823494000048253		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.009823494000048253 | validation: 0.03111070511737958]
	TIME [epoch: 13 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009760954745927473		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.009760954745927473 | validation: 0.02758260287482871]
	TIME [epoch: 13 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01150834711634379		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.01150834711634379 | validation: 0.02625556306856998]
	TIME [epoch: 13 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008374994323045675		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.008374994323045675 | validation: 0.027710247895246034]
	TIME [epoch: 13 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009401779607622564		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.009401779607622564 | validation: 0.02951141532708146]
	TIME [epoch: 13 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00959905494055531		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.00959905494055531 | validation: 0.021203725483330327]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_1082.pth
	Model improved!!!
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008564929383120163		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.008564929383120163 | validation: 0.039929837931791196]
	TIME [epoch: 13 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010150153582102115		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.010150153582102115 | validation: 0.03324730009938366]
	TIME [epoch: 13 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010080754683796224		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.010080754683796224 | validation: 0.0253341405557718]
	TIME [epoch: 13 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008885178842058446		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.008885178842058446 | validation: 0.044810210918822216]
	TIME [epoch: 13 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011986350325247282		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.011986350325247282 | validation: 0.03438730466635306]
	TIME [epoch: 13 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011359176655857903		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.011359176655857903 | validation: 0.029812065535325772]
	TIME [epoch: 13 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010658525619908779		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.010658525619908779 | validation: 0.027488186369475566]
	TIME [epoch: 13 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008371779992417542		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.008371779992417542 | validation: 0.026785098769199413]
	TIME [epoch: 13 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010810411997003205		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.010810411997003205 | validation: 0.04119351158934373]
	TIME [epoch: 13 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011049203632626202		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.011049203632626202 | validation: 0.029081022529480072]
	TIME [epoch: 13 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010610141772626735		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.010610141772626735 | validation: 0.03507902801366625]
	TIME [epoch: 13 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010718175004502863		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.010718175004502863 | validation: 0.02531865785230544]
	TIME [epoch: 13 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009596583565190333		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.009596583565190333 | validation: 0.019997511452834918]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_1095.pth
	Model improved!!!
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009300687136183568		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.009300687136183568 | validation: 0.0296955509257673]
	TIME [epoch: 13 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009789918233048823		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.009789918233048823 | validation: 0.020314592615239793]
	TIME [epoch: 13 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009615933456988688		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.009615933456988688 | validation: 0.03573820919827386]
	TIME [epoch: 13 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009955251512915254		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.009955251512915254 | validation: 0.034536209296731124]
	TIME [epoch: 13 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010069209754014694		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.010069209754014694 | validation: 0.03410691374506113]
	TIME [epoch: 13 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009862789712948229		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.009862789712948229 | validation: 0.03459757916496638]
	TIME [epoch: 13 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01182770851810563		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.01182770851810563 | validation: 0.03046184345822858]
	TIME [epoch: 13 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010831727064247699		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.010831727064247699 | validation: 0.035217608898439105]
	TIME [epoch: 13 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009963693653276817		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.009963693653276817 | validation: 0.029147797595529514]
	TIME [epoch: 13 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009933977246872217		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.009933977246872217 | validation: 0.03021269121740975]
	TIME [epoch: 13 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008511281395421148		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.008511281395421148 | validation: 0.028824234020576024]
	TIME [epoch: 13 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009246871600298903		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.009246871600298903 | validation: 0.03312056843123069]
	TIME [epoch: 13 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009714622609764035		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.009714622609764035 | validation: 0.027446804140035443]
	TIME [epoch: 13 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009135358922323065		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.009135358922323065 | validation: 0.020723506721851026]
	TIME [epoch: 13 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00985841388172147		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.00985841388172147 | validation: 0.02786829240217431]
	TIME [epoch: 13 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009676006890975428		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.009676006890975428 | validation: 0.028118407548220137]
	TIME [epoch: 13 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009166875626505453		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.009166875626505453 | validation: 0.022451635978772158]
	TIME [epoch: 13 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008508197054542144		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.008508197054542144 | validation: 0.025745705977087585]
	TIME [epoch: 13 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009771432126147027		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.009771432126147027 | validation: 0.038967732979909186]
	TIME [epoch: 13 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008863834281612555		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.008863834281612555 | validation: 0.0375992802789636]
	TIME [epoch: 13 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012433579464454064		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.012433579464454064 | validation: 0.02860520010567127]
	TIME [epoch: 13 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00897599449985754		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.00897599449985754 | validation: 0.030257406058741124]
	TIME [epoch: 13 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009598416006813864		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.009598416006813864 | validation: 0.037793434531650696]
	TIME [epoch: 13 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009980847105847514		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.009980847105847514 | validation: 0.03126816268223011]
	TIME [epoch: 13 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011250992481145994		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.011250992481145994 | validation: 0.02500780439745892]
	TIME [epoch: 13 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009932970603170818		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.009932970603170818 | validation: 0.029863947900909252]
	TIME [epoch: 13 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010397393596127902		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.010397393596127902 | validation: 0.028545281149780347]
	TIME [epoch: 13 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011501350442406015		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.011501350442406015 | validation: 0.027377252264185904]
	TIME [epoch: 13 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009637884709486481		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.009637884709486481 | validation: 0.027740780748265893]
	TIME [epoch: 13 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010441914401104535		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.010441914401104535 | validation: 0.02537634452952278]
	TIME [epoch: 13 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009331194467810595		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.009331194467810595 | validation: 0.0332326700133315]
	TIME [epoch: 13 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012524728728379569		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.012524728728379569 | validation: 0.029358856031877302]
	TIME [epoch: 13 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00959123790388318		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.00959123790388318 | validation: 0.02619569375723251]
	TIME [epoch: 13 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008856298216430483		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.008856298216430483 | validation: 0.03800852422689657]
	TIME [epoch: 13 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008771885419277115		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.008771885419277115 | validation: 0.03298852468394636]
	TIME [epoch: 12.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00953843914485639		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.00953843914485639 | validation: 0.034144855982990475]
	TIME [epoch: 13 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009822593158443289		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.009822593158443289 | validation: 0.0267622073880614]
	TIME [epoch: 13 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009273535721183872		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.009273535721183872 | validation: 0.027451136326387793]
	TIME [epoch: 13 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010036467852217283		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.010036467852217283 | validation: 0.031028180650169226]
	TIME [epoch: 13 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009281779057710252		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.009281779057710252 | validation: 0.02909433776001881]
	TIME [epoch: 13 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00954605872378262		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.00954605872378262 | validation: 0.03182373676620972]
	TIME [epoch: 13 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009546946060516068		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.009546946060516068 | validation: 0.032748511394319224]
	TIME [epoch: 13 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008271625242959468		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.008271625242959468 | validation: 0.024375828497770025]
	TIME [epoch: 13 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009625719996529742		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.009625719996529742 | validation: 0.0301351179860225]
	TIME [epoch: 13 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010071477174080839		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.010071477174080839 | validation: 0.026315507943153273]
	TIME [epoch: 13 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00945557026463566		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.00945557026463566 | validation: 0.024722055325599415]
	TIME [epoch: 13 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010332358604772223		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.010332358604772223 | validation: 0.03991278755752632]
	TIME [epoch: 13 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009779748709723682		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.009779748709723682 | validation: 0.02472476040548152]
	TIME [epoch: 13 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008634504300803805		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.008634504300803805 | validation: 0.028911713526577766]
	TIME [epoch: 13 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009411171823401279		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.009411171823401279 | validation: 0.026828145508647462]
	TIME [epoch: 13 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008773486278133402		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.008773486278133402 | validation: 0.02478896024677515]
	TIME [epoch: 13 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00915592035671606		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.00915592035671606 | validation: 0.04535368717248444]
	TIME [epoch: 13 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009680013785901356		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.009680013785901356 | validation: 0.0372854069938087]
	TIME [epoch: 13 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010126738269402255		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.010126738269402255 | validation: 0.025216228532726773]
	TIME [epoch: 13 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010636060957829825		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.010636060957829825 | validation: 0.029687102067848394]
	TIME [epoch: 13 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010488400405057038		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.010488400405057038 | validation: 0.025882681408414845]
	TIME [epoch: 13 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010309350727938642		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.010309350727938642 | validation: 0.03993358005425421]
	TIME [epoch: 13 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009340889546739332		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.009340889546739332 | validation: 0.04413530410476954]
	TIME [epoch: 13 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00975388145250071		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.00975388145250071 | validation: 0.02695133143490345]
	TIME [epoch: 13 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008663450936616593		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.008663450936616593 | validation: 0.025819610335851118]
	TIME [epoch: 13 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010696514847307458		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.010696514847307458 | validation: 0.03229552491177802]
	TIME [epoch: 13 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01027991972748316		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.01027991972748316 | validation: 0.033089447948146]
	TIME [epoch: 13 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008744036755868852		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.008744036755868852 | validation: 0.02671629670276191]
	TIME [epoch: 13 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010053934463439098		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.010053934463439098 | validation: 0.026549598557327993]
	TIME [epoch: 13 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009344475596445839		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.009344475596445839 | validation: 0.024831003059440228]
	TIME [epoch: 13 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0102214794157496		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.0102214794157496 | validation: 0.021859353444101382]
	TIME [epoch: 13 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008983878186251576		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.008983878186251576 | validation: 0.02814750864378567]
	TIME [epoch: 13 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009075650708295455		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.009075650708295455 | validation: 0.023952052130867242]
	TIME [epoch: 13 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00877193631010184		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.00877193631010184 | validation: 0.024996709209609527]
	TIME [epoch: 13 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013008136071906049		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.013008136071906049 | validation: 0.03368965972826695]
	TIME [epoch: 13 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012173106604403428		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.012173106604403428 | validation: 0.04210372142730856]
	TIME [epoch: 13 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008704312250777764		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.008704312250777764 | validation: 0.027165462222222214]
	TIME [epoch: 13 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009656927512394135		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.009656927512394135 | validation: 0.02413059114491324]
	TIME [epoch: 13 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009402463573293487		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.009402463573293487 | validation: 0.027777104610973116]
	TIME [epoch: 13 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010376456627818114		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.010376456627818114 | validation: 0.025507816242233795]
	TIME [epoch: 13 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008940185354540704		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.008940185354540704 | validation: 0.025608401011770934]
	TIME [epoch: 13 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008916352138120974		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.008916352138120974 | validation: 0.030838611111324223]
	TIME [epoch: 13 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00873873683459065		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.00873873683459065 | validation: 0.026434186216298117]
	TIME [epoch: 13 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009739330863180887		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.009739330863180887 | validation: 0.030409059902838043]
	TIME [epoch: 13 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010501425170439425		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.010501425170439425 | validation: 0.028577071887502428]
	TIME [epoch: 13 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009359913649331338		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.009359913649331338 | validation: 0.023992510092996522]
	TIME [epoch: 13 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009146544837084361		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.009146544837084361 | validation: 0.024356519588506764]
	TIME [epoch: 13 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009000263687784203		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.009000263687784203 | validation: 0.027979071730187512]
	TIME [epoch: 13 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009339565849870144		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.009339565849870144 | validation: 0.020769056734011096]
	TIME [epoch: 13 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008818656698291993		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.008818656698291993 | validation: 0.02119471034972308]
	TIME [epoch: 13 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00896149169783442		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.00896149169783442 | validation: 0.0283752080408778]
	TIME [epoch: 13 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009702105366769128		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.009702105366769128 | validation: 0.027735344397639583]
	TIME [epoch: 12.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008791850137785846		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.008791850137785846 | validation: 0.025398236626742832]
	TIME [epoch: 13 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009340882392522136		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.009340882392522136 | validation: 0.024886762813776254]
	TIME [epoch: 13 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00966885247085491		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.00966885247085491 | validation: 0.06793328122211718]
	TIME [epoch: 13 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014647642991150255		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.014647642991150255 | validation: 0.061491924281207366]
	TIME [epoch: 13 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013006532136744442		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.013006532136744442 | validation: 0.039333645163562414]
	TIME [epoch: 13 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009804143440090974		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.009804143440090974 | validation: 0.02562248820596723]
	TIME [epoch: 13 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008992547277159662		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.008992547277159662 | validation: 0.025858156199071003]
	TIME [epoch: 13 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00848891865207067		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.00848891865207067 | validation: 0.029200767980537334]
	TIME [epoch: 13 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00901884679922548		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.00901884679922548 | validation: 0.026045175753622085]
	TIME [epoch: 13 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008731998858455478		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.008731998858455478 | validation: 0.021977054190470427]
	TIME [epoch: 13 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008198494578075534		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.008198494578075534 | validation: 0.03257920596507753]
	TIME [epoch: 13 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00883272034922768		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.00883272034922768 | validation: 0.027093168272501224]
	TIME [epoch: 13 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009155689132977193		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.009155689132977193 | validation: 0.025350573551862043]
	TIME [epoch: 12.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008401633272972246		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.008401633272972246 | validation: 0.021691808105092747]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_160745/states/model_phi1_4b_v_mmd1_1196.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7607.972 seconds.
