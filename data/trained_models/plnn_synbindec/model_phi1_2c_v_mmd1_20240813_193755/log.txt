Args:
Namespace(name='model_phi1_2c_v_mmd1', outdir='out/model_training/model_phi1_2c_v_mmd1', training_data='data/training_data/basic/data_phi1_2c/training', validation_data='data/training_data/basic/data_phi1_2c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4011532720

Training model...

Saving initial model state to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.466842754742286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466842754742286 | validation: 3.8487772351355503]
	TIME [epoch: 108 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.978119886341212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.978119886341212 | validation: 3.2312690805665505]
	TIME [epoch: 6.72 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.218850892777273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.218850892777273 | validation: 2.8514472379748774]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.851446162208843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.851446162208843 | validation: 2.637377532234833]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8252065322410225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8252065322410225 | validation: 2.2744256497122817]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.392493539055938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.392493539055938 | validation: 2.074242720155043]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.17087198961402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.17087198961402 | validation: 2.056706573743468]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1758955626006875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1758955626006875 | validation: 2.017663665225109]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.078642287010188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.078642287010188 | validation: 1.933854873925326]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0148151157894105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0148151157894105 | validation: 1.930042957894032]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.937857250721605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.937857250721605 | validation: 2.189772921419707]
	TIME [epoch: 6.63 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.027414360474141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.027414360474141 | validation: 1.8741839429356277]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9660753317759738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9660753317759738 | validation: 1.790257037846871]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8660461929360057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8660461929360057 | validation: 1.8817819252716868]
	TIME [epoch: 6.65 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9945340316189863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9945340316189863 | validation: 1.8707358895581014]
	TIME [epoch: 6.65 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9589719850185197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9589719850185197 | validation: 2.0518674072530128]
	TIME [epoch: 6.63 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.031605973608323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.031605973608323 | validation: 1.7386060256619595]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7806063244950954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7806063244950954 | validation: 1.68038059469243]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7033134062583115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7033134062583115 | validation: 1.738735032740779]
	TIME [epoch: 6.63 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.702247042902965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.702247042902965 | validation: 1.6058706819679422]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6465899590996789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6465899590996789 | validation: 1.5854277005618398]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5742475014096242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5742475014096242 | validation: 1.5527534826410374]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5845917389452469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5845917389452469 | validation: 1.5440224994169611]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5471010356672146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5471010356672146 | validation: 1.5905724191857247]
	TIME [epoch: 6.63 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5467974437538237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5467974437538237 | validation: 1.5106069006670002]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.491515316629623		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.491515316629623 | validation: 1.4380412711150479]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4161449177112515		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.4161449177112515 | validation: 2.1777311193408737]
	TIME [epoch: 6.63 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.00891215876905		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.00891215876905 | validation: 1.4533049471531994]
	TIME [epoch: 6.63 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5202736224506452		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5202736224506452 | validation: 1.4286138597329294]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5142240445526245		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.5142240445526245 | validation: 1.3867436968909055]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3566445037778432		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.3566445037778432 | validation: 1.3883353780325516]
	TIME [epoch: 6.67 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3895433123896193		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.3895433123896193 | validation: 1.3940646035362627]
	TIME [epoch: 6.64 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3777073720236954		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3777073720236954 | validation: 1.3436509329166904]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3206563915027705		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3206563915027705 | validation: 2.3828846556079255]
	TIME [epoch: 6.64 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2294922478470784		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.2294922478470784 | validation: 1.6821585347930386]
	TIME [epoch: 6.63 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5684448251966887		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.5684448251966887 | validation: 1.4839512467482319]
	TIME [epoch: 6.64 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4275945201517681		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.4275945201517681 | validation: 1.3508186293009707]
	TIME [epoch: 6.65 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2821917608962394		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2821917608962394 | validation: 1.3465706798244161]
	TIME [epoch: 6.64 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5032455906574185		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.5032455906574185 | validation: 1.4669806049354506]
	TIME [epoch: 6.64 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4588684124507152		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.4588684124507152 | validation: 1.3710168860484986]
	TIME [epoch: 6.63 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2998309840040652		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2998309840040652 | validation: 1.3412813543305457]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.312348439196157		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.312348439196157 | validation: 1.6350314094739584]
	TIME [epoch: 6.67 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7357628987890343		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.7357628987890343 | validation: 1.681119548302917]
	TIME [epoch: 6.63 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6363683024999935		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6363683024999935 | validation: 1.4367966747661864]
	TIME [epoch: 6.63 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3690158351371715		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3690158351371715 | validation: 1.3850524238529758]
	TIME [epoch: 6.63 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4145138189583266		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.4145138189583266 | validation: 1.3504281179331095]
	TIME [epoch: 6.63 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3262754141992352		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3262754141992352 | validation: 1.3502554642021343]
	TIME [epoch: 6.65 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.339258885996848		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.339258885996848 | validation: 1.3332588760284778]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2669989987002523		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2669989987002523 | validation: 1.314404326888328]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3074918453656204		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.3074918453656204 | validation: 1.473185345775412]
	TIME [epoch: 6.65 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.376805247399234		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.376805247399234 | validation: 1.3936614554173024]
	TIME [epoch: 6.64 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3320826615639823		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3320826615639823 | validation: 1.3124764126316908]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3350365053940125		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.3350365053940125 | validation: 1.2993479902389078]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.284890451068677		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.284890451068677 | validation: 1.2976968116772387]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2923162544568307		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.2923162544568307 | validation: 1.2884515799287801]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2445174800568433		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2445174800568433 | validation: 1.278813818890284]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2174375703515063		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2174375703515063 | validation: 1.3008210787660084]
	TIME [epoch: 6.64 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2871262595392636		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2871262595392636 | validation: 1.2633284814811823]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2586298303293884		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2586298303293884 | validation: 1.2628062422097959]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2339332264479925		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2339332264479925 | validation: 1.260988156644867]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1763103009965785		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1763103009965785 | validation: 1.2994706824012403]
	TIME [epoch: 6.64 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2299243314228567		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2299243314228567 | validation: 1.3061855532462843]
	TIME [epoch: 6.63 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3297100525335293		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3297100525335293 | validation: 1.2412921207657854]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1984047158566082		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.1984047158566082 | validation: 1.271834073933857]
	TIME [epoch: 6.66 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1764168661936025		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1764168661936025 | validation: 1.3030510613749948]
	TIME [epoch: 6.63 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.228363043595529		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.228363043595529 | validation: 1.2552156982411165]
	TIME [epoch: 6.63 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1510457988811131		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1510457988811131 | validation: 1.2270161430291258]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1521476481025412		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.1521476481025412 | validation: 1.201597597217732]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1086448442752594		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1086448442752594 | validation: 1.197743978702588]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1024886116586532		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1024886116586532 | validation: 1.1822717970281127]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1018193989309042		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1018193989309042 | validation: 1.1869264729278746]
	TIME [epoch: 6.63 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1334804548600204		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1334804548600204 | validation: 1.0810071271714976]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.075291508303156		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.075291508303156 | validation: 1.189264203037364]
	TIME [epoch: 6.64 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.085871116447961		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.085871116447961 | validation: 1.0447953816748559]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0648109177830598		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0648109177830598 | validation: 1.0097109697051299]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9898245371427875		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.9898245371427875 | validation: 1.06197662308062]
	TIME [epoch: 6.64 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.070233927267643		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.070233927267643 | validation: 1.3623611729870384]
	TIME [epoch: 6.63 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1901820030645665		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1901820030645665 | validation: 1.1049982797439923]
	TIME [epoch: 6.64 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0297784773761753		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.0297784773761753 | validation: 0.9876852169587957]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0389393439996208		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.0389393439996208 | validation: 1.0273476907555295]
	TIME [epoch: 6.67 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.956643005048021		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.956643005048021 | validation: 1.0474007021712404]
	TIME [epoch: 6.63 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0147362060049616		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0147362060049616 | validation: 1.0495833371020673]
	TIME [epoch: 6.63 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0287039101209106		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0287039101209106 | validation: 0.9510319607300141]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9104707315376808		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9104707315376808 | validation: 1.0437485237401551]
	TIME [epoch: 6.64 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0105466011625528		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0105466011625528 | validation: 0.9880497371317827]
	TIME [epoch: 6.67 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9122452816537849		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9122452816537849 | validation: 0.9125004707769788]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0228138903252757		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0228138903252757 | validation: 0.914810613024946]
	TIME [epoch: 6.63 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8795578845205676		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8795578845205676 | validation: 1.0561270978343509]
	TIME [epoch: 6.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9301806937405341		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9301806937405341 | validation: 1.225064454113353]
	TIME [epoch: 6.63 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.117600197647142		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.117600197647142 | validation: 0.9499776842625463]
	TIME [epoch: 6.65 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9593742510654677		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9593742510654677 | validation: 0.8983150339099404]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9108063492194214		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9108063492194214 | validation: 1.6993537716547273]
	TIME [epoch: 6.63 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4439956343600429		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.4439956343600429 | validation: 1.015067172618772]
	TIME [epoch: 6.63 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9663377996018411		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.9663377996018411 | validation: 0.9214872821196199]
	TIME [epoch: 6.63 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8448524181517794		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8448524181517794 | validation: 0.9058418719331519]
	TIME [epoch: 6.63 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9192043843735169		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9192043843735169 | validation: 0.9149220107626773]
	TIME [epoch: 6.66 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9049688140022736		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9049688140022736 | validation: 0.9150310366318298]
	TIME [epoch: 6.64 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8183274829039189		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8183274829039189 | validation: 1.1016760774579568]
	TIME [epoch: 6.63 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9732152683231343		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.9732152683231343 | validation: 0.9622244622292775]
	TIME [epoch: 6.63 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8335509408344047		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.8335509408344047 | validation: 0.8921651540855827]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7914517932233301		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7914517932233301 | validation: 1.002491105971793]
	TIME [epoch: 6.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0412020946929073		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0412020946929073 | validation: 0.9820268443314566]
	TIME [epoch: 6.67 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9365223010011535		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9365223010011535 | validation: 1.0457052301725847]
	TIME [epoch: 6.64 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0071223995998235		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.0071223995998235 | validation: 0.9806585009702534]
	TIME [epoch: 6.64 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8176344995372522		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8176344995372522 | validation: 0.8963383484839256]
	TIME [epoch: 6.64 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9345706573622693		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9345706573622693 | validation: 0.8778145554820768]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.984134017743443		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.984134017743443 | validation: 1.2201256771882019]
	TIME [epoch: 6.68 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.055811206154056		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.055811206154056 | validation: 1.1071811876049453]
	TIME [epoch: 6.64 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.973955545584031		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.973955545584031 | validation: 0.9300564682905738]
	TIME [epoch: 6.63 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8291501053974488		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8291501053974488 | validation: 0.9001025745876021]
	TIME [epoch: 6.63 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7811763906802429		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7811763906802429 | validation: 0.8736891869862626]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7447303157203499		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7447303157203499 | validation: 0.9029939055301877]
	TIME [epoch: 6.66 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7627862831895669		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7627862831895669 | validation: 0.8585435206227706]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8259776268438124		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8259776268438124 | validation: 1.8148837249846328]
	TIME [epoch: 6.65 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.847365880874698		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.847365880874698 | validation: 1.5619685962862508]
	TIME [epoch: 6.65 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8207989441657235		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.8207989441657235 | validation: 2.4745221815424134]
	TIME [epoch: 6.64 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.356138303999705		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 2.356138303999705 | validation: 2.1644232120631153]
	TIME [epoch: 6.65 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9328270778606575		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.9328270778606575 | validation: 1.5592276672616618]
	TIME [epoch: 6.68 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3847458507545842		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.3847458507545842 | validation: 1.1190413803600814]
	TIME [epoch: 6.65 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0790064188922943		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0790064188922943 | validation: 1.2126911131450275]
	TIME [epoch: 6.64 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2086768146388633		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 1.2086768146388633 | validation: 1.1364964824488757]
	TIME [epoch: 6.63 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1168442454186343		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1168442454186343 | validation: 1.386625952350112]
	TIME [epoch: 6.64 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2363658794818329		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.2363658794818329 | validation: 1.2467344769677693]
	TIME [epoch: 6.67 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.085290499837184		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.085290499837184 | validation: 1.044763935044905]
	TIME [epoch: 6.66 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9685588016893167		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9685588016893167 | validation: 1.028964589492923]
	TIME [epoch: 6.64 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9317484986279003		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9317484986279003 | validation: 1.0356966482929397]
	TIME [epoch: 6.65 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9550699078757532		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9550699078757532 | validation: 0.9942870042965457]
	TIME [epoch: 6.65 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9245007716120025		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9245007716120025 | validation: 1.0073808840765583]
	TIME [epoch: 6.65 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9021968131642205		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9021968131642205 | validation: 0.9610217839360392]
	TIME [epoch: 6.66 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8621988324559525		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8621988324559525 | validation: 1.054914243646443]
	TIME [epoch: 6.68 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9452378034561393		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9452378034561393 | validation: 0.9999404556972795]
	TIME [epoch: 6.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9266805065387089		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9266805065387089 | validation: 0.9842361287430648]
	TIME [epoch: 6.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.920538948099997		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.920538948099997 | validation: 0.9436785501139083]
	TIME [epoch: 6.65 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8421581104371484		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8421581104371484 | validation: 1.5779028138998377]
	TIME [epoch: 6.66 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5086093289968368		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.5086093289968368 | validation: 1.103305823710145]
	TIME [epoch: 6.7 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.978680030473402		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.978680030473402 | validation: 0.956331668959673]
	TIME [epoch: 6.66 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8733754158791244		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8733754158791244 | validation: 0.9118301709880436]
	TIME [epoch: 6.66 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7784654592266684		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7784654592266684 | validation: 0.9115233986716289]
	TIME [epoch: 6.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.840914116972431		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.840914116972431 | validation: 0.8661001695157474]
	TIME [epoch: 6.64 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7726763526008997		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7726763526008997 | validation: 0.867245341666925]
	TIME [epoch: 6.66 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8243580642825721		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8243580642825721 | validation: 0.8910670477817698]
	TIME [epoch: 6.67 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8000554566567056		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8000554566567056 | validation: 0.8725924339839404]
	TIME [epoch: 6.65 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8057079181134391		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8057079181134391 | validation: 0.8623500193765825]
	TIME [epoch: 6.64 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7467631978550956		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7467631978550956 | validation: 0.8722410933469231]
	TIME [epoch: 6.63 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7463875983517245		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7463875983517245 | validation: 0.8411370380588921]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7665254211893461		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7665254211893461 | validation: 0.8595728659983882]
	TIME [epoch: 6.68 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7582047234377609		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7582047234377609 | validation: 0.8552351402244711]
	TIME [epoch: 6.64 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.747789166070188		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.747789166070188 | validation: 0.8504718207638992]
	TIME [epoch: 6.63 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6995705115146981		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6995705115146981 | validation: 0.8430143321214074]
	TIME [epoch: 6.63 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0957657029888908		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.0957657029888908 | validation: 1.2639577882055768]
	TIME [epoch: 6.64 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1527861617715813		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1527861617715813 | validation: 1.25729057485629]
	TIME [epoch: 6.64 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.033699081768722		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.033699081768722 | validation: 0.9533560315576054]
	TIME [epoch: 6.67 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7834543859092299		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7834543859092299 | validation: 0.8887980597860872]
	TIME [epoch: 6.63 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.728657155501252		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.728657155501252 | validation: 0.8743723763577411]
	TIME [epoch: 6.64 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6912595782946456		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6912595782946456 | validation: 0.8596481435191312]
	TIME [epoch: 6.64 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6869041499278006		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6869041499278006 | validation: 0.8533740535553967]
	TIME [epoch: 6.64 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6848675328652357		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6848675328652357 | validation: 0.8436029475613127]
	TIME [epoch: 6.65 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.698281807983272		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.698281807983272 | validation: 0.8239025583515244]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6743559420155418		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6743559420155418 | validation: 0.8313005545109372]
	TIME [epoch: 6.64 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6902030314120498		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6902030314120498 | validation: 0.8184543441465175]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6540294060484342		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6540294060484342 | validation: 0.8235685182916161]
	TIME [epoch: 6.64 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6816962283356747		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6816962283356747 | validation: 0.830911391368143]
	TIME [epoch: 6.64 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6812904709774403		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6812904709774403 | validation: 0.7989072909038044]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6317728764567635		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6317728764567635 | validation: 0.7862878401678594]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6483958767663298		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6483958767663298 | validation: 0.7925764745079973]
	TIME [epoch: 6.63 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7073031663491331		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.7073031663491331 | validation: 0.8062940380132844]
	TIME [epoch: 6.63 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6411001260441573		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6411001260441573 | validation: 0.8650186903909983]
	TIME [epoch: 6.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6899337959236931		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6899337959236931 | validation: 0.8450247711216541]
	TIME [epoch: 6.67 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6274020058007272		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6274020058007272 | validation: 0.7438558957874332]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5985579102144956		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.5985579102144956 | validation: 0.7482033378925466]
	TIME [epoch: 6.62 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5410198791982682		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5410198791982682 | validation: 0.7370991126849527]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7389865607708181		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7389865607708181 | validation: 0.8251384395384206]
	TIME [epoch: 6.63 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7413105136055416		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7413105136055416 | validation: 0.7175747973795178]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5716778650701111		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5716778650701111 | validation: 0.6535498885759704]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5370136735749723		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5370136735749723 | validation: 0.6549568146074813]
	TIME [epoch: 6.63 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5194353770104028		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5194353770104028 | validation: 0.5990974159481762]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4875749445071572		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4875749445071572 | validation: 0.6479723977628047]
	TIME [epoch: 6.63 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5598096074396208		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5598096074396208 | validation: 0.6535535447500203]
	TIME [epoch: 6.64 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5495721125661227		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.5495721125661227 | validation: 0.6582513986317211]
	TIME [epoch: 6.68 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5953742520420255		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5953742520420255 | validation: 0.687930697225549]
	TIME [epoch: 6.64 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5681872378869623		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5681872378869623 | validation: 0.7431895191122878]
	TIME [epoch: 6.63 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5681745858426259		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5681745858426259 | validation: 0.8842745209138244]
	TIME [epoch: 6.63 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9973279724368569		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.9973279724368569 | validation: 0.7102071774807079]
	TIME [epoch: 6.63 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6429926808334712		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6429926808334712 | validation: 0.714183354445636]
	TIME [epoch: 6.65 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5505396890724353		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5505396890724353 | validation: 0.6768078796293936]
	TIME [epoch: 6.66 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5452610055562073		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5452610055562073 | validation: 0.582008302196308]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4839868942024794		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4839868942024794 | validation: 0.5595179615678505]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46718024985556833		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.46718024985556833 | validation: 0.600058442697781]
	TIME [epoch: 6.64 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49601486741867507		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.49601486741867507 | validation: 0.6285910281129188]
	TIME [epoch: 6.64 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47458414118779413		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.47458414118779413 | validation: 0.5397912416391383]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4317250803790591		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.4317250803790591 | validation: 0.548066839792379]
	TIME [epoch: 6.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5220014057458217		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5220014057458217 | validation: 0.5331076400785596]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49024290142587706		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.49024290142587706 | validation: 0.5266902087546351]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4345746552903109		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4345746552903109 | validation: 0.5716249466461454]
	TIME [epoch: 6.63 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4331352734504582		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.4331352734504582 | validation: 0.9195271205626562]
	TIME [epoch: 6.65 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8656364811534402		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.8656364811534402 | validation: 0.5548658073790118]
	TIME [epoch: 6.66 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46626199545305325		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.46626199545305325 | validation: 0.6674721962534602]
	TIME [epoch: 6.63 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4923205378208263		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4923205378208263 | validation: 0.5553250261179238]
	TIME [epoch: 6.66 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45231356815275053		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.45231356815275053 | validation: 0.4776307522377411]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.404269533865175		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.404269533865175 | validation: 0.5393161190815879]
	TIME [epoch: 6.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44094985863645475		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.44094985863645475 | validation: 0.620084784481085]
	TIME [epoch: 115 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.572267991771528		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.572267991771528 | validation: 0.5682735968929417]
	TIME [epoch: 14.6 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4363114212623228		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.4363114212623228 | validation: 0.4821227071004178]
	TIME [epoch: 14.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3932347462237997		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.3932347462237997 | validation: 0.5364764745791936]
	TIME [epoch: 14.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40453182979948155		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.40453182979948155 | validation: 0.41432686940773744]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37800093356704534		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.37800093356704534 | validation: 0.47273367235591857]
	TIME [epoch: 14.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38943826045841146		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.38943826045841146 | validation: 0.39674158513687663]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4070488386843837		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4070488386843837 | validation: 1.1026384239033704]
	TIME [epoch: 14.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9449111904014476		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9449111904014476 | validation: 1.1650928360872184]
	TIME [epoch: 14.6 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9275161060394921		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.9275161060394921 | validation: 0.8422943300580275]
	TIME [epoch: 14.6 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5375658541937567		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5375658541937567 | validation: 0.367298305886718]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3692442355713569		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3692442355713569 | validation: 0.4427004524585451]
	TIME [epoch: 14.6 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4057604633068067		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.4057604633068067 | validation: 0.975438557688865]
	TIME [epoch: 14.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8434665232042289		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.8434665232042289 | validation: 1.0499998759048161]
	TIME [epoch: 14.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8436692608061258		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.8436692608061258 | validation: 0.984201711434403]
	TIME [epoch: 14.6 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6803743898425967		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.6803743898425967 | validation: 0.55854432691666]
	TIME [epoch: 14.6 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4485226198659781		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.4485226198659781 | validation: 0.439116225187394]
	TIME [epoch: 14.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41027685514706624		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.41027685514706624 | validation: 0.48385702146950466]
	TIME [epoch: 14.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38024171673111806		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.38024171673111806 | validation: 0.417849521250909]
	TIME [epoch: 14.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4280501979638681		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.4280501979638681 | validation: 0.39744130214347506]
	TIME [epoch: 14.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.381569755831632		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.381569755831632 | validation: 0.45710903202934405]
	TIME [epoch: 14.6 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3578840430551552		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.3578840430551552 | validation: 0.3869534848150088]
	TIME [epoch: 14.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3670315002746921		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.3670315002746921 | validation: 0.5903478613429903]
	TIME [epoch: 14.6 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4345331358289611		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.4345331358289611 | validation: 0.4481575775541822]
	TIME [epoch: 14.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32241664638889933		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.32241664638889933 | validation: 0.35808162730230747]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30903110174377535		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.30903110174377535 | validation: 0.3384959418507396]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30961045363769235		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.30961045363769235 | validation: 0.4456430962667911]
	TIME [epoch: 14.5 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3700055007032853		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.3700055007032853 | validation: 0.5445033651045711]
	TIME [epoch: 14.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3570274572214564		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.3570274572214564 | validation: 0.3122640394602753]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.279357435007736		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.279357435007736 | validation: 0.8175725876682837]
	TIME [epoch: 14.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6519913368507387		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.6519913368507387 | validation: 0.7722520700202237]
	TIME [epoch: 14.6 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5269252551971647		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5269252551971647 | validation: 0.3337742386451243]
	TIME [epoch: 14.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2877332486386319		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2877332486386319 | validation: 0.3148318353702835]
	TIME [epoch: 14.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26536747507394054		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.26536747507394054 | validation: 0.36180288209832023]
	TIME [epoch: 14.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2726317307415117		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.2726317307415117 | validation: 0.3704965566909686]
	TIME [epoch: 14.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3006947525157988		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.3006947525157988 | validation: 0.26219205900881754]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25965996941294567		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.25965996941294567 | validation: 0.28191237364242133]
	TIME [epoch: 14.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2293437368376301		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2293437368376301 | validation: 0.24291426086737067]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22058473299555775		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.22058473299555775 | validation: 0.390322820635292]
	TIME [epoch: 14.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2469813904972164		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.2469813904972164 | validation: 0.30616981585070796]
	TIME [epoch: 14.6 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21254369771735804		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.21254369771735804 | validation: 0.7008507646157096]
	TIME [epoch: 14.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4926468483526526		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.4926468483526526 | validation: 0.42347337636995186]
	TIME [epoch: 14.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3741003487723545		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.3741003487723545 | validation: 0.3894973397553013]
	TIME [epoch: 14.6 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28133006461807575		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.28133006461807575 | validation: 0.24606807155398303]
	TIME [epoch: 14.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21033279886523054		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.21033279886523054 | validation: 0.2694141084337818]
	TIME [epoch: 14.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2181381193422704		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2181381193422704 | validation: 0.3280244355132862]
	TIME [epoch: 14.6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23005897644549284		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.23005897644549284 | validation: 0.2559757577228363]
	TIME [epoch: 14.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1980794383140657		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.1980794383140657 | validation: 0.29631644701657184]
	TIME [epoch: 14.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23532976451573745		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.23532976451573745 | validation: 0.19199463559539678]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17837608834715846		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.17837608834715846 | validation: 0.2732645904377508]
	TIME [epoch: 14.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20083466797046454		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.20083466797046454 | validation: 0.2024168804519656]
	TIME [epoch: 14.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17734293756495212		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.17734293756495212 | validation: 0.3024843900694453]
	TIME [epoch: 14.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23853707548606046		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.23853707548606046 | validation: 0.2641022778591348]
	TIME [epoch: 14.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26437403433688		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.26437403433688 | validation: 0.28452905550362495]
	TIME [epoch: 14.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1955024371481675		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1955024371481675 | validation: 0.2208037212339483]
	TIME [epoch: 14.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1736334155259761		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.1736334155259761 | validation: 0.25669032965727784]
	TIME [epoch: 14.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16405732794256653		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.16405732794256653 | validation: 0.23908384681333514]
	TIME [epoch: 14.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2043801797297528		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.2043801797297528 | validation: 0.24958665649609443]
	TIME [epoch: 14.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1959292673006861		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1959292673006861 | validation: 0.2123998050528555]
	TIME [epoch: 14.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16438001070269015		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.16438001070269015 | validation: 0.1560228372688115]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19933970797524717		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.19933970797524717 | validation: 0.31425101519635357]
	TIME [epoch: 14.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22094376940946098		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.22094376940946098 | validation: 0.16437940796054692]
	TIME [epoch: 14.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.164617670185547		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.164617670185547 | validation: 0.1617417224647284]
	TIME [epoch: 14.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13483355841488084		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.13483355841488084 | validation: 0.16965502075858693]
	TIME [epoch: 14.6 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.133732663748474		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.133732663748474 | validation: 0.20742808148131564]
	TIME [epoch: 14.6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1331912857287097		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1331912857287097 | validation: 0.18873327380576876]
	TIME [epoch: 14.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1377173461381634		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1377173461381634 | validation: 0.5189788240275249]
	TIME [epoch: 14.6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23191873171290023		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.23191873171290023 | validation: 0.3425841585183099]
	TIME [epoch: 14.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33078385646512853		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.33078385646512853 | validation: 0.1560821801522154]
	TIME [epoch: 14.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11960295508250385		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.11960295508250385 | validation: 0.2987241068228701]
	TIME [epoch: 14.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19954741794667186		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.19954741794667186 | validation: 0.18379692727846933]
	TIME [epoch: 14.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13871202739009053		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.13871202739009053 | validation: 0.12129330143591895]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12073214263412066		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.12073214263412066 | validation: 0.25477884480723817]
	TIME [epoch: 14.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16884398161800884		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.16884398161800884 | validation: 0.22861027688056978]
	TIME [epoch: 14.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1563804096121486		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.1563804096121486 | validation: 0.14034251081739804]
	TIME [epoch: 14.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14338421802024845		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.14338421802024845 | validation: 0.1601668766684572]
	TIME [epoch: 14.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11654218732749		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.11654218732749 | validation: 0.1118207999836919]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11197385932601955		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11197385932601955 | validation: 0.13330814014865464]
	TIME [epoch: 14.6 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15080996421616857		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.15080996421616857 | validation: 0.18358430185067423]
	TIME [epoch: 14.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12370474314929074		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.12370474314929074 | validation: 0.249890985853033]
	TIME [epoch: 14.6 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13213362772310297		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.13213362772310297 | validation: 0.1723668860433991]
	TIME [epoch: 14.6 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10740182663131781		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.10740182663131781 | validation: 0.09697976594324752]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09396773078151273		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.09396773078151273 | validation: 0.7873946365627693]
	TIME [epoch: 14.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.54786045416079		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.54786045416079 | validation: 0.46264790489102303]
	TIME [epoch: 14.6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6003011216767907		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.6003011216767907 | validation: 1.1328519441412712]
	TIME [epoch: 14.6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7764719114189733		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.7764719114189733 | validation: 1.077863031067382]
	TIME [epoch: 14.6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7205557698296774		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.7205557698296774 | validation: 0.873905446829482]
	TIME [epoch: 14.6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44274411264599484		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.44274411264599484 | validation: 0.22289100116203633]
	TIME [epoch: 14.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12045585355914837		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.12045585355914837 | validation: 0.10716897920302466]
	TIME [epoch: 14.6 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10151583902778501		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.10151583902778501 | validation: 0.11470129384782256]
	TIME [epoch: 14.6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08724778972588793		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.08724778972588793 | validation: 0.09345553125294728]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09153865925390534		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.09153865925390534 | validation: 0.10331761647754663]
	TIME [epoch: 14.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08044504754054926		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.08044504754054926 | validation: 0.17023465284068992]
	TIME [epoch: 14.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10736967467413928		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.10736967467413928 | validation: 0.10348285591067292]
	TIME [epoch: 14.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0988686083329982		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.0988686083329982 | validation: 0.09480205096136715]
	TIME [epoch: 14.6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07965322502066037		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.07965322502066037 | validation: 0.24351912871365436]
	TIME [epoch: 14.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17481574935202435		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.17481574935202435 | validation: 0.11257105220716795]
	TIME [epoch: 14.6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08542565792169152		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.08542565792169152 | validation: 0.09183775325691401]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07600588008338136		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.07600588008338136 | validation: 0.09903318009810419]
	TIME [epoch: 14.5 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08537086529540394		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.08537086529540394 | validation: 0.14834138999923271]
	TIME [epoch: 14.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27414129045697744		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.27414129045697744 | validation: 0.10142834274021702]
	TIME [epoch: 14.6 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18365598296468563		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.18365598296468563 | validation: 0.21586091132644986]
	TIME [epoch: 14.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11656020367653283		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.11656020367653283 | validation: 0.09282921234044171]
	TIME [epoch: 14.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07450381212176874		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.07450381212176874 | validation: 0.13779591829349014]
	TIME [epoch: 14.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11277411120206302		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.11277411120206302 | validation: 0.14844977003065654]
	TIME [epoch: 14.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10262920479073148		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10262920479073148 | validation: 0.09953436423064027]
	TIME [epoch: 14.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13658711952230343		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.13658711952230343 | validation: 0.0980989960693235]
	TIME [epoch: 14.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0796201025897488		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.0796201025897488 | validation: 0.0926220280597681]
	TIME [epoch: 14.6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14696919211766143		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.14696919211766143 | validation: 0.23445980412758669]
	TIME [epoch: 14.6 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1930347191731256		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.1930347191731256 | validation: 0.21083815175672027]
	TIME [epoch: 14.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17593156178732827		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.17593156178732827 | validation: 0.23436734172020068]
	TIME [epoch: 14.6 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12102242070297342		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.12102242070297342 | validation: 0.11926439276674072]
	TIME [epoch: 14.6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08449384992078127		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.08449384992078127 | validation: 0.1294791505805197]
	TIME [epoch: 14.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08406063063003201		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.08406063063003201 | validation: 0.08981805488079121]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08443913246185464		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.08443913246185464 | validation: 0.10422403790499966]
	TIME [epoch: 14.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08524399160033966		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.08524399160033966 | validation: 0.08222546446653611]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07560495934553946		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07560495934553946 | validation: 0.11714809588906112]
	TIME [epoch: 14.6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07581562137166692		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.07581562137166692 | validation: 0.08209782322982798]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06565155355027655		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06565155355027655 | validation: 0.0965279457002921]
	TIME [epoch: 14.6 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07250637124501316		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.07250637124501316 | validation: 0.07869287675838665]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07321997980825604		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.07321997980825604 | validation: 0.07817321681568974]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06474436624559914		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.06474436624559914 | validation: 0.10830251704148007]
	TIME [epoch: 14.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07884642371926187		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.07884642371926187 | validation: 0.08785067859563507]
	TIME [epoch: 14.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06995107647362567		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06995107647362567 | validation: 0.07352205059256096]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06157645603888484		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.06157645603888484 | validation: 0.06793112859707773]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06995319622834058		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06995319622834058 | validation: 0.07658331345950535]
	TIME [epoch: 14.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08003286322267913		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.08003286322267913 | validation: 0.07109572948137814]
	TIME [epoch: 14.6 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06799573797163648		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06799573797163648 | validation: 0.27995940971663436]
	TIME [epoch: 14.6 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17324232675522191		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.17324232675522191 | validation: 0.07719849816167802]
	TIME [epoch: 14.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06877979627458725		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.06877979627458725 | validation: 0.09163046859968861]
	TIME [epoch: 14.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07821272870188858		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.07821272870188858 | validation: 0.08757555820369839]
	TIME [epoch: 14.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09426689623740643		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.09426689623740643 | validation: 0.14785235707968714]
	TIME [epoch: 14.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09286468949000595		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.09286468949000595 | validation: 0.07365490833006404]
	TIME [epoch: 14.6 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10078237120714492		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.10078237120714492 | validation: 0.14611282562554126]
	TIME [epoch: 14.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10844355431640276		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.10844355431640276 | validation: 0.1714804168979375]
	TIME [epoch: 14.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08825925620408699		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.08825925620408699 | validation: 0.06986305716833759]
	TIME [epoch: 14.6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0828948003985699		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.0828948003985699 | validation: 0.06877534621672439]
	TIME [epoch: 14.6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055329076609661784		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.055329076609661784 | validation: 0.23627022741922074]
	TIME [epoch: 14.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1080652412516285		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.1080652412516285 | validation: 0.07916100071264834]
	TIME [epoch: 14.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07029500476646211		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.07029500476646211 | validation: 0.0660814094062283]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06369941866703703		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06369941866703703 | validation: 0.0812766252103592]
	TIME [epoch: 14.6 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06344722376487252		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06344722376487252 | validation: 0.08444429621880256]
	TIME [epoch: 14.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06138566589763016		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.06138566589763016 | validation: 0.09442379176483491]
	TIME [epoch: 14.6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06812191608598311		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.06812191608598311 | validation: 0.061493711978915434]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06545922743007149		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06545922743007149 | validation: 0.04819919818044818]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048023262562110085		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.048023262562110085 | validation: 0.08939297570158967]
	TIME [epoch: 14.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06421790219134303		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06421790219134303 | validation: 0.07207339886992066]
	TIME [epoch: 14.6 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05521565444374665		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.05521565444374665 | validation: 0.0633406728175373]
	TIME [epoch: 14.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05709290379325725		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.05709290379325725 | validation: 0.2832108901689676]
	TIME [epoch: 14.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12050686159074198		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.12050686159074198 | validation: 0.061791709391212236]
	TIME [epoch: 14.5 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09878913701880097		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.09878913701880097 | validation: 0.12566945400437443]
	TIME [epoch: 14.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0952683283951044		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.0952683283951044 | validation: 0.07149864944156246]
	TIME [epoch: 14.6 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05569434909833562		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05569434909833562 | validation: 0.05478128415558086]
	TIME [epoch: 14.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05054459145603524		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05054459145603524 | validation: 0.05971400161473359]
	TIME [epoch: 14.6 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1654551592645536		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.1654551592645536 | validation: 0.1242433452792831]
	TIME [epoch: 14.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10818679968898605		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.10818679968898605 | validation: 0.06893783855320093]
	TIME [epoch: 14.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05245756282670014		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.05245756282670014 | validation: 0.05905591026850046]
	TIME [epoch: 14.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04697318384471534		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.04697318384471534 | validation: 0.04863952591012055]
	TIME [epoch: 14.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04460402540075377		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.04460402540075377 | validation: 0.05238793599113154]
	TIME [epoch: 14.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05064011940947316		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05064011940947316 | validation: 0.05959540981055261]
	TIME [epoch: 14.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0524738386293644		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.0524738386293644 | validation: 0.30517409378002924]
	TIME [epoch: 14.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1860730180348827		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1860730180348827 | validation: 0.166253468140777]
	TIME [epoch: 14.6 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1202074447562272		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.1202074447562272 | validation: 0.05864424794970258]
	TIME [epoch: 14.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056078949369356784		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.056078949369356784 | validation: 0.15805390481007736]
	TIME [epoch: 14.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10261950270734738		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.10261950270734738 | validation: 0.05720865057591136]
	TIME [epoch: 14.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04956435892266567		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04956435892266567 | validation: 0.0616529966546783]
	TIME [epoch: 14.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05227412890993224		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05227412890993224 | validation: 0.05009327768758694]
	TIME [epoch: 14.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07246841440043394		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.07246841440043394 | validation: 0.05903681443777087]
	TIME [epoch: 14.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05000124994376747		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.05000124994376747 | validation: 0.05845429312641608]
	TIME [epoch: 14.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049348425539547616		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.049348425539547616 | validation: 0.04949788271963044]
	TIME [epoch: 14.6 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0457881749433116		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0457881749433116 | validation: 0.05016883067403306]
	TIME [epoch: 14.6 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046596769739414255		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.046596769739414255 | validation: 0.0469085225683501]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04738088008876991		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.04738088008876991 | validation: 0.048187476434022804]
	TIME [epoch: 14.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043234752238437674		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.043234752238437674 | validation: 0.13452762418204242]
	TIME [epoch: 14.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08246799258585438		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.08246799258585438 | validation: 0.0550091137572311]
	TIME [epoch: 14.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04429544093594452		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.04429544093594452 | validation: 0.07047192906101661]
	TIME [epoch: 14.6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11041245331206202		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.11041245331206202 | validation: 0.0614550786758153]
	TIME [epoch: 14.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0867677373348923		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0867677373348923 | validation: 0.05383260906690954]
	TIME [epoch: 14.6 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045085049659133916		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.045085049659133916 | validation: 0.05418833085129846]
	TIME [epoch: 14.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048198847943310544		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.048198847943310544 | validation: 0.07748489714751339]
	TIME [epoch: 14.6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05163514177040811		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05163514177040811 | validation: 0.10764696704822858]
	TIME [epoch: 14.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08721426748927882		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.08721426748927882 | validation: 0.04819631216810022]
	TIME [epoch: 14.6 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06023486676456076		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.06023486676456076 | validation: 0.04274542696111488]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04097377116635656		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.04097377116635656 | validation: 0.05302624712769696]
	TIME [epoch: 14.6 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043027379516718464		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.043027379516718464 | validation: 0.043255678160768134]
	TIME [epoch: 14.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03834227270857368		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.03834227270857368 | validation: 0.04692553700463872]
	TIME [epoch: 14.6 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040183446065452114		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.040183446065452114 | validation: 0.05383440981826342]
	TIME [epoch: 14.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039027565223825625		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.039027565223825625 | validation: 0.042515409411307686]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04471699679952812		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04471699679952812 | validation: 0.038750905399074304]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03796186095902973		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.03796186095902973 | validation: 0.17964172947358717]
	TIME [epoch: 14.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10605411382519986		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.10605411382519986 | validation: 0.18804342407586333]
	TIME [epoch: 14.6 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11048235043692035		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.11048235043692035 | validation: 0.12143242369866936]
	TIME [epoch: 14.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07248754165914179		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.07248754165914179 | validation: 0.11105773233476979]
	TIME [epoch: 14.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0738626960329077		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.0738626960329077 | validation: 0.06609905143818526]
	TIME [epoch: 14.6 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04678945432379384		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04678945432379384 | validation: 0.04025125623152546]
	TIME [epoch: 14.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035026366424905364		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.035026366424905364 | validation: 0.04677572016191344]
	TIME [epoch: 14.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04370116984577124		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.04370116984577124 | validation: 0.04329551593303053]
	TIME [epoch: 14.6 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03735569300280863		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.03735569300280863 | validation: 0.040163528991397454]
	TIME [epoch: 14.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03543443999957318		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.03543443999957318 | validation: 0.05043405253887348]
	TIME [epoch: 14.6 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04525624740267667		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04525624740267667 | validation: 0.061477457004987905]
	TIME [epoch: 14.6 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07634817493496734		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.07634817493496734 | validation: 0.058246198913841286]
	TIME [epoch: 14.6 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050399547551817275		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.050399547551817275 | validation: 0.05975843595728926]
	TIME [epoch: 14.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04540796606504406		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.04540796606504406 | validation: 0.04355321687410858]
	TIME [epoch: 14.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03266030479661716		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.03266030479661716 | validation: 0.06302183397097204]
	TIME [epoch: 14.6 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04445486539193594		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.04445486539193594 | validation: 0.04444442904938988]
	TIME [epoch: 14.6 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03653555349500253		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.03653555349500253 | validation: 0.03465290998097169]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02939878657882439		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.02939878657882439 | validation: 0.048781187006513044]
	TIME [epoch: 14.6 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03681991254274488		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.03681991254274488 | validation: 0.040809886723257756]
	TIME [epoch: 14.6 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028932823921956674		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.028932823921956674 | validation: 0.03841166711169589]
	TIME [epoch: 14.6 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03782823882760761		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.03782823882760761 | validation: 0.20864357127894595]
	TIME [epoch: 14.6 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10849896740220835		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.10849896740220835 | validation: 0.29882339650241585]
	TIME [epoch: 14.6 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21514612797382132		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.21514612797382132 | validation: 0.09076201859563077]
	TIME [epoch: 14.6 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0976702991641262		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.0976702991641262 | validation: 0.08810549721893679]
	TIME [epoch: 14.6 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08863405754100599		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.08863405754100599 | validation: 0.10639929747310906]
	TIME [epoch: 14.6 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07007300109808853		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.07007300109808853 | validation: 0.07278015093020478]
	TIME [epoch: 14.6 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050398188260765135		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.050398188260765135 | validation: 0.034071704354387035]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03135251640643248		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.03135251640643248 | validation: 0.04238086904661078]
	TIME [epoch: 14.6 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037655358249517025		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.037655358249517025 | validation: 0.038810883376757616]
	TIME [epoch: 14.6 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031981645997609665		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.031981645997609665 | validation: 0.033383704763356135]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04486510245911696		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04486510245911696 | validation: 0.0855505702156032]
	TIME [epoch: 14.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06967229847450948		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.06967229847450948 | validation: 0.034587139527871]
	TIME [epoch: 14.6 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03409310955374301		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.03409310955374301 | validation: 0.038659827938481404]
	TIME [epoch: 14.6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03543319185593453		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.03543319185593453 | validation: 0.034460148722045295]
	TIME [epoch: 14.6 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02951767444498342		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.02951767444498342 | validation: 0.03939829004449391]
	TIME [epoch: 14.6 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03399407032842565		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03399407032842565 | validation: 0.03605239198058577]
	TIME [epoch: 14.6 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030441339532575804		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.030441339532575804 | validation: 0.04501073378432982]
	TIME [epoch: 14.6 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1122205680907103		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.1122205680907103 | validation: 0.1705025726242595]
	TIME [epoch: 14.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10976973535651413		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.10976973535651413 | validation: 0.1331384171910134]
	TIME [epoch: 14.6 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0787344095854023		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.0787344095854023 | validation: 0.04683955528293916]
	TIME [epoch: 14.6 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03529735685034842		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.03529735685034842 | validation: 0.03700599000089514]
	TIME [epoch: 14.6 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029253597665764054		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.029253597665764054 | validation: 0.034808188175958175]
	TIME [epoch: 14.6 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028682882359750993		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.028682882359750993 | validation: 0.03815872186176576]
	TIME [epoch: 14.6 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030763683465172147		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.030763683465172147 | validation: 0.03740178790039159]
	TIME [epoch: 14.6 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030129652171327254		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.030129652171327254 | validation: 0.04222493526990783]
	TIME [epoch: 14.6 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03788035149701584		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.03788035149701584 | validation: 0.042336743478770235]
	TIME [epoch: 14.6 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03346650294852935		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03346650294852935 | validation: 0.031915057001822796]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030405529164979794		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.030405529164979794 | validation: 0.03578850923257321]
	TIME [epoch: 14.6 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029362508339283958		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.029362508339283958 | validation: 0.04710167643575746]
	TIME [epoch: 14.6 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03126695066604944		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.03126695066604944 | validation: 0.0311093586756474]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02979272937295946		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.02979272937295946 | validation: 0.03489025971641416]
	TIME [epoch: 14.6 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028346548528453187		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.028346548528453187 | validation: 0.030318970345608195]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030399896937928425		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.030399896937928425 | validation: 0.035861360293189044]
	TIME [epoch: 14.6 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027885849785836234		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.027885849785836234 | validation: 0.0326129643317078]
	TIME [epoch: 14.6 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027723494655719605		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.027723494655719605 | validation: 0.03070419046722043]
	TIME [epoch: 14.6 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04941011520257091		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04941011520257091 | validation: 0.040373579187784725]
	TIME [epoch: 14.6 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0658892222755105		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.0658892222755105 | validation: 0.047084189695065067]
	TIME [epoch: 14.6 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055596486923169326		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.055596486923169326 | validation: 0.03286150073319271]
	TIME [epoch: 14.6 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02896046100811111		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.02896046100811111 | validation: 0.03554962108105456]
	TIME [epoch: 14.6 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02705533467645073		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.02705533467645073 | validation: 0.034272108770856395]
	TIME [epoch: 14.6 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027707656009992053		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.027707656009992053 | validation: 0.03512245635115799]
	TIME [epoch: 14.6 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028947842351512612		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.028947842351512612 | validation: 0.03043254762654667]
	TIME [epoch: 14.6 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02714253341713431		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.02714253341713431 | validation: 0.10731406644387256]
	TIME [epoch: 14.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06648341379408212		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06648341379408212 | validation: 0.03638699633557835]
	TIME [epoch: 14.6 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04351670741407454		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04351670741407454 | validation: 0.028892732605292227]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02902895157912805		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.02902895157912805 | validation: 0.028593808092626596]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02810161880330706		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.02810161880330706 | validation: 0.035047492453286706]
	TIME [epoch: 14.6 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034054352543975236		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.034054352543975236 | validation: 0.08124289220492023]
	TIME [epoch: 14.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053486300447189467		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.053486300447189467 | validation: 0.035097939390862996]
	TIME [epoch: 14.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03150479771731376		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03150479771731376 | validation: 0.0571990510151573]
	TIME [epoch: 14.6 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05226296520346721		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.05226296520346721 | validation: 0.033214033525841576]
	TIME [epoch: 14.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03139465798424682		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03139465798424682 | validation: 0.031654441943367306]
	TIME [epoch: 14.6 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02586663929395537		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.02586663929395537 | validation: 0.06922110835237637]
	TIME [epoch: 14.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045316481139043116		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.045316481139043116 | validation: 0.029361653394540596]
	TIME [epoch: 14.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026753719287184393		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.026753719287184393 | validation: 0.031237968641995958]
	TIME [epoch: 14.6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030115547037896692		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.030115547037896692 | validation: 0.03407336462732918]
	TIME [epoch: 14.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033391250290248475		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.033391250290248475 | validation: 0.030014192293245145]
	TIME [epoch: 14.6 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029196997436076483		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.029196997436076483 | validation: 0.03586075177660746]
	TIME [epoch: 14.5 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0325679675818271		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0325679675818271 | validation: 0.030428096462387085]
	TIME [epoch: 14.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02579048028300569		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.02579048028300569 | validation: 0.04340125178756231]
	TIME [epoch: 14.6 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031888575537717664		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.031888575537717664 | validation: 0.034604043414827965]
	TIME [epoch: 14.5 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03020919298375506		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03020919298375506 | validation: 0.02610561346052357]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027276922449906555		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.027276922449906555 | validation: 0.03772980774064412]
	TIME [epoch: 14.6 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03146245688419047		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.03146245688419047 | validation: 0.1807772603150415]
	TIME [epoch: 14.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11116101360015468		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.11116101360015468 | validation: 0.03498664115602453]
	TIME [epoch: 14.6 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030990464069146326		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.030990464069146326 | validation: 0.032015042051819056]
	TIME [epoch: 14.6 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0256488726778722		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0256488726778722 | validation: 0.029114444892676252]
	TIME [epoch: 14.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02609202118117472		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.02609202118117472 | validation: 0.02788951538231141]
	TIME [epoch: 14.6 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09891695486049253		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.09891695486049253 | validation: 0.23816696001656168]
	TIME [epoch: 14.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17299625885017217		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.17299625885017217 | validation: 0.059787047223288495]
	TIME [epoch: 14.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04883358038687455		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.04883358038687455 | validation: 0.03713779291303728]
	TIME [epoch: 14.6 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029995786725885924		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.029995786725885924 | validation: 0.0349749123984133]
	TIME [epoch: 14.5 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030759656199221518		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.030759656199221518 | validation: 0.028094732274973035]
	TIME [epoch: 14.6 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026701588453579256		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.026701588453579256 | validation: 0.02774747621381587]
	TIME [epoch: 14.6 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024907425940509813		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.024907425940509813 | validation: 0.02378661016082025]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02324922076318062		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.02324922076318062 | validation: 0.026755326500818483]
	TIME [epoch: 14.6 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02418821609216308		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.02418821609216308 | validation: 0.026208193393409646]
	TIME [epoch: 14.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02619646884479594		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.02619646884479594 | validation: 0.028810528276037407]
	TIME [epoch: 14.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026806470595543835		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.026806470595543835 | validation: 0.03144791169413637]
	TIME [epoch: 14.6 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024189541610715327		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.024189541610715327 | validation: 0.0358608333271734]
	TIME [epoch: 14.6 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030374694148603977		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.030374694148603977 | validation: 0.029019713133200987]
	TIME [epoch: 14.6 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02538753954859868		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.02538753954859868 | validation: 0.024549021078771962]
	TIME [epoch: 14.6 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0298716718965367		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.0298716718965367 | validation: 0.028964859634069296]
	TIME [epoch: 14.6 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024344725610007358		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.024344725610007358 | validation: 0.041625352733099814]
	TIME [epoch: 14.6 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03259135857405353		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03259135857405353 | validation: 0.02864719694638119]
	TIME [epoch: 14.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025722112206687235		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.025722112206687235 | validation: 0.029957420257588054]
	TIME [epoch: 14.6 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02660949520900765		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.02660949520900765 | validation: 0.04094287291078783]
	TIME [epoch: 14.6 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029903022608156485		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.029903022608156485 | validation: 0.03097759055530627]
	TIME [epoch: 14.6 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023169856460566355		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.023169856460566355 | validation: 0.026652368348554025]
	TIME [epoch: 14.6 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02710872236488264		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.02710872236488264 | validation: 0.041945212222790226]
	TIME [epoch: 14.6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035872622037250994		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.035872622037250994 | validation: 0.027119007002066948]
	TIME [epoch: 14.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02624543329062271		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.02624543329062271 | validation: 0.029195852828652443]
	TIME [epoch: 130 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022417778223190077		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.022417778223190077 | validation: 0.027359834927960226]
	TIME [epoch: 31.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02250002519744946		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.02250002519744946 | validation: 0.03471357932101639]
	TIME [epoch: 31.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033916184959794426		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.033916184959794426 | validation: 0.02673466666618097]
	TIME [epoch: 31.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029418059058023662		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.029418059058023662 | validation: 0.021235357456640472]
	TIME [epoch: 31.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02478958742999027		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.02478958742999027 | validation: 0.024747283290933397]
	TIME [epoch: 31.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023772682359833944		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.023772682359833944 | validation: 0.02859088125226287]
	TIME [epoch: 31.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024812704451108045		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.024812704451108045 | validation: 0.029434840390934483]
	TIME [epoch: 31.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024863907346870127		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.024863907346870127 | validation: 0.02861767553379397]
	TIME [epoch: 31.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02612002150222957		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.02612002150222957 | validation: 0.027639224071890026]
	TIME [epoch: 31.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0217544229427465		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.0217544229427465 | validation: 0.025220152954527522]
	TIME [epoch: 31.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02163522496039371		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.02163522496039371 | validation: 0.02825404541909457]
	TIME [epoch: 31.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025109460839409688		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.025109460839409688 | validation: 0.031814256370615045]
	TIME [epoch: 31.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024141548527698613		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.024141548527698613 | validation: 0.023747754764131768]
	TIME [epoch: 31.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02193779786984043		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.02193779786984043 | validation: 0.02843525417549285]
	TIME [epoch: 31.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025072786751365518		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.025072786751365518 | validation: 0.06487675384066012]
	TIME [epoch: 31.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04628643878040894		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.04628643878040894 | validation: 0.03776484734012487]
	TIME [epoch: 31.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028688361628839507		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.028688361628839507 | validation: 0.049429508842647504]
	TIME [epoch: 31.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03589302081112564		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03589302081112564 | validation: 0.04854598566631137]
	TIME [epoch: 31.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033027628923282475		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.033027628923282475 | validation: 0.029628018935316015]
	TIME [epoch: 31.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022952897310118098		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.022952897310118098 | validation: 0.026757942751918146]
	TIME [epoch: 31.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024520148511416838		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.024520148511416838 | validation: 0.025090803087030784]
	TIME [epoch: 31.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023264907932645365		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.023264907932645365 | validation: 0.04112244827296138]
	TIME [epoch: 31.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04376559295804777		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.04376559295804777 | validation: 0.07742947066986033]
	TIME [epoch: 31.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06764371921934886		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06764371921934886 | validation: 0.03909681733805762]
	TIME [epoch: 31.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0310320622483952		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0310320622483952 | validation: 0.025564710368244897]
	TIME [epoch: 31.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023812751306406192		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.023812751306406192 | validation: 0.023979317605101965]
	TIME [epoch: 31.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024723092557965134		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.024723092557965134 | validation: 0.0227879881239717]
	TIME [epoch: 31.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0213055234230621		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.0213055234230621 | validation: 0.025311583795324572]
	TIME [epoch: 31.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021865713946039644		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.021865713946039644 | validation: 0.024910778019553194]
	TIME [epoch: 31.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021163494297488976		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.021163494297488976 | validation: 0.023122589216385547]
	TIME [epoch: 31.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019631883512962425		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.019631883512962425 | validation: 0.027229300194177453]
	TIME [epoch: 31.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021998628994824722		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.021998628994824722 | validation: 0.0219792151902938]
	TIME [epoch: 31.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025937716403411616		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.025937716403411616 | validation: 0.025180107636637552]
	TIME [epoch: 31.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022715509484634473		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.022715509484634473 | validation: 0.020166908095877612]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020409920432097772		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.020409920432097772 | validation: 0.02315674882552471]
	TIME [epoch: 31.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02142878263317826		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.02142878263317826 | validation: 0.02393075092331412]
	TIME [epoch: 31.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02380833907397911		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.02380833907397911 | validation: 0.04547598320306836]
	TIME [epoch: 31.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033834386407376627		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.033834386407376627 | validation: 0.028917885656674594]
	TIME [epoch: 31.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028815278125549914		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.028815278125549914 | validation: 0.026179198986301885]
	TIME [epoch: 31.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020655251690635194		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.020655251690635194 | validation: 0.026945860564934733]
	TIME [epoch: 31.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020940013076394683		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.020940013076394683 | validation: 0.02739255887224424]
	TIME [epoch: 31.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02013803793625107		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.02013803793625107 | validation: 0.027894680341721476]
	TIME [epoch: 31.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021826549563846095		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.021826549563846095 | validation: 0.023340177632562423]
	TIME [epoch: 31.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022139738226856543		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.022139738226856543 | validation: 0.023560965047664112]
	TIME [epoch: 31.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02082953085006331		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.02082953085006331 | validation: 0.023397360048462885]
	TIME [epoch: 31.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02310533157043272		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.02310533157043272 | validation: 0.02628769832565843]
	TIME [epoch: 31.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02734391498216312		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02734391498216312 | validation: 0.0742812257037656]
	TIME [epoch: 31.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07948136978413706		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.07948136978413706 | validation: 0.05635347306093387]
	TIME [epoch: 31.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042772827827380615		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.042772827827380615 | validation: 0.02633632883896597]
	TIME [epoch: 31.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027663559247792437		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.027663559247792437 | validation: 0.026042097246220277]
	TIME [epoch: 31.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027569212078634858		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.027569212078634858 | validation: 0.02815683262485105]
	TIME [epoch: 31.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021908847698067512		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.021908847698067512 | validation: 0.023417727185866002]
	TIME [epoch: 31.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01903796764490457		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.01903796764490457 | validation: 0.02542587571141307]
	TIME [epoch: 31.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0210736783298984		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.0210736783298984 | validation: 0.02389237382511657]
	TIME [epoch: 31.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020943493723902556		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.020943493723902556 | validation: 0.02703939824758958]
	TIME [epoch: 31.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01927059142666916		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.01927059142666916 | validation: 0.02294028751668087]
	TIME [epoch: 31.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02083498695827614		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.02083498695827614 | validation: 0.027226591778339443]
	TIME [epoch: 31.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019993001673021336		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.019993001673021336 | validation: 0.027587600441027423]
	TIME [epoch: 31.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023606099494676895		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.023606099494676895 | validation: 0.025335777899111813]
	TIME [epoch: 31.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01986569192298892		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.01986569192298892 | validation: 0.02454766747249622]
	TIME [epoch: 31.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020666483585408152		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.020666483585408152 | validation: 0.027048669060285038]
	TIME [epoch: 31.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022457466445590653		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.022457466445590653 | validation: 0.09891758750857083]
	TIME [epoch: 31.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10624080254403073		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.10624080254403073 | validation: 0.037051028317543803]
	TIME [epoch: 31.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0322354031659726		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.0322354031659726 | validation: 0.03024018591695096]
	TIME [epoch: 31.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02189990639964582		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.02189990639964582 | validation: 0.027694473657672095]
	TIME [epoch: 31.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023371361777301695		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.023371361777301695 | validation: 0.024852372772970862]
	TIME [epoch: 31.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020812379635954226		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.020812379635954226 | validation: 0.025552645958134987]
	TIME [epoch: 31.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019823940908040776		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.019823940908040776 | validation: 0.026611121844766817]
	TIME [epoch: 31.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018736957724901485		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.018736957724901485 | validation: 0.02053410962104752]
	TIME [epoch: 31.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020211368874769684		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.020211368874769684 | validation: 0.023895878117576874]
	TIME [epoch: 31.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0186825024468675		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.0186825024468675 | validation: 0.022215945653510684]
	TIME [epoch: 31.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019706970106896093		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.019706970106896093 | validation: 0.023719781703519946]
	TIME [epoch: 31.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021160526062091557		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.021160526062091557 | validation: 0.025863368967942352]
	TIME [epoch: 31.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02095768947664546		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.02095768947664546 | validation: 0.027009257618296312]
	TIME [epoch: 31.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020704279762872556		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.020704279762872556 | validation: 0.028811542399917317]
	TIME [epoch: 31.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020457258734237812		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.020457258734237812 | validation: 0.02457120724318584]
	TIME [epoch: 31.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021831626560556573		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.021831626560556573 | validation: 0.026363286486345734]
	TIME [epoch: 31.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019455480645482207		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.019455480645482207 | validation: 0.02247700699908255]
	TIME [epoch: 31.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019972204439529982		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.019972204439529982 | validation: 0.02252713930355528]
	TIME [epoch: 31.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020601795930838322		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.020601795930838322 | validation: 0.02216206452818617]
	TIME [epoch: 31.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02073787023529636		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.02073787023529636 | validation: 0.023437351462470266]
	TIME [epoch: 31.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022833160623870735		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.022833160623870735 | validation: 0.03604275856312809]
	TIME [epoch: 31.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026932383878881902		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.026932383878881902 | validation: 0.02291532115576795]
	TIME [epoch: 31.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02040634181610217		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02040634181610217 | validation: 0.024897611811636123]
	TIME [epoch: 31.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02058231733543956		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02058231733543956 | validation: 0.042019510332486826]
	TIME [epoch: 31.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02722073695624322		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02722073695624322 | validation: 0.025129045975545185]
	TIME [epoch: 31.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02014554620268639		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02014554620268639 | validation: 0.026344024334595345]
	TIME [epoch: 31.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019787259578214464		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.019787259578214464 | validation: 0.02603210277082925]
	TIME [epoch: 31.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017987266861016003		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.017987266861016003 | validation: 0.02641880542910139]
	TIME [epoch: 31.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022878365530665627		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.022878365530665627 | validation: 0.024241555140452016]
	TIME [epoch: 31.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019954077473010777		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.019954077473010777 | validation: 0.02266015106261199]
	TIME [epoch: 31.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01976181669162988		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.01976181669162988 | validation: 0.025218334972607222]
	TIME [epoch: 31.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021014316208653967		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.021014316208653967 | validation: 0.024116886522302834]
	TIME [epoch: 31.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021332257742440783		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.021332257742440783 | validation: 0.022686247370151583]
	TIME [epoch: 31.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02380245346933578		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.02380245346933578 | validation: 0.029942556786046837]
	TIME [epoch: 31.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023453991272620177		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.023453991272620177 | validation: 0.023501547729697893]
	TIME [epoch: 31.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021120036819301385		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.021120036819301385 | validation: 0.02599512358668257]
	TIME [epoch: 31.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02228098645526952		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.02228098645526952 | validation: 0.01992787073698292]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018660232517581022		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.018660232517581022 | validation: 0.02450634596967488]
	TIME [epoch: 31.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02091817563702162		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02091817563702162 | validation: 0.023713621533706215]
	TIME [epoch: 31.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020128430824295142		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.020128430824295142 | validation: 0.022081729076888518]
	TIME [epoch: 31.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02006711492571918		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.02006711492571918 | validation: 0.025826280788503964]
	TIME [epoch: 31.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02253898950106297		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.02253898950106297 | validation: 0.02180306134591889]
	TIME [epoch: 31.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028610988571053153		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.028610988571053153 | validation: 0.02466304062619737]
	TIME [epoch: 31.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026984555477926622		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.026984555477926622 | validation: 0.02348920832409242]
	TIME [epoch: 31.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019739826668707872		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.019739826668707872 | validation: 0.02389763813205989]
	TIME [epoch: 31.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0205204939545725		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.0205204939545725 | validation: 0.023504817016084423]
	TIME [epoch: 31.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018105576610886315		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.018105576610886315 | validation: 0.02514794843012177]
	TIME [epoch: 31.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021835567331379903		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.021835567331379903 | validation: 0.03347499433137196]
	TIME [epoch: 31.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024394598734281566		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.024394598734281566 | validation: 0.025039872016515252]
	TIME [epoch: 31.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022775443793697336		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.022775443793697336 | validation: 0.02173080635017663]
	TIME [epoch: 31.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018469107962292786		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.018469107962292786 | validation: 0.01980622326326223]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020139316676387006		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.020139316676387006 | validation: 0.02242343955639546]
	TIME [epoch: 31.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018783222007510102		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.018783222007510102 | validation: 0.019429328292232314]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018607573463831642		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.018607573463831642 | validation: 0.020075522388860345]
	TIME [epoch: 31.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0181134731352309		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.0181134731352309 | validation: 0.020104515685173553]
	TIME [epoch: 31.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018805583550275708		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.018805583550275708 | validation: 0.021049435740153402]
	TIME [epoch: 31.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019057867383393857		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.019057867383393857 | validation: 0.023425689964136555]
	TIME [epoch: 31.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03127439178259351		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.03127439178259351 | validation: 0.024500287020673025]
	TIME [epoch: 31.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02070362114168408		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02070362114168408 | validation: 0.03032438002238068]
	TIME [epoch: 31.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022794206413868166		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.022794206413868166 | validation: 0.022446340311724364]
	TIME [epoch: 31.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02093551692489353		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.02093551692489353 | validation: 0.022852624365207616]
	TIME [epoch: 31.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01843774735890673		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.01843774735890673 | validation: 0.021884147736520216]
	TIME [epoch: 31.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019537747177373443		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.019537747177373443 | validation: 0.019465099110360224]
	TIME [epoch: 31.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021351185517674474		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.021351185517674474 | validation: 0.021943877712041357]
	TIME [epoch: 31.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020211265679616713		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.020211265679616713 | validation: 0.020241001194463634]
	TIME [epoch: 31.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017044688653073646		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.017044688653073646 | validation: 0.02496381404459985]
	TIME [epoch: 31.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01905912909966688		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.01905912909966688 | validation: 0.023357489377311094]
	TIME [epoch: 31.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020381159776655604		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.020381159776655604 | validation: 0.020372658852593387]
	TIME [epoch: 31.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017394703860930914		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.017394703860930914 | validation: 0.02131718893394929]
	TIME [epoch: 31.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0242547712339946		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.0242547712339946 | validation: 0.021668470802307733]
	TIME [epoch: 31.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019343090158006133		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.019343090158006133 | validation: 0.020374274107507518]
	TIME [epoch: 31.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01754958524675223		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.01754958524675223 | validation: 0.020562172128442687]
	TIME [epoch: 31.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01801865064875733		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.01801865064875733 | validation: 0.0508257565379088]
	TIME [epoch: 31.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045077575096395074		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.045077575096395074 | validation: 0.026728292659870208]
	TIME [epoch: 31.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02239115541793236		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.02239115541793236 | validation: 0.026635118633485968]
	TIME [epoch: 31.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020924749098497987		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.020924749098497987 | validation: 0.021180482344105862]
	TIME [epoch: 31.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019644435380969053		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.019644435380969053 | validation: 0.022984447554823692]
	TIME [epoch: 31.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01976923114270661		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.01976923114270661 | validation: 0.02176461117830665]
	TIME [epoch: 31.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024223878769675068		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.024223878769675068 | validation: 0.034772818025615576]
	TIME [epoch: 31.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027881286241528663		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.027881286241528663 | validation: 0.02002665739666984]
	TIME [epoch: 31.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018496412845060217		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.018496412845060217 | validation: 0.02229183155972289]
	TIME [epoch: 31.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01618464692621937		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.01618464692621937 | validation: 0.019945241012146783]
	TIME [epoch: 31.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01981253989640883		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.01981253989640883 | validation: 0.022817413815506976]
	TIME [epoch: 31.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019185014810700063		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.019185014810700063 | validation: 0.020628339411011967]
	TIME [epoch: 31.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01913034018731965		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.01913034018731965 | validation: 0.019958208708229754]
	TIME [epoch: 31.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017786187359285534		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.017786187359285534 | validation: 0.01635107270583234]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017487948955029087		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.017487948955029087 | validation: 0.021048796882313037]
	TIME [epoch: 31.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01819793063545297		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.01819793063545297 | validation: 0.017754357794293515]
	TIME [epoch: 31.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01768180608384231		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.01768180608384231 | validation: 0.02022366943909877]
	TIME [epoch: 31.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020021584230072123		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.020021584230072123 | validation: 0.01768335691886194]
	TIME [epoch: 31.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01793835147768487		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.01793835147768487 | validation: 0.018012448139426828]
	TIME [epoch: 31.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01807954964478388		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.01807954964478388 | validation: 0.022726056070510998]
	TIME [epoch: 31.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020277758181698916		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.020277758181698916 | validation: 0.021340188558386597]
	TIME [epoch: 31.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01815854971382623		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.01815854971382623 | validation: 0.020406500308264026]
	TIME [epoch: 31.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020004159984792866		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.020004159984792866 | validation: 0.02471193360152507]
	TIME [epoch: 31.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02113503114559935		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.02113503114559935 | validation: 0.019918207396471023]
	TIME [epoch: 31.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018360508082494335		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.018360508082494335 | validation: 0.02163058898655922]
	TIME [epoch: 31.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019436059403628338		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.019436059403628338 | validation: 0.022253801170329025]
	TIME [epoch: 31.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018086486082549716		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.018086486082549716 | validation: 0.0228773540640152]
	TIME [epoch: 31.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017299488899262035		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.017299488899262035 | validation: 0.022633279147097392]
	TIME [epoch: 31.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01808029862521881		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.01808029862521881 | validation: 0.02181760228132873]
	TIME [epoch: 31.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017048298076052848		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.017048298076052848 | validation: 0.022398682108500947]
	TIME [epoch: 31.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016860771293430308		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.016860771293430308 | validation: 0.020023598485386204]
	TIME [epoch: 31.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018844875715850095		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.018844875715850095 | validation: 0.01787639008506665]
	TIME [epoch: 31.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017381210351606478		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.017381210351606478 | validation: 0.023118607588620572]
	TIME [epoch: 31.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015825360500047358		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.015825360500047358 | validation: 0.022571314374050577]
	TIME [epoch: 31.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019421568698867554		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.019421568698867554 | validation: 0.019986802624072715]
	TIME [epoch: 31.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018635176038424205		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.018635176038424205 | validation: 0.019164334668096666]
	TIME [epoch: 31.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01729167132708338		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.01729167132708338 | validation: 0.02159803941180929]
	TIME [epoch: 31.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019636065165998525		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.019636065165998525 | validation: 0.023376227611954015]
	TIME [epoch: 31.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01659715725933652		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.01659715725933652 | validation: 0.019300604423772533]
	TIME [epoch: 31.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01867202913877047		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.01867202913877047 | validation: 0.02087421702570809]
	TIME [epoch: 31.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01978816771253093		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.01978816771253093 | validation: 0.021244457162745006]
	TIME [epoch: 31.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018108817158273748		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.018108817158273748 | validation: 0.022331592618945517]
	TIME [epoch: 31.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01678684597011692		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.01678684597011692 | validation: 0.021649787191406523]
	TIME [epoch: 31.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017231405875803327		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.017231405875803327 | validation: 0.02224296037612123]
	TIME [epoch: 31.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01682280147265944		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.01682280147265944 | validation: 0.02264287711084109]
	TIME [epoch: 31.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017564400294051964		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.017564400294051964 | validation: 0.023432423484304993]
	TIME [epoch: 31.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02153822108488936		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.02153822108488936 | validation: 0.018938845576215252]
	TIME [epoch: 31.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019507964736841334		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.019507964736841334 | validation: 0.020828416601581436]
	TIME [epoch: 31.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018210841390857775		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.018210841390857775 | validation: 0.021773068018329136]
	TIME [epoch: 31.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019096799152461142		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.019096799152461142 | validation: 0.019610759813756706]
	TIME [epoch: 31.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016631128109911568		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.016631128109911568 | validation: 0.025523317697850146]
	TIME [epoch: 31.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02303965684916766		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.02303965684916766 | validation: 0.01946330035573203]
	TIME [epoch: 31.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019150644494600407		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.019150644494600407 | validation: 0.019884687484446446]
	TIME [epoch: 31.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017808732812134867		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.017808732812134867 | validation: 0.02014410236343758]
	TIME [epoch: 31.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017400056375155		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.017400056375155 | validation: 0.019257270226911528]
	TIME [epoch: 31.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016573713905727323		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.016573713905727323 | validation: 0.02286457117910632]
	TIME [epoch: 31.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019253577565749046		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.019253577565749046 | validation: 0.0203028693997944]
	TIME [epoch: 31.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017308475069116213		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.017308475069116213 | validation: 0.018949058103183782]
	TIME [epoch: 31.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02545517579658095		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.02545517579658095 | validation: 0.024137144778611282]
	TIME [epoch: 31.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022821364301530055		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.022821364301530055 | validation: 0.02047173535219232]
	TIME [epoch: 31.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018577159951531273		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.018577159951531273 | validation: 0.023726714681327844]
	TIME [epoch: 31.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018605669483242868		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.018605669483242868 | validation: 0.02421054399749823]
	TIME [epoch: 31.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01798642030138868		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.01798642030138868 | validation: 0.017925545838808]
	TIME [epoch: 31.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017642445095745255		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.017642445095745255 | validation: 0.018387587760164203]
	TIME [epoch: 31.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016929052965475595		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.016929052965475595 | validation: 0.019508027188867835]
	TIME [epoch: 31.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018010583399620805		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.018010583399620805 | validation: 0.019921358610127213]
	TIME [epoch: 31.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017453016968994636		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.017453016968994636 | validation: 0.02001535479836001]
	TIME [epoch: 31.4 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01808261545226529		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.01808261545226529 | validation: 0.016854008034166507]
	TIME [epoch: 31.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017245660047132683		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.017245660047132683 | validation: 0.01939970772372031]
	TIME [epoch: 31.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01711835967375072		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.01711835967375072 | validation: 0.01942503344446276]
	TIME [epoch: 31.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017197518288616986		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.017197518288616986 | validation: 0.023814011015448736]
	TIME [epoch: 31.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020010205433524473		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.020010205433524473 | validation: 0.02123334571889578]
	TIME [epoch: 31.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016414058945467075		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.016414058945467075 | validation: 0.019562188716626253]
	TIME [epoch: 31.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017854232002007316		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.017854232002007316 | validation: 0.019914946072949843]
	TIME [epoch: 31.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017567507142353285		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.017567507142353285 | validation: 0.02135600428427702]
	TIME [epoch: 31.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01673747442501487		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.01673747442501487 | validation: 0.017599394114536777]
	TIME [epoch: 31.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018248918998460525		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.018248918998460525 | validation: 0.01941865388774422]
	TIME [epoch: 31.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016311758681200182		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.016311758681200182 | validation: 0.029312855944370587]
	TIME [epoch: 31.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022801754501692285		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.022801754501692285 | validation: 0.020183370588094354]
	TIME [epoch: 31.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018228315037486273		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.018228315037486273 | validation: 0.021347905604753137]
	TIME [epoch: 31.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01807121288726868		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.01807121288726868 | validation: 0.01910504069082506]
	TIME [epoch: 31.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015471805450136367		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.015471805450136367 | validation: 0.02230334364821923]
	TIME [epoch: 31.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017543931113292777		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.017543931113292777 | validation: 0.02028631338814184]
	TIME [epoch: 31.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01666762455686561		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.01666762455686561 | validation: 0.018601347748483096]
	TIME [epoch: 31.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015556150531216551		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.015556150531216551 | validation: 0.021559374765752606]
	TIME [epoch: 31.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01561525631062571		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.01561525631062571 | validation: 0.021369130375770623]
	TIME [epoch: 31.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01625594121232618		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.01625594121232618 | validation: 0.020883304093661015]
	TIME [epoch: 31.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01513823584525723		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.01513823584525723 | validation: 0.019537136778451616]
	TIME [epoch: 31.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01728495797421204		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.01728495797421204 | validation: 0.018777020840520825]
	TIME [epoch: 31.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01618088853953507		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.01618088853953507 | validation: 0.020405815749469286]
	TIME [epoch: 31.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017107533071585884		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.017107533071585884 | validation: 0.018327577907937095]
	TIME [epoch: 31.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01637559452710629		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.01637559452710629 | validation: 0.017638932013968583]
	TIME [epoch: 31.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01676282492197871		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.01676282492197871 | validation: 0.017881498482024444]
	TIME [epoch: 31.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019969363451196626		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.019969363451196626 | validation: 0.02011090114772699]
	TIME [epoch: 31.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01811195335128829		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.01811195335128829 | validation: 0.019150577591632363]
	TIME [epoch: 31.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019633652162727153		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.019633652162727153 | validation: 0.020199639342052534]
	TIME [epoch: 31.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02379581271566509		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.02379581271566509 | validation: 0.034766789256514635]
	TIME [epoch: 31.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02627422674485433		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.02627422674485433 | validation: 0.02222634702102319]
	TIME [epoch: 31.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019889093113389694		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.019889093113389694 | validation: 0.020072268663769724]
	TIME [epoch: 31.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017301179737363247		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.017301179737363247 | validation: 0.02077184409303627]
	TIME [epoch: 31.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02088866245669173		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.02088866245669173 | validation: 0.018652537452352232]
	TIME [epoch: 31.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01790421091064669		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.01790421091064669 | validation: 0.016983415356197556]
	TIME [epoch: 31.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015745802361266804		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.015745802361266804 | validation: 0.01904176090349329]
	TIME [epoch: 31.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020456963227390518		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.020456963227390518 | validation: 0.021472368039229223]
	TIME [epoch: 31.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01771531588827942		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.01771531588827942 | validation: 0.017789487835629083]
	TIME [epoch: 31.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01563201925193845		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.01563201925193845 | validation: 0.01832010273327959]
	TIME [epoch: 31.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015621321838506605		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.015621321838506605 | validation: 0.022312928850265663]
	TIME [epoch: 31.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017351027795315464		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.017351027795315464 | validation: 0.01929213231705216]
	TIME [epoch: 31.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015932918677371042		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.015932918677371042 | validation: 0.01922552462146995]
	TIME [epoch: 31.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01785495121097774		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.01785495121097774 | validation: 0.022858759182053885]
	TIME [epoch: 31.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01810337829781663		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.01810337829781663 | validation: 0.018755339006382057]
	TIME [epoch: 31.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01694438050369014		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.01694438050369014 | validation: 0.01926242745356853]
	TIME [epoch: 31.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01826516021635694		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.01826516021635694 | validation: 0.014778065042076361]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014405751365209215		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.014405751365209215 | validation: 0.018859998526754948]
	TIME [epoch: 31.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016649730398254237		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.016649730398254237 | validation: 0.018743689738885554]
	TIME [epoch: 31.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01661139617734547		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.01661139617734547 | validation: 0.018275897444883237]
	TIME [epoch: 31.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01703375197998164		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.01703375197998164 | validation: 0.02054308119759764]
	TIME [epoch: 31.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01890743271884635		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.01890743271884635 | validation: 0.01891665774137239]
	TIME [epoch: 31.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015934256769404954		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.015934256769404954 | validation: 0.01759020733849854]
	TIME [epoch: 31.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0154384073727159		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.0154384073727159 | validation: 0.021237768323999852]
	TIME [epoch: 31.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016316777860557276		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.016316777860557276 | validation: 0.02228830656494182]
	TIME [epoch: 31.4 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0164442586251671		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.0164442586251671 | validation: 0.02112688832162554]
	TIME [epoch: 31.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014015798646051323		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.014015798646051323 | validation: 0.01846787345799048]
	TIME [epoch: 31.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017263125263940388		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.017263125263940388 | validation: 0.017145221649829734]
	TIME [epoch: 31.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015794282563900818		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.015794282563900818 | validation: 0.018879840585826254]
	TIME [epoch: 31.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01793965249319147		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.01793965249319147 | validation: 0.016749336221035565]
	TIME [epoch: 31.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014862733985965484		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.014862733985965484 | validation: 0.021035777467290375]
	TIME [epoch: 31.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0169668064846398		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.0169668064846398 | validation: 0.021623167343320777]
	TIME [epoch: 31.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017197124872800177		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.017197124872800177 | validation: 0.02117480096637265]
	TIME [epoch: 31.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0168839764098138		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.0168839764098138 | validation: 0.021070831287734466]
	TIME [epoch: 31.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01655031939652042		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.01655031939652042 | validation: 0.020330283470877803]
	TIME [epoch: 31.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01593468512282036		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.01593468512282036 | validation: 0.017777231378656746]
	TIME [epoch: 31.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01730664939388641		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.01730664939388641 | validation: 0.020532887673403374]
	TIME [epoch: 31.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016071347806465866		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.016071347806465866 | validation: 0.020232896687543217]
	TIME [epoch: 31.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01597623256509976		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.01597623256509976 | validation: 0.020917119453529212]
	TIME [epoch: 31.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016876940422442983		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.016876940422442983 | validation: 0.016100271043252325]
	TIME [epoch: 31.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016157628328854257		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.016157628328854257 | validation: 0.019920976799270358]
	TIME [epoch: 31.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01664861375022814		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.01664861375022814 | validation: 0.019995007113908093]
	TIME [epoch: 31.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016530569259928282		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.016530569259928282 | validation: 0.017265015264193325]
	TIME [epoch: 31.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015432878617871468		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.015432878617871468 | validation: 0.018291409335104036]
	TIME [epoch: 31.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016166951716423406		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.016166951716423406 | validation: 0.018318959534254133]
	TIME [epoch: 31.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015240090459164532		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.015240090459164532 | validation: 0.02034925393292847]
	TIME [epoch: 31.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018935300351991882		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.018935300351991882 | validation: 0.022262777393749624]
	TIME [epoch: 31.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01616923306441319		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.01616923306441319 | validation: 0.018061619974568906]
	TIME [epoch: 31.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01667213855391643		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.01667213855391643 | validation: 0.019033490252503282]
	TIME [epoch: 31.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01568842565559038		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.01568842565559038 | validation: 0.01593036717577655]
	TIME [epoch: 31.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01770274140918983		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.01770274140918983 | validation: 0.01971538288578035]
	TIME [epoch: 31.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015666196498189122		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.015666196498189122 | validation: 0.019795546446259735]
	TIME [epoch: 31.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015932783560109576		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.015932783560109576 | validation: 0.022236065158254353]
	TIME [epoch: 31.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015887918848853495		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.015887918848853495 | validation: 0.017393784550153373]
	TIME [epoch: 31.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01696183546194803		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.01696183546194803 | validation: 0.018678359646487813]
	TIME [epoch: 31.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01618285699543488		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.01618285699543488 | validation: 0.020176455607017776]
	TIME [epoch: 31.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01710226375852429		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.01710226375852429 | validation: 0.020514151091683176]
	TIME [epoch: 31.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016263218918251873		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.016263218918251873 | validation: 0.019671307598773415]
	TIME [epoch: 31.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0159265874377307		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.0159265874377307 | validation: 0.02089879879715454]
	TIME [epoch: 31.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016921420469653885		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.016921420469653885 | validation: 0.017186959639013923]
	TIME [epoch: 31.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015293080264469733		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.015293080264469733 | validation: 0.017528165858028872]
	TIME [epoch: 31.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015687671699067828		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.015687671699067828 | validation: 0.019479856210273195]
	TIME [epoch: 31.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01743408680127638		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.01743408680127638 | validation: 0.017096733048104912]
	TIME [epoch: 31.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015712509297093568		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.015712509297093568 | validation: 0.01810728363942355]
	TIME [epoch: 31.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015549168430312912		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.015549168430312912 | validation: 0.018159847258745112]
	TIME [epoch: 31.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015471266969224625		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.015471266969224625 | validation: 0.017916576836800726]
	TIME [epoch: 31.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01641707358725341		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.01641707358725341 | validation: 0.018091793376562488]
	TIME [epoch: 31.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015337740330647004		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.015337740330647004 | validation: 0.01734735018952351]
	TIME [epoch: 31.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015898602599597376		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.015898602599597376 | validation: 0.021419383929449712]
	TIME [epoch: 31.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014581974423024967		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.014581974423024967 | validation: 0.02286314175607423]
	TIME [epoch: 31.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016796904317441252		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.016796904317441252 | validation: 0.017655589403199175]
	TIME [epoch: 31.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015979107263540855		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.015979107263540855 | validation: 0.020317370676042917]
	TIME [epoch: 31.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019515842726713403		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.019515842726713403 | validation: 0.018757699266633766]
	TIME [epoch: 31.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015966630462343933		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.015966630462343933 | validation: 0.01842974583031656]
	TIME [epoch: 31.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017534183637306595		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.017534183637306595 | validation: 0.016390254466471584]
	TIME [epoch: 31.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015372395443500851		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.015372395443500851 | validation: 0.017157448577684198]
	TIME [epoch: 31.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015764914995468948		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.015764914995468948 | validation: 0.017282589571164565]
	TIME [epoch: 31.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016636351096562047		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.016636351096562047 | validation: 0.019101145644585173]
	TIME [epoch: 31.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014404539803180356		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.014404539803180356 | validation: 0.01986088083100957]
	TIME [epoch: 31.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016257863040426097		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.016257863040426097 | validation: 0.017927856966079587]
	TIME [epoch: 31.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01672324415745597		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.01672324415745597 | validation: 0.021605097986194373]
	TIME [epoch: 31.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016135295177477187		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.016135295177477187 | validation: 0.02056235414509225]
	TIME [epoch: 31.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01653125450479179		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.01653125450479179 | validation: 0.020299590677556014]
	TIME [epoch: 31.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016100541808685524		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.016100541808685524 | validation: 0.019441949045090258]
	TIME [epoch: 31.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015496809744002912		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.015496809744002912 | validation: 0.023763923588888444]
	TIME [epoch: 31.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016937730515522168		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.016937730515522168 | validation: 0.017618686769846636]
	TIME [epoch: 31.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015378931329391724		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.015378931329391724 | validation: 0.024364743621635307]
	TIME [epoch: 31.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01573132590201323		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.01573132590201323 | validation: 0.01820791359980823]
	TIME [epoch: 31.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015719878881968158		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.015719878881968158 | validation: 0.01730307558505743]
	TIME [epoch: 31.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0148628187553502		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.0148628187553502 | validation: 0.0190687190720584]
	TIME [epoch: 31.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015786984562858444		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.015786984562858444 | validation: 0.019682210157418535]
	TIME [epoch: 31.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01502353533860994		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.01502353533860994 | validation: 0.019695723382850285]
	TIME [epoch: 31.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01657027009566365		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.01657027009566365 | validation: 0.01935274473924394]
	TIME [epoch: 31.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014193995057211932		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.014193995057211932 | validation: 0.019608220585340293]
	TIME [epoch: 31.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014605262688957097		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.014605262688957097 | validation: 0.018128544391202463]
	TIME [epoch: 31.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01579298011736183		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.01579298011736183 | validation: 0.021696989902921272]
	TIME [epoch: 31.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01639438151877746		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.01639438151877746 | validation: 0.019754990717331684]
	TIME [epoch: 31.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01629794599463681		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.01629794599463681 | validation: 0.02108650968480498]
	TIME [epoch: 31.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01539999371584884		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.01539999371584884 | validation: 0.017431153000038453]
	TIME [epoch: 31.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01666867159568443		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.01666867159568443 | validation: 0.02475063463602773]
	TIME [epoch: 31.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016585937644664622		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.016585937644664622 | validation: 0.01899751957726577]
	TIME [epoch: 31.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01583374774544348		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.01583374774544348 | validation: 0.01665785622091584]
	TIME [epoch: 31.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016358654913652154		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.016358654913652154 | validation: 0.019835290734438245]
	TIME [epoch: 31.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016708252328005636		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.016708252328005636 | validation: 0.019003261765418264]
	TIME [epoch: 31.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015294095891036742		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.015294095891036742 | validation: 0.01901076224617822]
	TIME [epoch: 31.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015111730635534764		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.015111730635534764 | validation: 0.01710302288171587]
	TIME [epoch: 31.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016826581901872025		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.016826581901872025 | validation: 0.018302033465652345]
	TIME [epoch: 31.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016508113134967813		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.016508113134967813 | validation: 0.016216837682791415]
	TIME [epoch: 31.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014600655248070464		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.014600655248070464 | validation: 0.018276196102200503]
	TIME [epoch: 31.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014603421138311257		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.014603421138311257 | validation: 0.01522649669145555]
	TIME [epoch: 31.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017145236272120126		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.017145236272120126 | validation: 0.019032259024910743]
	TIME [epoch: 31.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01509538640577877		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.01509538640577877 | validation: 0.017221259091427944]
	TIME [epoch: 31.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01884110426885835		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.01884110426885835 | validation: 0.023428762850632997]
	TIME [epoch: 31.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01610087343121517		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.01610087343121517 | validation: 0.016808493880685005]
	TIME [epoch: 31.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014871453492871178		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.014871453492871178 | validation: 0.01897031857442388]
	TIME [epoch: 31.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015480014690302957		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.015480014690302957 | validation: 0.019591756195008848]
	TIME [epoch: 31.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014822066437921715		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.014822066437921715 | validation: 0.020162064438387785]
	TIME [epoch: 31.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0159818399247995		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.0159818399247995 | validation: 0.018219409227443414]
	TIME [epoch: 31.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_193755/states/model_phi1_2c_v_mmd1_848.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 17047.522 seconds.
