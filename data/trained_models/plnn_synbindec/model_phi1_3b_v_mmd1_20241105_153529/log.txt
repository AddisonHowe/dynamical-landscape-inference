Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2659071999

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.797318754848127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.797318754848127 | validation: 5.8048041739149205]
	TIME [epoch: 254 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.325063760488307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.325063760488307 | validation: 5.800883454794203]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9114961832105815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9114961832105815 | validation: 5.518136465495285]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.830909958395648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.830909958395648 | validation: 5.346270183828195]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.412203512007911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.412203512007911 | validation: 5.448094337875995]
	TIME [epoch: 1.4 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.436147626347971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.436147626347971 | validation: 5.290349651258992]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.282230036098735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.282230036098735 | validation: 5.2219725135024415]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.232687336398122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.232687336398122 | validation: 5.196879125298672]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.174767660607205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.174767660607205 | validation: 5.140381083024154]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.124783236453191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124783236453191 | validation: 5.125285445512712]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.070491783405373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.070491783405373 | validation: 5.0511833160104365]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.023198895414471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.023198895414471 | validation: 5.051332850552582]
	TIME [epoch: 1.4 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9747330108155263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9747330108155263 | validation: 4.9924829006725]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.937389241470645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.937389241470645 | validation: 5.035021230973985]
	TIME [epoch: 1.4 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.922883678365554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.922883678365554 | validation: 4.9628171837985136]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.996127310753984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.996127310753984 | validation: 4.950658329573716]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8298179681874878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8298179681874878 | validation: 4.889788927277356]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7842228857497395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7842228857497395 | validation: 4.867023266328447]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.744927351726701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.744927351726701 | validation: 4.822147206976357]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.723516660793948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.723516660793948 | validation: 4.849328130746234]
	TIME [epoch: 1.4 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7233865509727457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7233865509727457 | validation: 4.815716534306697]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.761022221507308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.761022221507308 | validation: 4.762866406641955]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6340364708206176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6340364708206176 | validation: 4.730372257138618]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5936161449856847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5936161449856847 | validation: 4.699523716982237]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5725645993570834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5725645993570834 | validation: 4.6820482098018985]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.545854750197161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.545854750197161 | validation: 4.6532385302284025]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5211108917278024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5211108917278024 | validation: 4.62992755487172]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.498705714845828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.498705714845828 | validation: 4.621822885962601]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.487624364826047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.487624364826047 | validation: 4.625753161791962]
	TIME [epoch: 1.39 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5525746014575583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5525746014575583 | validation: 4.760560072094227]
	TIME [epoch: 1.39 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.675809818273146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.675809818273146 | validation: 4.603555367324302]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6031992475880332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6031992475880332 | validation: 4.519644433386653]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4462100849251738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4462100849251738 | validation: 4.63765882820305]
	TIME [epoch: 1.4 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5404654603598136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5404654603598136 | validation: 4.499580307035813]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4367173005062024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4367173005062024 | validation: 4.444906198863893]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.373520263320521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.373520263320521 | validation: 4.48409865260557]
	TIME [epoch: 1.39 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3751153146685886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3751153146685886 | validation: 4.426438776574218]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3367458781572568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3367458781572568 | validation: 4.391040443681409]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.310754656941283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.310754656941283 | validation: 4.351621957785711]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.286736877465196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.286736877465196 | validation: 4.301036656632386]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2696484621793913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2696484621793913 | validation: 4.250844056951442]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.245390660493123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.245390660493123 | validation: 4.121090320359889]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.164982029354377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.164982029354377 | validation: 3.91607773782632]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.981576150333441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.981576150333441 | validation: 3.6958026301630498]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8654679961774594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8654679961774594 | validation: 3.9542159568304514]
	TIME [epoch: 1.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3007855296944126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3007855296944126 | validation: 3.3475734920541864]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6750602199199602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6750602199199602 | validation: 2.803148935723595]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.264339790675937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.264339790675937 | validation: 2.3214369444567895]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9417818153432067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9417818153432067 | validation: 1.9909368683282112]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6706375651229093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6706375651229093 | validation: 1.5996798556260317]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4519195777866873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4519195777866873 | validation: 1.3746489861436648]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1239024749702833		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.1239024749702833 | validation: 1.0949223853357661]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516574354828842		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9516574354828842 | validation: 1.1184306245254063]
	TIME [epoch: 1.4 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9572390168644723		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.9572390168644723 | validation: 1.2002936510752318]
	TIME [epoch: 1.4 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1907262250099984		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.1907262250099984 | validation: 1.2213942300889113]
	TIME [epoch: 1.39 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077581598083653		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.077581598083653 | validation: 0.9409021092293927]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388445750825387		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9388445750825387 | validation: 1.0039109939160067]
	TIME [epoch: 1.39 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452581260402869		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.8452581260402869 | validation: 0.9059568076579491]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813546219985378		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.813546219985378 | validation: 0.8597097411336574]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909565375559775		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.7909565375559775 | validation: 0.9884755984276392]
	TIME [epoch: 1.39 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472033828213458		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.8472033828213458 | validation: 0.8432275522013793]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893150007575941		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.8893150007575941 | validation: 0.8838006608294813]
	TIME [epoch: 1.39 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791278545342885		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.791278545342885 | validation: 0.8073654117686959]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574704395593136		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.7574704395593136 | validation: 0.8025275778139854]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449352413015317		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.7449352413015317 | validation: 0.7998212193795826]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428227217244774		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.7428227217244774 | validation: 0.7779070008244059]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420826498864136		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.7420826498864136 | validation: 0.7775505631148532]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7442236897114469		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.7442236897114469 | validation: 0.7919064994292778]
	TIME [epoch: 1.4 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658766791278415		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7658766791278415 | validation: 0.867384339737916]
	TIME [epoch: 1.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454129285301901		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8454129285301901 | validation: 0.9979836368871917]
	TIME [epoch: 1.39 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757064698765757		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.8757064698765757 | validation: 0.8553355013491185]
	TIME [epoch: 1.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418804943452495		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8418804943452495 | validation: 0.7702641054243249]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424933803486203		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.7424933803486203 | validation: 0.7864040317566019]
	TIME [epoch: 1.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484110848425563		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.7484110848425563 | validation: 0.7512660946620096]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351778159442023		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.7351778159442023 | validation: 0.8100068725318295]
	TIME [epoch: 1.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745805561778566		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.745805561778566 | validation: 0.7789909856968187]
	TIME [epoch: 1.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024724607789699		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8024724607789699 | validation: 1.0175735099877257]
	TIME [epoch: 1.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782748656424303		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8782748656424303 | validation: 0.7878533970907335]
	TIME [epoch: 1.4 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7716913344055712		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.7716913344055712 | validation: 0.7657040427527674]
	TIME [epoch: 1.39 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339281677631481		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7339281677631481 | validation: 0.7796350270936642]
	TIME [epoch: 1.39 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444968229275013		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.7444968229275013 | validation: 0.7725600036004127]
	TIME [epoch: 1.39 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706301196778842		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7706301196778842 | validation: 0.8467706970515881]
	TIME [epoch: 1.39 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706258090848559		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.7706258090848559 | validation: 0.7898742338009402]
	TIME [epoch: 1.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7789539811121615		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.7789539811121615 | validation: 0.8203683214946367]
	TIME [epoch: 1.39 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817827577300043		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7817827577300043 | validation: 0.8670677789966587]
	TIME [epoch: 1.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359225659233722		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8359225659233722 | validation: 0.799443912574353]
	TIME [epoch: 1.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764676977308803		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.764676977308803 | validation: 0.7926755446683453]
	TIME [epoch: 1.39 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467243863890084		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7467243863890084 | validation: 0.7476316145176167]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414730479760323		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.7414730479760323 | validation: 0.7727372562273894]
	TIME [epoch: 1.39 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298142720005604		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7298142720005604 | validation: 0.7755860973795239]
	TIME [epoch: 1.39 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278548337583286		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7278548337583286 | validation: 0.7644446687258345]
	TIME [epoch: 1.39 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374496488811181		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.7374496488811181 | validation: 0.8289414038826323]
	TIME [epoch: 1.39 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535139284962867		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7535139284962867 | validation: 0.7885404656883166]
	TIME [epoch: 1.39 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748549551429247		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.7748549551429247 | validation: 1.0232292600693005]
	TIME [epoch: 1.39 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580190790269921		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8580190790269921 | validation: 0.8025133165535895]
	TIME [epoch: 1.39 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8247507342407308		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8247507342407308 | validation: 0.8239807117788207]
	TIME [epoch: 1.39 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718051635277982		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.7718051635277982 | validation: 0.8072137004408225]
	TIME [epoch: 1.39 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979030365535853		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.7979030365535853 | validation: 0.8486371815809314]
	TIME [epoch: 1.39 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914699315606183		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.7914699315606183 | validation: 0.7656851853867471]
	TIME [epoch: 1.39 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386894166763694		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.7386894166763694 | validation: 0.7515137451078031]
	TIME [epoch: 1.39 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267266456212925		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7267266456212925 | validation: 0.7603596514455966]
	TIME [epoch: 1.39 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7273076317740089		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7273076317740089 | validation: 0.7459036239422664]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250166987813792		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7250166987813792 | validation: 0.7645759892593347]
	TIME [epoch: 1.39 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244740682106325		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7244740682106325 | validation: 0.756065072441261]
	TIME [epoch: 1.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311481321446965		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7311481321446965 | validation: 0.8078583029471693]
	TIME [epoch: 1.39 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633733645121581		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7633733645121581 | validation: 0.8411304350000303]
	TIME [epoch: 1.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856040413128011		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7856040413128011 | validation: 0.8443877327189487]
	TIME [epoch: 1.39 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106215124906864		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8106215124906864 | validation: 0.7783776939723878]
	TIME [epoch: 1.39 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298240060947094		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.7298240060947094 | validation: 0.7733486610871324]
	TIME [epoch: 1.39 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168503047730317		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.7168503047730317 | validation: 0.7627663135527234]
	TIME [epoch: 1.39 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336427572990795		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7336427572990795 | validation: 0.9195491267751819]
	TIME [epoch: 1.39 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960944649275171		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.7960944649275171 | validation: 0.8971491596257274]
	TIME [epoch: 1.39 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9328247442958022		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9328247442958022 | validation: 0.8850371813434781]
	TIME [epoch: 1.39 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754974193193916		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7754974193193916 | validation: 0.7945215557645826]
	TIME [epoch: 1.39 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366184727096408		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7366184727096408 | validation: 0.7689920816620952]
	TIME [epoch: 1.39 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399689030282718		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7399689030282718 | validation: 0.7952080662404103]
	TIME [epoch: 1.39 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218760377615377		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7218760377615377 | validation: 0.771215931178113]
	TIME [epoch: 1.39 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7147410096962304		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.7147410096962304 | validation: 0.7661815580297624]
	TIME [epoch: 1.39 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236189173158482		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7236189173158482 | validation: 0.7754786729237925]
	TIME [epoch: 1.39 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156336172443093		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.7156336172443093 | validation: 0.7654232957896486]
	TIME [epoch: 1.39 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716083209917639		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.716083209917639 | validation: 0.7619031361302463]
	TIME [epoch: 1.39 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091659671678089		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7091659671678089 | validation: 0.7891010116380253]
	TIME [epoch: 1.4 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437471525395362		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7437471525395362 | validation: 0.8223416729994848]
	TIME [epoch: 1.4 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741879260884648		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.7741879260884648 | validation: 0.8749266080371783]
	TIME [epoch: 1.4 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183580673805204		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8183580673805204 | validation: 0.7652723558511594]
	TIME [epoch: 1.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286918145095628		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7286918145095628 | validation: 1.0386481764159863]
	TIME [epoch: 1.4 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593655069674361		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8593655069674361 | validation: 0.7979935733353156]
	TIME [epoch: 1.39 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058482945235039		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8058482945235039 | validation: 0.7687268789124926]
	TIME [epoch: 1.4 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7208901651724796		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.7208901651724796 | validation: 0.8033198114379673]
	TIME [epoch: 1.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476940770614655		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7476940770614655 | validation: 0.7563520338959571]
	TIME [epoch: 1.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243050826982312		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.7243050826982312 | validation: 0.7548928990150675]
	TIME [epoch: 1.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013644900238151		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7013644900238151 | validation: 0.7510819490528418]
	TIME [epoch: 1.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038506337085026		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7038506337085026 | validation: 0.7389182652055424]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922809779387756		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6922809779387756 | validation: 0.7535331971208822]
	TIME [epoch: 1.39 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038004730602154		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.7038004730602154 | validation: 0.8300030650700209]
	TIME [epoch: 1.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348211439651213		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7348211439651213 | validation: 0.8418179507696053]
	TIME [epoch: 1.39 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240505623203245		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8240505623203245 | validation: 0.9001879004615477]
	TIME [epoch: 1.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069274807547901		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8069274807547901 | validation: 0.7295619858567542]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010829920502227		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7010829920502227 | validation: 0.7284251290608335]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6971151872507504		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6971151872507504 | validation: 0.7775847003637288]
	TIME [epoch: 1.4 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082506435178351		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7082506435178351 | validation: 0.7487177032777774]
	TIME [epoch: 1.4 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718916766022072		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.718916766022072 | validation: 0.770804397983357]
	TIME [epoch: 1.4 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963738903600333		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6963738903600333 | validation: 0.744356133427751]
	TIME [epoch: 1.4 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925289553358605		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6925289553358605 | validation: 0.7366208287511964]
	TIME [epoch: 1.4 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842504208202334		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.6842504208202334 | validation: 0.7781669355234746]
	TIME [epoch: 1.4 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991220116786692		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6991220116786692 | validation: 0.74062785543678]
	TIME [epoch: 1.4 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224357280544487		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7224357280544487 | validation: 0.9339690039587119]
	TIME [epoch: 1.4 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093056310608483		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8093056310608483 | validation: 0.7848629192524591]
	TIME [epoch: 1.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740589238445007		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7740589238445007 | validation: 0.7372995222126367]
	TIME [epoch: 1.4 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881352400391657		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6881352400391657 | validation: 0.7277701220077836]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883992488503282		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6883992488503282 | validation: 0.7102856692878359]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695328483267816		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.695328483267816 | validation: 0.7340983295072917]
	TIME [epoch: 1.39 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870528578640629		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6870528578640629 | validation: 0.7099710609073533]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777664677498985		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6777664677498985 | validation: 0.7499295678492672]
	TIME [epoch: 1.4 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034916308040701		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7034916308040701 | validation: 0.7788548425874403]
	TIME [epoch: 1.4 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960189589932694		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6960189589932694 | validation: 0.753362791476811]
	TIME [epoch: 1.4 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371684023296308		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7371684023296308 | validation: 0.8309733953014066]
	TIME [epoch: 1.4 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7137273294568166		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7137273294568166 | validation: 0.863965571895109]
	TIME [epoch: 1.4 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621683798687719		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8621683798687719 | validation: 0.8400278095237872]
	TIME [epoch: 1.39 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371715678764623		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7371715678764623 | validation: 0.699391223379405]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601861207069857		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6601861207069857 | validation: 0.6898371116435418]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568137612806391		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.6568137612806391 | validation: 0.7311942467232927]
	TIME [epoch: 1.39 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692476215064674		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6692476215064674 | validation: 0.683457860758202]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665390308411461		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.6665390308411461 | validation: 0.69778784622219]
	TIME [epoch: 1.39 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505982254888946		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.6505982254888946 | validation: 0.6566948941799325]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637709915665343		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.637709915665343 | validation: 0.7287293025437501]
	TIME [epoch: 1.39 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551552918683724		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6551552918683724 | validation: 0.7097615703218119]
	TIME [epoch: 1.39 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115038288680583		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7115038288680583 | validation: 1.0534165482721372]
	TIME [epoch: 1.4 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853083710697284		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.853083710697284 | validation: 0.7215324051337274]
	TIME [epoch: 1.39 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687266738430815		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7687266738430815 | validation: 0.6711759619353461]
	TIME [epoch: 1.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838537707289538		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.6838537707289538 | validation: 0.7356723243695927]
	TIME [epoch: 1.4 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671054490463934		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.6671054490463934 | validation: 0.648444030886216]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363160248960442		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6363160248960442 | validation: 0.6359967412433915]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405599468409062		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6405599468409062 | validation: 0.6673670998986448]
	TIME [epoch: 1.4 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248965692252094		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6248965692252094 | validation: 0.6378065899382231]
	TIME [epoch: 1.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6101466513069557		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6101466513069557 | validation: 0.6581312903543499]
	TIME [epoch: 1.4 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069683690001251		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6069683690001251 | validation: 0.6208166309441627]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120510807820932		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6120510807820932 | validation: 0.7759674734348692]
	TIME [epoch: 1.4 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6717800000915122		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6717800000915122 | validation: 0.8410669397090929]
	TIME [epoch: 1.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901593774941929		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8901593774941929 | validation: 0.6277296714859709]
	TIME [epoch: 1.39 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395100800714182		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6395100800714182 | validation: 0.7917892300488781]
	TIME [epoch: 1.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875832162392638		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6875832162392638 | validation: 0.6134548514763589]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680730554913552		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6680730554913552 | validation: 0.5798683334654509]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6004993837715763		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6004993837715763 | validation: 0.7156300116931155]
	TIME [epoch: 1.4 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314399293472845		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6314399293472845 | validation: 0.5718470222912151]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223742047468916		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6223742047468916 | validation: 0.5867091002391203]
	TIME [epoch: 1.39 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568750125407295		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.568750125407295 | validation: 0.6035511708124347]
	TIME [epoch: 1.4 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5778363238029565		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.5778363238029565 | validation: 0.5540696317837477]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5656781230750394		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.5656781230750394 | validation: 0.6207075430343049]
	TIME [epoch: 1.39 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726547664626344		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5726547664626344 | validation: 0.5630831583861647]
	TIME [epoch: 1.4 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926426581369288		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5926426581369288 | validation: 0.6793470756442597]
	TIME [epoch: 1.39 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6023483942931123		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6023483942931123 | validation: 0.6201327376618039]
	TIME [epoch: 1.4 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992421664067167		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6992421664067167 | validation: 0.605099215848886]
	TIME [epoch: 1.4 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6093908147062091		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6093908147062091 | validation: 0.5844432444538155]
	TIME [epoch: 1.4 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608904886724768		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.5608904886724768 | validation: 0.5187584671667173]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651725718392179		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.5651725718392179 | validation: 0.6006340121026872]
	TIME [epoch: 1.39 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.556868867385913		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.556868867385913 | validation: 0.5209470215622461]
	TIME [epoch: 1.39 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5697576979415218		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.5697576979415218 | validation: 0.611405220787318]
	TIME [epoch: 1.39 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5602879787733103		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.5602879787733103 | validation: 0.5138930900096494]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5618893744670002		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5618893744670002 | validation: 0.5178253701013397]
	TIME [epoch: 1.4 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5273687011860185		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.5273687011860185 | validation: 0.4809676664145368]
	TIME [epoch: 265 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504675384640142		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.504675384640142 | validation: 0.4892262474102421]
	TIME [epoch: 2.77 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4993468731409784		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.4993468731409784 | validation: 0.48801319928250947]
	TIME [epoch: 2.76 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49810803558383526		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.49810803558383526 | validation: 0.4645887609511101]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960019285763555		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.4960019285763555 | validation: 0.5395218136252827]
	TIME [epoch: 2.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154195822384814		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5154195822384814 | validation: 0.529869720392726]
	TIME [epoch: 2.76 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5630219093379458		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.5630219093379458 | validation: 0.625483783750294]
	TIME [epoch: 2.77 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567873979839861		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5567873979839861 | validation: 0.4592300354882335]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5235223740354951		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.5235223740354951 | validation: 0.4670360142643567]
	TIME [epoch: 2.77 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49053375760802953		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.49053375760802953 | validation: 0.4680385920152221]
	TIME [epoch: 2.76 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4758879147263103		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.4758879147263103 | validation: 0.43156499848694807]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4520830574756806		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4520830574756806 | validation: 0.4313511426279698]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4533565217742971		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.4533565217742971 | validation: 0.417841765997753]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44836544668739026		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.44836544668739026 | validation: 0.4307322258272717]
	TIME [epoch: 2.76 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44683268075952476		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.44683268075952476 | validation: 0.40227440982277574]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351004553221719		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.4351004553221719 | validation: 0.404924723618848]
	TIME [epoch: 2.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41849001076733133		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.41849001076733133 | validation: 0.39917181540602154]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41815583908810733		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.41815583908810733 | validation: 0.3964062844085506]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40932667744044404		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.40932667744044404 | validation: 0.36334802920674003]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40838140968005165		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.40838140968005165 | validation: 0.40942053048434945]
	TIME [epoch: 2.76 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40332058814744715		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.40332058814744715 | validation: 0.5041687777975862]
	TIME [epoch: 2.76 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44277928700341834		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.44277928700341834 | validation: 0.6438714384076537]
	TIME [epoch: 2.76 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664848523747091		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5664848523747091 | validation: 0.6666079634895172]
	TIME [epoch: 2.76 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5319232850185209		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.5319232850185209 | validation: 0.4963420953189848]
	TIME [epoch: 2.76 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42721330443340433		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.42721330443340433 | validation: 0.4108384542823442]
	TIME [epoch: 2.76 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4053113792850502		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4053113792850502 | validation: 0.3710482295077583]
	TIME [epoch: 2.76 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37552298232729964		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.37552298232729964 | validation: 0.34890169539575494]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37271559636514057		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.37271559636514057 | validation: 0.36642620543604737]
	TIME [epoch: 2.78 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3641745598714241		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.3641745598714241 | validation: 0.3527237815645963]
	TIME [epoch: 2.77 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35137337260382906		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.35137337260382906 | validation: 0.3531827523455553]
	TIME [epoch: 2.77 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.346546667418395		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.346546667418395 | validation: 0.35562650533262863]
	TIME [epoch: 2.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34573702053313143		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.34573702053313143 | validation: 0.34052565618435354]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392151939422398		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.3392151939422398 | validation: 0.36744855267107446]
	TIME [epoch: 2.76 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33910023698812736		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.33910023698812736 | validation: 0.37085659722079867]
	TIME [epoch: 2.76 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3880807713535427		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.3880807713535427 | validation: 0.42658100805124877]
	TIME [epoch: 2.76 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239232734370829		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.4239232734370829 | validation: 0.3936877908885862]
	TIME [epoch: 2.75 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3573742053858753		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.3573742053858753 | validation: 0.3421141682665265]
	TIME [epoch: 2.76 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018142132486782		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.3018142132486782 | validation: 0.3461991355277808]
	TIME [epoch: 2.77 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31194260010569896		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.31194260010569896 | validation: 0.33003499446989437]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945306211019477		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.2945306211019477 | validation: 0.33762418391914173]
	TIME [epoch: 2.77 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295713469769098		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.295713469769098 | validation: 0.3259496973810154]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3006675441978765		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.3006675441978765 | validation: 0.3819419155602126]
	TIME [epoch: 2.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39294002172113596		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.39294002172113596 | validation: 0.386432760593195]
	TIME [epoch: 2.78 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42687965647824755		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.42687965647824755 | validation: 0.3723785569482134]
	TIME [epoch: 2.78 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31366272397506795		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.31366272397506795 | validation: 0.35669218217436716]
	TIME [epoch: 2.77 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32280339473060077		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.32280339473060077 | validation: 0.31807247956647666]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457484368311691		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.3457484368311691 | validation: 0.43334744477184295]
	TIME [epoch: 2.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33835930767773414		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.33835930767773414 | validation: 0.46572133620106215]
	TIME [epoch: 2.76 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3400119548133352		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.3400119548133352 | validation: 0.3474246687377483]
	TIME [epoch: 2.76 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2868921248200768		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.2868921248200768 | validation: 0.33759123790930473]
	TIME [epoch: 2.76 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.309532321355001		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.309532321355001 | validation: 0.32630787848073894]
	TIME [epoch: 2.76 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2883773921334015		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.2883773921334015 | validation: 0.3344143451388968]
	TIME [epoch: 2.75 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935493696977965		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2935493696977965 | validation: 0.3485547339344149]
	TIME [epoch: 2.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30925324643685337		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.30925324643685337 | validation: 0.3812188782831381]
	TIME [epoch: 2.77 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328812059686058		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.328812059686058 | validation: 0.3536923614518426]
	TIME [epoch: 2.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3118069347781702		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.3118069347781702 | validation: 0.32190850006565763]
	TIME [epoch: 2.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28488354047482717		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.28488354047482717 | validation: 0.3171951317389734]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629747829369658		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.2629747829369658 | validation: 0.3156801142669383]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25064806166367		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.25064806166367 | validation: 0.31829495696399007]
	TIME [epoch: 2.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562635304226372		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.2562635304226372 | validation: 0.29860645334477404]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29228189192737086		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.29228189192737086 | validation: 0.3747078809221941]
	TIME [epoch: 2.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37629923054653136		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.37629923054653136 | validation: 0.2934959386162549]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858920210700034		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.2858920210700034 | validation: 0.3422943405357382]
	TIME [epoch: 2.77 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26144150977449265		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.26144150977449265 | validation: 0.31951802562048703]
	TIME [epoch: 2.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713037352442026		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.2713037352442026 | validation: 0.4144300364066341]
	TIME [epoch: 2.78 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33015089570466055		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.33015089570466055 | validation: 0.39003154907906695]
	TIME [epoch: 2.77 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209663599386609		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.3209663599386609 | validation: 0.31937695617747597]
	TIME [epoch: 2.77 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27206617402291794		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.27206617402291794 | validation: 0.4308312434896219]
	TIME [epoch: 2.77 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3191329717797593		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.3191329717797593 | validation: 0.32018854294326343]
	TIME [epoch: 2.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24815160920001705		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.24815160920001705 | validation: 0.32633810378412664]
	TIME [epoch: 2.78 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24354437597245793		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.24354437597245793 | validation: 0.2989936222730611]
	TIME [epoch: 2.78 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28362169338632887		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.28362169338632887 | validation: 0.3557687613612444]
	TIME [epoch: 2.78 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3261277226473206		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.3261277226473206 | validation: 0.3775298183387003]
	TIME [epoch: 2.78 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3057907590671782		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.3057907590671782 | validation: 0.30244024283740206]
	TIME [epoch: 2.77 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24777155189794514		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.24777155189794514 | validation: 0.35383260378324366]
	TIME [epoch: 2.78 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574064082301347		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.2574064082301347 | validation: 0.28804845767933396]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22654005802353247		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.22654005802353247 | validation: 0.29162194649385526]
	TIME [epoch: 2.77 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22037392424516847		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.22037392424516847 | validation: 0.3054827709481931]
	TIME [epoch: 2.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571181651042921		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.2571181651042921 | validation: 0.36389946697495046]
	TIME [epoch: 2.77 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3394101513815889		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3394101513815889 | validation: 0.36546927373307986]
	TIME [epoch: 2.77 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31068740143679124		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.31068740143679124 | validation: 0.319568218001087]
	TIME [epoch: 2.78 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25542374239401805		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.25542374239401805 | validation: 0.3329176643755587]
	TIME [epoch: 2.78 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26665441475648755		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.26665441475648755 | validation: 0.2997607558569075]
	TIME [epoch: 2.79 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22220657173455935		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.22220657173455935 | validation: 0.2907724500765077]
	TIME [epoch: 2.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21086150984693994		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.21086150984693994 | validation: 0.3188143081924075]
	TIME [epoch: 2.78 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23266238315545137		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.23266238315545137 | validation: 0.3252262898951877]
	TIME [epoch: 2.78 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504058745969849		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.2504058745969849 | validation: 0.34112562577268807]
	TIME [epoch: 2.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3276358522680878		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3276358522680878 | validation: 0.316015935077339]
	TIME [epoch: 2.77 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28560844999360696		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.28560844999360696 | validation: 0.30590620594123513]
	TIME [epoch: 2.79 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22215367903260724		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.22215367903260724 | validation: 0.3230671785989696]
	TIME [epoch: 2.78 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23509341165730482		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.23509341165730482 | validation: 0.3155968515867501]
	TIME [epoch: 2.79 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23862359258072544		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.23862359258072544 | validation: 0.3536229229867334]
	TIME [epoch: 2.78 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27164306687158074		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.27164306687158074 | validation: 0.31460329646065244]
	TIME [epoch: 2.78 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2794903466951701		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.2794903466951701 | validation: 0.3309642018260156]
	TIME [epoch: 2.77 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24127459457878794		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.24127459457878794 | validation: 0.27684782193022245]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20317125908811784		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.20317125908811784 | validation: 0.2992862177884063]
	TIME [epoch: 2.78 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22652851021999784		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.22652851021999784 | validation: 0.3088014858150905]
	TIME [epoch: 2.78 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2637934790239551		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.2637934790239551 | validation: 0.3507853171381557]
	TIME [epoch: 2.77 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277120499128334		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.277120499128334 | validation: 0.3648126834916418]
	TIME [epoch: 2.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26640466189887113		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.26640466189887113 | validation: 0.30161323638601073]
	TIME [epoch: 2.77 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217070162110155		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.217070162110155 | validation: 0.2953448317309251]
	TIME [epoch: 2.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21791550579989236		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.21791550579989236 | validation: 0.352921157066189]
	TIME [epoch: 2.77 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23570410373880202		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.23570410373880202 | validation: 0.30181839272933764]
	TIME [epoch: 2.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20602018162914504		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.20602018162914504 | validation: 0.29276685791908014]
	TIME [epoch: 2.77 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19990547710965595		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.19990547710965595 | validation: 0.29511567216166495]
	TIME [epoch: 2.78 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22165523554836028		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.22165523554836028 | validation: 0.32758081145255513]
	TIME [epoch: 2.78 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29717040035836434		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.29717040035836434 | validation: 0.3403687007261282]
	TIME [epoch: 2.77 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28587513340126913		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.28587513340126913 | validation: 0.29573751167005585]
	TIME [epoch: 2.77 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2291075889047908		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.2291075889047908 | validation: 0.2939892436298758]
	TIME [epoch: 2.78 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.213719550001503		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.213719550001503 | validation: 0.2883331903422856]
	TIME [epoch: 2.77 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21919581810622368		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.21919581810622368 | validation: 0.30091707118194]
	TIME [epoch: 2.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2133826483935756		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2133826483935756 | validation: 0.3064001048186765]
	TIME [epoch: 2.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089441204746165		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.2089441204746165 | validation: 0.29635820903380455]
	TIME [epoch: 2.78 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20932498051146084		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.20932498051146084 | validation: 0.2791771302081713]
	TIME [epoch: 2.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923481963765213		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.1923481963765213 | validation: 0.277330126089269]
	TIME [epoch: 2.78 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19612428574610374		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.19612428574610374 | validation: 0.28092221920303284]
	TIME [epoch: 2.77 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20359491823907708		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.20359491823907708 | validation: 0.30218237381081225]
	TIME [epoch: 2.78 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26445734826720413		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.26445734826720413 | validation: 0.3188512694845496]
	TIME [epoch: 2.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3216218959642395		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.3216218959642395 | validation: 0.3337358744832257]
	TIME [epoch: 2.78 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2267113795047325		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2267113795047325 | validation: 0.2909578489767044]
	TIME [epoch: 2.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2092623135745461		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.2092623135745461 | validation: 0.26124012734715146]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1915785320611417		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.1915785320611417 | validation: 0.28621383139667145]
	TIME [epoch: 2.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20078736473275993		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.20078736473275993 | validation: 0.3232814713353881]
	TIME [epoch: 2.77 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376845107962997		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.2376845107962997 | validation: 0.2844141329479099]
	TIME [epoch: 2.77 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1971805890373136		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.1971805890373136 | validation: 0.34037410490498565]
	TIME [epoch: 2.77 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22405810606611465		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.22405810606611465 | validation: 0.3129419229651223]
	TIME [epoch: 2.78 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21610977120735342		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.21610977120735342 | validation: 0.27990606972427273]
	TIME [epoch: 2.77 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20727859338211482		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.20727859338211482 | validation: 0.2860126724875051]
	TIME [epoch: 2.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28561414807113455		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.28561414807113455 | validation: 0.30723500574197044]
	TIME [epoch: 2.77 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2422770927230182		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.2422770927230182 | validation: 0.30394857073685105]
	TIME [epoch: 2.77 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23104123940959884		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.23104123940959884 | validation: 0.2993215423151988]
	TIME [epoch: 2.78 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21212978660959778		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.21212978660959778 | validation: 0.2603206037357067]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17637807558447724		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.17637807558447724 | validation: 0.2949576641855007]
	TIME [epoch: 2.77 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19665377815114946		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.19665377815114946 | validation: 0.2561591988442076]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17158369641146126		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.17158369641146126 | validation: 0.2603266733279223]
	TIME [epoch: 2.77 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1865449548376087		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.1865449548376087 | validation: 0.28437746026421734]
	TIME [epoch: 2.77 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566310141299767		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.2566310141299767 | validation: 0.32597073448694025]
	TIME [epoch: 2.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3270764663159925		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3270764663159925 | validation: 0.322097091645239]
	TIME [epoch: 2.77 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21208611193264795		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.21208611193264795 | validation: 0.2452876473431104]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16905195360941605		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.16905195360941605 | validation: 0.2848177628360371]
	TIME [epoch: 2.75 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18153508444538888		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.18153508444538888 | validation: 0.2796735890518745]
	TIME [epoch: 2.76 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19445752075207892		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.19445752075207892 | validation: 0.2708800779043144]
	TIME [epoch: 2.77 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21569629551851408		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.21569629551851408 | validation: 0.2989820777085582]
	TIME [epoch: 2.76 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26553926956878976		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.26553926956878976 | validation: 0.27412752329895124]
	TIME [epoch: 2.77 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165762376544447		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2165762376544447 | validation: 0.28551068449410455]
	TIME [epoch: 2.75 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18613180192849554		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.18613180192849554 | validation: 0.2616484365342901]
	TIME [epoch: 2.77 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17465128708596056		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.17465128708596056 | validation: 0.2652178173330721]
	TIME [epoch: 2.76 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16566580905240827		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.16566580905240827 | validation: 0.2528774224629369]
	TIME [epoch: 2.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699449078742207		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1699449078742207 | validation: 0.2818642325879269]
	TIME [epoch: 2.77 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20333103811590073		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20333103811590073 | validation: 0.2652184534133777]
	TIME [epoch: 2.77 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21018867643375333		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.21018867643375333 | validation: 0.3113017308974585]
	TIME [epoch: 2.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173459401631664		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2173459401631664 | validation: 0.28644685137368286]
	TIME [epoch: 2.76 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114858920733417		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.2114858920733417 | validation: 0.2856245561814272]
	TIME [epoch: 2.76 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19584133284125052		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.19584133284125052 | validation: 0.29615559597080054]
	TIME [epoch: 2.77 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21941790233447298		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.21941790233447298 | validation: 0.3122827772162862]
	TIME [epoch: 2.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19427801963612873		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.19427801963612873 | validation: 0.2555436491623211]
	TIME [epoch: 2.77 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15970497878703266		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.15970497878703266 | validation: 0.25084182817957584]
	TIME [epoch: 2.75 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17291021028170547		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.17291021028170547 | validation: 0.26637327436052305]
	TIME [epoch: 2.76 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688276106294887		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.1688276106294887 | validation: 0.2525200036101494]
	TIME [epoch: 2.76 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15650871596406046		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.15650871596406046 | validation: 0.24828472289162873]
	TIME [epoch: 2.77 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16587177959804258		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.16587177959804258 | validation: 0.2538072151628214]
	TIME [epoch: 2.76 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800293918551667		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.1800293918551667 | validation: 0.2895237787472643]
	TIME [epoch: 2.76 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26129254728106416		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.26129254728106416 | validation: 0.2749844724950643]
	TIME [epoch: 2.76 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23365522760998403		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.23365522760998403 | validation: 0.26813586251481497]
	TIME [epoch: 2.77 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18042879183522967		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.18042879183522967 | validation: 0.27754388199248836]
	TIME [epoch: 2.76 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18895621600871304		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.18895621600871304 | validation: 0.3710655449815887]
	TIME [epoch: 2.75 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2597452356062277		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.2597452356062277 | validation: 0.29040040186235644]
	TIME [epoch: 2.76 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18933397097827606		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.18933397097827606 | validation: 0.27749635080347973]
	TIME [epoch: 2.75 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19590451637223077		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.19590451637223077 | validation: 0.2850578921991686]
	TIME [epoch: 2.75 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18915809848438664		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.18915809848438664 | validation: 0.2336468719164529]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18342891568229056		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.18342891568229056 | validation: 0.34675817015413063]
	TIME [epoch: 2.77 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2333075982744861		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.2333075982744861 | validation: 0.26123590998889135]
	TIME [epoch: 2.77 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16704394955226673		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.16704394955226673 | validation: 0.2438274608630891]
	TIME [epoch: 2.77 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16014619660874752		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.16014619660874752 | validation: 0.2780276685544493]
	TIME [epoch: 2.78 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18406475525236132		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.18406475525236132 | validation: 0.25420364050390676]
	TIME [epoch: 2.77 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2095634041001035		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2095634041001035 | validation: 0.31504032916941127]
	TIME [epoch: 2.78 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23315570958087065		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.23315570958087065 | validation: 0.2858980692665211]
	TIME [epoch: 2.78 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894380511941214		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1894380511941214 | validation: 0.26524910387708206]
	TIME [epoch: 2.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15998118241384174		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.15998118241384174 | validation: 0.23557688171769478]
	TIME [epoch: 2.78 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14928533412008704		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.14928533412008704 | validation: 0.25688796735853753]
	TIME [epoch: 2.78 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17505609980911127		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.17505609980911127 | validation: 0.2640178278095082]
	TIME [epoch: 2.78 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19253329833047614		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.19253329833047614 | validation: 0.25508471597643306]
	TIME [epoch: 2.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22529745799191858		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.22529745799191858 | validation: 0.2588358386469355]
	TIME [epoch: 2.78 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19638869288377214		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.19638869288377214 | validation: 0.25627417485739673]
	TIME [epoch: 2.77 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18788595597385294		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.18788595597385294 | validation: 0.2514894696986019]
	TIME [epoch: 2.77 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500553590678196		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1500553590678196 | validation: 0.2526002791215089]
	TIME [epoch: 2.77 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15156551961166073		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.15156551961166073 | validation: 0.24857489395399207]
	TIME [epoch: 2.77 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15161077811545892		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.15161077811545892 | validation: 0.24332618618138202]
	TIME [epoch: 2.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14427564741177665		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.14427564741177665 | validation: 0.24175319386285388]
	TIME [epoch: 2.76 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1393527724595384		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.1393527724595384 | validation: 0.2516672133491425]
	TIME [epoch: 2.77 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16199983076439728		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.16199983076439728 | validation: 0.5088933179764455]
	TIME [epoch: 2.76 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39308742480560954		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.39308742480560954 | validation: 0.265403730032655]
	TIME [epoch: 2.76 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17025761799297334		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.17025761799297334 | validation: 0.2846004136652939]
	TIME [epoch: 2.77 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28475591083438895		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.28475591083438895 | validation: 0.2620136873941126]
	TIME [epoch: 2.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16116380540850234		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.16116380540850234 | validation: 0.2733578083421299]
	TIME [epoch: 2.77 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20393919445360303		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.20393919445360303 | validation: 0.25220496724780767]
	TIME [epoch: 2.77 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16892306758584197		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.16892306758584197 | validation: 0.22813682948237748]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14541971448428315		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.14541971448428315 | validation: 0.23743459067376388]
	TIME [epoch: 2.77 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740349238810615		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.1740349238810615 | validation: 0.24495224910444532]
	TIME [epoch: 2.77 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15802226167419314		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.15802226167419314 | validation: 0.2531493527649068]
	TIME [epoch: 2.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15084487040293154		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.15084487040293154 | validation: 0.25902532798864114]
	TIME [epoch: 2.76 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17577743448135688		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.17577743448135688 | validation: 0.23084435415929538]
	TIME [epoch: 2.77 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14387170103426467		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.14387170103426467 | validation: 0.22787982131680776]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14380835542363238		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.14380835542363238 | validation: 0.2657772742707285]
	TIME [epoch: 2.77 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16801634972673385		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.16801634972673385 | validation: 0.24717282729953102]
	TIME [epoch: 2.77 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17223273521776641		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.17223273521776641 | validation: 0.2360355411416034]
	TIME [epoch: 2.77 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13749147746698953		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.13749147746698953 | validation: 0.22872631637495677]
	TIME [epoch: 2.77 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13134372184123635		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.13134372184123635 | validation: 0.21986578838557788]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417117364334746		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.13417117364334746 | validation: 0.23549180269884717]
	TIME [epoch: 2.75 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15819758885548307		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.15819758885548307 | validation: 0.23514390479005365]
	TIME [epoch: 2.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14116189307974567		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.14116189307974567 | validation: 0.25439404383481257]
	TIME [epoch: 2.75 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22983482297000607		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.22983482297000607 | validation: 0.3164256854411936]
	TIME [epoch: 2.75 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586950137752561		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2586950137752561 | validation: 0.23702718234464148]
	TIME [epoch: 2.76 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17193567720868594		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.17193567720868594 | validation: 0.2641168439300142]
	TIME [epoch: 2.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18499371782841217		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.18499371782841217 | validation: 0.25500876556208507]
	TIME [epoch: 2.76 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17775257093176272		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.17775257093176272 | validation: 0.2568934949130847]
	TIME [epoch: 2.76 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19435402374392474		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.19435402374392474 | validation: 0.2874340618119587]
	TIME [epoch: 2.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1966387918017011		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.1966387918017011 | validation: 0.25496497861377904]
	TIME [epoch: 2.76 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677781236470869		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.1677781236470869 | validation: 0.22287310155355483]
	TIME [epoch: 2.77 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15354169779517357		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.15354169779517357 | validation: 0.21154758178288302]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14293975122143057		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.14293975122143057 | validation: 0.24400128061570725]
	TIME [epoch: 2.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629854857325402		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.1629854857325402 | validation: 0.2417329238246985]
	TIME [epoch: 2.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15815338687664623		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.15815338687664623 | validation: 0.23081045126094735]
	TIME [epoch: 2.75 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449407065212136		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.14449407065212136 | validation: 0.2165528470709751]
	TIME [epoch: 2.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389594222719249		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.1389594222719249 | validation: 0.22310338364169102]
	TIME [epoch: 2.76 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13672875418141112		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.13672875418141112 | validation: 0.22869295872806747]
	TIME [epoch: 2.76 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896697694795907		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.14896697694795907 | validation: 0.22747166307579533]
	TIME [epoch: 2.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13944896395110928		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.13944896395110928 | validation: 0.228418705855957]
	TIME [epoch: 2.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426634937737795		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1426634937737795 | validation: 0.22504817857715645]
	TIME [epoch: 2.76 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13633901691654923		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.13633901691654923 | validation: 0.23260493654888106]
	TIME [epoch: 2.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14545300138466996		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.14545300138466996 | validation: 0.229490617994946]
	TIME [epoch: 2.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16987354848278485		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.16987354848278485 | validation: 0.2617459127852482]
	TIME [epoch: 2.75 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768907665259281		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.1768907665259281 | validation: 0.23214415335756355]
	TIME [epoch: 2.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247954419921545		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1247954419921545 | validation: 0.2146174599662246]
	TIME [epoch: 2.75 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1352067393874883		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.1352067393874883 | validation: 0.24006530546681595]
	TIME [epoch: 2.76 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14865174406659554		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.14865174406659554 | validation: 0.22016745202107701]
	TIME [epoch: 2.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13125147256208144		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.13125147256208144 | validation: 0.23595785897688085]
	TIME [epoch: 2.75 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14355386077454876		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.14355386077454876 | validation: 0.24582025267757557]
	TIME [epoch: 2.76 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844633730737541		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.1844633730737541 | validation: 0.3013640984143428]
	TIME [epoch: 2.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20904796987929858		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.20904796987929858 | validation: 0.23561552849641623]
	TIME [epoch: 2.76 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150828173770881		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.150828173770881 | validation: 0.20591601022038308]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296434436305479		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.1296434436305479 | validation: 0.22130632869497618]
	TIME [epoch: 2.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13963207044735884		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.13963207044735884 | validation: 0.19152577477382923]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13655838691330202		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.13655838691330202 | validation: 0.21265840229311894]
	TIME [epoch: 2.76 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12006067819692748		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.12006067819692748 | validation: 0.2030171294989292]
	TIME [epoch: 2.76 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11777783819236178		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.11777783819236178 | validation: 0.2156918251215883]
	TIME [epoch: 2.76 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11735173085122412		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.11735173085122412 | validation: 0.20162864980780784]
	TIME [epoch: 2.75 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12631197320492057		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.12631197320492057 | validation: 0.24614970326764657]
	TIME [epoch: 2.76 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16880115460666845		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.16880115460666845 | validation: 0.23007767955620514]
	TIME [epoch: 2.76 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21357156704835717		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.21357156704835717 | validation: 0.2553982181541947]
	TIME [epoch: 2.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696953963323733		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1696953963323733 | validation: 0.2898549727716422]
	TIME [epoch: 2.75 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1884633372457878		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.1884633372457878 | validation: 0.2262317005077147]
	TIME [epoch: 2.76 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15321187831909		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.15321187831909 | validation: 0.21532520009330358]
	TIME [epoch: 2.76 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12137322946376548		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.12137322946376548 | validation: 0.20519380056666048]
	TIME [epoch: 2.76 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11618215676725631		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.11618215676725631 | validation: 0.21497057349865464]
	TIME [epoch: 2.76 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11879820508720541		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.11879820508720541 | validation: 0.20803292112891697]
	TIME [epoch: 2.76 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11512352484879905		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.11512352484879905 | validation: 0.20522479596462573]
	TIME [epoch: 2.75 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12305536899695747		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.12305536899695747 | validation: 0.20548425017896144]
	TIME [epoch: 2.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060032321179822		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.11060032321179822 | validation: 0.2116521690790183]
	TIME [epoch: 2.76 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12258922870546997		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.12258922870546997 | validation: 0.20838693599812044]
	TIME [epoch: 2.75 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13355731890708264		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.13355731890708264 | validation: 0.2516967527627088]
	TIME [epoch: 2.75 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15978439389935234		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.15978439389935234 | validation: 0.22463154660846213]
	TIME [epoch: 2.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1938315101635498		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1938315101635498 | validation: 0.2371728738481902]
	TIME [epoch: 2.76 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14897407149691058		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.14897407149691058 | validation: 0.21372927177742113]
	TIME [epoch: 2.75 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11413290419651405		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.11413290419651405 | validation: 0.526029147131475]
	TIME [epoch: 2.75 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5996190340856339		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5996190340856339 | validation: 0.47998463093086396]
	TIME [epoch: 2.76 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49665996379230715		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.49665996379230715 | validation: 0.2760064390520784]
	TIME [epoch: 2.76 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18775431704948883		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.18775431704948883 | validation: 0.1975253632648825]
	TIME [epoch: 2.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135679961344664		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.135679961344664 | validation: 0.1918795511844006]
	TIME [epoch: 2.76 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13362465907993742		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.13362465907993742 | validation: 0.20199455695654836]
	TIME [epoch: 2.75 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1218395987117478		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.1218395987117478 | validation: 0.18677952252585725]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626974129748142		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.12626974129748142 | validation: 0.18113499018525692]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11915226353084822		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.11915226353084822 | validation: 0.20565064803812252]
	TIME [epoch: 2.75 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13113535008138075		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.13113535008138075 | validation: 0.20074816557297248]
	TIME [epoch: 2.75 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12325110903533823		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.12325110903533823 | validation: 0.2043014430141911]
	TIME [epoch: 2.76 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1224678083047144		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.1224678083047144 | validation: 0.20975116076754882]
	TIME [epoch: 2.76 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16356496646799454		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.16356496646799454 | validation: 0.21200339539098514]
	TIME [epoch: 2.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13325920801842536		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.13325920801842536 | validation: 0.19100912240160428]
	TIME [epoch: 2.75 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153228078417735		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.1153228078417735 | validation: 0.18885977881125093]
	TIME [epoch: 2.75 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245089407093577		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.11245089407093577 | validation: 0.1810852667540932]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212558079489961		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.11212558079489961 | validation: 0.23547697211724228]
	TIME [epoch: 2.75 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15782691209585714		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.15782691209585714 | validation: 0.2085086618731838]
	TIME [epoch: 2.76 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12303859538753821		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.12303859538753821 | validation: 0.19186130698023388]
	TIME [epoch: 2.76 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11514431078233507		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.11514431078233507 | validation: 0.1914730920813648]
	TIME [epoch: 2.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362917572614851		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.12362917572614851 | validation: 0.18342805233084053]
	TIME [epoch: 2.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11037674228105246		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.11037674228105246 | validation: 0.17590605408116164]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10352494153820346		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.10352494153820346 | validation: 0.18521900593096485]
	TIME [epoch: 2.75 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10137294828750922		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.10137294828750922 | validation: 0.17064787980866702]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10648915949552022		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.10648915949552022 | validation: 0.17440960308224215]
	TIME [epoch: 2.75 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10233267742176057		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.10233267742176057 | validation: 0.186904342409753]
	TIME [epoch: 2.75 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10571715650351245		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.10571715650351245 | validation: 0.23110333038547398]
	TIME [epoch: 2.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15824265343870178		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.15824265343870178 | validation: 0.25570851186454063]
	TIME [epoch: 2.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34391316539142963		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.34391316539142963 | validation: 0.19270946039980294]
	TIME [epoch: 2.75 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487551032295584		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1487551032295584 | validation: 0.2925001811663827]
	TIME [epoch: 2.75 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24403547648056428		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.24403547648056428 | validation: 0.21276208871954183]
	TIME [epoch: 2.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12487253954775483		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.12487253954775483 | validation: 0.2249135979160514]
	TIME [epoch: 2.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15485939927184575		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.15485939927184575 | validation: 0.20631988652968172]
	TIME [epoch: 2.75 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373200868354814		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.11373200868354814 | validation: 0.19932605680965515]
	TIME [epoch: 2.75 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11403204746724338		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.11403204746724338 | validation: 0.22039670548573273]
	TIME [epoch: 2.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382695361312446		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1382695361312446 | validation: 0.19400116139058834]
	TIME [epoch: 2.75 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10689681355147583		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.10689681355147583 | validation: 0.18305591210495853]
	TIME [epoch: 270 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224888430846807		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.10224888430846807 | validation: 0.19292020839160315]
	TIME [epoch: 5.95 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246224647434513		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.11246224647434513 | validation: 0.1901632770427126]
	TIME [epoch: 5.93 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387160900301774		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.10387160900301774 | validation: 0.17678526944501394]
	TIME [epoch: 5.93 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10463391493905931		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.10463391493905931 | validation: 0.19539006525361535]
	TIME [epoch: 5.94 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10658423219256997		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.10658423219256997 | validation: 0.19533572090723164]
	TIME [epoch: 5.94 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14153521046182654		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.14153521046182654 | validation: 0.25234003290399265]
	TIME [epoch: 5.94 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16193715226990577		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.16193715226990577 | validation: 0.19008017763333762]
	TIME [epoch: 5.94 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10959823927950595		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.10959823927950595 | validation: 0.17905056580527226]
	TIME [epoch: 5.93 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839000868398502		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.09839000868398502 | validation: 0.22899163549217608]
	TIME [epoch: 5.94 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14653988580929692		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.14653988580929692 | validation: 0.19218411055466644]
	TIME [epoch: 5.94 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11757909154029735		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.11757909154029735 | validation: 0.18751454476235663]
	TIME [epoch: 5.94 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558411418163013		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.10558411418163013 | validation: 0.1920696712477148]
	TIME [epoch: 5.94 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810552713968713		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09810552713968713 | validation: 0.19422231239125604]
	TIME [epoch: 5.93 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10771179180376722		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.10771179180376722 | validation: 0.1961358632167076]
	TIME [epoch: 5.94 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13146416144673714		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.13146416144673714 | validation: 0.23432047703554046]
	TIME [epoch: 5.95 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16279736669789097		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.16279736669789097 | validation: 0.5527610836042937]
	TIME [epoch: 5.94 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4022088903424184		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.4022088903424184 | validation: 0.4285849788249582]
	TIME [epoch: 5.96 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29267884275905404		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.29267884275905404 | validation: 0.18333056208589182]
	TIME [epoch: 5.94 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12073573178694305		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.12073573178694305 | validation: 0.21729638502454446]
	TIME [epoch: 5.95 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14171820599073928		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.14171820599073928 | validation: 0.2108378193403834]
	TIME [epoch: 5.94 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12335284628556185		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.12335284628556185 | validation: 0.18558576921775197]
	TIME [epoch: 5.95 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10279568275603852		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.10279568275603852 | validation: 0.17924815026965746]
	TIME [epoch: 5.94 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10173666295695302		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.10173666295695302 | validation: 0.17250158398064236]
	TIME [epoch: 5.95 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10342162639539741		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.10342162639539741 | validation: 0.1658013299918302]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10366071089574912		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.10366071089574912 | validation: 0.1859313259094794]
	TIME [epoch: 5.94 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10432692267393583		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.10432692267393583 | validation: 0.1716887155173369]
	TIME [epoch: 5.94 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10195724831820599		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.10195724831820599 | validation: 0.1743383578007588]
	TIME [epoch: 5.94 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10457965230205965		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.10457965230205965 | validation: 0.1682639475715323]
	TIME [epoch: 5.94 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767024957662235		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.09767024957662235 | validation: 0.170077265394491]
	TIME [epoch: 5.94 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600135219111973		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.09600135219111973 | validation: 0.19127127287836865]
	TIME [epoch: 5.94 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946570763311259		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.09946570763311259 | validation: 0.1661286505304338]
	TIME [epoch: 5.94 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728039429656772		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.09728039429656772 | validation: 0.17166624100960537]
	TIME [epoch: 5.94 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09824195770369466		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.09824195770369466 | validation: 0.18335771510599905]
	TIME [epoch: 5.93 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10477492130579229		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.10477492130579229 | validation: 0.19786331630027512]
	TIME [epoch: 5.94 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12650369516917132		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.12650369516917132 | validation: 0.16311286928594818]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382436097730926		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.1382436097730926 | validation: 0.22531030907412616]
	TIME [epoch: 5.94 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467088272566496		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.1467088272566496 | validation: 0.1797881313770729]
	TIME [epoch: 5.93 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09958036390439187		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.09958036390439187 | validation: 0.1761417304608165]
	TIME [epoch: 5.94 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12057961512607694		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.12057961512607694 | validation: 0.19813289112994684]
	TIME [epoch: 5.94 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309858545422815		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.1309858545422815 | validation: 0.1772609068503641]
	TIME [epoch: 5.93 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.095325194953128		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.095325194953128 | validation: 0.17767041347142487]
	TIME [epoch: 5.93 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1045189746885869		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1045189746885869 | validation: 0.17844780275799976]
	TIME [epoch: 5.93 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11277209191399995		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.11277209191399995 | validation: 0.1810817064527473]
	TIME [epoch: 5.95 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11182363926531139		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.11182363926531139 | validation: 0.1771712913828799]
	TIME [epoch: 5.95 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09765380311395966		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09765380311395966 | validation: 0.1781748881645255]
	TIME [epoch: 5.93 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09012930894064974		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.09012930894064974 | validation: 0.17421061127253715]
	TIME [epoch: 5.94 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0904045906913331		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.0904045906913331 | validation: 0.16789705585756678]
	TIME [epoch: 5.94 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09156111769487454		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.09156111769487454 | validation: 0.18719857340742976]
	TIME [epoch: 5.94 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947598208097011		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.0947598208097011 | validation: 0.18306865301404848]
	TIME [epoch: 5.96 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229675154507282		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.09229675154507282 | validation: 0.18335905652447249]
	TIME [epoch: 5.94 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10533993870396036		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.10533993870396036 | validation: 0.1740089796845374]
	TIME [epoch: 5.95 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12615190643086163		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.12615190643086163 | validation: 0.2235050906411077]
	TIME [epoch: 5.94 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13248449320477726		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.13248449320477726 | validation: 0.17640723632404454]
	TIME [epoch: 5.94 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13898225679624027		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.13898225679624027 | validation: 0.2032784487920771]
	TIME [epoch: 5.94 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11725785496949895		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11725785496949895 | validation: 0.1833960445858298]
	TIME [epoch: 5.94 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283933477321938		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.08283933477321938 | validation: 0.18807527025780263]
	TIME [epoch: 5.94 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11662701834042913		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.11662701834042913 | validation: 0.1923918665302862]
	TIME [epoch: 5.94 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11338571771947813		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.11338571771947813 | validation: 0.18413023524556682]
	TIME [epoch: 5.94 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08927636037199044		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.08927636037199044 | validation: 0.17503262182942816]
	TIME [epoch: 5.95 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08670088377281454		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.08670088377281454 | validation: 0.17036392732007918]
	TIME [epoch: 5.94 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10035119184080955		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.10035119184080955 | validation: 0.2010974885466036]
	TIME [epoch: 5.94 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13054091701548298		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.13054091701548298 | validation: 0.1725854259022412]
	TIME [epoch: 5.94 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09852793868819695		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.09852793868819695 | validation: 0.1632672712252372]
	TIME [epoch: 5.95 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08459693396927882		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.08459693396927882 | validation: 0.16473728208612812]
	TIME [epoch: 5.94 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08235223669610939		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.08235223669610939 | validation: 0.18072580667468344]
	TIME [epoch: 5.95 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10199157637997366		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.10199157637997366 | validation: 0.1712948912363429]
	TIME [epoch: 5.94 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732350598000274		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.08732350598000274 | validation: 0.16971123050002107]
	TIME [epoch: 5.94 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08786567129694302		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.08786567129694302 | validation: 0.16381129397643168]
	TIME [epoch: 5.94 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08705850345805916		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.08705850345805916 | validation: 0.1800942067820946]
	TIME [epoch: 5.95 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691838998969999		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.10691838998969999 | validation: 0.18871052557493329]
	TIME [epoch: 5.94 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11767635816798064		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.11767635816798064 | validation: 0.20368216629669963]
	TIME [epoch: 5.94 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12293164945601058		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.12293164945601058 | validation: 0.18972459209046952]
	TIME [epoch: 5.93 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451934708970354		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.1451934708970354 | validation: 0.19007639950558564]
	TIME [epoch: 5.94 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10171817901741281		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10171817901741281 | validation: 0.17212311198019514]
	TIME [epoch: 5.93 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088988843647151		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.088988843647151 | validation: 0.16883935918486845]
	TIME [epoch: 5.94 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326611427967965		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.09326611427967965 | validation: 0.25238630988123906]
	TIME [epoch: 5.93 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15779762123964186		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.15779762123964186 | validation: 0.16550264242084237]
	TIME [epoch: 5.94 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902947261725228		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.0902947261725228 | validation: 0.18626848825564946]
	TIME [epoch: 5.93 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839818528753028		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.11839818528753028 | validation: 0.17276464235179115]
	TIME [epoch: 5.95 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002594807074607		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.1002594807074607 | validation: 0.15930744320852414]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09435840878914516		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.09435840878914516 | validation: 0.15176139717663198]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08993599999406747		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.08993599999406747 | validation: 0.1743693369893914]
	TIME [epoch: 5.95 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08563965789979835		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.08563965789979835 | validation: 0.16539969206058597]
	TIME [epoch: 5.94 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08262938917383096		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.08262938917383096 | validation: 0.15138277849510762]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08231456436736678		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.08231456436736678 | validation: 0.16242263032040954]
	TIME [epoch: 5.92 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908899548879969		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.07908899548879969 | validation: 0.1737422189241996]
	TIME [epoch: 5.91 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876365351954534		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.08876365351954534 | validation: 0.16389144615594692]
	TIME [epoch: 5.91 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09434780043608088		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.09434780043608088 | validation: 0.18535728095414]
	TIME [epoch: 5.91 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259072923504734		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.11259072923504734 | validation: 0.16783421401683737]
	TIME [epoch: 5.94 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08422325313303737		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.08422325313303737 | validation: 0.1644389587804024]
	TIME [epoch: 5.93 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08300829216048836		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.08300829216048836 | validation: 0.16451885098713884]
	TIME [epoch: 5.94 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11121708969754698		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.11121708969754698 | validation: 0.18089238013960948]
	TIME [epoch: 5.94 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760149269879066		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.10760149269879066 | validation: 0.1572358758255744]
	TIME [epoch: 5.95 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08821145145144624		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.08821145145144624 | validation: 0.1662947971034875]
	TIME [epoch: 5.94 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09973561225305634		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.09973561225305634 | validation: 0.18101704379827155]
	TIME [epoch: 5.94 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10893595976349545		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.10893595976349545 | validation: 0.16713520461895445]
	TIME [epoch: 5.94 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243756928369539		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.08243756928369539 | validation: 0.16298427693322215]
	TIME [epoch: 5.94 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07783126197859101		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.07783126197859101 | validation: 0.15181156168804907]
	TIME [epoch: 5.94 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774295901256045		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0774295901256045 | validation: 0.14062061464264772]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07873468883042993		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.07873468883042993 | validation: 0.17532739724784552]
	TIME [epoch: 5.94 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09784842476471833		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.09784842476471833 | validation: 0.16654136872710967]
	TIME [epoch: 5.95 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316995706597344		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.1316995706597344 | validation: 0.17870633142253078]
	TIME [epoch: 5.94 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10376951113766865		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.10376951113766865 | validation: 0.1533940779582266]
	TIME [epoch: 5.94 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760960891962343		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07760960891962343 | validation: 0.1520625754220243]
	TIME [epoch: 5.94 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468640570651634		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08468640570651634 | validation: 0.16673129339278625]
	TIME [epoch: 5.94 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470932244016208		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.08470932244016208 | validation: 0.16530090620221405]
	TIME [epoch: 5.94 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925664100363824		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.0925664100363824 | validation: 0.15552954115399253]
	TIME [epoch: 5.94 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0778984686881167		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.0778984686881167 | validation: 0.1604467081116956]
	TIME [epoch: 5.93 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040801936625691		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.08040801936625691 | validation: 0.15234141568027934]
	TIME [epoch: 5.94 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07912819030352997		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.07912819030352997 | validation: 0.17105424171922984]
	TIME [epoch: 5.94 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737694911281613		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10737694911281613 | validation: 0.1547697207953169]
	TIME [epoch: 5.94 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0786158427507887		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.0786158427507887 | validation: 0.16266778663879056]
	TIME [epoch: 5.95 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839195314599565		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.09839195314599565 | validation: 0.17078485838719218]
	TIME [epoch: 5.95 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11572380642350372		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.11572380642350372 | validation: 0.1548718435519819]
	TIME [epoch: 5.94 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299878069022265		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08299878069022265 | validation: 0.15956873812983874]
	TIME [epoch: 5.94 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08316903587579492		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.08316903587579492 | validation: 0.151280212203659]
	TIME [epoch: 5.94 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256825919359463		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.08256825919359463 | validation: 0.1628955000153957]
	TIME [epoch: 5.96 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958250525550997		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.0958250525550997 | validation: 0.20066225517225555]
	TIME [epoch: 5.94 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.167123326046232		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.167123326046232 | validation: 0.1719490973035769]
	TIME [epoch: 5.94 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09309023692484193		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.09309023692484193 | validation: 0.17351178235829404]
	TIME [epoch: 5.94 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0953052901619315		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.0953052901619315 | validation: 0.16169584048938077]
	TIME [epoch: 5.94 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07470286866996773		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.07470286866996773 | validation: 0.12325496471331934]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489370716652184		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07489370716652184 | validation: 0.1523739187407617]
	TIME [epoch: 5.93 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08572276187650911		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08572276187650911 | validation: 0.14672011962383605]
	TIME [epoch: 5.93 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07585054814440512		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07585054814440512 | validation: 0.14498058828417912]
	TIME [epoch: 5.93 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359464315753057		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.07359464315753057 | validation: 0.1468702428448906]
	TIME [epoch: 5.93 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255606180296194		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.07255606180296194 | validation: 0.13956634272090762]
	TIME [epoch: 5.93 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07060975789853662		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07060975789853662 | validation: 0.1481483112834103]
	TIME [epoch: 5.94 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693715427432622		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.0693715427432622 | validation: 0.15204313364180577]
	TIME [epoch: 5.93 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071182025975216		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.07071182025975216 | validation: 0.1510266437315127]
	TIME [epoch: 5.94 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009604337463958		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.08009604337463958 | validation: 0.14600045947643042]
	TIME [epoch: 5.94 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838540487496216		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07838540487496216 | validation: 0.16366265928335205]
	TIME [epoch: 5.94 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577416765069323		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.09577416765069323 | validation: 0.14983731278850285]
	TIME [epoch: 5.94 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11750588730998814		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.11750588730998814 | validation: 0.1621303498372873]
	TIME [epoch: 5.94 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952389381181548		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.08952389381181548 | validation: 0.1588177972242928]
	TIME [epoch: 5.94 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08978164195180956		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.08978164195180956 | validation: 0.17745883553140984]
	TIME [epoch: 5.94 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14139979368695885		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.14139979368695885 | validation: 0.14356390854759674]
	TIME [epoch: 5.94 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149367096297647		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.07149367096297647 | validation: 0.1609954935835748]
	TIME [epoch: 5.94 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09307739255715643		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.09307739255715643 | validation: 0.14525175134901902]
	TIME [epoch: 5.94 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848755120938553		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.0848755120938553 | validation: 0.14803018438508087]
	TIME [epoch: 5.94 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888173526435312		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06888173526435312 | validation: 0.14628049258684375]
	TIME [epoch: 5.95 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064000389629417		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.07064000389629417 | validation: 0.14223019872367945]
	TIME [epoch: 5.94 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07862101742020498		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07862101742020498 | validation: 0.14231525556954977]
	TIME [epoch: 5.94 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366519439493123		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.07366519439493123 | validation: 0.15484924919027745]
	TIME [epoch: 5.94 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08272146026846251		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.08272146026846251 | validation: 0.14187759414271137]
	TIME [epoch: 5.94 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537353040738233		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.08537353040738233 | validation: 0.2012563431974974]
	TIME [epoch: 5.94 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15995795247881345		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.15995795247881345 | validation: 0.1787080310371692]
	TIME [epoch: 5.94 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11437440957576246		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.11437440957576246 | validation: 0.15001572604087654]
	TIME [epoch: 5.94 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359323276821128		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.07359323276821128 | validation: 0.13386342989915004]
	TIME [epoch: 5.95 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125331725039552		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.08125331725039552 | validation: 0.15957684271816913]
	TIME [epoch: 5.94 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887674935205072		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.0887674935205072 | validation: 0.14297950616083063]
	TIME [epoch: 5.94 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06845788804083607		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06845788804083607 | validation: 0.12920363367230822]
	TIME [epoch: 5.94 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07553433174022303		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.07553433174022303 | validation: 0.14922190321964887]
	TIME [epoch: 5.94 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791054798565541		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.0791054798565541 | validation: 0.13788165522183005]
	TIME [epoch: 5.95 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06808301433183161		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06808301433183161 | validation: 0.13000917029370412]
	TIME [epoch: 5.94 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968315996944931		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.06968315996944931 | validation: 0.15695427472336787]
	TIME [epoch: 5.94 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321510204534062		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07321510204534062 | validation: 0.12490391375036346]
	TIME [epoch: 5.94 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286328091236942		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.06286328091236942 | validation: 0.13249175544582012]
	TIME [epoch: 5.94 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06904100790653028		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.06904100790653028 | validation: 0.16827736847785058]
	TIME [epoch: 5.94 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09248244640755665		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.09248244640755665 | validation: 0.16191229996649753]
	TIME [epoch: 5.94 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11645269431658604		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.11645269431658604 | validation: 0.13961478587821943]
	TIME [epoch: 5.94 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057030611629645		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.08057030611629645 | validation: 0.14636646497202047]
	TIME [epoch: 5.94 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07598396842530385		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.07598396842530385 | validation: 0.13665760650363812]
	TIME [epoch: 5.94 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07581439936198385		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.07581439936198385 | validation: 0.1329130859049441]
	TIME [epoch: 5.94 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07194127936617173		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07194127936617173 | validation: 0.12628160511236183]
	TIME [epoch: 5.93 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508729792457492		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.06508729792457492 | validation: 0.14384795570677852]
	TIME [epoch: 5.94 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047978326398296		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.07047978326398296 | validation: 0.13528511970357227]
	TIME [epoch: 5.95 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396287600868994		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.06396287600868994 | validation: 0.1265864129933287]
	TIME [epoch: 5.95 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549460995750647		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06549460995750647 | validation: 0.12882641405337578]
	TIME [epoch: 5.95 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700115485757203		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06700115485757203 | validation: 0.14278974281852097]
	TIME [epoch: 5.95 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0716343653992021		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.0716343653992021 | validation: 0.12738761431153295]
	TIME [epoch: 5.94 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07410041844780436		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07410041844780436 | validation: 0.1473597449968855]
	TIME [epoch: 5.95 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09020974952015223		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.09020974952015223 | validation: 0.13080504744869975]
	TIME [epoch: 5.95 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707819905308826		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.09707819905308826 | validation: 0.17283772452112045]
	TIME [epoch: 5.94 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11493154636658895		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.11493154636658895 | validation: 0.13379232769776536]
	TIME [epoch: 5.95 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639661120756699		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.0639661120756699 | validation: 0.140570060178654]
	TIME [epoch: 5.94 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193840116862458		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.08193840116862458 | validation: 0.13702949091176028]
	TIME [epoch: 5.96 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06774467641847999		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.06774467641847999 | validation: 0.1391633023508286]
	TIME [epoch: 5.95 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07062604245346947		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.07062604245346947 | validation: 0.1417402523176828]
	TIME [epoch: 5.95 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06819951403348128		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06819951403348128 | validation: 0.16427361216566294]
	TIME [epoch: 5.95 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09476714403993147		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09476714403993147 | validation: 0.13982528852769047]
	TIME [epoch: 5.95 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0673742957526465		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.0673742957526465 | validation: 0.1310658701878323]
	TIME [epoch: 5.95 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319020724149191		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06319020724149191 | validation: 0.14891271996490868]
	TIME [epoch: 5.96 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07193629317450718		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07193629317450718 | validation: 0.13694209440191205]
	TIME [epoch: 5.95 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06767896229877854		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.06767896229877854 | validation: 0.12750671107537528]
	TIME [epoch: 5.96 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06267861186027134		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.06267861186027134 | validation: 0.1445049481463869]
	TIME [epoch: 5.94 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721242906625569		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.08721242906625569 | validation: 0.20603661486581473]
	TIME [epoch: 5.96 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13257989103166815		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.13257989103166815 | validation: 0.1453899666367303]
	TIME [epoch: 5.97 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168865371715463		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.07168865371715463 | validation: 0.1464443809640084]
	TIME [epoch: 5.98 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867366289473697		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.07867366289473697 | validation: 0.13064280232354694]
	TIME [epoch: 5.97 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060137109890569154		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.060137109890569154 | validation: 0.1483787635111054]
	TIME [epoch: 5.96 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754238496323196		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06754238496323196 | validation: 0.13313550137701796]
	TIME [epoch: 5.96 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567563700119361		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06567563700119361 | validation: 0.1494230692871472]
	TIME [epoch: 5.97 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07327847959634612		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.07327847959634612 | validation: 0.13130851270610672]
	TIME [epoch: 5.96 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06168698877203038		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06168698877203038 | validation: 0.1355249964172158]
	TIME [epoch: 5.98 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06184899903760185		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.06184899903760185 | validation: 0.14417876647110614]
	TIME [epoch: 5.96 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061147097648818684		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.061147097648818684 | validation: 0.14083777181879084]
	TIME [epoch: 5.95 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217806876124727		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.07217806876124727 | validation: 0.12677780651542708]
	TIME [epoch: 5.96 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06529965741456166		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.06529965741456166 | validation: 0.1323186657738787]
	TIME [epoch: 5.97 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05850724880207883		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.05850724880207883 | validation: 0.1297767595252781]
	TIME [epoch: 5.95 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244310019637706		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.06244310019637706 | validation: 0.1359708942653144]
	TIME [epoch: 5.94 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061328437626203026		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.061328437626203026 | validation: 0.1271960349890371]
	TIME [epoch: 5.94 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05959614797290204		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.05959614797290204 | validation: 0.12710741841579437]
	TIME [epoch: 5.94 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062197391152418756		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.062197391152418756 | validation: 0.13397771250101342]
	TIME [epoch: 5.95 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399952209878648		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06399952209878648 | validation: 0.1281984125704325]
	TIME [epoch: 5.96 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732548455107116		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.0732548455107116 | validation: 0.168774041786947]
	TIME [epoch: 5.94 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09189484515491322		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.09189484515491322 | validation: 0.2630945423462933]
	TIME [epoch: 5.96 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3516714168420749		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3516714168420749 | validation: 0.24639871791104737]
	TIME [epoch: 5.94 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2954837917540216		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.2954837917540216 | validation: 0.13258012742651515]
	TIME [epoch: 5.95 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434392168194046		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.06434392168194046 | validation: 0.17526691546655085]
	TIME [epoch: 5.95 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12020043900808175		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.12020043900808175 | validation: 0.14146363278052887]
	TIME [epoch: 5.95 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484106162886071		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.07484106162886071 | validation: 0.13070849477726987]
	TIME [epoch: 5.95 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07219184568911208		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.07219184568911208 | validation: 0.1403486617485083]
	TIME [epoch: 5.95 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06991252076690022		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06991252076690022 | validation: 0.130923474789888]
	TIME [epoch: 5.95 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058302422682475576		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.058302422682475576 | validation: 0.13614123179191293]
	TIME [epoch: 5.96 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059192933866890306		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.059192933866890306 | validation: 0.13944514581328127]
	TIME [epoch: 5.95 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13393980613051493		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.13393980613051493 | validation: 0.1450647598474001]
	TIME [epoch: 5.96 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09052855902730823		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.09052855902730823 | validation: 0.12928635802123176]
	TIME [epoch: 5.95 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06595278043488431		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.06595278043488431 | validation: 0.11921711001046753]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05997083569191296		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.05997083569191296 | validation: 0.1265086626547571]
	TIME [epoch: 5.94 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05768197467233268		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.05768197467233268 | validation: 0.12504370335435117]
	TIME [epoch: 5.94 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05840264540642823		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.05840264540642823 | validation: 0.12458179515725878]
	TIME [epoch: 5.94 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958588320026519		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.05958588320026519 | validation: 0.13279692323809714]
	TIME [epoch: 5.94 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06241972653940696		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06241972653940696 | validation: 0.12788243126079762]
	TIME [epoch: 5.94 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10303727900762066		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.10303727900762066 | validation: 0.13876355028384801]
	TIME [epoch: 5.95 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293313811767152		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.06293313811767152 | validation: 0.1246921888991453]
	TIME [epoch: 5.94 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540368504310773		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.06540368504310773 | validation: 0.1242044221129288]
	TIME [epoch: 5.95 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748283718351889		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.05748283718351889 | validation: 0.1311628421644303]
	TIME [epoch: 5.95 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05963452879885393		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.05963452879885393 | validation: 0.1268752028671187]
	TIME [epoch: 5.95 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831678771359654		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.05831678771359654 | validation: 0.131240482409746]
	TIME [epoch: 5.97 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05956916087340832		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05956916087340832 | validation: 0.11916981182084219]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06019095672238921		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.06019095672238921 | validation: 0.1324235296212371]
	TIME [epoch: 5.97 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980589908853136		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05980589908853136 | validation: 0.11640860669643523]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061976896607102976		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.061976896607102976 | validation: 0.1339124166459522]
	TIME [epoch: 5.96 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276744952585388		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.06276744952585388 | validation: 0.1275155410890035]
	TIME [epoch: 5.97 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589429815575784		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.05589429815575784 | validation: 0.13226349983891447]
	TIME [epoch: 5.97 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0553789456394659		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.0553789456394659 | validation: 0.13182342782067918]
	TIME [epoch: 5.97 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783499759686537		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.05783499759686537 | validation: 0.2692293768052535]
	TIME [epoch: 5.97 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23008300316116403		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.23008300316116403 | validation: 0.1655085434100151]
	TIME [epoch: 5.97 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11766751424698527		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.11766751424698527 | validation: 0.14544404856924983]
	TIME [epoch: 5.98 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07330579070269104		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.07330579070269104 | validation: 0.1477679861639524]
	TIME [epoch: 5.98 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180042314329695		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.09180042314329695 | validation: 0.13786904725577873]
	TIME [epoch: 5.99 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567605579868843		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.06567605579868843 | validation: 0.1356471602934528]
	TIME [epoch: 5.98 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060357221393994555		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.060357221393994555 | validation: 0.13211660880386097]
	TIME [epoch: 5.97 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05784271184425733		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05784271184425733 | validation: 0.128815302255718]
	TIME [epoch: 5.97 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053689610637771945		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.053689610637771945 | validation: 0.11717302335054991]
	TIME [epoch: 5.97 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05616125885507848		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05616125885507848 | validation: 0.13140518285406227]
	TIME [epoch: 5.96 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05617599655754072		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.05617599655754072 | validation: 0.11830240314286522]
	TIME [epoch: 5.97 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054669768962980536		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.054669768962980536 | validation: 0.1152652598521436]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05661378028037499		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05661378028037499 | validation: 0.1262086614449843]
	TIME [epoch: 5.97 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554214608325356		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05554214608325356 | validation: 0.12285277975260578]
	TIME [epoch: 5.96 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05297348885596611		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.05297348885596611 | validation: 0.12210996766675825]
	TIME [epoch: 5.97 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378517055998613		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05378517055998613 | validation: 0.11766915917540977]
	TIME [epoch: 5.97 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055631312382005284		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.055631312382005284 | validation: 0.11405175971602155]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05701277491811564		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.05701277491811564 | validation: 0.12848897853852517]
	TIME [epoch: 5.96 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06561539419902453		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.06561539419902453 | validation: 0.1381061723367018]
	TIME [epoch: 5.97 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08956506338161781		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.08956506338161781 | validation: 0.12100172204227017]
	TIME [epoch: 5.97 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057376575156519005		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.057376575156519005 | validation: 0.11648901993776571]
	TIME [epoch: 5.96 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056673998690096		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.056673998690096 | validation: 0.12390166156098492]
	TIME [epoch: 5.98 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07646656390620557		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07646656390620557 | validation: 0.11833430870758539]
	TIME [epoch: 5.97 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05979667167337953		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05979667167337953 | validation: 0.11940954033085337]
	TIME [epoch: 5.97 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397988191302341		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.05397988191302341 | validation: 0.20583468750650238]
	TIME [epoch: 5.97 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26872225673052824		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.26872225673052824 | validation: 0.20850452083902982]
	TIME [epoch: 5.98 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23356062329157823		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.23356062329157823 | validation: 0.1405531882574811]
	TIME [epoch: 5.96 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245049001035865		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.08245049001035865 | validation: 0.22211921890282307]
	TIME [epoch: 5.97 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20812074265240918		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.20812074265240918 | validation: 0.2836690898905972]
	TIME [epoch: 5.96 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2229769631190406		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.2229769631190406 | validation: 0.15913986371883407]
	TIME [epoch: 5.97 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12549563001262765		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.12549563001262765 | validation: 0.11763780735568208]
	TIME [epoch: 5.96 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627981283718981		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0627981283718981 | validation: 0.12717788471768948]
	TIME [epoch: 5.97 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07139508144657519		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.07139508144657519 | validation: 0.12546779160447105]
	TIME [epoch: 5.97 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343969377187594		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.06343969377187594 | validation: 0.12328359910637648]
	TIME [epoch: 5.97 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06109985147958653		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06109985147958653 | validation: 0.10472947979754484]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05706827961217226		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.05706827961217226 | validation: 0.10948103392896202]
	TIME [epoch: 5.97 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815316896117535		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.05815316896117535 | validation: 0.11719787083717118]
	TIME [epoch: 5.97 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384494357357509		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.05384494357357509 | validation: 0.11173104407649764]
	TIME [epoch: 5.98 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053231713372941976		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.053231713372941976 | validation: 0.11235033183135856]
	TIME [epoch: 5.98 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05606622081083945		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.05606622081083945 | validation: 0.11999759701046463]
	TIME [epoch: 5.98 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360985046273239		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.05360985046273239 | validation: 0.11311413126477293]
	TIME [epoch: 5.97 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055070498342304805		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.055070498342304805 | validation: 0.1258848047305222]
	TIME [epoch: 5.97 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05871666550850134		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.05871666550850134 | validation: 0.1123459183746955]
	TIME [epoch: 5.97 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05233861529783435		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05233861529783435 | validation: 0.121826031356907]
	TIME [epoch: 5.98 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371167970582759		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05371167970582759 | validation: 0.11798544831513268]
	TIME [epoch: 5.98 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538575243906824		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.0538575243906824 | validation: 0.13952941927569507]
	TIME [epoch: 5.97 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11786066919842599		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.11786066919842599 | validation: 0.10714763242575898]
	TIME [epoch: 5.97 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714426280569235		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.0714426280569235 | validation: 0.11730950320578915]
	TIME [epoch: 5.97 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05801237030115761		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.05801237030115761 | validation: 0.12242895158528255]
	TIME [epoch: 5.97 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076644933851215		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.06076644933851215 | validation: 0.15458744754942133]
	TIME [epoch: 5.97 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10442418920790048		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.10442418920790048 | validation: 0.12551846718524867]
	TIME [epoch: 5.97 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827853483479349		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.06827853483479349 | validation: 0.10767004904612923]
	TIME [epoch: 5.97 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681691197615779		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.05681691197615779 | validation: 0.12308079648798874]
	TIME [epoch: 5.96 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06047850862496107		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06047850862496107 | validation: 0.1154297042028074]
	TIME [epoch: 5.97 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714123114651391		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05714123114651391 | validation: 0.1180424875951554]
	TIME [epoch: 5.96 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185298473950692		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05185298473950692 | validation: 0.11036637226860926]
	TIME [epoch: 5.97 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053521088332804966		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.053521088332804966 | validation: 0.10861070776428411]
	TIME [epoch: 5.96 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053862926977213295		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.053862926977213295 | validation: 0.11890160470660614]
	TIME [epoch: 5.97 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05698687611780512		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.05698687611780512 | validation: 0.11403339976218758]
	TIME [epoch: 5.97 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05341693928440016		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.05341693928440016 | validation: 0.11781382318437904]
	TIME [epoch: 5.96 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05374691466889304		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.05374691466889304 | validation: 0.13414403902157554]
	TIME [epoch: 5.96 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07402239497191246		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.07402239497191246 | validation: 0.123113502627786]
	TIME [epoch: 5.96 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05722488876250023		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.05722488876250023 | validation: 0.12452367399436562]
	TIME [epoch: 5.97 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157739595591366		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.06157739595591366 | validation: 0.10997500405287637]
	TIME [epoch: 5.97 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112997882674744		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.05112997882674744 | validation: 0.11701272253282857]
	TIME [epoch: 5.97 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054998425085595964		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.054998425085595964 | validation: 0.12423798299863964]
	TIME [epoch: 5.97 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0516423511094656		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.0516423511094656 | validation: 0.12431186821472873]
	TIME [epoch: 5.96 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608432914227539		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.05608432914227539 | validation: 0.15752896425788052]
	TIME [epoch: 5.97 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11708885271661007		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.11708885271661007 | validation: 0.13526868895461738]
	TIME [epoch: 5.96 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08372854190098623		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.08372854190098623 | validation: 0.11962678476843691]
	TIME [epoch: 5.97 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167315820824683		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.05167315820824683 | validation: 0.12237270872799183]
	TIME [epoch: 5.96 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676357685965691		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.0676357685965691 | validation: 0.1194173064605155]
	TIME [epoch: 5.97 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05408988250467546		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.05408988250467546 | validation: 0.11898560956841978]
	TIME [epoch: 5.97 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05632365223475308		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.05632365223475308 | validation: 0.10617220895891362]
	TIME [epoch: 5.97 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05201379369928976		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05201379369928976 | validation: 0.11401155432920543]
	TIME [epoch: 5.98 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800097235399736		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.04800097235399736 | validation: 0.12110241279259225]
	TIME [epoch: 5.98 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05179537922742794		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.05179537922742794 | validation: 0.1195989050267869]
	TIME [epoch: 5.97 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256405245462137		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05256405245462137 | validation: 0.10913801926262667]
	TIME [epoch: 5.97 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053889935818769746		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.053889935818769746 | validation: 0.10478407936765471]
	TIME [epoch: 5.97 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0531514789558106		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.0531514789558106 | validation: 0.11140699579776464]
	TIME [epoch: 5.98 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0564594679862699		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.0564594679862699 | validation: 0.11363559804991011]
	TIME [epoch: 5.97 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112280357904164		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.05112280357904164 | validation: 0.11296594363344076]
	TIME [epoch: 5.98 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346292059054953		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.05346292059054953 | validation: 0.1182131071068266]
	TIME [epoch: 5.97 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053023218880511216		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.053023218880511216 | validation: 0.11092080520355979]
	TIME [epoch: 5.96 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205169096229282		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.05205169096229282 | validation: 0.12086377656520647]
	TIME [epoch: 5.97 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256585262926865		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.05256585262926865 | validation: 0.10819529451383858]
	TIME [epoch: 5.96 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532459261438111		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.05532459261438111 | validation: 0.10600048283998556]
	TIME [epoch: 5.97 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052447875408269846		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.052447875408269846 | validation: 0.11713359054821076]
	TIME [epoch: 5.96 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05252587312832108		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.05252587312832108 | validation: 0.09744306164996197]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142113871890933		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.05142113871890933 | validation: 0.12554511560547685]
	TIME [epoch: 5.93 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610116066716214		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05610116066716214 | validation: 0.11406528384360624]
	TIME [epoch: 5.95 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04882318269893956		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.04882318269893956 | validation: 0.11043471616204381]
	TIME [epoch: 5.95 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051346737801943924		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.051346737801943924 | validation: 0.135571201061441]
	TIME [epoch: 5.94 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06397619317471422		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.06397619317471422 | validation: 0.11016562120769757]
	TIME [epoch: 5.94 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047776962134792		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.047776962134792 | validation: 0.11464351329496232]
	TIME [epoch: 5.95 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660499124958216		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.05660499124958216 | validation: 0.12978358197680243]
	TIME [epoch: 5.94 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08042945625588628		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.08042945625588628 | validation: 0.12454658988667527]
	TIME [epoch: 5.94 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327182814670604		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.06327182814670604 | validation: 0.10134318544399933]
	TIME [epoch: 5.94 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093567659015915		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.05093567659015915 | validation: 0.10448065186621612]
	TIME [epoch: 5.94 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05324782866631895		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.05324782866631895 | validation: 0.12098981501643125]
	TIME [epoch: 5.94 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05651744253481396		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.05651744253481396 | validation: 0.13785818590225576]
	TIME [epoch: 5.94 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305760880582675		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.1305760880582675 | validation: 0.11933393547636464]
	TIME [epoch: 5.94 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508287960640625		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07508287960640625 | validation: 0.12218498824249358]
	TIME [epoch: 5.94 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05313874963756644		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.05313874963756644 | validation: 0.11263192285740185]
	TIME [epoch: 5.95 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06515712082492009		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.06515712082492009 | validation: 0.10587556691283234]
	TIME [epoch: 5.94 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049371002993375		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.049371002993375 | validation: 0.10487924398399491]
	TIME [epoch: 5.96 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496012595359411		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.0496012595359411 | validation: 0.11355768312356858]
	TIME [epoch: 5.94 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773574088782819		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04773574088782819 | validation: 0.10964745180832347]
	TIME [epoch: 5.95 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049660779641724884		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.049660779641724884 | validation: 0.10607858686647514]
	TIME [epoch: 5.94 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04866918064405949		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.04866918064405949 | validation: 0.11020645165675079]
	TIME [epoch: 5.95 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048021849350958164		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.048021849350958164 | validation: 0.12261195232840337]
	TIME [epoch: 5.94 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04847128457647948		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.04847128457647948 | validation: 0.1107655970676718]
	TIME [epoch: 5.95 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778975362399221		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.04778975362399221 | validation: 0.1018057921264699]
	TIME [epoch: 5.95 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047918818301790796		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.047918818301790796 | validation: 0.12589366356964332]
	TIME [epoch: 5.94 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058686010346019835		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.058686010346019835 | validation: 0.10201012543839288]
	TIME [epoch: 5.94 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047134648802699054		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.047134648802699054 | validation: 0.10602742784189784]
	TIME [epoch: 5.95 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05220462279963336		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.05220462279963336 | validation: 0.14030502376648984]
	TIME [epoch: 5.94 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789143482923602		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.0789143482923602 | validation: 0.11326346593206714]
	TIME [epoch: 5.94 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04630352327406326		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04630352327406326 | validation: 0.11474212385437497]
	TIME [epoch: 5.93 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05021652374553467		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.05021652374553467 | validation: 0.10538074053871996]
	TIME [epoch: 5.94 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045615812367423174		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.045615812367423174 | validation: 0.10665049269624466]
	TIME [epoch: 5.94 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05459615501795126		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.05459615501795126 | validation: 0.11043124908086469]
	TIME [epoch: 5.95 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047644310437442085		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.047644310437442085 | validation: 0.11048518323808723]
	TIME [epoch: 5.94 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726741828546834		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.04726741828546834 | validation: 0.10614335752161007]
	TIME [epoch: 5.94 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579880379245901		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.04579880379245901 | validation: 0.1011885224831135]
	TIME [epoch: 5.94 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514580228477116		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.04514580228477116 | validation: 0.10098073029008384]
	TIME [epoch: 5.94 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841950621114563		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.04841950621114563 | validation: 0.11522161144740464]
	TIME [epoch: 5.94 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051153650765463896		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.051153650765463896 | validation: 0.10610520910747118]
	TIME [epoch: 5.93 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044270774498396975		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.044270774498396975 | validation: 0.10701571266212118]
	TIME [epoch: 5.94 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053884057872740046		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.053884057872740046 | validation: 0.11235546228635472]
	TIME [epoch: 5.94 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613375170091908		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.04613375170091908 | validation: 0.10265575437404476]
	TIME [epoch: 5.96 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04496952135703113		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.04496952135703113 | validation: 0.10779725899208532]
	TIME [epoch: 5.93 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731802141113825		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04731802141113825 | validation: 0.08898609160504517]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049354457631461644		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.049354457631461644 | validation: 0.10141940216018518]
	TIME [epoch: 5.94 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579341111579462		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04579341111579462 | validation: 0.10973191642709397]
	TIME [epoch: 5.95 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043468255541074356		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.043468255541074356 | validation: 0.11551331928188713]
	TIME [epoch: 5.94 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07035881036266525		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.07035881036266525 | validation: 0.10664021233349832]
	TIME [epoch: 5.94 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04643795699671848		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.04643795699671848 | validation: 0.11299149171828843]
	TIME [epoch: 5.95 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048723099810250206		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.048723099810250206 | validation: 0.09880577218871202]
	TIME [epoch: 5.94 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044918071775600074		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.044918071775600074 | validation: 0.09790677068145903]
	TIME [epoch: 5.96 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04749828408720942		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04749828408720942 | validation: 0.10337260264659937]
	TIME [epoch: 5.94 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045768049419641826		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.045768049419641826 | validation: 0.10744176705594928]
	TIME [epoch: 5.95 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809289560364066		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.04809289560364066 | validation: 0.09912782452171205]
	TIME [epoch: 5.95 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752765085451646		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.04752765085451646 | validation: 0.11180962081022204]
	TIME [epoch: 5.94 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052092099532831226		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.052092099532831226 | validation: 0.10767531085293545]
	TIME [epoch: 5.94 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597000605082591		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04597000605082591 | validation: 0.11184350382881698]
	TIME [epoch: 5.94 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049135326302135685		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.049135326302135685 | validation: 0.0928081557121431]
	TIME [epoch: 5.94 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04805994114015864		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.04805994114015864 | validation: 0.10022962469514425]
	TIME [epoch: 5.94 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565407265460099		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.04565407265460099 | validation: 0.09845022029050471]
	TIME [epoch: 5.94 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666178600944793		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.04666178600944793 | validation: 0.09930551566436414]
	TIME [epoch: 5.94 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041027501159982716		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.041027501159982716 | validation: 0.09763811290781389]
	TIME [epoch: 5.95 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657574412716506		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04657574412716506 | validation: 0.11035283399693943]
	TIME [epoch: 5.94 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818049016227262		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04818049016227262 | validation: 0.09495800060109016]
	TIME [epoch: 5.95 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0443183119910061		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.0443183119910061 | validation: 0.0956202521816432]
	TIME [epoch: 5.94 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043813262753567915		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.043813262753567915 | validation: 0.08649575627650354]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041606094804068815		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.041606094804068815 | validation: 0.08991806152888838]
	TIME [epoch: 5.94 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641670941830274		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.04641670941830274 | validation: 0.102622692829603]
	TIME [epoch: 5.94 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045362371153233515		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.045362371153233515 | validation: 0.10210618940520416]
	TIME [epoch: 5.95 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04812813958734131		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.04812813958734131 | validation: 0.09778143955286338]
	TIME [epoch: 5.94 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590226536557542		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.04590226536557542 | validation: 0.09110754801048626]
	TIME [epoch: 5.94 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05335777851783711		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.05335777851783711 | validation: 0.11436389139217454]
	TIME [epoch: 5.95 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05246152350929505		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.05246152350929505 | validation: 0.09865396938991011]
	TIME [epoch: 5.94 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042373655937568755		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.042373655937568755 | validation: 0.09769923959705784]
	TIME [epoch: 5.9 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044215741311696845		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.044215741311696845 | validation: 0.10543763409429117]
	TIME [epoch: 5.91 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411328301485114		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.04411328301485114 | validation: 0.09117122565882455]
	TIME [epoch: 5.91 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440404881775504		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.0440404881775504 | validation: 0.10305098332622553]
	TIME [epoch: 5.92 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05087456916827711		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.05087456916827711 | validation: 0.1082859310042069]
	TIME [epoch: 5.95 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04401379011099181		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04401379011099181 | validation: 0.10249717673491686]
	TIME [epoch: 5.95 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301050898191677		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04301050898191677 | validation: 0.09857325140805484]
	TIME [epoch: 5.94 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0508868358662688		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.0508868358662688 | validation: 0.10734438162989123]
	TIME [epoch: 5.95 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04874782157758434		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.04874782157758434 | validation: 0.08771063947923748]
	TIME [epoch: 5.95 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041339104785779436		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.041339104785779436 | validation: 0.09127336678457448]
	TIME [epoch: 5.95 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042284394944750815		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.042284394944750815 | validation: 0.10033227926436394]
	TIME [epoch: 5.95 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04561784286733749		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.04561784286733749 | validation: 0.10060124420236671]
	TIME [epoch: 5.94 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432644396091967		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.04432644396091967 | validation: 0.09287458750460675]
	TIME [epoch: 5.94 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0415284395150011		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.0415284395150011 | validation: 0.0830590300396308]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674236813032856		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.04674236813032856 | validation: 0.09124479657588003]
	TIME [epoch: 5.93 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046615891767712025		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.046615891767712025 | validation: 0.09335597016600634]
	TIME [epoch: 5.93 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519882377755556		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.04519882377755556 | validation: 0.09793959685841318]
	TIME [epoch: 5.93 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840122871639868		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03840122871639868 | validation: 0.09758303396182916]
	TIME [epoch: 5.94 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04523662764143335		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.04523662764143335 | validation: 0.08741107684619127]
	TIME [epoch: 5.94 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043167757776207696		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.043167757776207696 | validation: 0.1042787211037366]
	TIME [epoch: 5.93 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04386856212330237		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.04386856212330237 | validation: 0.14809633717055745]
	TIME [epoch: 5.94 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15314061331798573		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.15314061331798573 | validation: 0.11869583983667305]
	TIME [epoch: 5.93 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07436145585116946		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.07436145585116946 | validation: 0.08892206272164822]
	TIME [epoch: 5.93 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597074144648317		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.04597074144648317 | validation: 0.11081416012960915]
	TIME [epoch: 5.94 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05888020204827901		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.05888020204827901 | validation: 0.10179302652389427]
	TIME [epoch: 5.94 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04561584163522449		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.04561584163522449 | validation: 0.09233545580940819]
	TIME [epoch: 5.94 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061128363646978		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.04061128363646978 | validation: 0.10463043517835802]
	TIME [epoch: 5.93 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503463747998655		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0503463747998655 | validation: 0.09990057688116187]
	TIME [epoch: 5.95 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994172116017486		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04994172116017486 | validation: 0.09997709797907091]
	TIME [epoch: 5.93 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04383161989602186		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.04383161989602186 | validation: 0.09454643465852647]
	TIME [epoch: 5.94 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04435983812388435		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.04435983812388435 | validation: 0.10291818868497114]
	TIME [epoch: 5.93 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952434921143855		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.04952434921143855 | validation: 0.10093158704946276]
	TIME [epoch: 5.94 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432478469985429		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0432478469985429 | validation: 0.09351294592104577]
	TIME [epoch: 5.94 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04349262512294962		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.04349262512294962 | validation: 0.0902582483601991]
	TIME [epoch: 5.95 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042738619472074614		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.042738619472074614 | validation: 0.10022322973488121]
	TIME [epoch: 5.94 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04113749621890996		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.04113749621890996 | validation: 0.09755443723541898]
	TIME [epoch: 5.92 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396953183190666		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.04396953183190666 | validation: 0.0926195630282495]
	TIME [epoch: 5.94 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042461571829408296		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.042461571829408296 | validation: 0.09587290943847662]
	TIME [epoch: 5.93 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039297148320620846		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.039297148320620846 | validation: 0.0802722002898726]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048698932921431126		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.048698932921431126 | validation: 0.10182826981282883]
	TIME [epoch: 5.95 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046319299186953274		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.046319299186953274 | validation: 0.10048368987246843]
	TIME [epoch: 5.96 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04332681805945834		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.04332681805945834 | validation: 0.09775471643596051]
	TIME [epoch: 5.95 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042999671794952184		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.042999671794952184 | validation: 0.2765212049909283]
	TIME [epoch: 5.96 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185054587113584		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.185054587113584 | validation: 0.2628491354641145]
	TIME [epoch: 5.96 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21249059102066256		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.21249059102066256 | validation: 0.1702659659670335]
	TIME [epoch: 5.95 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1301458360800642		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.1301458360800642 | validation: 0.11186524258590197]
	TIME [epoch: 5.94 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056861213429489806		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.056861213429489806 | validation: 0.0977026939635995]
	TIME [epoch: 5.93 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04105729778151433		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.04105729778151433 | validation: 0.10909414509613834]
	TIME [epoch: 5.95 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04907382129340451		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04907382129340451 | validation: 0.10049362138053651]
	TIME [epoch: 5.94 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509007936663179		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.04509007936663179 | validation: 0.08825560759532487]
	TIME [epoch: 5.95 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188430737914855		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.04188430737914855 | validation: 0.09145078799181994]
	TIME [epoch: 5.94 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142115335076769		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.04142115335076769 | validation: 0.09274162597261026]
	TIME [epoch: 5.96 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04029036497844727		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.04029036497844727 | validation: 0.08987226218816845]
	TIME [epoch: 5.95 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04044038472717049		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.04044038472717049 | validation: 0.10377101451437372]
	TIME [epoch: 5.96 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820484614801467		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.03820484614801467 | validation: 0.09156131813023806]
	TIME [epoch: 5.94 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03900894691811846		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03900894691811846 | validation: 0.09980238270590003]
	TIME [epoch: 5.94 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909117272580442		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.03909117272580442 | validation: 0.09714553063701103]
	TIME [epoch: 5.94 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04091229186897826		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.04091229186897826 | validation: 0.09880291918416213]
	TIME [epoch: 5.94 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042196469610223185		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.042196469610223185 | validation: 0.08562668231403463]
	TIME [epoch: 5.93 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868392416410926		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.03868392416410926 | validation: 0.09586117801130645]
	TIME [epoch: 5.94 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040756887420513234		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.040756887420513234 | validation: 0.09760545712433095]
	TIME [epoch: 5.94 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472910951371943		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.04472910951371943 | validation: 0.08873386997634594]
	TIME [epoch: 5.94 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042244117191532		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.042244117191532 | validation: 0.09481924032410127]
	TIME [epoch: 5.95 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03935790851488862		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03935790851488862 | validation: 0.08420026906527821]
	TIME [epoch: 5.94 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04052098043022584		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.04052098043022584 | validation: 0.09651567915552578]
	TIME [epoch: 5.95 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932037527234283		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.03932037527234283 | validation: 0.08684911846871501]
	TIME [epoch: 5.93 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03734049217307285		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.03734049217307285 | validation: 0.08787773901618726]
	TIME [epoch: 5.94 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03686581070197766		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03686581070197766 | validation: 0.10656298623844136]
	TIME [epoch: 5.95 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03981727224613102		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.03981727224613102 | validation: 0.09276497542384358]
	TIME [epoch: 5.93 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037855081100893855		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.037855081100893855 | validation: 0.09328285682158383]
	TIME [epoch: 5.93 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041192069776092154		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.041192069776092154 | validation: 0.10039269545673682]
	TIME [epoch: 5.93 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04464656975589794		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.04464656975589794 | validation: 0.09795101769814854]
	TIME [epoch: 5.93 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994118619262717		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.03994118619262717 | validation: 0.09791170383999975]
	TIME [epoch: 5.94 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037895874143181955		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.037895874143181955 | validation: 0.08281741786626334]
	TIME [epoch: 5.91 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03965572349386463		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.03965572349386463 | validation: 0.0981503827358269]
	TIME [epoch: 5.95 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864811814508605		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03864811814508605 | validation: 0.09464602376686462]
	TIME [epoch: 5.94 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041533457316012205		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.041533457316012205 | validation: 0.0899953370884958]
	TIME [epoch: 5.95 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03773313810162632		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03773313810162632 | validation: 0.11738208458996745]
	TIME [epoch: 5.94 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594033963054882		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0594033963054882 | validation: 0.10961557162270484]
	TIME [epoch: 5.95 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682745431893347		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.04682745431893347 | validation: 0.08542277641501203]
	TIME [epoch: 5.94 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039461292083267895		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.039461292083267895 | validation: 0.08099312901912559]
	TIME [epoch: 5.96 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023526942879351		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.04023526942879351 | validation: 0.09552239951775733]
	TIME [epoch: 5.95 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03755387660908171		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03755387660908171 | validation: 0.08957392663902643]
	TIME [epoch: 5.97 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811017283574797		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03811017283574797 | validation: 0.09504204689810157]
	TIME [epoch: 5.94 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03953916947216241		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.03953916947216241 | validation: 0.09885903850464928]
	TIME [epoch: 5.96 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865937968270924		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03865937968270924 | validation: 0.08856950495911042]
	TIME [epoch: 5.95 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166024370419644		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.04166024370419644 | validation: 0.10250967678871095]
	TIME [epoch: 5.96 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442037065321149		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.0442037065321149 | validation: 0.0989840482880291]
	TIME [epoch: 5.95 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041763334863696676		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.041763334863696676 | validation: 0.08860030131749516]
	TIME [epoch: 5.95 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04465666245759582		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.04465666245759582 | validation: 0.09626298280901713]
	TIME [epoch: 5.95 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380148148404543		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0380148148404543 | validation: 0.0995952240044588]
	TIME [epoch: 5.95 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036685468835491945		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.036685468835491945 | validation: 0.08490058133290876]
	TIME [epoch: 5.95 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04031554181104968		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.04031554181104968 | validation: 0.08687152938686532]
	TIME [epoch: 5.96 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369758834487152		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.0369758834487152 | validation: 0.09407827654655004]
	TIME [epoch: 5.96 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03984704691973733		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.03984704691973733 | validation: 0.13075892243115347]
	TIME [epoch: 5.95 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08911088530117485		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.08911088530117485 | validation: 0.12714644195067093]
	TIME [epoch: 5.95 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08783948983454545		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.08783948983454545 | validation: 0.09488631672817704]
	TIME [epoch: 5.95 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049773993316178204		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.049773993316178204 | validation: 0.08235754084984238]
	TIME [epoch: 5.95 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916389738402376		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.03916389738402376 | validation: 0.09295758309747734]
	TIME [epoch: 5.95 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404219651680678		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0404219651680678 | validation: 0.08930789342493578]
	TIME [epoch: 5.95 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623394312740791		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.03623394312740791 | validation: 0.08838774749875891]
	TIME [epoch: 279 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038198290249631686		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.038198290249631686 | validation: 0.0863380504641444]
	TIME [epoch: 12.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035729134241670436		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.035729134241670436 | validation: 0.08601524017080821]
	TIME [epoch: 12.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03698408645042505		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03698408645042505 | validation: 0.08914384217537173]
	TIME [epoch: 12.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040669821889460014		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.040669821889460014 | validation: 0.09059623952646757]
	TIME [epoch: 12.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166287548991748		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.04166287548991748 | validation: 0.09620421072828982]
	TIME [epoch: 12.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03360612025720609		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.03360612025720609 | validation: 0.09611434262048091]
	TIME [epoch: 12.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342008084038572		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.0342008084038572 | validation: 0.12896132471059393]
	TIME [epoch: 12.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09375838193634216		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.09375838193634216 | validation: 0.12023453457156773]
	TIME [epoch: 12.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07860275607128815		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.07860275607128815 | validation: 0.09797753606615654]
	TIME [epoch: 12.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04199901499182186		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.04199901499182186 | validation: 0.08424767751753778]
	TIME [epoch: 12.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037240933414171985		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.037240933414171985 | validation: 0.08321135154036773]
	TIME [epoch: 12.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04108059307416454		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.04108059307416454 | validation: 0.08133226189921519]
	TIME [epoch: 12.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04002391833571944		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.04002391833571944 | validation: 0.10058182993408714]
	TIME [epoch: 12.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354126878717904		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.0354126878717904 | validation: 0.09052804983966892]
	TIME [epoch: 12.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03854463315497859		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.03854463315497859 | validation: 0.09102502413495461]
	TIME [epoch: 12.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448162266467118		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03448162266467118 | validation: 0.08170443128680016]
	TIME [epoch: 12.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03946917838096276		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03946917838096276 | validation: 0.08485787035746684]
	TIME [epoch: 12.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0356175206278242		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.0356175206278242 | validation: 0.08840542276723075]
	TIME [epoch: 12.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03901296397578837		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03901296397578837 | validation: 0.07383759174933362]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038078345513826925		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.038078345513826925 | validation: 0.09281294940215373]
	TIME [epoch: 12.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778983674675186		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03778983674675186 | validation: 0.09738590751425105]
	TIME [epoch: 12.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043245581516017755		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.043245581516017755 | validation: 0.08011227201355889]
	TIME [epoch: 12.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556071483543611		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.03556071483543611 | validation: 0.08381633928877431]
	TIME [epoch: 12.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722582069066124		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.03722582069066124 | validation: 0.08086032055244335]
	TIME [epoch: 12.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04141049795928601		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.04141049795928601 | validation: 0.08633116012889319]
	TIME [epoch: 12.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533803571188292		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.04533803571188292 | validation: 0.08883641811400617]
	TIME [epoch: 12.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037924464909768994		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.037924464909768994 | validation: 0.07943427086986982]
	TIME [epoch: 12.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641714105039061		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.03641714105039061 | validation: 0.08784328378924115]
	TIME [epoch: 12.6 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842368881502136		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03842368881502136 | validation: 0.09328574739355414]
	TIME [epoch: 12.6 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03875979467904929		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.03875979467904929 | validation: 0.08793758315838618]
	TIME [epoch: 12.6 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574789754869701		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.03574789754869701 | validation: 0.08812068224072922]
	TIME [epoch: 12.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033149190639289804		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.033149190639289804 | validation: 0.08774423606075626]
	TIME [epoch: 12.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478429454676727		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03478429454676727 | validation: 0.06721366125189222]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568616192146554		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.03568616192146554 | validation: 0.08929003183896955]
	TIME [epoch: 12.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03895737253502987		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03895737253502987 | validation: 0.07249583474384719]
	TIME [epoch: 12.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419789989662381		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.03419789989662381 | validation: 0.07966221543423746]
	TIME [epoch: 12.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035815697011947596		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.035815697011947596 | validation: 0.0728645307271285]
	TIME [epoch: 12.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159670722970458		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.03159670722970458 | validation: 0.09149668828857978]
	TIME [epoch: 12.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565261746660211		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03565261746660211 | validation: 0.07563538158431703]
	TIME [epoch: 12.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035893928419201165		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.035893928419201165 | validation: 0.0841347726804651]
	TIME [epoch: 12.6 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549842358456105		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.03549842358456105 | validation: 0.08554114487501978]
	TIME [epoch: 12.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552808200720334		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.03552808200720334 | validation: 0.0898357442507724]
	TIME [epoch: 12.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03666904656950869		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.03666904656950869 | validation: 0.08227868984410952]
	TIME [epoch: 12.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03599299828264675		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.03599299828264675 | validation: 0.08219550333294588]
	TIME [epoch: 12.6 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03678994148740404		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03678994148740404 | validation: 0.08931127447481679]
	TIME [epoch: 12.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03636848299899738		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.03636848299899738 | validation: 0.09278192394790619]
	TIME [epoch: 12.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060462273403584954		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.060462273403584954 | validation: 0.09137207120658232]
	TIME [epoch: 12.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05302618229189614		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.05302618229189614 | validation: 0.0769583358772162]
	TIME [epoch: 12.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419953124499282		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.03419953124499282 | validation: 0.09311245933863573]
	TIME [epoch: 12.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360323828822315		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.0360323828822315 | validation: 0.09763628464768416]
	TIME [epoch: 12.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03844496487205933		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.03844496487205933 | validation: 0.08159085207071312]
	TIME [epoch: 12.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372773839123954		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.03372773839123954 | validation: 0.09419814724024425]
	TIME [epoch: 12.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03615344628864014		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.03615344628864014 | validation: 0.08750759821908381]
	TIME [epoch: 12.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03327238626887441		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.03327238626887441 | validation: 0.07786732188518354]
	TIME [epoch: 12.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036218894006540325		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.036218894006540325 | validation: 0.08617201134277569]
	TIME [epoch: 12.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033790467674574755		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.033790467674574755 | validation: 0.08599120295154987]
	TIME [epoch: 12.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03251450362461311		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03251450362461311 | validation: 0.08502619553687595]
	TIME [epoch: 12.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03805500056444304		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.03805500056444304 | validation: 0.08701064665111709]
	TIME [epoch: 12.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032788561423896065		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.032788561423896065 | validation: 0.07407066496929735]
	TIME [epoch: 12.6 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416987897050057		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.03416987897050057 | validation: 0.0835323596514751]
	TIME [epoch: 12.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03633119957332298		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.03633119957332298 | validation: 0.10024990303281643]
	TIME [epoch: 12.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03775266096822399		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.03775266096822399 | validation: 0.09688785337098865]
	TIME [epoch: 12.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04032544167854615		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.04032544167854615 | validation: 0.0766072199145609]
	TIME [epoch: 12.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0337221700572289		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.0337221700572289 | validation: 0.08012388802161566]
	TIME [epoch: 12.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04663058547222942		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.04663058547222942 | validation: 0.08365728780024682]
	TIME [epoch: 12.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03683516416842905		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.03683516416842905 | validation: 0.08903240644261445]
	TIME [epoch: 12.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344265835219126		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.0344265835219126 | validation: 0.06996190373685522]
	TIME [epoch: 12.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614748006595695		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.03614748006595695 | validation: 0.08090327588932406]
	TIME [epoch: 12.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037457951436183155		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.037457951436183155 | validation: 0.08082544109468054]
	TIME [epoch: 12.6 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03361838906240584		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.03361838906240584 | validation: 0.08204355751083153]
	TIME [epoch: 12.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034265499616464724		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.034265499616464724 | validation: 0.08091391895274028]
	TIME [epoch: 12.6 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502516079785029		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.03502516079785029 | validation: 0.087910818535343]
	TIME [epoch: 12.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372831900156764		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.03372831900156764 | validation: 0.07795389570886048]
	TIME [epoch: 12.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141767461941803		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.03141767461941803 | validation: 0.0797388951341193]
	TIME [epoch: 12.6 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386438048263799		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.03386438048263799 | validation: 0.08580768618127996]
	TIME [epoch: 12.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035938473440249016		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.035938473440249016 | validation: 0.07606480828571455]
	TIME [epoch: 12.6 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03436150843331553		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03436150843331553 | validation: 0.0746431012209143]
	TIME [epoch: 12.6 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549032993742287		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.03549032993742287 | validation: 0.08612824472473742]
	TIME [epoch: 12.6 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03803969025782651		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.03803969025782651 | validation: 0.08842794672964785]
	TIME [epoch: 12.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03368506706208614		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.03368506706208614 | validation: 0.07903874852226511]
	TIME [epoch: 12.6 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034644727127857926		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.034644727127857926 | validation: 0.08273780360983687]
	TIME [epoch: 12.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03488765140961563		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.03488765140961563 | validation: 0.07583694001003882]
	TIME [epoch: 12.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03360333856988326		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.03360333856988326 | validation: 0.06894633229214701]
	TIME [epoch: 12.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396784899248178		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.03396784899248178 | validation: 0.08212492994534405]
	TIME [epoch: 12.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549369912737929		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.03549369912737929 | validation: 0.08715857011565215]
	TIME [epoch: 12.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334187125295872		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.03334187125295872 | validation: 0.07513463069141782]
	TIME [epoch: 12.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03587382882608026		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.03587382882608026 | validation: 0.0766239016966041]
	TIME [epoch: 12.6 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552613262059816		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.03552613262059816 | validation: 0.07624606947614435]
	TIME [epoch: 12.6 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033143276385855194		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.033143276385855194 | validation: 0.07436071586620366]
	TIME [epoch: 12.6 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463234550585023		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.03463234550585023 | validation: 0.08287833355045646]
	TIME [epoch: 12.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03315728650637594		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.03315728650637594 | validation: 0.09422218614390312]
	TIME [epoch: 12.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037459194848368		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.04037459194848368 | validation: 0.08681164817564362]
	TIME [epoch: 12.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462921411879927		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.03462921411879927 | validation: 0.08119232965867564]
	TIME [epoch: 12.6 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653448118166696		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.03653448118166696 | validation: 0.07829865146687276]
	TIME [epoch: 12.6 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03246696363754722		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.03246696363754722 | validation: 0.08122698058627162]
	TIME [epoch: 12.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0329916687889948		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.0329916687889948 | validation: 0.07931630433721577]
	TIME [epoch: 12.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285870705433691		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.03285870705433691 | validation: 0.08059383654523067]
	TIME [epoch: 12.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03327241815909644		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.03327241815909644 | validation: 0.08407157934902458]
	TIME [epoch: 12.6 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034064678507032736		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.034064678507032736 | validation: 0.07076589228165141]
	TIME [epoch: 12.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031176379615992653		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.031176379615992653 | validation: 0.08943063002139968]
	TIME [epoch: 12.6 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058082320693917895		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.058082320693917895 | validation: 0.10107849445381074]
	TIME [epoch: 12.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06273241633017139		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.06273241633017139 | validation: 0.09293837210964535]
	TIME [epoch: 12.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0423305771633055		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.0423305771633055 | validation: 0.07207580665017568]
	TIME [epoch: 12.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032209421810637774		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.032209421810637774 | validation: 0.07278752090643477]
	TIME [epoch: 12.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034426600847822585		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.034426600847822585 | validation: 0.0764072862595287]
	TIME [epoch: 12.6 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03660843644095885		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.03660843644095885 | validation: 0.07909263214693169]
	TIME [epoch: 12.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566654591390939		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.03566654591390939 | validation: 0.06644509772184859]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1108.pth
	Model improved!!!
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0349778411629584		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.0349778411629584 | validation: 0.07445706247659196]
	TIME [epoch: 12.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03390158288368352		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.03390158288368352 | validation: 0.07308284484543358]
	TIME [epoch: 12.6 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032082671794746706		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.032082671794746706 | validation: 0.08365148611457673]
	TIME [epoch: 12.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03348141734387599		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03348141734387599 | validation: 0.08379325363326975]
	TIME [epoch: 12.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031042982249361088		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.031042982249361088 | validation: 0.08491335225691457]
	TIME [epoch: 12.6 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03280932960425608		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.03280932960425608 | validation: 0.07555455577584302]
	TIME [epoch: 12.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03150434876543879		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.03150434876543879 | validation: 0.07888364897236855]
	TIME [epoch: 12.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029652041470222153		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.029652041470222153 | validation: 0.06961871148834983]
	TIME [epoch: 12.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03236474307843886		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.03236474307843886 | validation: 0.07228542947265827]
	TIME [epoch: 12.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03327868752252627		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.03327868752252627 | validation: 0.0701670635013126]
	TIME [epoch: 12.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194760984270845		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.03194760984270845 | validation: 0.069149371449357]
	TIME [epoch: 12.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030978350972990898		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.030978350972990898 | validation: 0.08291772080191818]
	TIME [epoch: 12.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166211020974971		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.03166211020974971 | validation: 0.07858555728218142]
	TIME [epoch: 12.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03126909522342923		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.03126909522342923 | validation: 0.07404588741578647]
	TIME [epoch: 12.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030435003693787585		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.030435003693787585 | validation: 0.08366398255287241]
	TIME [epoch: 12.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242638698490413		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.03242638698490413 | validation: 0.07201448369417111]
	TIME [epoch: 12.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03236218729223017		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.03236218729223017 | validation: 0.0819263596158586]
	TIME [epoch: 12.6 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033443416248685624		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.033443416248685624 | validation: 0.08940684774477776]
	TIME [epoch: 12.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03196723260611739		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.03196723260611739 | validation: 0.06923649779093259]
	TIME [epoch: 12.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03164481575112279		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.03164481575112279 | validation: 0.07259389044596454]
	TIME [epoch: 12.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03263372527218485		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.03263372527218485 | validation: 0.07356616606360479]
	TIME [epoch: 12.6 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031263706205948934		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.031263706205948934 | validation: 0.0704029077973808]
	TIME [epoch: 12.6 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030933243151407625		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.030933243151407625 | validation: 0.06877899292207738]
	TIME [epoch: 12.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03301296562663551		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.03301296562663551 | validation: 0.07493702380290955]
	TIME [epoch: 12.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030632850118794002		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.030632850118794002 | validation: 0.07071513966657014]
	TIME [epoch: 12.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542494175655158		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.03542494175655158 | validation: 0.08168604036868277]
	TIME [epoch: 12.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033986932206826984		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.033986932206826984 | validation: 0.07093060931179011]
	TIME [epoch: 12.6 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202832056025408		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.03202832056025408 | validation: 0.07190232822412297]
	TIME [epoch: 12.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034152573553742606		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.034152573553742606 | validation: 0.07142346167087212]
	TIME [epoch: 12.6 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331756650936064		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.0331756650936064 | validation: 0.07630831647874639]
	TIME [epoch: 12.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03297823028678064		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.03297823028678064 | validation: 0.06257132377660178]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03413037436363636		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.03413037436363636 | validation: 0.06822580568407326]
	TIME [epoch: 12.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030713897947120834		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.030713897947120834 | validation: 0.07343950945720927]
	TIME [epoch: 12.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030814089946814047		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.030814089946814047 | validation: 0.06410573883737178]
	TIME [epoch: 12.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03111652023765652		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.03111652023765652 | validation: 0.07262190606684857]
	TIME [epoch: 12.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02786152755416599		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02786152755416599 | validation: 0.0780094796252335]
	TIME [epoch: 12.6 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372363041067194		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.04372363041067194 | validation: 0.07453125769695568]
	TIME [epoch: 12.6 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897467801024312		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.03897467801024312 | validation: 0.08007671261150272]
	TIME [epoch: 12.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03210299272053192		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.03210299272053192 | validation: 0.07072376820421455]
	TIME [epoch: 12.6 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030342122077808		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.030342122077808 | validation: 0.06654252059012054]
	TIME [epoch: 12.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030871153750890574		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.030871153750890574 | validation: 0.07376802619917959]
	TIME [epoch: 12.6 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030336877046501107		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.030336877046501107 | validation: 0.07862197079709918]
	TIME [epoch: 12.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02878221094067363		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.02878221094067363 | validation: 0.07480019282522728]
	TIME [epoch: 12.6 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03087889319411322		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.03087889319411322 | validation: 0.06941955470433107]
	TIME [epoch: 12.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03106437967355838		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.03106437967355838 | validation: 0.07316541156052926]
	TIME [epoch: 12.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02900357852374467		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.02900357852374467 | validation: 0.08147677580292968]
	TIME [epoch: 12.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032911269408053626		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.032911269408053626 | validation: 0.07758633241120146]
	TIME [epoch: 12.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04182077644304501		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04182077644304501 | validation: 0.0756878094176113]
	TIME [epoch: 12.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048517778370176415		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.048517778370176415 | validation: 0.07864952492943375]
	TIME [epoch: 12.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039050031815479044		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.039050031815479044 | validation: 0.0678493311242745]
	TIME [epoch: 12.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032886643186284134		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.032886643186284134 | validation: 0.07149347781246132]
	TIME [epoch: 12.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241670998598243		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.03241670998598243 | validation: 0.06745154301872221]
	TIME [epoch: 12.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03182118691993598		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.03182118691993598 | validation: 0.07583854530174805]
	TIME [epoch: 12.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03218491633186816		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.03218491633186816 | validation: 0.07863387040748926]
	TIME [epoch: 12.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0305826238035527		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.0305826238035527 | validation: 0.07224135791762912]
	TIME [epoch: 12.6 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031661078358801616		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.031661078358801616 | validation: 0.07346623963169184]
	TIME [epoch: 12.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031122125313502027		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.031122125313502027 | validation: 0.06557956546001274]
	TIME [epoch: 12.6 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338364381891613		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.03338364381891613 | validation: 0.07041162999439869]
	TIME [epoch: 12.6 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030803868766143806		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.030803868766143806 | validation: 0.07219805265698924]
	TIME [epoch: 12.6 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029730004441520533		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.029730004441520533 | validation: 0.07400880993253021]
	TIME [epoch: 12.6 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030649765585063733		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.030649765585063733 | validation: 0.08329744404125317]
	TIME [epoch: 12.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168344646833455		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.03168344646833455 | validation: 0.07296085878155205]
	TIME [epoch: 12.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02989230440083063		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.02989230440083063 | validation: 0.07805093958417106]
	TIME [epoch: 12.6 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032689810617873745		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.032689810617873745 | validation: 0.0749179927427803]
	TIME [epoch: 12.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02910273040144149		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.02910273040144149 | validation: 0.07444944044669424]
	TIME [epoch: 12.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030276278193047266		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.030276278193047266 | validation: 0.08443087877050984]
	TIME [epoch: 12.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030264688766547424		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.030264688766547424 | validation: 0.07068867689934015]
	TIME [epoch: 12.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03149184788598078		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.03149184788598078 | validation: 0.08022278756576708]
	TIME [epoch: 12.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02953151135415566		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.02953151135415566 | validation: 0.07522438475140358]
	TIME [epoch: 12.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028756578706718043		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.028756578706718043 | validation: 0.08086522021304304]
	TIME [epoch: 12.6 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187457102115812		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.03187457102115812 | validation: 0.0753126599331354]
	TIME [epoch: 12.6 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030839863334511417		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.030839863334511417 | validation: 0.08422139647367355]
	TIME [epoch: 12.6 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028968704387671754		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.028968704387671754 | validation: 0.07405021053476223]
	TIME [epoch: 12.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030847833718875416		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.030847833718875416 | validation: 0.07775496624950541]
	TIME [epoch: 12.6 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02941052974674138		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.02941052974674138 | validation: 0.06910474949958273]
	TIME [epoch: 12.6 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030478822191069056		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.030478822191069056 | validation: 0.07743799742938853]
	TIME [epoch: 12.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031030154805471843		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.031030154805471843 | validation: 0.06930187373350231]
	TIME [epoch: 12.6 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03032375710072402		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.03032375710072402 | validation: 0.06116738957731878]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1186.pth
	Model improved!!!
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031388481579982364		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.031388481579982364 | validation: 0.07740089020225116]
	TIME [epoch: 12.6 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03163796685813046		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.03163796685813046 | validation: 0.07922444578249137]
	TIME [epoch: 12.6 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028954859038314823		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.028954859038314823 | validation: 0.07188294030871041]
	TIME [epoch: 12.6 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02946857050871489		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.02946857050871489 | validation: 0.07124391589313571]
	TIME [epoch: 12.6 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02990065913612603		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.02990065913612603 | validation: 0.07978324847022158]
	TIME [epoch: 12.6 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180054120505532		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.03180054120505532 | validation: 0.0682240462589399]
	TIME [epoch: 12.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03073571984080476		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.03073571984080476 | validation: 0.0755691458312707]
	TIME [epoch: 12.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03329047475934203		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.03329047475934203 | validation: 0.07150431494974825]
	TIME [epoch: 12.6 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030857171445848063		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.030857171445848063 | validation: 0.0646476748109931]
	TIME [epoch: 12.6 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02760339492680543		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.02760339492680543 | validation: 0.06854035423620652]
	TIME [epoch: 12.6 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034817642887713655		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.034817642887713655 | validation: 0.07312331436803875]
	TIME [epoch: 12.6 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030432184735228202		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.030432184735228202 | validation: 0.07183079293894433]
	TIME [epoch: 12.6 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03046520154610433		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.03046520154610433 | validation: 0.07407681769758916]
	TIME [epoch: 12.6 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178579465748227		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.03178579465748227 | validation: 0.06423367663005244]
	TIME [epoch: 12.6 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029574086967455688		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.029574086967455688 | validation: 0.06250135065981888]
	TIME [epoch: 12.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02876200682676985		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02876200682676985 | validation: 0.07582932979329271]
	TIME [epoch: 12.6 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02976198611295871		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.02976198611295871 | validation: 0.0719765314275862]
	TIME [epoch: 12.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03188372477790324		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.03188372477790324 | validation: 0.06682979792904772]
	TIME [epoch: 12.6 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02924955180567767		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.02924955180567767 | validation: 0.07522816080742376]
	TIME [epoch: 12.6 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03054885433897152		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.03054885433897152 | validation: 0.0660035671731706]
	TIME [epoch: 12.6 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03006358968090685		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.03006358968090685 | validation: 0.06832231126871517]
	TIME [epoch: 12.6 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03015166599727478		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.03015166599727478 | validation: 0.08381177951712242]
	TIME [epoch: 12.6 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028952838710038495		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.028952838710038495 | validation: 0.0727263499888578]
	TIME [epoch: 12.6 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032289845254262116		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.032289845254262116 | validation: 0.07260426111682679]
	TIME [epoch: 12.6 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028225807770333393		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.028225807770333393 | validation: 0.06995018967892495]
	TIME [epoch: 12.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02878371867814208		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.02878371867814208 | validation: 0.0702531009427464]
	TIME [epoch: 12.6 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02908475734820951		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.02908475734820951 | validation: 0.06869635979226119]
	TIME [epoch: 12.6 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029182432504983386		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.029182432504983386 | validation: 0.06835788083247342]
	TIME [epoch: 12.6 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028665073218826453		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.028665073218826453 | validation: 0.08877155407313135]
	TIME [epoch: 12.6 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041914280461460986		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.041914280461460986 | validation: 0.06806549770168352]
	TIME [epoch: 12.6 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032516725617028376		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.032516725617028376 | validation: 0.07218375101952168]
	TIME [epoch: 12.6 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02757849540586933		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.02757849540586933 | validation: 0.0759995555876535]
	TIME [epoch: 12.6 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030016692492412903		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.030016692492412903 | validation: 0.07225112376761862]
	TIME [epoch: 12.6 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030428493384138316		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.030428493384138316 | validation: 0.07064104190017038]
	TIME [epoch: 12.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030746281140077745		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.030746281140077745 | validation: 0.07090785297035943]
	TIME [epoch: 12.6 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030411863948349875		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.030411863948349875 | validation: 0.05825639932829923]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028252389300219113		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.028252389300219113 | validation: 0.06465905925589553]
	TIME [epoch: 12.6 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029688108670233124		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.029688108670233124 | validation: 0.06499008136472616]
	TIME [epoch: 12.6 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028118750292970476		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.028118750292970476 | validation: 0.07012690403887481]
	TIME [epoch: 12.6 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029129984646704833		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.029129984646704833 | validation: 0.06761687022921388]
	TIME [epoch: 12.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028951725112816844		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.028951725112816844 | validation: 0.06746836696311452]
	TIME [epoch: 12.6 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029207886473300543		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.029207886473300543 | validation: 0.07046524288562404]
	TIME [epoch: 12.6 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02861799362386891		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.02861799362386891 | validation: 0.06272303921609754]
	TIME [epoch: 12.6 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02924745470849391		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02924745470849391 | validation: 0.06643629495257594]
	TIME [epoch: 12.6 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03062653570558507		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.03062653570558507 | validation: 0.06698320335241262]
	TIME [epoch: 12.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028631153917071193		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.028631153917071193 | validation: 0.06746593187838652]
	TIME [epoch: 12.6 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02933089615066991		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.02933089615066991 | validation: 0.06621969275236438]
	TIME [epoch: 12.6 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028873597049849868		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.028873597049849868 | validation: 0.0641648652938896]
	TIME [epoch: 12.6 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030658212140280697		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.030658212140280697 | validation: 0.06952060238271839]
	TIME [epoch: 12.6 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028826303909556775		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.028826303909556775 | validation: 0.06075335241479819]
	TIME [epoch: 12.6 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031211391052477923		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.031211391052477923 | validation: 0.06925807605515562]
	TIME [epoch: 12.6 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02827323445413411		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.02827323445413411 | validation: 0.06130387553032225]
	TIME [epoch: 12.6 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027457938651675937		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.027457938651675937 | validation: 0.06509591162560094]
	TIME [epoch: 12.6 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028280727280324058		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.028280727280324058 | validation: 0.06947424412385067]
	TIME [epoch: 12.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028555251784082664		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.028555251784082664 | validation: 0.06367668979414885]
	TIME [epoch: 12.6 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02700455030470839		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02700455030470839 | validation: 0.07955113986149831]
	TIME [epoch: 12.6 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030523360050031		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.04030523360050031 | validation: 0.08646654255720773]
	TIME [epoch: 12.6 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03730125787173146		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.03730125787173146 | validation: 0.07415947091982397]
	TIME [epoch: 12.6 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02964114750228548		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02964114750228548 | validation: 0.06107856426488204]
	TIME [epoch: 12.6 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027482393826420617		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.027482393826420617 | validation: 0.06796993007042078]
	TIME [epoch: 12.6 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03056430068524516		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.03056430068524516 | validation: 0.06860191581934863]
	TIME [epoch: 12.6 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026775989063131868		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.026775989063131868 | validation: 0.06374454858424443]
	TIME [epoch: 12.6 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02681769520749028		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.02681769520749028 | validation: 0.06792236375131029]
	TIME [epoch: 12.6 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028147281784574974		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.028147281784574974 | validation: 0.07482312387630466]
	TIME [epoch: 12.6 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030138064833269366		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.030138064833269366 | validation: 0.0561124879643034]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1251.pth
	Model improved!!!
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027964227039149466		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.027964227039149466 | validation: 0.06136349311652051]
	TIME [epoch: 12.6 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028116992071529765		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.028116992071529765 | validation: 0.06565042938163118]
	TIME [epoch: 12.6 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028251968659485597		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.028251968659485597 | validation: 0.06756519696997955]
	TIME [epoch: 12.6 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02697888840819369		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.02697888840819369 | validation: 0.06360933437174036]
	TIME [epoch: 12.6 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027540920199583194		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.027540920199583194 | validation: 0.055201262485551794]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1256.pth
	Model improved!!!
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027652396760893767		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.027652396760893767 | validation: 0.05941854862293]
	TIME [epoch: 12.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03548449678999127		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.03548449678999127 | validation: 0.05789320122476113]
	TIME [epoch: 12.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166935575620844		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.03166935575620844 | validation: 0.06466587490620775]
	TIME [epoch: 12.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028138795331802614		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.028138795331802614 | validation: 0.06115974127697639]
	TIME [epoch: 12.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030351650298140822		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.030351650298140822 | validation: 0.05761021293403967]
	TIME [epoch: 12.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02696253757538421		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02696253757538421 | validation: 0.06621805523902309]
	TIME [epoch: 12.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027928289617014223		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.027928289617014223 | validation: 0.05755292386664867]
	TIME [epoch: 12.7 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02967345066787588		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.02967345066787588 | validation: 0.06115124236110843]
	TIME [epoch: 12.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028561984087323914		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.028561984087323914 | validation: 0.0680122557806976]
	TIME [epoch: 12.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027855759397590774		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.027855759397590774 | validation: 0.06152077824754031]
	TIME [epoch: 12.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0269503294875903		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.0269503294875903 | validation: 0.061891898387090194]
	TIME [epoch: 12.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02850265420037176		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.02850265420037176 | validation: 0.0546287054486862]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1268.pth
	Model improved!!!
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035147544998252414		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.035147544998252414 | validation: 0.05911134138735152]
	TIME [epoch: 12.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032461358184380666		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.032461358184380666 | validation: 0.05811226904829619]
	TIME [epoch: 12.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030042735716474842		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.030042735716474842 | validation: 0.07099845422042564]
	TIME [epoch: 12.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028521141831129145		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.028521141831129145 | validation: 0.06032372864972522]
	TIME [epoch: 12.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029548987406569038		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.029548987406569038 | validation: 0.07616489437191758]
	TIME [epoch: 12.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026133121223210924		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.026133121223210924 | validation: 0.06504507361318988]
	TIME [epoch: 12.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0294813659814775		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.0294813659814775 | validation: 0.060534431725720865]
	TIME [epoch: 12.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026956640284488846		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.026956640284488846 | validation: 0.062077045978089565]
	TIME [epoch: 12.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029095336721433763		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.029095336721433763 | validation: 0.06610628371031975]
	TIME [epoch: 12.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026852629900668654		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.026852629900668654 | validation: 0.06159019017744615]
	TIME [epoch: 12.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026504377225551247		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.026504377225551247 | validation: 0.06057157829459105]
	TIME [epoch: 12.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028422571476095993		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.028422571476095993 | validation: 0.057915807656868004]
	TIME [epoch: 12.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02679047811599263		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.02679047811599263 | validation: 0.06505497150140248]
	TIME [epoch: 12.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02729111791718724		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.02729111791718724 | validation: 0.05917392516905186]
	TIME [epoch: 12.7 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028894530028405106		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.028894530028405106 | validation: 0.06363638064910936]
	TIME [epoch: 12.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026321884963805477		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.026321884963805477 | validation: 0.056588837510010516]
	TIME [epoch: 12.7 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02964027327630868		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.02964027327630868 | validation: 0.055639162220821874]
	TIME [epoch: 12.6 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030161471044245436		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.030161471044245436 | validation: 0.05722091418499467]
	TIME [epoch: 12.6 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028061150844812487		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.028061150844812487 | validation: 0.058397629811280063]
	TIME [epoch: 12.6 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027979544049485678		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.027979544049485678 | validation: 0.06663394083510119]
	TIME [epoch: 12.6 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028484785581797872		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.028484785581797872 | validation: 0.08540257869317033]
	TIME [epoch: 12.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498378744558371		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.03498378744558371 | validation: 0.08804101982736767]
	TIME [epoch: 12.6 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03251998848879252		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.03251998848879252 | validation: 0.0660318929340909]
	TIME [epoch: 12.6 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03025353290490303		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.03025353290490303 | validation: 0.06718291530243507]
	TIME [epoch: 12.6 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026969758736121834		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.026969758736121834 | validation: 0.06377812977978796]
	TIME [epoch: 12.7 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02672926557694976		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.02672926557694976 | validation: 0.06355975965743135]
	TIME [epoch: 12.6 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026908714412948384		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.026908714412948384 | validation: 0.06445459585920486]
	TIME [epoch: 12.6 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0267661947160998		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.0267661947160998 | validation: 0.07519485935464296]
	TIME [epoch: 12.6 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036783340129792556		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.036783340129792556 | validation: 0.07739975425206624]
	TIME [epoch: 12.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037794301887361756		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.037794301887361756 | validation: 0.06279579865184154]
	TIME [epoch: 12.6 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03077346574964218		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.03077346574964218 | validation: 0.06069066656041085]
	TIME [epoch: 12.6 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02440701197658436		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.02440701197658436 | validation: 0.0641545187703007]
	TIME [epoch: 12.6 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028710935751174775		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.028710935751174775 | validation: 0.05762802884831474]
	TIME [epoch: 12.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027426537253955337		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.027426537253955337 | validation: 0.05826293424488055]
	TIME [epoch: 12.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02973666898533965		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.02973666898533965 | validation: 0.06526930124320403]
	TIME [epoch: 12.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02625276295301534		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.02625276295301534 | validation: 0.05468929266732939]
	TIME [epoch: 12.6 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02856444334305122		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.02856444334305122 | validation: 0.06308315199959302]
	TIME [epoch: 12.6 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029279330223527692		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.029279330223527692 | validation: 0.06790512853570958]
	TIME [epoch: 12.6 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025706080416399286		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.025706080416399286 | validation: 0.06394274163865783]
	TIME [epoch: 12.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026136949483600944		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.026136949483600944 | validation: 0.07129885069286332]
	TIME [epoch: 12.6 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027828736150570795		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.027828736150570795 | validation: 0.0599088030130605]
	TIME [epoch: 12.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028418065007465822		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.028418065007465822 | validation: 0.06517275773255723]
	TIME [epoch: 12.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02657423411402921		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.02657423411402921 | validation: 0.06561754079903591]
	TIME [epoch: 12.6 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027954432167667922		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.027954432167667922 | validation: 0.06063196843920873]
	TIME [epoch: 12.6 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026971030531992214		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.026971030531992214 | validation: 0.0674997652681779]
	TIME [epoch: 12.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02809500548366658		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.02809500548366658 | validation: 0.0698167597099017]
	TIME [epoch: 12.6 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025999955492571773		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.025999955492571773 | validation: 0.05628768293009793]
	TIME [epoch: 12.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024538826683068126		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.024538826683068126 | validation: 0.06068112309923274]
	TIME [epoch: 12.6 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026061131015407822		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.026061131015407822 | validation: 0.06833395708915284]
	TIME [epoch: 12.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027633288288235195		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.027633288288235195 | validation: 0.07148713546411183]
	TIME [epoch: 12.6 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02841396079983106		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.02841396079983106 | validation: 0.06407966122081207]
	TIME [epoch: 12.7 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03287770949439278		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.03287770949439278 | validation: 0.05424146254003085]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1320.pth
	Model improved!!!
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026319164429336586		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.026319164429336586 | validation: 0.06245735013992442]
	TIME [epoch: 12.6 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030165217204421		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.03030165217204421 | validation: 0.06822449643221813]
	TIME [epoch: 12.6 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02746289214595525		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02746289214595525 | validation: 0.056519767302208195]
	TIME [epoch: 12.6 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0267605965006488		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.0267605965006488 | validation: 0.06629477604832733]
	TIME [epoch: 12.6 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02744544485175174		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.02744544485175174 | validation: 0.06287737471697354]
	TIME [epoch: 12.6 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0281286318368992		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.0281286318368992 | validation: 0.06551059539963056]
	TIME [epoch: 12.6 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026821479277924744		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.026821479277924744 | validation: 0.06256085890218174]
	TIME [epoch: 12.6 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027247427505355422		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.027247427505355422 | validation: 0.05103499235095776]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1328.pth
	Model improved!!!
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02617628358181087		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.02617628358181087 | validation: 0.0573190380607705]
	TIME [epoch: 12.6 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026896524550723645		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.026896524550723645 | validation: 0.06737528686050981]
	TIME [epoch: 12.6 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852948042321535		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.02852948042321535 | validation: 0.06088182984921864]
	TIME [epoch: 12.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02826197321721844		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.02826197321721844 | validation: 0.061438470339709145]
	TIME [epoch: 12.6 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029075517408252136		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.029075517408252136 | validation: 0.06988485740500923]
	TIME [epoch: 12.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025739098576986967		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.025739098576986967 | validation: 0.059696865154829616]
	TIME [epoch: 12.6 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024376499010566845		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.024376499010566845 | validation: 0.05458268254871595]
	TIME [epoch: 12.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027817163622247017		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.027817163622247017 | validation: 0.06819029634913754]
	TIME [epoch: 12.6 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02855282574287517		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.02855282574287517 | validation: 0.0681402195222145]
	TIME [epoch: 12.6 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027142888777535595		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.027142888777535595 | validation: 0.06824363330136796]
	TIME [epoch: 12.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027134225593913597		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.027134225593913597 | validation: 0.05868285405783755]
	TIME [epoch: 12.6 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02702470848752046		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.02702470848752046 | validation: 0.07150493330092104]
	TIME [epoch: 12.6 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027243872579758514		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.027243872579758514 | validation: 0.06510760209009142]
	TIME [epoch: 12.6 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030805581008226134		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.030805581008226134 | validation: 0.06511465773330184]
	TIME [epoch: 12.6 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03006376911794552		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.03006376911794552 | validation: 0.05874297294696214]
	TIME [epoch: 12.6 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026751471833245978		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.026751471833245978 | validation: 0.05833573972277878]
	TIME [epoch: 12.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0267687205015124		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.0267687205015124 | validation: 0.059844152838657495]
	TIME [epoch: 12.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027919141986941877		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.027919141986941877 | validation: 0.06662293172545565]
	TIME [epoch: 12.6 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024816204404840017		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.024816204404840017 | validation: 0.06398086228631039]
	TIME [epoch: 12.6 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02720693038871509		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02720693038871509 | validation: 0.06126609814932452]
	TIME [epoch: 12.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027644372699851996		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.027644372699851996 | validation: 0.0663783489274646]
	TIME [epoch: 12.6 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026348543276926577		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.026348543276926577 | validation: 0.06285021509846904]
	TIME [epoch: 12.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026567283480846342		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.026567283480846342 | validation: 0.06258749903335939]
	TIME [epoch: 12.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030874804998249156		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.030874804998249156 | validation: 0.05495573934818019]
	TIME [epoch: 12.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03014584169701017		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.03014584169701017 | validation: 0.059030150295680955]
	TIME [epoch: 12.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027883003807737252		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.027883003807737252 | validation: 0.0632686670443402]
	TIME [epoch: 12.6 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027329059040088165		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.027329059040088165 | validation: 0.06129221556215684]
	TIME [epoch: 12.6 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03054662562875647		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.03054662562875647 | validation: 0.06510792805939133]
	TIME [epoch: 12.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027176386014408367		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.027176386014408367 | validation: 0.0705417952272672]
	TIME [epoch: 12.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026843141423837816		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.026843141423837816 | validation: 0.05797880552467876]
	TIME [epoch: 12.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025051050198698275		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.025051050198698275 | validation: 0.053895593925118696]
	TIME [epoch: 12.6 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025556353330854542		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.025556353330854542 | validation: 0.0774061795777649]
	TIME [epoch: 12.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02603644065856701		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.02603644065856701 | validation: 0.06071951038056013]
	TIME [epoch: 12.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811194768186406		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.02811194768186406 | validation: 0.06561783505087908]
	TIME [epoch: 12.6 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023412397599701578		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.023412397599701578 | validation: 0.05796800482826478]
	TIME [epoch: 12.6 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02676777685669654		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.02676777685669654 | validation: 0.05601279155128091]
	TIME [epoch: 12.6 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025672063466233696		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.025672063466233696 | validation: 0.06098557532489795]
	TIME [epoch: 12.6 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02630863547004796		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02630863547004796 | validation: 0.06487043644229155]
	TIME [epoch: 12.6 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027191662268616273		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.027191662268616273 | validation: 0.06155717277331041]
	TIME [epoch: 12.6 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028347439251867557		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.028347439251867557 | validation: 0.071759144235696]
	TIME [epoch: 12.6 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024530035809469157		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.024530035809469157 | validation: 0.06398402768366161]
	TIME [epoch: 12.6 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022900417846284295		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.022900417846284295 | validation: 0.05750165901435081]
	TIME [epoch: 12.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02466301637674503		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.02466301637674503 | validation: 0.05725377751117292]
	TIME [epoch: 12.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025240789614002834		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.025240789614002834 | validation: 0.054290745340535775]
	TIME [epoch: 12.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026274978563818278		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.026274978563818278 | validation: 0.06919791433854884]
	TIME [epoch: 12.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027827322618775886		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.027827322618775886 | validation: 0.06312793711491232]
	TIME [epoch: 12.6 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02890271918864351		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.02890271918864351 | validation: 0.07272302334215956]
	TIME [epoch: 12.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025786951802688442		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.025786951802688442 | validation: 0.06679024599130766]
	TIME [epoch: 12.6 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02506992943473786		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.02506992943473786 | validation: 0.06431154843702379]
	TIME [epoch: 12.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02576956816783035		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.02576956816783035 | validation: 0.06127459047645672]
	TIME [epoch: 12.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02650259996090687		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.02650259996090687 | validation: 0.06731650796276593]
	TIME [epoch: 12.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024282469528947664		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.024282469528947664 | validation: 0.07401119931284066]
	TIME [epoch: 12.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026925241448348496		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.026925241448348496 | validation: 0.06466275933894779]
	TIME [epoch: 12.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028317096842463228		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.028317096842463228 | validation: 0.05724547387772999]
	TIME [epoch: 12.6 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03293642752914042		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.03293642752914042 | validation: 0.05441539188328385]
	TIME [epoch: 12.6 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03266982324389518		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.03266982324389518 | validation: 0.05588537593152794]
	TIME [epoch: 12.6 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03500667846204905		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.03500667846204905 | validation: 0.05908559248984677]
	TIME [epoch: 12.6 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028140388443199802		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.028140388443199802 | validation: 0.056992054858491674]
	TIME [epoch: 12.6 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029380263666109366		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.029380263666109366 | validation: 0.06599055093978828]
	TIME [epoch: 12.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026105747954770945		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.026105747954770945 | validation: 0.053062749139005605]
	TIME [epoch: 12.6 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02893024671770566		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.02893024671770566 | validation: 0.05965084026689711]
	TIME [epoch: 12.6 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02723106607592264		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.02723106607592264 | validation: 0.05608666063729596]
	TIME [epoch: 12.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02561213512151842		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.02561213512151842 | validation: 0.060099273591215634]
	TIME [epoch: 12.6 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026722644746031636		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.026722644746031636 | validation: 0.06731764003910973]
	TIME [epoch: 12.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024427728390853238		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.024427728390853238 | validation: 0.06789362618977755]
	TIME [epoch: 12.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023999583537045992		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.023999583537045992 | validation: 0.07229787120843643]
	TIME [epoch: 12.6 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026044701745046677		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.026044701745046677 | validation: 0.06168710938412985]
	TIME [epoch: 12.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024960204763820784		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.024960204763820784 | validation: 0.057097713529974205]
	TIME [epoch: 12.6 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02440769952970972		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.02440769952970972 | validation: 0.06010799003714656]
	TIME [epoch: 12.6 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026130386652551626		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.026130386652551626 | validation: 0.06124097300424369]
	TIME [epoch: 12.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02268943327658527		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.02268943327658527 | validation: 0.05005655765764581]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1399.pth
	Model improved!!!
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02410801067991125		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.02410801067991125 | validation: 0.06272512880402552]
	TIME [epoch: 12.6 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025448325198284914		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.025448325198284914 | validation: 0.06504991737423124]
	TIME [epoch: 12.5 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02472798329252731		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02472798329252731 | validation: 0.04926436577297351]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1402.pth
	Model improved!!!
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02474405042335528		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.02474405042335528 | validation: 0.06370683015774234]
	TIME [epoch: 12.5 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024512745694194794		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.024512745694194794 | validation: 0.05625123532215129]
	TIME [epoch: 12.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02468009417339486		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.02468009417339486 | validation: 0.060056588158996185]
	TIME [epoch: 12.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027282037675837083		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.027282037675837083 | validation: 0.06054661712008008]
	TIME [epoch: 12.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024956594778610456		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.024956594778610456 | validation: 0.06245791666584444]
	TIME [epoch: 12.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026605080176123872		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.026605080176123872 | validation: 0.06296780854522085]
	TIME [epoch: 12.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02441659292770659		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.02441659292770659 | validation: 0.056059899289806536]
	TIME [epoch: 12.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02564994132509173		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.02564994132509173 | validation: 0.06688055860867385]
	TIME [epoch: 12.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023234851743237757		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.023234851743237757 | validation: 0.06217416614995322]
	TIME [epoch: 12.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028551113338373027		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.028551113338373027 | validation: 0.055313970935109175]
	TIME [epoch: 12.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029443975847899335		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.029443975847899335 | validation: 0.06207928355989604]
	TIME [epoch: 12.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02671574565544118		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.02671574565544118 | validation: 0.05352550471677795]
	TIME [epoch: 12.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026124744120606426		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.026124744120606426 | validation: 0.06346331132632736]
	TIME [epoch: 12.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02556722234666501		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.02556722234666501 | validation: 0.07064187053255243]
	TIME [epoch: 12.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0258834256177396		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.0258834256177396 | validation: 0.06383337258947751]
	TIME [epoch: 12.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02435285196986957		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.02435285196986957 | validation: 0.057279581549537655]
	TIME [epoch: 12.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026784999104951693		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.026784999104951693 | validation: 0.05809191037302763]
	TIME [epoch: 12.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026129109571484677		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.026129109571484677 | validation: 0.054824639838525424]
	TIME [epoch: 12.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02607994634995316		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.02607994634995316 | validation: 0.06230279814152966]
	TIME [epoch: 12.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025300954808627044		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.025300954808627044 | validation: 0.054016346811417495]
	TIME [epoch: 12.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02610496735113021		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.02610496735113021 | validation: 0.05678973880898911]
	TIME [epoch: 12.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025456006426271147		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.025456006426271147 | validation: 0.06250621505896889]
	TIME [epoch: 12.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025684728747797828		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.025684728747797828 | validation: 0.05905975762506373]
	TIME [epoch: 12.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02455114602970998		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.02455114602970998 | validation: 0.06081055993996677]
	TIME [epoch: 12.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027307056976969238		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.027307056976969238 | validation: 0.06332900684706996]
	TIME [epoch: 12.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02724198053807005		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.02724198053807005 | validation: 0.06326094147365265]
	TIME [epoch: 12.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024636050187768214		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.024636050187768214 | validation: 0.06519943655551974]
	TIME [epoch: 12.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025073337105028137		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.025073337105028137 | validation: 0.06460219986565176]
	TIME [epoch: 12.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026871563322209715		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.026871563322209715 | validation: 0.0656256426841915]
	TIME [epoch: 12.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02718594859291613		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.02718594859291613 | validation: 0.07590210864976911]
	TIME [epoch: 12.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02671891661162551		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.02671891661162551 | validation: 0.06436169040678927]
	TIME [epoch: 12.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02409383134795779		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.02409383134795779 | validation: 0.05533442737660052]
	TIME [epoch: 12.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024543917919848775		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.024543917919848775 | validation: 0.06303934748157562]
	TIME [epoch: 12.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02537185601650087		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.02537185601650087 | validation: 0.058201764631886144]
	TIME [epoch: 12.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025270281895569503		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.025270281895569503 | validation: 0.06822649449660882]
	TIME [epoch: 12.5 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02612398860444232		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.02612398860444232 | validation: 0.05330842018606861]
	TIME [epoch: 12.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024766052037036176		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.024766052037036176 | validation: 0.0659069068077703]
	TIME [epoch: 12.6 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02554292569426429		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.02554292569426429 | validation: 0.06424368314775024]
	TIME [epoch: 12.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02619670900892158		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.02619670900892158 | validation: 0.06002003032558454]
	TIME [epoch: 12.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024905823202854154		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.024905823202854154 | validation: 0.05852058827309974]
	TIME [epoch: 12.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029235063397818704		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.029235063397818704 | validation: 0.051141808982702264]
	TIME [epoch: 12.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028905795773152356		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.028905795773152356 | validation: 0.05820443546798379]
	TIME [epoch: 12.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033208811520800435		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.033208811520800435 | validation: 0.06377079702103505]
	TIME [epoch: 12.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030260885464395332		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.030260885464395332 | validation: 0.06214632304576526]
	TIME [epoch: 12.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024696482673631667		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.024696482673631667 | validation: 0.07091051007206102]
	TIME [epoch: 12.5 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027050155831631062		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.027050155831631062 | validation: 0.07140881671458421]
	TIME [epoch: 12.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023282800201109424		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.023282800201109424 | validation: 0.059472257771079166]
	TIME [epoch: 12.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025004760393512834		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.025004760393512834 | validation: 0.05849618164482121]
	TIME [epoch: 12.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024784481821374667		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.024784481821374667 | validation: 0.06447090645849346]
	TIME [epoch: 12.5 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026198996789304287		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.026198996789304287 | validation: 0.06016607416273132]
	TIME [epoch: 12.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025593258824828293		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.025593258824828293 | validation: 0.06793473964284368]
	TIME [epoch: 12.5 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02367005500602937		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.02367005500602937 | validation: 0.05499071050631245]
	TIME [epoch: 12.5 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02281804265418203		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.02281804265418203 | validation: 0.05992246078373497]
	TIME [epoch: 12.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024567524970822312		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.024567524970822312 | validation: 0.05690630479422531]
	TIME [epoch: 12.5 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02813187923628924		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.02813187923628924 | validation: 0.06021803149214264]
	TIME [epoch: 12.5 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02572193210931488		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.02572193210931488 | validation: 0.06454271716273244]
	TIME [epoch: 12.5 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025043204153683326		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.025043204153683326 | validation: 0.05710768964539875]
	TIME [epoch: 12.5 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02684898913707742		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.02684898913707742 | validation: 0.0568298157837986]
	TIME [epoch: 12.5 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02357440370597294		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.02357440370597294 | validation: 0.06568801892721092]
	TIME [epoch: 12.5 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02479311624171743		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.02479311624171743 | validation: 0.06737168066138186]
	TIME [epoch: 12.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024745515455827816		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.024745515455827816 | validation: 0.07054920002901964]
	TIME [epoch: 12.5 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02655348343404064		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.02655348343404064 | validation: 0.0660579279776477]
	TIME [epoch: 12.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025472601676732046		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.025472601676732046 | validation: 0.06920557248313282]
	TIME [epoch: 12.5 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0257052552804716		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.0257052552804716 | validation: 0.06508170882115101]
	TIME [epoch: 12.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022755368261186324		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.022755368261186324 | validation: 0.0571816718299327]
	TIME [epoch: 12.5 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025055460963963792		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.025055460963963792 | validation: 0.05775239117848673]
	TIME [epoch: 12.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024760816803116256		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.024760816803116256 | validation: 0.06995360382321841]
	TIME [epoch: 12.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02403654872178878		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.02403654872178878 | validation: 0.063682409368394]
	TIME [epoch: 12.6 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024889881246072897		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.024889881246072897 | validation: 0.06864197458037034]
	TIME [epoch: 12.6 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022832662495438596		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.022832662495438596 | validation: 0.055230896920092455]
	TIME [epoch: 12.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02269876553548357		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.02269876553548357 | validation: 0.059394671661843625]
	TIME [epoch: 12.6 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027128184095194494		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.027128184095194494 | validation: 0.06214709992828348]
	TIME [epoch: 12.6 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023527714517868743		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.023527714517868743 | validation: 0.058225789624448215]
	TIME [epoch: 12.6 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024332175416735685		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.024332175416735685 | validation: 0.05525041006127197]
	TIME [epoch: 12.6 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02441061920722878		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.02441061920722878 | validation: 0.0669900774694948]
	TIME [epoch: 12.6 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023100145485387455		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.023100145485387455 | validation: 0.05920204449358066]
	TIME [epoch: 12.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0244661712909661		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.0244661712909661 | validation: 0.07185749555830188]
	TIME [epoch: 12.6 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02521637584643699		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.02521637584643699 | validation: 0.06254418525765583]
	TIME [epoch: 12.6 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02428119749689083		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.02428119749689083 | validation: 0.05807843826520875]
	TIME [epoch: 12.6 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025427059432292988		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.025427059432292988 | validation: 0.06230535682089843]
	TIME [epoch: 12.6 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024853567175328228		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.024853567175328228 | validation: 0.0642836358237041]
	TIME [epoch: 12.6 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022565435218170435		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.022565435218170435 | validation: 0.07264134668943602]
	TIME [epoch: 12.6 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028471040483598664		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.028471040483598664 | validation: 0.079491439603808]
	TIME [epoch: 12.6 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03420201741334249		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.03420201741334249 | validation: 0.07612417092882173]
	TIME [epoch: 12.6 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552350958499579		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.03552350958499579 | validation: 0.0715248148831587]
	TIME [epoch: 12.6 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02809514269082725		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.02809514269082725 | validation: 0.07138229993130685]
	TIME [epoch: 12.6 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02703925504608406		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.02703925504608406 | validation: 0.0624084577033365]
	TIME [epoch: 12.6 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025789639839793585		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.025789639839793585 | validation: 0.05594242728858029]
	TIME [epoch: 12.6 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025794634853679436		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.025794634853679436 | validation: 0.053624273405449646]
	TIME [epoch: 12.6 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025170006008530654		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.025170006008530654 | validation: 0.06356825897333143]
	TIME [epoch: 12.6 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025170853341574408		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.025170853341574408 | validation: 0.061397127120720145]
	TIME [epoch: 12.6 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023395993464729235		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.023395993464729235 | validation: 0.05790133987578187]
	TIME [epoch: 12.6 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024343268288691454		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.024343268288691454 | validation: 0.053116176708576206]
	TIME [epoch: 12.5 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02429627098350751		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.02429627098350751 | validation: 0.05671723896703891]
	TIME [epoch: 12.5 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023763898595571133		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.023763898595571133 | validation: 0.05687732463786948]
	TIME [epoch: 12.5 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02410528355566466		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.02410528355566466 | validation: 0.06412755336281406]
	TIME [epoch: 12.5 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02496239938824675		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.02496239938824675 | validation: 0.06603692560542128]
	TIME [epoch: 12.5 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02299836645762215		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.02299836645762215 | validation: 0.061165259280197816]
	TIME [epoch: 12.5 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023720723779498833		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.023720723779498833 | validation: 0.05798117479097164]
	TIME [epoch: 12.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02557092483625051		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.02557092483625051 | validation: 0.0670533808118403]
	TIME [epoch: 12.6 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024091548697572084		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.024091548697572084 | validation: 0.05497516734418576]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_153529/states/model_phi1_3b_v_mmd1_1503.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 11598.792 seconds.
