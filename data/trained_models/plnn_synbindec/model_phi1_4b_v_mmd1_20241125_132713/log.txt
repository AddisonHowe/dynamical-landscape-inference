Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1314334870

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.3913379539405515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3913379539405515 | validation: 5.096207161984448]
	TIME [epoch: 158 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.918297249677297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.918297249677297 | validation: 5.4717993277395705]
	TIME [epoch: 1.36 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.513871439359116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.513871439359116 | validation: 4.674001224613646]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.77308422403037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.77308422403037 | validation: 5.280386879538452]
	TIME [epoch: 1.35 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.397252223995072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.397252223995072 | validation: 4.930464545395811]
	TIME [epoch: 1.35 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.081857637146836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.081857637146836 | validation: 4.628541099929383]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.601657104132648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.601657104132648 | validation: 4.546822175616256]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.683502160162523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.683502160162523 | validation: 4.4257754588748055]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.697061269060664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.697061269060664 | validation: 4.481293520038736]
	TIME [epoch: 1.36 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.46895790879744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.46895790879744 | validation: 4.493725257532673]
	TIME [epoch: 1.35 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.438469540674342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.438469540674342 | validation: 4.295632768312458]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.414610792504063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.414610792504063 | validation: 4.376342818311972]
	TIME [epoch: 1.35 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.381830006355308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.381830006355308 | validation: 4.267749080284008]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3639407564910275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3639407564910275 | validation: 4.304812983862989]
	TIME [epoch: 1.35 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.278545247069044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.278545247069044 | validation: 4.150855416046826]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.233229220032842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.233229220032842 | validation: 4.17837957210992]
	TIME [epoch: 1.35 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1907733894071555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1907733894071555 | validation: 4.10421981469751]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.170640638177341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.170640638177341 | validation: 4.178724050087428]
	TIME [epoch: 1.35 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.150880628927086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.150880628927086 | validation: 4.027699959918626]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1631551318203535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1631551318203535 | validation: 4.087087410575648]
	TIME [epoch: 1.35 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.047566295313861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.047566295313861 | validation: 3.93400131776493]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.991483782888221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.991483782888221 | validation: 3.9847788784576155]
	TIME [epoch: 1.34 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.941504457323038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.941504457323038 | validation: 3.8912382384633446]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9101521720231713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9101521720231713 | validation: 4.027175518487366]
	TIME [epoch: 1.35 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.913570551300858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.913570551300858 | validation: 4.059296973807932]
	TIME [epoch: 1.43 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0761378642788335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0761378642788335 | validation: 3.9023816819669683]
	TIME [epoch: 1.35 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.832205156759526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.832205156759526 | validation: 3.896837154930884]
	TIME [epoch: 1.35 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7833059098426247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7833059098426247 | validation: 3.8265194444626114]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7825583385646833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7825583385646833 | validation: 3.9442212493596864]
	TIME [epoch: 1.35 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.805559798031209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.805559798031209 | validation: 3.869900989435368]
	TIME [epoch: 1.34 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.863259371492305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.863259371492305 | validation: 3.8229447017161764]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7154346548254025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7154346548254025 | validation: 3.7816056157531115]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.685329032683851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.685329032683851 | validation: 3.7442454733827293]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.676292118945831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.676292118945831 | validation: 3.7807596449248098]
	TIME [epoch: 1.35 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.665444085941639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.665444085941639 | validation: 3.724546796066507]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6671875285669095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6671875285669095 | validation: 3.7939474578093724]
	TIME [epoch: 1.35 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6714989792586126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6714989792586126 | validation: 3.770530228864918]
	TIME [epoch: 1.35 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.720539361265029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.720539361265029 | validation: 3.717561588411334]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.61975238363873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.61975238363873 | validation: 3.674066904388333]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5779267947764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5779267947764 | validation: 3.6488113422103425]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.555672584781118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.555672584781118 | validation: 3.6273177795624045]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5446099914250837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5446099914250837 | validation: 3.6129150573365836]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5357970290229175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5357970290229175 | validation: 3.622740044168463]
	TIME [epoch: 1.35 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5248498943407127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5248498943407127 | validation: 3.6208922705409594]
	TIME [epoch: 1.35 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.554626297954382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.554626297954382 | validation: 3.7024385576604377]
	TIME [epoch: 1.35 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.616757999434524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.616757999434524 | validation: 3.638150361764449]
	TIME [epoch: 1.35 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5956280604647963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5956280604647963 | validation: 3.5398363831283066]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4617829407038085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4617829407038085 | validation: 3.587811695124848]
	TIME [epoch: 1.35 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.497717567497041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497717567497041 | validation: 3.571168076206206]
	TIME [epoch: 1.34 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5002229843019403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5002229843019403 | validation: 3.501299306063901]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4288594272134483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4288594272134483 | validation: 3.4878223166029154]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.419725370968091		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.419725370968091 | validation: 3.4791743829186377]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.407165909708009		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.407165909708009 | validation: 3.4827589539920876]
	TIME [epoch: 1.37 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4068089058918303		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.4068089058918303 | validation: 3.469091008576465]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.394978893664817		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.394978893664817 | validation: 3.475778350920189]
	TIME [epoch: 1.35 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3941164682743317		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.3941164682743317 | validation: 3.445780395257291]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.380139641384965		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.380139641384965 | validation: 3.397254513706807]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3337634641783884		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.3337634641783884 | validation: 3.3815109572202178]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.309217690957201		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.309217690957201 | validation: 3.3596516210322136]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2834258505035168		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.2834258505035168 | validation: 3.3161035120494464]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2508692345147256		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.2508692345147256 | validation: 3.239896098488183]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.182624894905526		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.182624894905526 | validation: 3.06135444057175]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0338209381522625		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.0338209381522625 | validation: 3.070012553132253]
	TIME [epoch: 1.35 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8777729737983306		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8777729737983306 | validation: 2.6000784951955094]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.607330528995806		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.607330528995806 | validation: 2.1979093078595797]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0478562054871743		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.0478562054871743 | validation: 2.0300055573592433]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.874240508751316		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.874240508751316 | validation: 1.8215405279748322]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6921887875705084		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.6921887875705084 | validation: 1.4045945527901509]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2749849487729303		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.2749849487729303 | validation: 1.0264677767372326]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8976192566067185		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8976192566067185 | validation: 1.2712924262476553]
	TIME [epoch: 1.35 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1720557878568967		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.1720557878568967 | validation: 1.3983686718174553]
	TIME [epoch: 1.35 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4902151554675742		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.4902151554675742 | validation: 1.2565229215069107]
	TIME [epoch: 1.35 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1828432996981315		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.1828432996981315 | validation: 1.0601986062704387]
	TIME [epoch: 1.34 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258034963570394		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9258034963570394 | validation: 0.9795936569183887]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9932529533552037		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9932529533552037 | validation: 0.9453095337806826]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8295055398635325		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8295055398635325 | validation: 0.984040720393693]
	TIME [epoch: 1.35 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426291946586341		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8426291946586341 | validation: 0.8593224865782192]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033768605377045		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8033768605377045 | validation: 0.8419878156814271]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000006158647892		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.8000006158647892 | validation: 0.85499979187107]
	TIME [epoch: 1.35 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896122192138281		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7896122192138281 | validation: 0.8964043066083122]
	TIME [epoch: 1.35 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047185063584206		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8047185063584206 | validation: 0.8422011585754536]
	TIME [epoch: 1.35 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946089340645486		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7946089340645486 | validation: 0.8901458486682258]
	TIME [epoch: 1.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902006465464032		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.7902006465464032 | validation: 0.8619556768794765]
	TIME [epoch: 1.35 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819900927975618		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.7819900927975618 | validation: 0.8305269181315439]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7789547459358873		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7789547459358873 | validation: 0.8656061400760061]
	TIME [epoch: 1.35 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819049832647005		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.7819049832647005 | validation: 0.8478013211555432]
	TIME [epoch: 1.35 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791270247948282		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.7791270247948282 | validation: 0.8486068358883485]
	TIME [epoch: 1.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678516151395351		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7678516151395351 | validation: 0.8370099746745225]
	TIME [epoch: 1.34 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770749692003175		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.770749692003175 | validation: 0.8581701076134238]
	TIME [epoch: 1.35 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741001287713882		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7741001287713882 | validation: 0.8490885811896031]
	TIME [epoch: 1.34 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758476289182008		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7758476289182008 | validation: 0.8746884009822851]
	TIME [epoch: 1.34 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962610815991531		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.7962610815991531 | validation: 0.9984229711535676]
	TIME [epoch: 1.34 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853326995840459		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8853326995840459 | validation: 1.0116556572527404]
	TIME [epoch: 1.34 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448570146026797		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9448570146026797 | validation: 0.8786818886415159]
	TIME [epoch: 1.34 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662154568969572		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7662154568969572 | validation: 0.9670757964123979]
	TIME [epoch: 1.34 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034323425948898		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8034323425948898 | validation: 0.8797635038318428]
	TIME [epoch: 1.34 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206904029744864		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8206904029744864 | validation: 0.9682646175245223]
	TIME [epoch: 1.35 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837794778731977		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.7837794778731977 | validation: 0.8470058793952691]
	TIME [epoch: 1.35 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325997190124477		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8325997190124477 | validation: 1.1092635929935728]
	TIME [epoch: 1.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005452938775446		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9005452938775446 | validation: 0.8416634956702906]
	TIME [epoch: 1.35 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657666684328708		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7657666684328708 | validation: 0.8520763037662331]
	TIME [epoch: 1.35 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737897296031924		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7737897296031924 | validation: 0.9298028739575748]
	TIME [epoch: 1.35 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862748838595315		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7862748838595315 | validation: 0.8351643434873243]
	TIME [epoch: 1.35 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572430860180944		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7572430860180944 | validation: 0.8425761895374102]
	TIME [epoch: 1.35 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540676996864301		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7540676996864301 | validation: 0.9005582148106523]
	TIME [epoch: 1.35 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600483743305322		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7600483743305322 | validation: 0.835896833291182]
	TIME [epoch: 1.35 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693843391803682		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7693843391803682 | validation: 0.914887463084284]
	TIME [epoch: 1.35 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700496397795662		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.7700496397795662 | validation: 0.8764278682755523]
	TIME [epoch: 1.35 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933467463523515		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.7933467463523515 | validation: 1.0293035321727675]
	TIME [epoch: 1.35 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8147299771087981		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8147299771087981 | validation: 0.8897754060241442]
	TIME [epoch: 1.35 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288610721586201		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8288610721586201 | validation: 0.9348687702337838]
	TIME [epoch: 1.35 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7510703810086085		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.7510703810086085 | validation: 0.8813505368595973]
	TIME [epoch: 1.35 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528182283834133		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7528182283834133 | validation: 0.897209207414591]
	TIME [epoch: 1.35 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814446809127523		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7814446809127523 | validation: 0.8440025306310388]
	TIME [epoch: 1.35 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930852241207916		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7930852241207916 | validation: 1.074678317932032]
	TIME [epoch: 1.35 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569655786237429		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8569655786237429 | validation: 0.8459088649062529]
	TIME [epoch: 1.35 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757728443880824		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7757728443880824 | validation: 0.9343278914467366]
	TIME [epoch: 1.35 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536731494951028		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.7536731494951028 | validation: 0.8764874064441989]
	TIME [epoch: 1.35 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418484919620455		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7418484919620455 | validation: 0.8313978674686624]
	TIME [epoch: 1.35 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527939794697946		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.7527939794697946 | validation: 0.9322430969593883]
	TIME [epoch: 1.35 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767794045548992		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.767794045548992 | validation: 0.8297768713894259]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729476542993683		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7729476542993683 | validation: 0.9218925445452035]
	TIME [epoch: 1.35 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775152477927847		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7775152477927847 | validation: 0.9580928466468781]
	TIME [epoch: 1.35 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894659218646303		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.7894659218646303 | validation: 0.8978057049011755]
	TIME [epoch: 1.35 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207297073713542		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8207297073713542 | validation: 1.0427006163875387]
	TIME [epoch: 1.35 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764909023418953		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7764909023418953 | validation: 0.8342077803376905]
	TIME [epoch: 1.35 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483308779421219		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7483308779421219 | validation: 0.9167194330666408]
	TIME [epoch: 1.35 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523639995748161		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7523639995748161 | validation: 0.8691207293689089]
	TIME [epoch: 1.35 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75761659190778		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.75761659190778 | validation: 0.9217610457381756]
	TIME [epoch: 1.35 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680418677918474		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7680418677918474 | validation: 0.8518980514828709]
	TIME [epoch: 1.35 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772306670365659		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.772306670365659 | validation: 1.0368600380654711]
	TIME [epoch: 1.35 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812693888554347		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.812693888554347 | validation: 0.8413931638848837]
	TIME [epoch: 1.35 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765975739521777		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.765975739521777 | validation: 0.9770859544722804]
	TIME [epoch: 1.35 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607050703315625		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7607050703315625 | validation: 0.8373255432878578]
	TIME [epoch: 1.35 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564351952738446		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.7564351952738446 | validation: 0.9092436835116399]
	TIME [epoch: 1.35 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663534696237693		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7663534696237693 | validation: 0.8937488238913684]
	TIME [epoch: 1.35 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970482827332002		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7970482827332002 | validation: 0.9632887024172746]
	TIME [epoch: 1.35 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746631147523455		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7746631147523455 | validation: 0.8727653681557452]
	TIME [epoch: 1.35 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417471682780851		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7417471682780851 | validation: 0.8684767553163149]
	TIME [epoch: 1.35 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314604172510168		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.7314604172510168 | validation: 0.8618053337550606]
	TIME [epoch: 1.35 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340020173262346		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7340020173262346 | validation: 0.8865222977761434]
	TIME [epoch: 1.35 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376464738233716		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7376464738233716 | validation: 0.8299198375726423]
	TIME [epoch: 1.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595590014553116		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.7595590014553116 | validation: 1.0592413972221182]
	TIME [epoch: 1.35 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322859276283845		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8322859276283845 | validation: 0.8560194683389796]
	TIME [epoch: 1.35 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899421910101551		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7899421910101551 | validation: 0.8943627621258227]
	TIME [epoch: 1.35 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7508122355154139		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7508122355154139 | validation: 0.8971290755708949]
	TIME [epoch: 1.35 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362192071322804		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7362192071322804 | validation: 0.8424372599461908]
	TIME [epoch: 1.35 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995414497725183		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7995414497725183 | validation: 1.1015800634130863]
	TIME [epoch: 1.35 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814069688422069		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.814069688422069 | validation: 0.8178667696811254]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287748757167318		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7287748757167318 | validation: 0.8218073573628204]
	TIME [epoch: 1.35 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415409121916899		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7415409121916899 | validation: 0.9474515911217423]
	TIME [epoch: 1.35 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7508271657986025		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7508271657986025 | validation: 0.8604013864045158]
	TIME [epoch: 1.35 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379946445199257		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7379946445199257 | validation: 0.8499114109416097]
	TIME [epoch: 1.35 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376735496966009		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7376735496966009 | validation: 0.8837818375509155]
	TIME [epoch: 1.35 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424544660566699		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7424544660566699 | validation: 0.854450179353631]
	TIME [epoch: 1.35 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525684551257575		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7525684551257575 | validation: 0.9238516553806095]
	TIME [epoch: 1.35 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680317273168676		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7680317273168676 | validation: 0.911994796389919]
	TIME [epoch: 1.35 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712054927445827		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7712054927445827 | validation: 0.8365772159110922]
	TIME [epoch: 1.35 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575491159546266		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7575491159546266 | validation: 0.9389086744878266]
	TIME [epoch: 1.35 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448143794880994		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7448143794880994 | validation: 0.8164706304951951]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456970088847016		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7456970088847016 | validation: 0.9409427126268742]
	TIME [epoch: 1.35 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7369884931969876		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7369884931969876 | validation: 0.8185632899185145]
	TIME [epoch: 1.35 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424384253109463		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7424384253109463 | validation: 0.9166200035616516]
	TIME [epoch: 1.35 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743787423446137		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.743787423446137 | validation: 0.8482480877872952]
	TIME [epoch: 1.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362264459662684		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7362264459662684 | validation: 0.8657257885974263]
	TIME [epoch: 1.35 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467322809209586		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7467322809209586 | validation: 0.9199465828873873]
	TIME [epoch: 1.35 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739959212104464		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7739959212104464 | validation: 0.847595240621402]
	TIME [epoch: 1.35 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544242883672968		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7544242883672968 | validation: 0.9074171918039664]
	TIME [epoch: 1.35 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382965694303935		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.7382965694303935 | validation: 0.8161273501945993]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221102861057795		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7221102861057795 | validation: 0.8468014852168626]
	TIME [epoch: 1.35 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127652521779324		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7127652521779324 | validation: 0.8344708183014613]
	TIME [epoch: 1.35 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098614534794373		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7098614534794373 | validation: 0.7959903899599037]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209109458214624		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7209109458214624 | validation: 0.8616690399948923]
	TIME [epoch: 1.35 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293106482356205		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7293106482356205 | validation: 0.8402482430539767]
	TIME [epoch: 1.35 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613770634930452		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7613770634930452 | validation: 0.9613673302531037]
	TIME [epoch: 1.35 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235868512097005		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8235868512097005 | validation: 0.9223421273429735]
	TIME [epoch: 1.35 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643930681136192		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7643930681136192 | validation: 0.8351632370479586]
	TIME [epoch: 1.35 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168389910153937		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7168389910153937 | validation: 0.873259523273946]
	TIME [epoch: 1.35 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7189766166020783		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7189766166020783 | validation: 0.8418594925834066]
	TIME [epoch: 1.35 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169717084117704		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7169717084117704 | validation: 0.8215569256922631]
	TIME [epoch: 1.35 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220035294044393		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7220035294044393 | validation: 0.8086988538268314]
	TIME [epoch: 1.35 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083587526539707		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7083587526539707 | validation: 0.9192531620264716]
	TIME [epoch: 1.35 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314156456062864		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7314156456062864 | validation: 0.8001408709392254]
	TIME [epoch: 1.35 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785016242161689		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7785016242161689 | validation: 1.1313571595556824]
	TIME [epoch: 1.35 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113540602416497		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8113540602416497 | validation: 0.8173647127867842]
	TIME [epoch: 1.35 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153577746786224		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7153577746786224 | validation: 0.7940478141391929]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295801260434729		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7295801260434729 | validation: 0.8711335839014744]
	TIME [epoch: 1.35 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284207815462026		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7284207815462026 | validation: 0.8348872235471497]
	TIME [epoch: 1.35 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7067744346411703		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7067744346411703 | validation: 0.7917028157178305]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093155479835886		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7093155479835886 | validation: 0.8657637850366268]
	TIME [epoch: 1.35 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085146709689132		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7085146709689132 | validation: 0.8270577064659668]
	TIME [epoch: 1.35 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714215326494555		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.714215326494555 | validation: 0.8424884962981198]
	TIME [epoch: 1.35 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220663872921754		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7220663872921754 | validation: 0.8678029291819075]
	TIME [epoch: 1.35 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350414564360213		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7350414564360213 | validation: 0.8619582310193945]
	TIME [epoch: 1.35 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326069821667127		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7326069821667127 | validation: 0.9083530432858823]
	TIME [epoch: 1.35 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217139452297553		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7217139452297553 | validation: 0.8167001269452474]
	TIME [epoch: 1.35 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7090979677208387		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7090979677208387 | validation: 0.8654018629538691]
	TIME [epoch: 1.35 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004746712259725		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7004746712259725 | validation: 0.7713245785705182]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032455149617968		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7032455149617968 | validation: 0.9335830431384466]
	TIME [epoch: 1.35 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281539048112226		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7281539048112226 | validation: 0.7680591657341728]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751750806164344		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.751750806164344 | validation: 0.9236354074060156]
	TIME [epoch: 166 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112721034178902		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7112721034178902 | validation: 0.7777791245176378]
	TIME [epoch: 2.69 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831470475117044		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6831470475117044 | validation: 0.7862014260592215]
	TIME [epoch: 2.66 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682057027107266		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.682057027107266 | validation: 0.8097314631779456]
	TIME [epoch: 2.67 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779486018964442		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6779486018964442 | validation: 0.7921992082291297]
	TIME [epoch: 2.65 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776404506744357		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6776404506744357 | validation: 0.7671079304531363]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838591962396984		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6838591962396984 | validation: 0.8434855511553103]
	TIME [epoch: 2.68 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7408659045894918		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7408659045894918 | validation: 1.0173696673497272]
	TIME [epoch: 2.68 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869278500133143		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.869278500133143 | validation: 0.9216329771717601]
	TIME [epoch: 2.66 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286678253764848		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7286678253764848 | validation: 0.7802841997721441]
	TIME [epoch: 2.68 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032263999696722		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7032263999696722 | validation: 0.9295522867693755]
	TIME [epoch: 2.66 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988882852924461		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6988882852924461 | validation: 0.8159979503524533]
	TIME [epoch: 2.67 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796149252512416		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6796149252512416 | validation: 0.7518514452642546]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6770341085741721		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6770341085741721 | validation: 0.8933981035161622]
	TIME [epoch: 2.67 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830641720145725		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6830641720145725 | validation: 0.7257617526965346]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942887263647609		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6942887263647609 | validation: 0.9236296037039028]
	TIME [epoch: 2.67 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119303814941307		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7119303814941307 | validation: 0.7557575713631874]
	TIME [epoch: 2.66 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047867971204329		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7047867971204329 | validation: 0.8398617591134878]
	TIME [epoch: 2.68 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893805799919062		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6893805799919062 | validation: 0.7915766931625836]
	TIME [epoch: 2.67 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749115608074208		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6749115608074208 | validation: 0.7635369455323663]
	TIME [epoch: 2.67 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744483985190565		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6744483985190565 | validation: 0.8460957713832298]
	TIME [epoch: 2.66 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721123412298659		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6721123412298659 | validation: 0.7333957027467495]
	TIME [epoch: 2.66 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676890233877454		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6676890233877454 | validation: 0.8879595598973392]
	TIME [epoch: 2.66 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594512421339931		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6594512421339931 | validation: 0.747792222704993]
	TIME [epoch: 2.67 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567142314908149		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6567142314908149 | validation: 0.8785238445826288]
	TIME [epoch: 2.66 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687633050555648		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6687633050555648 | validation: 0.7199788691520848]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717823050749908		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.717823050749908 | validation: 1.0176363285829286]
	TIME [epoch: 2.66 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802341733470461		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7802341733470461 | validation: 0.8051232395085033]
	TIME [epoch: 2.67 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773909166575339		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6773909166575339 | validation: 0.7600880962757985]
	TIME [epoch: 2.66 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788488639402944		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6788488639402944 | validation: 0.8648630652217112]
	TIME [epoch: 2.67 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589375868284032		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6589375868284032 | validation: 0.7655244649148223]
	TIME [epoch: 2.67 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415457367138838		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6415457367138838 | validation: 0.7721973158656433]
	TIME [epoch: 2.67 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636420806234088		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.636420806234088 | validation: 0.7786672143398369]
	TIME [epoch: 2.67 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427397292247216		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6427397292247216 | validation: 0.7423590161105564]
	TIME [epoch: 2.66 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351848268043184		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6351848268043184 | validation: 0.7725184620525296]
	TIME [epoch: 2.67 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526322733090999		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6526322733090999 | validation: 0.8107026780714712]
	TIME [epoch: 2.66 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987726514542313		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6987726514542313 | validation: 0.8203637435172122]
	TIME [epoch: 2.68 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287745133187739		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7287745133187739 | validation: 0.9220005725360032]
	TIME [epoch: 2.67 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910833585228735		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6910833585228735 | validation: 0.7062435296018155]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589931256641787		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6589931256641787 | validation: 0.8710358204185452]
	TIME [epoch: 2.67 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372870405871757		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6372870405871757 | validation: 0.7309746546391488]
	TIME [epoch: 2.68 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204118947340365		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6204118947340365 | validation: 0.7214832049843976]
	TIME [epoch: 2.67 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221211452836771		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6221211452836771 | validation: 0.8051793798228521]
	TIME [epoch: 2.68 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266780333507243		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6266780333507243 | validation: 0.6686835526968209]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385110247239963		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6385110247239963 | validation: 0.9604098727799887]
	TIME [epoch: 2.68 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900483794693565		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6900483794693565 | validation: 0.6835428847802023]
	TIME [epoch: 2.69 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568901227709592		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6568901227709592 | validation: 0.796489700927228]
	TIME [epoch: 2.68 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223537544149257		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6223537544149257 | validation: 0.7193331887399084]
	TIME [epoch: 2.68 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104655880675596		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6104655880675596 | validation: 0.7133509922968533]
	TIME [epoch: 2.67 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621041817540896		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.621041817540896 | validation: 0.7744763664617338]
	TIME [epoch: 2.68 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626169291899705		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.626169291899705 | validation: 0.7355338883743384]
	TIME [epoch: 2.67 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250439614688907		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6250439614688907 | validation: 0.763602538434512]
	TIME [epoch: 2.67 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194198348643112		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6194198348643112 | validation: 0.7727197344692218]
	TIME [epoch: 2.68 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048309021139067		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6048309021139067 | validation: 0.6968627594527053]
	TIME [epoch: 2.68 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961051928021045		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5961051928021045 | validation: 0.8272324842638348]
	TIME [epoch: 2.68 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208040544371325		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6208040544371325 | validation: 0.6591688329145621]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686202905401024		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6686202905401024 | validation: 0.9606723484453288]
	TIME [epoch: 2.67 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752607814046755		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6752607814046755 | validation: 0.6653041566419486]
	TIME [epoch: 2.68 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029704521796314		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6029704521796314 | validation: 0.7125663392672019]
	TIME [epoch: 2.67 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582742919354189		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.582742919354189 | validation: 0.7566383490309678]
	TIME [epoch: 2.68 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5918359555042727		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5918359555042727 | validation: 0.6480011067317979]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.599490323794052		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.599490323794052 | validation: 0.8115943626176882]
	TIME [epoch: 2.67 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048204957040041		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6048204957040041 | validation: 0.6679517472876173]
	TIME [epoch: 2.67 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5882297131536407		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5882297131536407 | validation: 0.7896382054882221]
	TIME [epoch: 2.67 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900765613546202		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.5900765613546202 | validation: 0.6727683561851799]
	TIME [epoch: 2.66 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5690636819868058		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.5690636819868058 | validation: 0.80400580086182]
	TIME [epoch: 2.67 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5864866168093021		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.5864866168093021 | validation: 0.6483706702826393]
	TIME [epoch: 2.67 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976216676072523		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5976216676072523 | validation: 0.9334265229705943]
	TIME [epoch: 2.67 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272689210450552		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6272689210450552 | validation: 0.6634163723828087]
	TIME [epoch: 2.67 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188360491132086		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6188360491132086 | validation: 0.8124284477145155]
	TIME [epoch: 2.67 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609124099625278		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5609124099625278 | validation: 0.7459934749823091]
	TIME [epoch: 2.67 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5495328894783533		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5495328894783533 | validation: 0.6131456271287063]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5692721176952612		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5692721176952612 | validation: 0.7590993496276778]
	TIME [epoch: 2.66 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.567292782085688		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.567292782085688 | validation: 0.6406391086367927]
	TIME [epoch: 2.67 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679074053551167		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5679074053551167 | validation: 0.754843932717226]
	TIME [epoch: 2.66 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933349470780885		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5933349470780885 | validation: 0.6990542714954754]
	TIME [epoch: 2.67 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5780405673623601		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5780405673623601 | validation: 0.7218754510577924]
	TIME [epoch: 2.66 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5533116036888782		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5533116036888782 | validation: 0.6722547759523331]
	TIME [epoch: 2.66 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5327403177004524		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.5327403177004524 | validation: 0.6859777340822706]
	TIME [epoch: 2.66 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248849014246781		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5248849014246781 | validation: 0.6210448443165677]
	TIME [epoch: 2.69 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261249099515408		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5261249099515408 | validation: 0.8084194472217142]
	TIME [epoch: 2.67 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508016899450294		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5508016899450294 | validation: 0.6122483796385865]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355128075340827		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6355128075340827 | validation: 0.9953843706663728]
	TIME [epoch: 2.68 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641886857425053		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.641886857425053 | validation: 0.6596594032569296]
	TIME [epoch: 2.68 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5200239864560371		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5200239864560371 | validation: 0.6339986410270244]
	TIME [epoch: 2.68 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280542494092496		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.5280542494092496 | validation: 0.7684571487519918]
	TIME [epoch: 2.68 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5296572170320444		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5296572170320444 | validation: 0.6402717587200493]
	TIME [epoch: 2.68 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5070603377401788		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5070603377401788 | validation: 0.6251258258510973]
	TIME [epoch: 2.68 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111191459250355		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5111191459250355 | validation: 0.6763244881865923]
	TIME [epoch: 2.68 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5070977767964371		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5070977767964371 | validation: 0.632415924284337]
	TIME [epoch: 2.68 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035541681054396		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5035541681054396 | validation: 0.6832752367754713]
	TIME [epoch: 2.68 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5262060480045591		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5262060480045591 | validation: 0.754506840317726]
	TIME [epoch: 2.65 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614572635108483		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5614572635108483 | validation: 0.6263395141747621]
	TIME [epoch: 2.68 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5630009992785866		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.5630009992785866 | validation: 0.7685071712618419]
	TIME [epoch: 2.68 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253000012033835		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5253000012033835 | validation: 0.5988524972352204]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5045996164980877		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5045996164980877 | validation: 0.7672568018914083]
	TIME [epoch: 2.67 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5002758656160425		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5002758656160425 | validation: 0.5700543351688158]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49880341409984874		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.49880341409984874 | validation: 0.7881253592239459]
	TIME [epoch: 2.65 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5052523506476448		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.5052523506476448 | validation: 0.55390686010363]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5151305774920416		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.5151305774920416 | validation: 0.7947785489985885]
	TIME [epoch: 2.67 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166886454560258		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5166886454560258 | validation: 0.5986499973127241]
	TIME [epoch: 2.68 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4943227028340274		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.4943227028340274 | validation: 0.6701485875771387]
	TIME [epoch: 2.67 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47242191262690386		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.47242191262690386 | validation: 0.5964011055853978]
	TIME [epoch: 2.67 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4634711888137494		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.4634711888137494 | validation: 0.6374023077611991]
	TIME [epoch: 2.65 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46398850352783755		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.46398850352783755 | validation: 0.557023582401383]
	TIME [epoch: 2.68 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46889950023372706		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.46889950023372706 | validation: 0.7860765125604211]
	TIME [epoch: 2.68 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5039413660765528		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.5039413660765528 | validation: 0.5290855174255624]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5736110304062206		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5736110304062206 | validation: 0.8226482262451347]
	TIME [epoch: 2.66 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5158241983504587		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.5158241983504587 | validation: 0.5957579868223174]
	TIME [epoch: 2.65 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617313974157481		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.4617313974157481 | validation: 0.5977864182416337]
	TIME [epoch: 2.66 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4490615622614033		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.4490615622614033 | validation: 0.6794842540772049]
	TIME [epoch: 2.67 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46238562700688224		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.46238562700688224 | validation: 0.5297953340008701]
	TIME [epoch: 2.68 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4632779806129888		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.4632779806129888 | validation: 0.6923326975731818]
	TIME [epoch: 2.67 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45863038649376836		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.45863038649376836 | validation: 0.5575473757016932]
	TIME [epoch: 2.68 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4329592734699454		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.4329592734699454 | validation: 0.6785557908419415]
	TIME [epoch: 2.67 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42934467269799714		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.42934467269799714 | validation: 0.5529257318366917]
	TIME [epoch: 2.66 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45269897704079065		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.45269897704079065 | validation: 0.8344179550195664]
	TIME [epoch: 2.68 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4904674801334439		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.4904674801334439 | validation: 0.5330409423244252]
	TIME [epoch: 2.66 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4390435575943542		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.4390435575943542 | validation: 0.6700733001346143]
	TIME [epoch: 2.67 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4253077373143208		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.4253077373143208 | validation: 0.5493733455050089]
	TIME [epoch: 2.63 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45474448206452456		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.45474448206452456 | validation: 0.7246651232113367]
	TIME [epoch: 2.64 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47360005743791816		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.47360005743791816 | validation: 0.4910577248263202]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4563258189424535		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4563258189424535 | validation: 0.731825845276652]
	TIME [epoch: 2.68 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4261514382954216		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4261514382954216 | validation: 0.5458237399640455]
	TIME [epoch: 2.66 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40698524601554215		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.40698524601554215 | validation: 0.6591947596585718]
	TIME [epoch: 2.67 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3972970738968292		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.3972970738968292 | validation: 0.5829056417270609]
	TIME [epoch: 2.67 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3822855866200544		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3822855866200544 | validation: 0.5542248228182497]
	TIME [epoch: 2.66 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767548574885304		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3767548574885304 | validation: 0.6005987056781423]
	TIME [epoch: 2.67 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3789183310909619		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.3789183310909619 | validation: 0.49461722591870755]
	TIME [epoch: 2.67 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40315224157488283		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.40315224157488283 | validation: 0.8230030037511841]
	TIME [epoch: 2.67 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4829366374571836		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.4829366374571836 | validation: 0.48634152445160467]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5267774031448326		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5267774031448326 | validation: 0.7262294310443439]
	TIME [epoch: 2.67 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3883203750456539		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.3883203750456539 | validation: 0.6192470044370058]
	TIME [epoch: 2.67 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698441772067594		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.3698441772067594 | validation: 0.4696537305115376]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015013840711271		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.4015013840711271 | validation: 0.6883408596132056]
	TIME [epoch: 2.67 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40065465340799933		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.40065465340799933 | validation: 0.4979233044340314]
	TIME [epoch: 2.66 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3686591415103219		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.3686591415103219 | validation: 0.587680353440845]
	TIME [epoch: 2.67 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473380860964841		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3473380860964841 | validation: 0.5229188535886946]
	TIME [epoch: 2.66 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3427379579516399		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.3427379579516399 | validation: 0.573403752163806]
	TIME [epoch: 2.67 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35454222891278825		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.35454222891278825 | validation: 0.5564311577416831]
	TIME [epoch: 2.67 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38855691627683925		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.38855691627683925 | validation: 0.6110277038082842]
	TIME [epoch: 2.67 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38752476561308913		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.38752476561308913 | validation: 0.5467179110158861]
	TIME [epoch: 2.66 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.358847965346536		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.358847965346536 | validation: 0.5919700847115816]
	TIME [epoch: 2.67 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3272129499330221		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.3272129499330221 | validation: 0.475969182257892]
	TIME [epoch: 2.66 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33844882727972947		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.33844882727972947 | validation: 0.8117115325661557]
	TIME [epoch: 2.67 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43217166061557705		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.43217166061557705 | validation: 0.43480872316049163]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46067155118345926		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.46067155118345926 | validation: 0.6889813568431786]
	TIME [epoch: 2.67 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36249079042970095		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.36249079042970095 | validation: 0.5261127202602218]
	TIME [epoch: 2.66 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31319203756280867		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.31319203756280867 | validation: 0.4607033561356675]
	TIME [epoch: 2.66 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30842773685854097		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.30842773685854097 | validation: 0.6057881734299783]
	TIME [epoch: 2.66 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197842646337661		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.3197842646337661 | validation: 0.4486025482335089]
	TIME [epoch: 2.67 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3272618400006009		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.3272618400006009 | validation: 0.6544702722421899]
	TIME [epoch: 2.66 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.342273672562387		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.342273672562387 | validation: 0.4402070645847531]
	TIME [epoch: 2.67 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33137316319432836		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.33137316319432836 | validation: 0.6317226367451467]
	TIME [epoch: 2.66 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3249216511275509		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.3249216511275509 | validation: 0.432825111760001]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194138618092382		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.3194138618092382 | validation: 0.6414660874932215]
	TIME [epoch: 2.65 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3185063354691135		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.3185063354691135 | validation: 0.44148876910560875]
	TIME [epoch: 2.66 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308432097684915		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.308432097684915 | validation: 0.6272476106943246]
	TIME [epoch: 2.66 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109707323842643		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.3109707323842643 | validation: 0.42904660639990144]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3085524936415619		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3085524936415619 | validation: 0.6728196743992167]
	TIME [epoch: 2.66 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31415950410878235		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.31415950410878235 | validation: 0.4430483587640425]
	TIME [epoch: 2.66 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893168255701866		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.2893168255701866 | validation: 0.5900641972959687]
	TIME [epoch: 2.66 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282633910776381		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.282633910776381 | validation: 0.4240739280648942]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262857276623178		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.262857276623178 | validation: 0.547540844789354]
	TIME [epoch: 2.66 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26698211986746445		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.26698211986746445 | validation: 0.42052991664911943]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789667509741914		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2789667509741914 | validation: 0.672313826079114]
	TIME [epoch: 2.65 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33998995333615667		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.33998995333615667 | validation: 0.39425700506776745]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3453044139484116		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3453044139484116 | validation: 0.6322427176606543]
	TIME [epoch: 2.66 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29528255922626057		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.29528255922626057 | validation: 0.4189317883525881]
	TIME [epoch: 2.66 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25430162721434796		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.25430162721434796 | validation: 0.5623093396177471]
	TIME [epoch: 2.66 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25441568230207195		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.25441568230207195 | validation: 0.42766554232113274]
	TIME [epoch: 2.67 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27731389433656345		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.27731389433656345 | validation: 0.6504094788825561]
	TIME [epoch: 2.66 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296059047251171		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.296059047251171 | validation: 0.4312368207176087]
	TIME [epoch: 2.67 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2317064073411529		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.2317064073411529 | validation: 0.48346728057383753]
	TIME [epoch: 2.66 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2353975948001627		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2353975948001627 | validation: 0.41954371325479567]
	TIME [epoch: 2.66 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2657730025656155		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2657730025656155 | validation: 0.6062848962482699]
	TIME [epoch: 2.66 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3049945660265353		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.3049945660265353 | validation: 0.38377149754039774]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781605719643168		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2781605719643168 | validation: 0.6899869464234676]
	TIME [epoch: 2.66 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025772773059925		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.3025772773059925 | validation: 0.3785512597186091]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26667608099260554		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.26667608099260554 | validation: 0.5486642396866371]
	TIME [epoch: 2.66 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23664679404811903		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.23664679404811903 | validation: 0.39956617116951865]
	TIME [epoch: 2.67 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2147422834092927		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.2147422834092927 | validation: 0.47721636279183466]
	TIME [epoch: 2.66 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20885603739216393		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.20885603739216393 | validation: 0.39538954483785277]
	TIME [epoch: 2.67 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2079911475569014		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.2079911475569014 | validation: 0.5216686941077017]
	TIME [epoch: 2.66 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2168464783610042		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.2168464783610042 | validation: 0.37048927126731585]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706089108420606		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.2706089108420606 | validation: 0.7192376650485377]
	TIME [epoch: 2.65 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379419682849048		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3379419682849048 | validation: 0.38232499307685763]
	TIME [epoch: 2.65 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23933047506008234		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.23933047506008234 | validation: 0.5098420618742174]
	TIME [epoch: 2.65 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20856260250595968		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.20856260250595968 | validation: 0.4041129420133942]
	TIME [epoch: 2.67 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20635448430522094		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.20635448430522094 | validation: 0.48721742841902194]
	TIME [epoch: 2.67 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21300398970289527		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.21300398970289527 | validation: 0.4149469706333026]
	TIME [epoch: 2.67 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19015110465979526		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.19015110465979526 | validation: 0.41503866298868897]
	TIME [epoch: 2.66 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18240411435891177		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.18240411435891177 | validation: 0.4755326203592629]
	TIME [epoch: 2.67 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19000245875592264		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.19000245875592264 | validation: 0.35575773239823294]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21398788185121273		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.21398788185121273 | validation: 0.7574162292965398]
	TIME [epoch: 2.65 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3315580219029481		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.3315580219029481 | validation: 0.35964307484761415]
	TIME [epoch: 2.66 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25188482566125464		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.25188482566125464 | validation: 0.5150729787246618]
	TIME [epoch: 2.65 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20214676129400957		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.20214676129400957 | validation: 0.37991050019204825]
	TIME [epoch: 2.65 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.177171214104373		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.177171214104373 | validation: 0.46225363012054965]
	TIME [epoch: 2.67 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18216918153646197		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.18216918153646197 | validation: 0.3720357649297816]
	TIME [epoch: 2.65 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17950541244185647		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.17950541244185647 | validation: 0.4888143844440565]
	TIME [epoch: 2.65 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1913767147647075		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.1913767147647075 | validation: 0.3432814883601929]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2441410013474456		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2441410013474456 | validation: 0.6643570326741035]
	TIME [epoch: 2.67 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811774801416916		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2811774801416916 | validation: 0.34498526410927427]
	TIME [epoch: 2.67 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20657656090955154		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.20657656090955154 | validation: 0.5223308305772347]
	TIME [epoch: 2.67 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19831066012275309		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.19831066012275309 | validation: 0.39636317765910034]
	TIME [epoch: 2.66 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16676658088596283		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.16676658088596283 | validation: 0.3947698597483296]
	TIME [epoch: 2.66 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15313350878759982		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.15313350878759982 | validation: 0.40917195956607916]
	TIME [epoch: 2.65 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15152169324030693		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.15152169324030693 | validation: 0.37269972604143203]
	TIME [epoch: 2.65 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14992360375857572		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.14992360375857572 | validation: 0.45536598793191996]
	TIME [epoch: 2.65 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15549292199399914		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.15549292199399914 | validation: 0.3338003423406768]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170211799870612		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2170211799870612 | validation: 0.7479157613152387]
	TIME [epoch: 2.67 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3223497520026815		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3223497520026815 | validation: 0.3370682166588174]
	TIME [epoch: 2.67 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22908812355827174		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.22908812355827174 | validation: 0.467507407158946]
	TIME [epoch: 2.67 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21865909624992752		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.21865909624992752 | validation: 0.40401320463692025]
	TIME [epoch: 2.68 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14812686423770005		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.14812686423770005 | validation: 0.35487092937672154]
	TIME [epoch: 2.67 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15371506631960521		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.15371506631960521 | validation: 0.4362379897287181]
	TIME [epoch: 2.68 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17472786876423807		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.17472786876423807 | validation: 0.35060208508254]
	TIME [epoch: 3.21 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16058440475689303		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.16058440475689303 | validation: 0.5061379158766042]
	TIME [epoch: 2.65 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19076373480344838		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.19076373480344838 | validation: 0.3218886009887009]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21533003991881244		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.21533003991881244 | validation: 0.6211035918197869]
	TIME [epoch: 2.65 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2380830566896952		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2380830566896952 | validation: 0.34219667599712045]
	TIME [epoch: 2.65 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14489672434113052		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.14489672434113052 | validation: 0.37069873428665434]
	TIME [epoch: 2.65 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13025136508871352		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.13025136508871352 | validation: 0.40265802628774844]
	TIME [epoch: 2.65 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13555364935685368		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.13555364935685368 | validation: 0.33894934935204424]
	TIME [epoch: 2.65 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14955032827817327		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.14955032827817327 | validation: 0.5235730864983045]
	TIME [epoch: 2.66 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18421372879749712		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.18421372879749712 | validation: 0.3165780369140512]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22126906696347007		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.22126906696347007 | validation: 0.5869482467197659]
	TIME [epoch: 2.65 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22423735725311353		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.22423735725311353 | validation: 0.34081466340005323]
	TIME [epoch: 2.67 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14091839820204424		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.14091839820204424 | validation: 0.3804325493494116]
	TIME [epoch: 2.67 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12859132103911186		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.12859132103911186 | validation: 0.39509548740506756]
	TIME [epoch: 2.66 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12726473720390236		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.12726473720390236 | validation: 0.33945615596425216]
	TIME [epoch: 2.66 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556313337089484		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.13556313337089484 | validation: 0.447403936316705]
	TIME [epoch: 2.67 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14666891502497753		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.14666891502497753 | validation: 0.3664467418896906]
	TIME [epoch: 2.65 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508559954784141		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.1508559954784141 | validation: 0.4363169217273763]
	TIME [epoch: 2.66 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162256601928747		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.162256601928747 | validation: 0.38976990131438166]
	TIME [epoch: 2.66 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14278222348033878		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.14278222348033878 | validation: 0.30759454538947933]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16885338555859383		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.16885338555859383 | validation: 0.6838056181133104]
	TIME [epoch: 2.65 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29258539780364173		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.29258539780364173 | validation: 0.3159842414176344]
	TIME [epoch: 2.66 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1862792516525083		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1862792516525083 | validation: 0.4275730878148669]
	TIME [epoch: 2.65 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13913242218498642		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.13913242218498642 | validation: 0.34092142453049995]
	TIME [epoch: 2.66 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11949981750336447		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.11949981750336447 | validation: 0.3667239070270638]
	TIME [epoch: 2.64 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11343167737803629		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.11343167737803629 | validation: 0.36349590177078367]
	TIME [epoch: 2.66 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11193863226219844		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.11193863226219844 | validation: 0.3518468325293454]
	TIME [epoch: 2.65 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11629874307721931		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.11629874307721931 | validation: 0.3631308970687588]
	TIME [epoch: 2.66 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693724189167633		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.11693724189167633 | validation: 0.3444438629331026]
	TIME [epoch: 2.66 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11638683566900589		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.11638683566900589 | validation: 0.39701167853241953]
	TIME [epoch: 2.67 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14041616277762056		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.14041616277762056 | validation: 0.3153246785587377]
	TIME [epoch: 2.66 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955021254110963		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.1955021254110963 | validation: 0.724459059947824]
	TIME [epoch: 2.66 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33877542329002747		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.33877542329002747 | validation: 0.3330266548326917]
	TIME [epoch: 2.67 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758358160646756		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1758358160646756 | validation: 0.476877401330596]
	TIME [epoch: 2.67 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208136497622606		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.2208136497622606 | validation: 0.4659114337258752]
	TIME [epoch: 2.66 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14286401541608532		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.14286401541608532 | validation: 0.3285130156163378]
	TIME [epoch: 2.67 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19305185982173054		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.19305185982173054 | validation: 0.4679153435986983]
	TIME [epoch: 2.66 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14878003535174666		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.14878003535174666 | validation: 0.35073806846830824]
	TIME [epoch: 2.66 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12187481769601584		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12187481769601584 | validation: 0.35074645634219437]
	TIME [epoch: 2.63 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12176666943116385		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.12176666943116385 | validation: 0.33834032369460076]
	TIME [epoch: 2.66 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11046646957991406		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.11046646957991406 | validation: 0.4002889764352766]
	TIME [epoch: 2.65 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11278526581580874		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.11278526581580874 | validation: 0.31839192847733455]
	TIME [epoch: 2.65 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1238061764735016		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.1238061764735016 | validation: 0.44826273704500696]
	TIME [epoch: 2.66 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531883550265723		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.1531883550265723 | validation: 0.30235662347268066]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1695157224009688		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.1695157224009688 | validation: 0.5619632087517886]
	TIME [epoch: 2.66 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.202478191790464		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.202478191790464 | validation: 0.31380334699311496]
	TIME [epoch: 2.67 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181362309120354		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.1181362309120354 | validation: 0.3483022303105944]
	TIME [epoch: 2.65 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10358432809289124		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.10358432809289124 | validation: 0.36178119863605573]
	TIME [epoch: 2.67 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10638522668848831		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.10638522668848831 | validation: 0.30724181445429427]
	TIME [epoch: 2.66 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266087582924018		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.1266087582924018 | validation: 0.5167217405743338]
	TIME [epoch: 2.68 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17321835693387677		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.17321835693387677 | validation: 0.2794592703155104]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15351890447794214		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.15351890447794214 | validation: 0.505135631300273]
	TIME [epoch: 2.66 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17268763188867986		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.17268763188867986 | validation: 0.3302884281332597]
	TIME [epoch: 2.65 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11549493569407478		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.11549493569407478 | validation: 0.35869633428741654]
	TIME [epoch: 2.66 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10269291933163277		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.10269291933163277 | validation: 0.34361030590386177]
	TIME [epoch: 2.65 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10180186971150895		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.10180186971150895 | validation: 0.33147767618996943]
	TIME [epoch: 2.66 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987733045052263		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.0987733045052263 | validation: 0.3742773367448072]
	TIME [epoch: 2.65 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10372101943758001		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.10372101943758001 | validation: 0.28624246565460093]
	TIME [epoch: 2.66 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325731777016937		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.1325731777016937 | validation: 0.5840524631777738]
	TIME [epoch: 2.67 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24810779087454243		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.24810779087454243 | validation: 0.286131154051567]
	TIME [epoch: 2.66 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394872199323293		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1394872199323293 | validation: 0.48093966492421175]
	TIME [epoch: 2.64 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14802272992602633		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.14802272992602633 | validation: 0.3235659206783217]
	TIME [epoch: 2.67 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1193401320177963		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1193401320177963 | validation: 0.34634343838522996]
	TIME [epoch: 2.65 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10372269464716297		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.10372269464716297 | validation: 0.31218909090412705]
	TIME [epoch: 2.68 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09605409157487081		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.09605409157487081 | validation: 0.31743024950067267]
	TIME [epoch: 2.66 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09128375190575945		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.09128375190575945 | validation: 0.3251600665530293]
	TIME [epoch: 2.66 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09644207807571825		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.09644207807571825 | validation: 0.33145773483252683]
	TIME [epoch: 2.66 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945120249020536		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.09945120249020536 | validation: 0.3109367024259563]
	TIME [epoch: 2.66 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081905801232107		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.11081905801232107 | validation: 0.43938389749576195]
	TIME [epoch: 2.66 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17076667290846673		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.17076667290846673 | validation: 0.2835737599727477]
	TIME [epoch: 2.66 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19893591878233277		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.19893591878233277 | validation: 0.6041420375168602]
	TIME [epoch: 2.66 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24651604708393116		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.24651604708393116 | validation: 0.3113503088231478]
	TIME [epoch: 2.67 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12248439432735683		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.12248439432735683 | validation: 0.3211157111532876]
	TIME [epoch: 2.64 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14710413770008024		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.14710413770008024 | validation: 0.42010000617919263]
	TIME [epoch: 2.65 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14450666218890665		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.14450666218890665 | validation: 0.30749300356673104]
	TIME [epoch: 2.64 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466040930829156		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.11466040930829156 | validation: 0.39602944099976134]
	TIME [epoch: 2.66 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261281498503268		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1261281498503268 | validation: 0.3211260976375142]
	TIME [epoch: 2.64 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0910695194943154		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.0910695194943154 | validation: 0.3146677117244006]
	TIME [epoch: 2.65 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10075237809364929		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.10075237809364929 | validation: 0.35993698770696253]
	TIME [epoch: 2.66 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10227293054893555		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.10227293054893555 | validation: 0.30275013900740133]
	TIME [epoch: 2.66 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10105774345832993		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.10105774345832993 | validation: 0.3791934291332388]
	TIME [epoch: 2.65 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10033238323098827		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.10033238323098827 | validation: 0.28563528091826995]
	TIME [epoch: 2.65 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10625444595718381		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.10625444595718381 | validation: 0.41052168808657363]
	TIME [epoch: 2.66 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11814494779548358		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.11814494779548358 | validation: 0.2600140890128624]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13633458356985764		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.13633458356985764 | validation: 0.5528611617918119]
	TIME [epoch: 5.78 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18599038388622918		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.18599038388622918 | validation: 0.2781837781311907]
	TIME [epoch: 5.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10061981038953029		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.10061981038953029 | validation: 0.30079478397587156]
	TIME [epoch: 5.78 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915464709939094		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.0915464709939094 | validation: 0.351405947378014]
	TIME [epoch: 5.77 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758714703195268		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.09758714703195268 | validation: 0.30613104223634596]
	TIME [epoch: 5.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09336712942837919		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.09336712942837919 | validation: 0.34560048572591584]
	TIME [epoch: 5.77 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615686635443023		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.09615686635443023 | validation: 0.2840170625873185]
	TIME [epoch: 5.78 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601866024802273		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.09601866024802273 | validation: 0.4183323983104524]
	TIME [epoch: 5.77 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11398881804562518		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.11398881804562518 | validation: 0.2573101681568903]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13098661934693998		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.13098661934693998 | validation: 0.5881140961900441]
	TIME [epoch: 5.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.229473631111149		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.229473631111149 | validation: 0.2777613487636051]
	TIME [epoch: 5.77 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09675892856284922		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.09675892856284922 | validation: 0.2806119036804594]
	TIME [epoch: 5.76 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09305218555923793		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09305218555923793 | validation: 0.4121448185393467]
	TIME [epoch: 5.78 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1363842382979109		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.1363842382979109 | validation: 0.270110800100977]
	TIME [epoch: 5.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10898773285511919		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.10898773285511919 | validation: 0.4250878245412974]
	TIME [epoch: 5.78 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11636774666684221		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.11636774666684221 | validation: 0.2714667502125224]
	TIME [epoch: 5.77 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908902451107917		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10908902451107917 | validation: 0.39933449128970366]
	TIME [epoch: 5.79 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757374145724324		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.10757374145724324 | validation: 0.2822824170919351]
	TIME [epoch: 5.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877326678405828		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.0877326678405828 | validation: 0.2872836524453987]
	TIME [epoch: 5.78 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639059982925666		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.08639059982925666 | validation: 0.32573771503308285]
	TIME [epoch: 5.77 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08630038144534764		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.08630038144534764 | validation: 0.2809664677235351]
	TIME [epoch: 5.78 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09158462345468418		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.09158462345468418 | validation: 0.37562407706040585]
	TIME [epoch: 5.78 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10969881184099918		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.10969881184099918 | validation: 0.2923436634873992]
	TIME [epoch: 5.78 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606760660638845		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.10606760660638845 | validation: 0.32893746141842617]
	TIME [epoch: 5.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317247685944463		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.10317247685944463 | validation: 0.3166163747693424]
	TIME [epoch: 5.76 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09453549366374812		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.09453549366374812 | validation: 0.24841954010245956]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12765688594762473		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.12765688594762473 | validation: 0.6290928484547456]
	TIME [epoch: 5.78 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2734525280354692		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.2734525280354692 | validation: 0.2601296958141594]
	TIME [epoch: 5.78 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477205088239165		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.08477205088239165 | validation: 0.2553897131078986]
	TIME [epoch: 5.78 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853533870632904		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.11853533870632904 | validation: 0.4666956891649388]
	TIME [epoch: 5.76 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637280607497255		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1637280607497255 | validation: 0.26021328634300167]
	TIME [epoch: 5.77 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09399835416226654		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.09399835416226654 | validation: 0.309232835576558]
	TIME [epoch: 5.77 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285142066228068		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.08285142066228068 | validation: 0.2931816504162265]
	TIME [epoch: 5.77 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09367120454389603		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.09367120454389603 | validation: 0.27514675444252756]
	TIME [epoch: 5.77 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09228847197877403		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.09228847197877403 | validation: 0.30913640666626363]
	TIME [epoch: 5.78 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08579792028210853		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.08579792028210853 | validation: 0.2831710741889461]
	TIME [epoch: 5.76 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361223413633619		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.08361223413633619 | validation: 0.31281746323225407]
	TIME [epoch: 5.77 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644735381327168		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.08644735381327168 | validation: 0.2671025767716401]
	TIME [epoch: 5.76 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09608549178204431		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.09608549178204431 | validation: 0.46845816694750453]
	TIME [epoch: 5.77 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1342057803671209		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.1342057803671209 | validation: 0.2525950894667192]
	TIME [epoch: 5.76 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09009752915948836		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.09009752915948836 | validation: 0.34342504402707186]
	TIME [epoch: 5.78 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1036048089557358		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1036048089557358 | validation: 0.2377410032243215]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153858161050927		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.153858161050927 | validation: 0.459245943812437]
	TIME [epoch: 5.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17808560871412715		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.17808560871412715 | validation: 0.28110361404002443]
	TIME [epoch: 5.78 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09917455054204247		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09917455054204247 | validation: 0.318637760273145]
	TIME [epoch: 5.77 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12211433313745286		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.12211433313745286 | validation: 0.30414088206474754]
	TIME [epoch: 5.74 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08312335826602855		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.08312335826602855 | validation: 0.26543709393826065]
	TIME [epoch: 5.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270183444002658		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.09270183444002658 | validation: 0.42488486002412684]
	TIME [epoch: 5.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032297171753383		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.11032297171753383 | validation: 0.26732564656841495]
	TIME [epoch: 5.77 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07975365750641797		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.07975365750641797 | validation: 0.2610849562185341]
	TIME [epoch: 5.78 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829219716445839		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0829219716445839 | validation: 0.32226933885848635]
	TIME [epoch: 5.75 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724888925612659		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.08724888925612659 | validation: 0.24390427698886008]
	TIME [epoch: 5.77 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10634810622726876		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.10634810622726876 | validation: 0.46638031546164815]
	TIME [epoch: 5.75 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15136356054229405		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.15136356054229405 | validation: 0.27061108529060035]
	TIME [epoch: 5.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24477947950961645		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.24477947950961645 | validation: 0.33620120398723413]
	TIME [epoch: 5.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10181792203321084		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.10181792203321084 | validation: 0.29630717874699625]
	TIME [epoch: 5.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260401151082342		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.10260401151082342 | validation: 0.27587318869483507]
	TIME [epoch: 5.75 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002622330242881		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1002622330242881 | validation: 0.2613425554021109]
	TIME [epoch: 5.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08038794365252631		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.08038794365252631 | validation: 0.2936594068869807]
	TIME [epoch: 5.75 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08391586672909825		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.08391586672909825 | validation: 0.26097709551318576]
	TIME [epoch: 5.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742566531963078		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.07742566531963078 | validation: 0.2764215434561345]
	TIME [epoch: 5.75 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821352238713458		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.07821352238713458 | validation: 0.2662069780490887]
	TIME [epoch: 5.76 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08019968630583196		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.08019968630583196 | validation: 0.3051337673659795]
	TIME [epoch: 5.76 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782988410698099		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.0782988410698099 | validation: 0.2618119882385657]
	TIME [epoch: 5.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08302059445381456		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.08302059445381456 | validation: 0.372521947193404]
	TIME [epoch: 5.75 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155017906796808		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.10155017906796808 | validation: 0.23736061809168768]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10569001783824344		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.10569001783824344 | validation: 0.4509494561746079]
	TIME [epoch: 5.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802938721021846		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.13802938721021846 | validation: 0.2449794354734728]
	TIME [epoch: 5.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09319009465393677		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.09319009465393677 | validation: 0.3023737703806091]
	TIME [epoch: 5.75 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08053605100307669		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.08053605100307669 | validation: 0.28684074322001873]
	TIME [epoch: 5.75 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07696685200466058		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07696685200466058 | validation: 0.2631514693094425]
	TIME [epoch: 5.77 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07556817473533958		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.07556817473533958 | validation: 0.28296840552934327]
	TIME [epoch: 5.75 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07367310944314129		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.07367310944314129 | validation: 0.259804834451489]
	TIME [epoch: 5.76 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07475545090279558		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.07475545090279558 | validation: 0.29931764561410085]
	TIME [epoch: 5.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0809925173659559		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.0809925173659559 | validation: 0.21557277505803799]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357197649567885		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.09357197649567885 | validation: 0.4209308090779651]
	TIME [epoch: 5.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13881800910920206		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.13881800910920206 | validation: 0.247020851372179]
	TIME [epoch: 5.78 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18064990772355302		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.18064990772355302 | validation: 0.35310948023359706]
	TIME [epoch: 5.75 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1171350218149703		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1171350218149703 | validation: 0.28955144685008966]
	TIME [epoch: 5.77 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08841052413035758		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.08841052413035758 | validation: 0.2560619063286862]
	TIME [epoch: 5.75 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11366798564547692		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.11366798564547692 | validation: 0.33650598311607716]
	TIME [epoch: 5.75 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11696395233150508		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.11696395233150508 | validation: 0.27836961295350887]
	TIME [epoch: 5.74 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083942172040719		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.08083942172040719 | validation: 0.23068389576292825]
	TIME [epoch: 5.75 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064571920540845		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.1064571920540845 | validation: 0.33909357856444483]
	TIME [epoch: 5.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338703757181648		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.09338703757181648 | validation: 0.2677124561507346]
	TIME [epoch: 5.77 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245385389741402		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.10245385389741402 | validation: 0.3557958971356043]
	TIME [epoch: 5.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606812759244666		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.10606812759244666 | validation: 0.2733000525133716]
	TIME [epoch: 5.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705313927922016		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.0705313927922016 | validation: 0.250723872388468]
	TIME [epoch: 5.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08097062870662129		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.08097062870662129 | validation: 0.36220930659578876]
	TIME [epoch: 5.76 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09275494233092807		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.09275494233092807 | validation: 0.23710551921092043]
	TIME [epoch: 5.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08398556601436205		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.08398556601436205 | validation: 0.3252831115010741]
	TIME [epoch: 5.77 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08457897638910362		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.08457897638910362 | validation: 0.2458902408517245]
	TIME [epoch: 5.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07695413757326447		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.07695413757326447 | validation: 0.2706915138397413]
	TIME [epoch: 5.76 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07130052665482452		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07130052665482452 | validation: 0.2457542553527312]
	TIME [epoch: 5.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07461894634310327		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.07461894634310327 | validation: 0.27774674470196875]
	TIME [epoch: 5.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07560721667906317		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.07560721667906317 | validation: 0.23579045395142595]
	TIME [epoch: 5.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08000826387467655		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.08000826387467655 | validation: 0.39948685434777986]
	TIME [epoch: 5.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10845318290508493		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.10845318290508493 | validation: 0.24846554037661228]
	TIME [epoch: 5.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961690848004148		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0961690848004148 | validation: 0.3713659483610779]
	TIME [epoch: 5.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11090233721341454		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.11090233721341454 | validation: 0.23564531941051323]
	TIME [epoch: 5.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07796978192956004		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.07796978192956004 | validation: 0.27878231186257746]
	TIME [epoch: 5.76 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855039265321233		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.07855039265321233 | validation: 0.2730887774095145]
	TIME [epoch: 5.73 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077271155527298		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.08077271155527298 | validation: 0.2595508769516234]
	TIME [epoch: 5.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07539536474340436		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07539536474340436 | validation: 0.30425024330418293]
	TIME [epoch: 5.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08385527747088421		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08385527747088421 | validation: 0.21913907495307924]
	TIME [epoch: 5.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08404250774461458		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.08404250774461458 | validation: 0.4063736331859705]
	TIME [epoch: 5.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11838881760286908		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.11838881760286908 | validation: 0.22864888351555526]
	TIME [epoch: 5.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025019310182116		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.1025019310182116 | validation: 0.3859512352246109]
	TIME [epoch: 5.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09889842040729847		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.09889842040729847 | validation: 0.2653923614085819]
	TIME [epoch: 5.74 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07639294845574052		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.07639294845574052 | validation: 0.21159949606207032]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093385782675812		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10093385782675812 | validation: 0.3962587125719981]
	TIME [epoch: 5.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1278819574789754		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.1278819574789754 | validation: 0.24776772730248844]
	TIME [epoch: 5.78 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058671599238822		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07058671599238822 | validation: 0.22279419053017546]
	TIME [epoch: 5.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07780062871092762		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.07780062871092762 | validation: 0.3489279758200188]
	TIME [epoch: 5.78 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09200116784349816		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.09200116784349816 | validation: 0.23596540218500772]
	TIME [epoch: 5.77 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08584460993477468		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.08584460993477468 | validation: 0.28863333068663655]
	TIME [epoch: 5.77 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007294564231768		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11007294564231768 | validation: 0.26406089277931927]
	TIME [epoch: 5.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06933606480261677		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.06933606480261677 | validation: 0.2404930855615766]
	TIME [epoch: 5.77 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07213857618704152		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.07213857618704152 | validation: 0.26291172509951866]
	TIME [epoch: 5.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08145048346510944		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.08145048346510944 | validation: 0.2552835170273489]
	TIME [epoch: 5.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08935974294678845		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.08935974294678845 | validation: 0.26004298776245227]
	TIME [epoch: 5.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514645797963198		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.08514645797963198 | validation: 0.2423470904511009]
	TIME [epoch: 5.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06509474528652018		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.06509474528652018 | validation: 0.24921155094346742]
	TIME [epoch: 5.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08036122581726853		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08036122581726853 | validation: 0.30553850240030855]
	TIME [epoch: 5.75 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08879125451355767		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.08879125451355767 | validation: 0.2352115342911045]
	TIME [epoch: 5.77 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819060381629938		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.0819060381629938 | validation: 0.36766362012565035]
	TIME [epoch: 5.77 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693139151924018		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.09693139151924018 | validation: 0.21707873168668976]
	TIME [epoch: 5.77 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657303912547088		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07657303912547088 | validation: 0.290515041079842]
	TIME [epoch: 5.77 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908176602162989		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.07908176602162989 | validation: 0.22781958752707948]
	TIME [epoch: 5.76 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07994405255904115		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.07994405255904115 | validation: 0.3192557347165236]
	TIME [epoch: 5.76 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10559431081441044		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.10559431081441044 | validation: 0.22185108608120757]
	TIME [epoch: 5.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10753731980663667		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.10753731980663667 | validation: 0.3396580893827259]
	TIME [epoch: 5.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687287156946038		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.10687287156946038 | validation: 0.2490216105900699]
	TIME [epoch: 5.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715069737921057		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.0715069737921057 | validation: 0.23351687992505507]
	TIME [epoch: 5.77 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213168172549695		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.08213168172549695 | validation: 0.2707757003207272]
	TIME [epoch: 5.71 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177380490269254		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.07177380490269254 | validation: 0.22413920752619187]
	TIME [epoch: 5.77 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612150613237235		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06612150613237235 | validation: 0.2632837480195729]
	TIME [epoch: 5.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07000505601427617		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.07000505601427617 | validation: 0.24145687129030496]
	TIME [epoch: 5.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349788973151075		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.07349788973151075 | validation: 0.3579643199197995]
	TIME [epoch: 5.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09177962614446473		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.09177962614446473 | validation: 0.22588257959464217]
	TIME [epoch: 5.76 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085997716159685		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.07085997716159685 | validation: 0.2409727608444229]
	TIME [epoch: 5.72 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06568685645359768		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.06568685645359768 | validation: 0.26861753726354115]
	TIME [epoch: 5.77 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093485117198332		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07093485117198332 | validation: 0.21941427335295513]
	TIME [epoch: 5.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689397249391416		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.0689397249391416 | validation: 0.30715251532989213]
	TIME [epoch: 5.78 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08249711701942297		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.08249711701942297 | validation: 0.21591868599584663]
	TIME [epoch: 5.76 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10440056054466584		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.10440056054466584 | validation: 0.4247630921382782]
	TIME [epoch: 5.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12102506891593866		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.12102506891593866 | validation: 0.2548137717261894]
	TIME [epoch: 5.76 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06493637477900588		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.06493637477900588 | validation: 0.20802190835941628]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07654076121375766		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.07654076121375766 | validation: 0.27585094124735837]
	TIME [epoch: 5.77 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07166604786175489		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.07166604786175489 | validation: 0.23273548738572825]
	TIME [epoch: 5.76 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06745778389292709		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06745778389292709 | validation: 0.26079087674257867]
	TIME [epoch: 5.78 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753129302227075		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06753129302227075 | validation: 0.22426460608175056]
	TIME [epoch: 5.76 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325758774818993		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.08325758774818993 | validation: 0.2741737982049085]
	TIME [epoch: 5.78 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325809651135276		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09325809651135276 | validation: 0.23920771773874205]
	TIME [epoch: 5.77 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652517167948481		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06652517167948481 | validation: 0.21964080188248306]
	TIME [epoch: 5.78 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666943876509262		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.0666943876509262 | validation: 0.28692671584239193]
	TIME [epoch: 5.75 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977195999517081		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.0977195999517081 | validation: 0.2128792025013774]
	TIME [epoch: 5.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08606859036709723		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.08606859036709723 | validation: 0.31324402835539633]
	TIME [epoch: 5.75 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09322797621478766		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.09322797621478766 | validation: 0.2274627142889387]
	TIME [epoch: 5.75 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07611279834361505		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.07611279834361505 | validation: 0.3468605119250395]
	TIME [epoch: 5.75 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871703107074641		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.08871703107074641 | validation: 0.2323322000220018]
	TIME [epoch: 5.76 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06613092243849375		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06613092243849375 | validation: 0.2207841978769663]
	TIME [epoch: 5.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661701792119677		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06661701792119677 | validation: 0.27261079751869854]
	TIME [epoch: 5.75 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705595792687608		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06705595792687608 | validation: 0.22476065457771593]
	TIME [epoch: 5.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06928659461026154		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06928659461026154 | validation: 0.29406682659747146]
	TIME [epoch: 5.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922289906416007		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.07922289906416007 | validation: 0.21098061878034333]
	TIME [epoch: 5.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759810654593263		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.0759810654593263 | validation: 0.32797227000316775]
	TIME [epoch: 5.75 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034802442584235		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.1034802442584235 | validation: 0.22786257427452022]
	TIME [epoch: 5.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838962110389604		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.07838962110389604 | validation: 0.23717895745908077]
	TIME [epoch: 5.77 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513796275245098		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.07513796275245098 | validation: 0.2601299890246897]
	TIME [epoch: 5.77 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630680640293872		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06630680640293872 | validation: 0.21016466673610543]
	TIME [epoch: 5.77 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06899831812333147		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.06899831812333147 | validation: 0.31214128554473874]
	TIME [epoch: 5.76 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327527189930463		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.08327527189930463 | validation: 0.20299561164657645]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859462545683035		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.0859462545683035 | validation: 0.3268763537068625]
	TIME [epoch: 5.78 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09200725122685045		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09200725122685045 | validation: 0.21734931870347796]
	TIME [epoch: 5.78 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06656355968393753		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06656355968393753 | validation: 0.23782442760684153]
	TIME [epoch: 5.79 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035175874318885		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.08035175874318885 | validation: 0.2517507736791175]
	TIME [epoch: 5.78 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.077535536628915		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.077535536628915 | validation: 0.23962228647044218]
	TIME [epoch: 5.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06974914539019036		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06974914539019036 | validation: 0.2134724355456588]
	TIME [epoch: 5.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359082181405977		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06359082181405977 | validation: 0.25356482005218267]
	TIME [epoch: 5.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06669743331804959		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06669743331804959 | validation: 0.20590570449092904]
	TIME [epoch: 5.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024024928911672		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.07024024928911672 | validation: 0.30427673598112115]
	TIME [epoch: 5.77 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727217127228494		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.0727217127228494 | validation: 0.2027456248087186]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350636812625734		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07350636812625734 | validation: 0.3143521027742794]
	TIME [epoch: 5.78 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365841768199359		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.08365841768199359 | validation: 0.2119175374852989]
	TIME [epoch: 5.78 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07511240365979265		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.07511240365979265 | validation: 0.2705505337677891]
	TIME [epoch: 5.79 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08032711212232879		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.08032711212232879 | validation: 0.21644285688349454]
	TIME [epoch: 5.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06556406595807535		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.06556406595807535 | validation: 0.24190295163393302]
	TIME [epoch: 5.71 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060685545985931795		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.060685545985931795 | validation: 0.2425863565430833]
	TIME [epoch: 5.79 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06255439607330116		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.06255439607330116 | validation: 0.21433670640902333]
	TIME [epoch: 5.71 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322687911820755		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06322687911820755 | validation: 0.26314453451406145]
	TIME [epoch: 5.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549892189106087		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06549892189106087 | validation: 0.22056926130008933]
	TIME [epoch: 5.71 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06816011340720729		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06816011340720729 | validation: 0.24800841563158293]
	TIME [epoch: 5.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07598941005035377		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.07598941005035377 | validation: 0.21046515012599995]
	TIME [epoch: 5.71 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09272632206358551		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.09272632206358551 | validation: 0.2994461895314888]
	TIME [epoch: 5.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10430523332052773		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.10430523332052773 | validation: 0.21484349924095586]
	TIME [epoch: 5.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07685492900807213		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.07685492900807213 | validation: 0.3064078949485955]
	TIME [epoch: 5.76 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08460241415340775		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.08460241415340775 | validation: 0.23811830324685523]
	TIME [epoch: 5.75 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228032847699197		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.06228032847699197 | validation: 0.20314395429348733]
	TIME [epoch: 5.77 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07578974820574425		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.07578974820574425 | validation: 0.3643521088987363]
	TIME [epoch: 5.76 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925366457389467		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0925366457389467 | validation: 0.2006827668923522]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661308204132221		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.06661308204132221 | validation: 0.20437665671513533]
	TIME [epoch: 5.75 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414946815608466		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06414946815608466 | validation: 0.260624158194631]
	TIME [epoch: 5.79 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703437552336036		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.0703437552336036 | validation: 0.20546240371179883]
	TIME [epoch: 5.79 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272055640055726		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06272055640055726 | validation: 0.25519891336171735]
	TIME [epoch: 5.76 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926364194357906		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06926364194357906 | validation: 0.22856626815931846]
	TIME [epoch: 5.78 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06589662479549728		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.06589662479549728 | validation: 0.2218026645482759]
	TIME [epoch: 5.76 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06156018875233972		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.06156018875233972 | validation: 0.19352256638083923]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431868786201492		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06431868786201492 | validation: 0.2555287112515034]
	TIME [epoch: 5.78 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763953449244401		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.0763953449244401 | validation: 0.20505523424805464]
	TIME [epoch: 5.79 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540399900217031		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.06540399900217031 | validation: 0.2741649885032646]
	TIME [epoch: 5.81 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701270421236312		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.0701270421236312 | validation: 0.20906627988657128]
	TIME [epoch: 5.79 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897874414832264		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.06897874414832264 | validation: 0.26216865679622575]
	TIME [epoch: 5.79 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662366702598929		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06662366702598929 | validation: 0.1953572880586335]
	TIME [epoch: 5.78 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623200051403049		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.0623200051403049 | validation: 0.22881110383290146]
	TIME [epoch: 5.79 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336239507213462		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.06336239507213462 | validation: 0.2110085960198516]
	TIME [epoch: 5.77 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591148061997506		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0591148061997506 | validation: 0.21656241501518325]
	TIME [epoch: 5.78 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925219424868947		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.05925219424868947 | validation: 0.2199402905777312]
	TIME [epoch: 5.79 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060264717241039815		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.060264717241039815 | validation: 0.23633054398611206]
	TIME [epoch: 5.79 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124873046232204		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.07124873046232204 | validation: 0.22054155012836363]
	TIME [epoch: 5.8 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08065626374649218		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.08065626374649218 | validation: 0.2772485544114719]
	TIME [epoch: 5.8 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458328644730568		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.09458328644730568 | validation: 0.18810739376573862]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434133447502627		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06434133447502627 | validation: 0.2965803112668768]
	TIME [epoch: 5.8 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08538557831904896		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.08538557831904896 | validation: 0.23279422175749362]
	TIME [epoch: 5.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07045720174918448		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.07045720174918448 | validation: 0.1950138003890909]
	TIME [epoch: 5.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06226678079642689		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.06226678079642689 | validation: 0.30191286638002657]
	TIME [epoch: 5.81 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761750003666779		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.0761750003666779 | validation: 0.2061036045001363]
	TIME [epoch: 5.81 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768164392646451		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.0768164392646451 | validation: 0.29599063672616166]
	TIME [epoch: 5.8 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303640850213556		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.09303640850213556 | validation: 0.21722358010375623]
	TIME [epoch: 5.8 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08814479067002771		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.08814479067002771 | validation: 0.23700376849784802]
	TIME [epoch: 5.8 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382640765006617		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.06382640765006617 | validation: 0.23569335985590023]
	TIME [epoch: 5.81 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510843673430877		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.06510843673430877 | validation: 0.19985993864668172]
	TIME [epoch: 5.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06590966242602238		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.06590966242602238 | validation: 0.24157396486001895]
	TIME [epoch: 5.8 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06411145815938799		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.06411145815938799 | validation: 0.20323081622552733]
	TIME [epoch: 5.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0610095767867103		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.0610095767867103 | validation: 0.2108538027660326]
	TIME [epoch: 5.81 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06159684039637167		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.06159684039637167 | validation: 0.20999039957549376]
	TIME [epoch: 5.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303770930520403		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.06303770930520403 | validation: 0.25022156977052745]
	TIME [epoch: 5.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0637150461667488		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.0637150461667488 | validation: 0.21065678471810623]
	TIME [epoch: 5.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950501692938929		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05950501692938929 | validation: 0.23367935521152927]
	TIME [epoch: 5.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945281450586129		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.05945281450586129 | validation: 0.18484928618918273]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057758992172562994		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.057758992172562994 | validation: 0.24139279293778892]
	TIME [epoch: 5.78 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061316603979196395		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.061316603979196395 | validation: 0.20679681030516442]
	TIME [epoch: 5.78 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0629543915336242		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.0629543915336242 | validation: 0.24018576089034074]
	TIME [epoch: 5.78 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324417973914506		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.06324417973914506 | validation: 0.23510046258790362]
	TIME [epoch: 5.79 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06232007046642588		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.06232007046642588 | validation: 0.19562313365464923]
	TIME [epoch: 5.78 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365218307221417		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06365218307221417 | validation: 0.26511450182323715]
	TIME [epoch: 5.78 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744272284141207		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.08744272284141207 | validation: 0.21028257841845838]
	TIME [epoch: 5.78 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06520080558652872		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.06520080558652872 | validation: 0.26658832457993603]
	TIME [epoch: 5.77 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186707614849917		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07186707614849917 | validation: 0.22379334163939998]
	TIME [epoch: 5.77 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06094082732901354		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.06094082732901354 | validation: 0.1959726874342583]
	TIME [epoch: 5.79 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06758131356096925		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06758131356096925 | validation: 0.3149482463464576]
	TIME [epoch: 5.79 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08919919901133706		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.08919919901133706 | validation: 0.2001336733613488]
	TIME [epoch: 5.78 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881495522064987		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05881495522064987 | validation: 0.2283768464746081]
	TIME [epoch: 5.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730579539623079		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.05730579539623079 | validation: 0.20279573065693876]
	TIME [epoch: 5.81 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059709943106133534		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.059709943106133534 | validation: 0.21141295548713643]
	TIME [epoch: 5.79 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055298273007096145		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.055298273007096145 | validation: 0.19556354345902102]
	TIME [epoch: 5.78 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065853238444262		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.065853238444262 | validation: 0.2572217429399406]
	TIME [epoch: 5.78 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875146277187071		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07875146277187071 | validation: 0.18416852560479016]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286917321942227		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.06286917321942227 | validation: 0.21163874526421714]
	TIME [epoch: 5.8 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614211014424428		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05614211014424428 | validation: 0.22177968126254136]
	TIME [epoch: 5.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746210486768382		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05746210486768382 | validation: 0.19714603364942052]
	TIME [epoch: 5.79 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06777928754692625		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.06777928754692625 | validation: 0.33357562903308785]
	TIME [epoch: 5.79 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.089508193394587		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.089508193394587 | validation: 0.1994905932466281]
	TIME [epoch: 5.78 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630645007732194		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05630645007732194 | validation: 0.19213432558024937]
	TIME [epoch: 5.79 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059978891426713137		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.059978891426713137 | validation: 0.2566346207069124]
	TIME [epoch: 5.78 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792388578838306		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.06792388578838306 | validation: 0.19542976538693188]
	TIME [epoch: 5.8 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06554990625026308		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.06554990625026308 | validation: 0.24296831040251796]
	TIME [epoch: 5.8 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07947176042685981		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.07947176042685981 | validation: 0.23110223798844798]
	TIME [epoch: 5.79 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065788359268957		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.065788359268957 | validation: 0.20729695436365972]
	TIME [epoch: 5.8 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657091021059973		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.05657091021059973 | validation: 0.2200962821691922]
	TIME [epoch: 5.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055990202340143576		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.055990202340143576 | validation: 0.22533120408472956]
	TIME [epoch: 5.79 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664536744395331		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.05664536744395331 | validation: 0.2179914356638548]
	TIME [epoch: 5.8 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055519460829192034		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.055519460829192034 | validation: 0.18689275694371166]
	TIME [epoch: 5.79 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057251631151350894		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.057251631151350894 | validation: 0.24349337562653714]
	TIME [epoch: 5.8 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553929018563177		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.06553929018563177 | validation: 0.19654983293030898]
	TIME [epoch: 5.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437285949957939		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.07437285949957939 | validation: 0.3150772992080616]
	TIME [epoch: 5.81 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122569175044957		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08122569175044957 | validation: 0.19466850324575674]
	TIME [epoch: 5.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053774304052664465		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.053774304052664465 | validation: 0.18680818452863193]
	TIME [epoch: 5.79 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337910021580768		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06337910021580768 | validation: 0.24676741954532944]
	TIME [epoch: 5.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212778124204029		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.07212778124204029 | validation: 0.21794769622860724]
	TIME [epoch: 5.8 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05837915153192352		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05837915153192352 | validation: 0.19608330467084303]
	TIME [epoch: 5.79 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05830441918702884		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05830441918702884 | validation: 0.25093084197055193]
	TIME [epoch: 5.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062228876255015726		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.062228876255015726 | validation: 0.18336778952943925]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060329779334948826		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.060329779334948826 | validation: 0.2540876131111644]
	TIME [epoch: 5.79 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814181292251116		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06814181292251116 | validation: 0.27641280128303575]
	TIME [epoch: 5.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2066029819878843		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.2066029819878843 | validation: 0.19968895974431794]
	TIME [epoch: 5.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403059940346024		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.06403059940346024 | validation: 0.29043283073223286]
	TIME [epoch: 5.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272173535443172		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.07272173535443172 | validation: 0.28102557422950203]
	TIME [epoch: 5.81 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196324611593957		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.07196324611593957 | validation: 0.21631920938545762]
	TIME [epoch: 5.8 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054896045547094874		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.054896045547094874 | validation: 0.18143501252075747]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343606583246707		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06343606583246707 | validation: 0.2053513034490762]
	TIME [epoch: 5.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054943431546618786		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.054943431546618786 | validation: 0.21566528854866426]
	TIME [epoch: 5.82 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05774566371831437		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05774566371831437 | validation: 0.19821793941457622]
	TIME [epoch: 5.79 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056039258864310576		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.056039258864310576 | validation: 0.21949230886519244]
	TIME [epoch: 5.8 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06378695442532249		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.06378695442532249 | validation: 0.19273049692066807]
	TIME [epoch: 5.79 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05308335857493276		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.05308335857493276 | validation: 0.19812188390789212]
	TIME [epoch: 5.81 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057071959188751284		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.057071959188751284 | validation: 0.2219828070142108]
	TIME [epoch: 5.79 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05651490078964781		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.05651490078964781 | validation: 0.18752245629005887]
	TIME [epoch: 5.81 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055905591268819055		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.055905591268819055 | validation: 0.20595467318394683]
	TIME [epoch: 5.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357932617271459		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.05357932617271459 | validation: 0.19403363354640613]
	TIME [epoch: 5.79 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060649577269898634		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.060649577269898634 | validation: 0.22880542349480706]
	TIME [epoch: 5.78 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846162825254874		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.06846162825254874 | validation: 0.191332101692835]
	TIME [epoch: 5.79 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06030159919623168		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.06030159919623168 | validation: 0.22034447271713004]
	TIME [epoch: 5.78 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328227630309954		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.06328227630309954 | validation: 0.20587939117062404]
	TIME [epoch: 5.79 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054443241553865346		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.054443241553865346 | validation: 0.19323578760232518]
	TIME [epoch: 5.78 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543584357659415		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.05543584357659415 | validation: 0.19197711223435984]
	TIME [epoch: 5.79 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057162598984002264		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.057162598984002264 | validation: 0.2050054085591395]
	TIME [epoch: 5.77 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057840097995524024		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.057840097995524024 | validation: 0.22662038255625183]
	TIME [epoch: 5.79 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06614204404435475		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.06614204404435475 | validation: 0.18130774301605512]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0587209197222262		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.0587209197222262 | validation: 0.22860007146607472]
	TIME [epoch: 5.77 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05857844555594565		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.05857844555594565 | validation: 0.18062767156766085]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735787208912781		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05735787208912781 | validation: 0.21202579107364478]
	TIME [epoch: 5.77 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05383067365514275		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.05383067365514275 | validation: 0.2102965690771706]
	TIME [epoch: 5.78 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054066005910223924		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.054066005910223924 | validation: 0.19588442830713826]
	TIME [epoch: 5.79 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05318050613013929		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05318050613013929 | validation: 0.19737912296534652]
	TIME [epoch: 5.78 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05418748255468752		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05418748255468752 | validation: 0.211163504404681]
	TIME [epoch: 5.79 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056270175533038515		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.056270175533038515 | validation: 0.1775499706939037]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07560186903573961		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.07560186903573961 | validation: 0.27791004607763287]
	TIME [epoch: 5.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08200161864512984		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.08200161864512984 | validation: 0.1911240562761436]
	TIME [epoch: 5.77 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473699337187132		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.05473699337187132 | validation: 0.19681855923648595]
	TIME [epoch: 5.76 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584693235284821		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0584693235284821 | validation: 0.21583812349884274]
	TIME [epoch: 5.79 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06312346180579265		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.06312346180579265 | validation: 0.1718153654644422]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057106263056858175		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.057106263056858175 | validation: 0.2076694313285554]
	TIME [epoch: 5.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05469904263812106		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.05469904263812106 | validation: 0.20412841574832888]
	TIME [epoch: 5.77 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511053855955246		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.05511053855955246 | validation: 0.2115087374369381]
	TIME [epoch: 5.76 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05381282934210482		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.05381282934210482 | validation: 0.1917224891024606]
	TIME [epoch: 5.76 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082436442326791		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.05082436442326791 | validation: 0.20255536901949492]
	TIME [epoch: 5.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292262906825347		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05292262906825347 | validation: 0.19456447047441144]
	TIME [epoch: 5.76 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0565711309276287		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.0565711309276287 | validation: 0.2061353361679398]
	TIME [epoch: 5.74 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06216406344651212		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06216406344651212 | validation: 0.1816747622577105]
	TIME [epoch: 5.76 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05376442507107749		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.05376442507107749 | validation: 0.283954152265461]
	TIME [epoch: 5.75 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07451995567683391		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.07451995567683391 | validation: 0.19454041494379923]
	TIME [epoch: 5.77 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116905386483556		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.07116905386483556 | validation: 0.19608829224192112]
	TIME [epoch: 5.76 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671390801681907		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.0671390801681907 | validation: 0.22685669296554656]
	TIME [epoch: 5.76 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058133834065368706		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.058133834065368706 | validation: 0.20313726407854535]
	TIME [epoch: 5.77 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729878943810743		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.05729878943810743 | validation: 0.19737792150584677]
	TIME [epoch: 5.77 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05236323639442011		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.05236323639442011 | validation: 0.18665821749613765]
	TIME [epoch: 5.76 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051275575210202255		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.051275575210202255 | validation: 0.17966079898792664]
	TIME [epoch: 5.76 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05188120415397253		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.05188120415397253 | validation: 0.20235459397111644]
	TIME [epoch: 5.76 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053524680908036644		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.053524680908036644 | validation: 0.1833890284589352]
	TIME [epoch: 5.77 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053110737724396326		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.053110737724396326 | validation: 0.20458934225916503]
	TIME [epoch: 5.76 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052017534630014066		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.052017534630014066 | validation: 0.18661487715183453]
	TIME [epoch: 5.76 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05362425874764099		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.05362425874764099 | validation: 0.20636103081018115]
	TIME [epoch: 5.76 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452208459274514		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.05452208459274514 | validation: 0.196497583623645]
	TIME [epoch: 5.77 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186633700019163		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.05186633700019163 | validation: 0.20701430036365798]
	TIME [epoch: 5.75 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522864600782937		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.05522864600782937 | validation: 0.1673411922710205]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925332059655516		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.06925332059655516 | validation: 0.24580481172594235]
	TIME [epoch: 5.76 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316774876989911		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.07316774876989911 | validation: 0.17839853720641252]
	TIME [epoch: 5.77 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369604822578659		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.05369604822578659 | validation: 0.21851705244398734]
	TIME [epoch: 5.77 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05272850123066088		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.05272850123066088 | validation: 0.18779903061927802]
	TIME [epoch: 5.76 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05342530234674875		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.05342530234674875 | validation: 0.21482031986380787]
	TIME [epoch: 5.71 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056137373232817006		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.056137373232817006 | validation: 0.2139248753980322]
	TIME [epoch: 5.76 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05308981091344597		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.05308981091344597 | validation: 0.1819834259122577]
	TIME [epoch: 5.73 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054676853556145204		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.054676853556145204 | validation: 0.24039164574195154]
	TIME [epoch: 5.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678404731689229		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.05678404731689229 | validation: 0.18085009372712993]
	TIME [epoch: 5.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05261151733882617		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.05261151733882617 | validation: 0.20885179207436907]
	TIME [epoch: 5.74 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052196567883514663		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.052196567883514663 | validation: 0.21557213493482782]
	TIME [epoch: 5.73 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051098634977725724		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.051098634977725724 | validation: 0.1824661656825198]
	TIME [epoch: 5.74 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05288614264114033		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.05288614264114033 | validation: 0.2515755586366421]
	TIME [epoch: 5.72 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331256934382165		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.06331256934382165 | validation: 0.18104741802203456]
	TIME [epoch: 5.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07051232307241523		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07051232307241523 | validation: 0.2111942351775994]
	TIME [epoch: 5.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061065681406189606		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.061065681406189606 | validation: 0.19528664983692912]
	TIME [epoch: 5.75 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04986157006703685		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.04986157006703685 | validation: 0.18353870233414712]
	TIME [epoch: 5.74 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243959799345321		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.05243959799345321 | validation: 0.1797768837548832]
	TIME [epoch: 5.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05429554538498646		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.05429554538498646 | validation: 0.22424376925940345]
	TIME [epoch: 5.76 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0555656186857075		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.0555656186857075 | validation: 0.1809475875843568]
	TIME [epoch: 5.75 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051725338349922136		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.051725338349922136 | validation: 0.21373456015958095]
	TIME [epoch: 5.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05811655357085629		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.05811655357085629 | validation: 0.16853249294762943]
	TIME [epoch: 5.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639683395892913		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.05639683395892913 | validation: 0.1988844461394297]
	TIME [epoch: 5.76 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05340234003407684		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.05340234003407684 | validation: 0.1908590063880018]
	TIME [epoch: 5.73 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052404691739364906		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.052404691739364906 | validation: 0.1805136987307762]
	TIME [epoch: 5.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056499779929132785		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.056499779929132785 | validation: 0.2732608661566186]
	TIME [epoch: 5.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004844155393487		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.07004844155393487 | validation: 0.18561734024654317]
	TIME [epoch: 5.75 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206283847705276		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.05206283847705276 | validation: 0.1795832600566651]
	TIME [epoch: 5.75 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527405234631582		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.05527405234631582 | validation: 0.21510849531383724]
	TIME [epoch: 5.76 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055681103639788516		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.055681103639788516 | validation: 0.19249786065762267]
	TIME [epoch: 5.71 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050899070453123495		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.050899070453123495 | validation: 0.1893087743980394]
	TIME [epoch: 5.75 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053987673352420135		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.053987673352420135 | validation: 0.1882800210090737]
	TIME [epoch: 5.71 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0512367123241882		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.0512367123241882 | validation: 0.2088997228523579]
	TIME [epoch: 5.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050514102493651604		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.050514102493651604 | validation: 0.18451268243794813]
	TIME [epoch: 5.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051022621788993625		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.051022621788993625 | validation: 0.20678826273157527]
	TIME [epoch: 5.75 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046959872545791585		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.046959872545791585 | validation: 0.18966081111589447]
	TIME [epoch: 5.77 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05191677222062586		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.05191677222062586 | validation: 0.17520807269396238]
	TIME [epoch: 5.78 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972186156431033		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.04972186156431033 | validation: 0.22277739867802027]
	TIME [epoch: 5.78 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059714477323517395		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.059714477323517395 | validation: 0.1767338314276569]
	TIME [epoch: 5.79 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08036387149430811		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.08036387149430811 | validation: 0.2318695474830336]
	TIME [epoch: 5.77 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059844188314378106		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.059844188314378106 | validation: 0.1900327035440546]
	TIME [epoch: 5.78 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052841641172711055		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.052841641172711055 | validation: 0.17957352257444326]
	TIME [epoch: 5.77 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431544762557731		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.06431544762557731 | validation: 0.19732623879726524]
	TIME [epoch: 5.78 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051067524201952104		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.051067524201952104 | validation: 0.19303029578294637]
	TIME [epoch: 5.77 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052646284405396114		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.052646284405396114 | validation: 0.19782234943236793]
	TIME [epoch: 5.78 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931579347685025		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.04931579347685025 | validation: 0.18020992737913497]
	TIME [epoch: 5.78 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051002244890829815		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.051002244890829815 | validation: 0.21287334394128138]
	TIME [epoch: 5.78 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051727913128385296		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.051727913128385296 | validation: 0.1800810777404619]
	TIME [epoch: 5.78 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05172496982279062		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.05172496982279062 | validation: 0.19456035836774296]
	TIME [epoch: 5.78 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04775837770581129		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.04775837770581129 | validation: 0.19247273982322774]
	TIME [epoch: 5.78 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971535026487172		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.04971535026487172 | validation: 0.1817895344181992]
	TIME [epoch: 5.79 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0533429540506515		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0533429540506515 | validation: 0.22334620950950132]
	TIME [epoch: 5.77 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929310816913839		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.05929310816913839 | validation: 0.18849712325746837]
	TIME [epoch: 5.78 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04876875896164956		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.04876875896164956 | validation: 0.1834352683356292]
	TIME [epoch: 5.77 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194402669722814		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.05194402669722814 | validation: 0.22212555730786004]
	TIME [epoch: 5.77 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565084252111161		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.05565084252111161 | validation: 0.17535856833433205]
	TIME [epoch: 5.78 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077836300354943		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.05077836300354943 | validation: 0.1890204631350991]
	TIME [epoch: 5.79 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04984929986756905		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04984929986756905 | validation: 0.1916063928027486]
	TIME [epoch: 5.77 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955442492828437		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04955442492828437 | validation: 0.19884266838396303]
	TIME [epoch: 5.78 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04846107339039704		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04846107339039704 | validation: 0.17550475803899057]
	TIME [epoch: 5.76 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509750568619934		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.05509750568619934 | validation: 0.2363374779636497]
	TIME [epoch: 5.79 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057734543971848414		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.057734543971848414 | validation: 0.16771844849857961]
	TIME [epoch: 5.78 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05217304737986231		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05217304737986231 | validation: 0.1958892593303425]
	TIME [epoch: 5.79 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05073981069785688		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.05073981069785688 | validation: 0.20177836751458655]
	TIME [epoch: 5.78 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058455233834231284		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.058455233834231284 | validation: 0.17358188512450107]
	TIME [epoch: 5.78 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05731961260332868		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.05731961260332868 | validation: 0.21084595730691716]
	TIME [epoch: 5.77 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025774908776872		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.05025774908776872 | validation: 0.1967456132014801]
	TIME [epoch: 5.79 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050379278266817915		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.050379278266817915 | validation: 0.1907835955760743]
	TIME [epoch: 5.77 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04992160656504419		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04992160656504419 | validation: 0.23785364186970293]
	TIME [epoch: 5.78 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13426195269711855		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.13426195269711855 | validation: 0.17695767761941317]
	TIME [epoch: 5.8 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058874130358633		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.05058874130358633 | validation: 0.24502884207320472]
	TIME [epoch: 5.76 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060186741975746874		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.060186741975746874 | validation: 0.22769166422578135]
	TIME [epoch: 5.78 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055874630746253325		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.055874630746253325 | validation: 0.16949625957105888]
	TIME [epoch: 5.78 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05053627384196561		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.05053627384196561 | validation: 0.19329776697469941]
	TIME [epoch: 5.74 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052277704451205054		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.052277704451205054 | validation: 0.17469725193149133]
	TIME [epoch: 5.77 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04637134781453162		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.04637134781453162 | validation: 0.19544904682767372]
	TIME [epoch: 5.75 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052255641858415684		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.052255641858415684 | validation: 0.17768448120932312]
	TIME [epoch: 5.75 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050462081234255905		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.050462081234255905 | validation: 0.19437325523624463]
	TIME [epoch: 5.75 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047863130183032045		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.047863130183032045 | validation: 0.18542278467809262]
	TIME [epoch: 5.75 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04813599464130713		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.04813599464130713 | validation: 0.19304559347144368]
	TIME [epoch: 5.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666997850443412		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.04666997850443412 | validation: 0.18658442706727826]
	TIME [epoch: 5.76 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04904859917487601		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04904859917487601 | validation: 0.18479505696713036]
	TIME [epoch: 5.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050399495663360466		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.050399495663360466 | validation: 0.17916482627062652]
	TIME [epoch: 5.76 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047496310562410965		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.047496310562410965 | validation: 0.1952833042967341]
	TIME [epoch: 5.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781869959644466		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.04781869959644466 | validation: 0.18829649836290058]
	TIME [epoch: 5.77 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048853882684345144		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.048853882684345144 | validation: 0.176458068006662]
	TIME [epoch: 5.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394201734640677		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.05394201734640677 | validation: 0.21536570494402274]
	TIME [epoch: 5.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056085810369157746		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.056085810369157746 | validation: 0.18323059281552811]
	TIME [epoch: 5.75 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05129094719144924		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.05129094719144924 | validation: 0.18005327034748528]
	TIME [epoch: 5.77 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04548344695248835		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.04548344695248835 | validation: 0.1721654175541587]
	TIME [epoch: 5.77 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022901453391418		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.05022901453391418 | validation: 0.19751208705005763]
	TIME [epoch: 5.78 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050334363749097		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.05050334363749097 | validation: 0.17894601758657613]
	TIME [epoch: 5.74 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557279081906029		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.05557279081906029 | validation: 0.2063016547866183]
	TIME [epoch: 5.78 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547661152576674		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.05547661152576674 | validation: 0.1725570410681926]
	TIME [epoch: 5.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048288271688160286		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.048288271688160286 | validation: 0.17500038027981202]
	TIME [epoch: 5.75 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04907774279129326		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.04907774279129326 | validation: 0.1928731179023293]
	TIME [epoch: 5.72 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949046837759911		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.04949046837759911 | validation: 0.17811193540722575]
	TIME [epoch: 5.74 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241774604734524		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.05241774604734524 | validation: 0.18593097414104928]
	TIME [epoch: 5.72 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051468124631759225		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.051468124631759225 | validation: 0.20473723946757805]
	TIME [epoch: 5.74 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04937075792706366		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.04937075792706366 | validation: 0.17930745742965573]
	TIME [epoch: 5.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04987323735510575		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.04987323735510575 | validation: 0.21755234130270598]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132713/states/model_phi1_4b_v_mmd1_948.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4242.950 seconds.
