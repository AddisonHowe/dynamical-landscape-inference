Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 904762345

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.811666886089372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.811666886089372 | validation: 3.631594523103997]
	TIME [epoch: 157 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.946730088362293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.946730088362293 | validation: 2.828837759689789]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.194974967298806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.194974967298806 | validation: 2.3699038897710167]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.694683110329127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.694683110329127 | validation: 2.1570354915329886]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5080056960094295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5080056960094295 | validation: 1.9087434942819705]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2953611887454124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2953611887454124 | validation: 1.730649997289471]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0638498868224886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0638498868224886 | validation: 1.4963917378140574]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7560044341846146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7560044341846146 | validation: 2.185816148403245]
	TIME [epoch: 2.67 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2207091655453928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2207091655453928 | validation: 2.535150089322663]
	TIME [epoch: 2.67 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.741636119425926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.741636119425926 | validation: 2.71995514169681]
	TIME [epoch: 2.67 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.532669224556956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.532669224556956 | validation: 2.629047757093433]
	TIME [epoch: 2.67 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.845847986612437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.845847986612437 | validation: 2.1216079026089902]
	TIME [epoch: 2.67 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3187032585408995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3187032585408995 | validation: 1.4114932386021959]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7759389000935664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7759389000935664 | validation: 1.3657824415083617]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6302680596400805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6302680596400805 | validation: 1.2386782994387522]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4950821463096202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4950821463096202 | validation: 1.2025123215980065]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41808817956204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.41808817956204 | validation: 1.205546149360342]
	TIME [epoch: 2.68 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3359937484252793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3359937484252793 | validation: 1.1170696239150553]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2580375953523928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2580375953523928 | validation: 1.1285762891786968]
	TIME [epoch: 2.68 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2141481920542878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2141481920542878 | validation: 1.1012611333061675]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1831241333559341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1831241333559341 | validation: 1.0516811826361028]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1601182282641074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1601182282641074 | validation: 1.0330877702874763]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310038347735034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1310038347735034 | validation: 1.0120673874246986]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1010298457994006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1010298457994006 | validation: 1.0496896049292808]
	TIME [epoch: 2.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324345938103428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1324345938103428 | validation: 0.9900288032629252]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143291872330034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.143291872330034 | validation: 1.0013786784118073]
	TIME [epoch: 2.67 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1214622811229595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1214622811229595 | validation: 1.0162039274486447]
	TIME [epoch: 2.67 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281242148506936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1281242148506936 | validation: 0.9450701519216111]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104097150056526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.104097150056526 | validation: 0.9697212345647602]
	TIME [epoch: 2.67 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253980369811951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1253980369811951 | validation: 1.0033194693378558]
	TIME [epoch: 2.67 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1218244614216588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1218244614216588 | validation: 0.9196440035380263]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0343422921978676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0343422921978676 | validation: 0.9203482473303612]
	TIME [epoch: 2.67 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0732780824118076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0732780824118076 | validation: 1.0101354561362417]
	TIME [epoch: 2.66 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2047969668581453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2047969668581453 | validation: 1.1197249275888932]
	TIME [epoch: 2.67 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2279254020432508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2279254020432508 | validation: 0.9160128836560593]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0369725206346272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0369725206346272 | validation: 0.9583306561700026]
	TIME [epoch: 2.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116449958928413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.116449958928413 | validation: 0.9390170431366811]
	TIME [epoch: 2.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.070765126703237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.070765126703237 | validation: 0.9210349453267717]
	TIME [epoch: 2.67 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9857855007277705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9857855007277705 | validation: 0.8955829551615815]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9811927309268336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9811927309268336 | validation: 0.9217819848935659]
	TIME [epoch: 2.67 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0140081454458851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0140081454458851 | validation: 0.8658284385915347]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0243283214446972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0243283214446972 | validation: 0.8444371610390538]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9759772922956631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9759772922956631 | validation: 0.853001008726439]
	TIME [epoch: 2.68 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9615879978368974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9615879978368974 | validation: 0.8831648863696376]
	TIME [epoch: 2.68 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9601531971969393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9601531971969393 | validation: 0.8465907726199934]
	TIME [epoch: 2.68 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577108331750236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9577108331750236 | validation: 0.8822595671480666]
	TIME [epoch: 2.68 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9501278529232662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9501278529232662 | validation: 0.8574199422186963]
	TIME [epoch: 2.67 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951198173659067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951198173659067 | validation: 0.904544660385115]
	TIME [epoch: 2.68 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0419549344751782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0419549344751782 | validation: 0.880782086581]
	TIME [epoch: 2.68 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0266790898003892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0266790898003892 | validation: 0.8414889220224309]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951751130239897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951751130239897 | validation: 0.8649183298157599]
	TIME [epoch: 2.68 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579461625848111		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9579461625848111 | validation: 0.8274113918374684]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0147177610470517		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.0147177610470517 | validation: 0.9096457381665961]
	TIME [epoch: 2.68 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0450180849060604		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.0450180849060604 | validation: 0.8837830491359324]
	TIME [epoch: 2.68 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9700459588505314		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.9700459588505314 | validation: 0.879222123422927]
	TIME [epoch: 2.68 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0440803026602823		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.0440803026602823 | validation: 0.8083954748191676]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915506319615367		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.915506319615367 | validation: 0.8108122055038518]
	TIME [epoch: 2.68 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9324964545839646		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9324964545839646 | validation: 0.8038897624365431]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265447492522964		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.9265447492522964 | validation: 0.8266850177699379]
	TIME [epoch: 2.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114996361474548		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9114996361474548 | validation: 0.8270592672078294]
	TIME [epoch: 2.67 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027806337473233		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.027806337473233 | validation: 0.9023136154900361]
	TIME [epoch: 2.67 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0744493784007012		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.0744493784007012 | validation: 0.8202405951016403]
	TIME [epoch: 2.67 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929251198994451		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.929251198994451 | validation: 0.7742982717985504]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440455063776341		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9440455063776341 | validation: 0.7988431005696439]
	TIME [epoch: 2.67 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9370067250502239		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9370067250502239 | validation: 0.8407051677600347]
	TIME [epoch: 2.67 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9695704644987345		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9695704644987345 | validation: 0.8037711440956408]
	TIME [epoch: 2.66 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957099205507144		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.957099205507144 | validation: 0.7666902774321006]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226459634169595		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.9226459634169595 | validation: 0.7917734605688077]
	TIME [epoch: 2.71 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131897101051502		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9131897101051502 | validation: 0.7935524045952449]
	TIME [epoch: 2.67 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966313208136653		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8966313208136653 | validation: 0.7571140112961945]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190401462121269		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.9190401462121269 | validation: 0.7662857179634601]
	TIME [epoch: 2.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907689806702925		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8907689806702925 | validation: 0.8434193841133926]
	TIME [epoch: 2.67 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374281978887278		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9374281978887278 | validation: 0.8194847084075537]
	TIME [epoch: 2.67 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666761901820564		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9666761901820564 | validation: 0.7947826015114314]
	TIME [epoch: 2.67 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9534354459404589		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9534354459404589 | validation: 0.8157153771748608]
	TIME [epoch: 2.67 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959786310968201		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.959786310968201 | validation: 0.7847521986773659]
	TIME [epoch: 2.66 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014578711827471		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.9014578711827471 | validation: 0.8549992119028436]
	TIME [epoch: 2.67 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0466160042517367		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.0466160042517367 | validation: 0.7751181559359215]
	TIME [epoch: 2.67 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9213679585385072		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9213679585385072 | validation: 0.8308478245737195]
	TIME [epoch: 2.7 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.955105891955803		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.955105891955803 | validation: 0.7360180592879038]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868612525306798		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8868612525306798 | validation: 0.7680911675402536]
	TIME [epoch: 2.67 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918985928476971		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.918985928476971 | validation: 0.7994129911962209]
	TIME [epoch: 2.67 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9456801991649999		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.9456801991649999 | validation: 0.7448140736047746]
	TIME [epoch: 2.67 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846394955078646		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8846394955078646 | validation: 0.7403173640825517]
	TIME [epoch: 2.67 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795675273166625		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8795675273166625 | validation: 0.7377039458372233]
	TIME [epoch: 2.66 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766102313443014		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8766102313443014 | validation: 0.7357772789567009]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788267164095729		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8788267164095729 | validation: 0.7464585153234312]
	TIME [epoch: 2.68 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814644448151927		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8814644448151927 | validation: 0.756371286136642]
	TIME [epoch: 2.67 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933282969718627		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8933282969718627 | validation: 0.750696522023783]
	TIME [epoch: 2.68 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722748832675338		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8722748832675338 | validation: 0.741669224077943]
	TIME [epoch: 2.67 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904492773565854		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.904492773565854 | validation: 0.7747890828791139]
	TIME [epoch: 2.67 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300740830437815		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9300740830437815 | validation: 0.7989315235603089]
	TIME [epoch: 2.67 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918429235596299		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.918429235596299 | validation: 0.7958571938457978]
	TIME [epoch: 2.67 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950821054468381		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.950821054468381 | validation: 0.7472668518192024]
	TIME [epoch: 2.68 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860331042914958		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8860331042914958 | validation: 0.8217740384949797]
	TIME [epoch: 2.68 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9946963128242351		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9946963128242351 | validation: 0.7539791998190288]
	TIME [epoch: 2.68 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887174250485302		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8887174250485302 | validation: 0.7709298779100301]
	TIME [epoch: 2.68 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085196897983034		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9085196897983034 | validation: 0.7628090717870415]
	TIME [epoch: 2.68 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064090897687618		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9064090897687618 | validation: 0.7178933066181525]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769549892008603		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8769549892008603 | validation: 0.7477340459353842]
	TIME [epoch: 2.67 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8835871971931257		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8835871971931257 | validation: 0.7666607885208894]
	TIME [epoch: 2.68 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194341601400254		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.9194341601400254 | validation: 0.7932692774348304]
	TIME [epoch: 2.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036611864657377		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9036611864657377 | validation: 0.7403689242415579]
	TIME [epoch: 2.67 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821112014167051		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8821112014167051 | validation: 0.7679209890100384]
	TIME [epoch: 2.68 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222762202810377		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9222762202810377 | validation: 0.7731007970274018]
	TIME [epoch: 2.68 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245862575568251		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.9245862575568251 | validation: 0.7971096083317053]
	TIME [epoch: 2.68 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9760776314739432		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9760776314739432 | validation: 0.7383093162169234]
	TIME [epoch: 2.68 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799778653783193		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8799778653783193 | validation: 0.7686045327176698]
	TIME [epoch: 3.07 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9230603156035997		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9230603156035997 | validation: 0.732122454133424]
	TIME [epoch: 2.68 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887906946007751		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8887906946007751 | validation: 0.7864339465851428]
	TIME [epoch: 2.68 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204377753884038		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9204377753884038 | validation: 0.7219727473566593]
	TIME [epoch: 2.68 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687230605316413		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8687230605316413 | validation: 0.7254571117473863]
	TIME [epoch: 2.68 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038526400885064		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9038526400885064 | validation: 0.7640866675097724]
	TIME [epoch: 2.68 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327829497375043		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9327829497375043 | validation: 0.733997737958524]
	TIME [epoch: 2.68 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685218480160343		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8685218480160343 | validation: 0.7565876862787767]
	TIME [epoch: 2.68 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099273198717883		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9099273198717883 | validation: 0.7475726297275349]
	TIME [epoch: 2.68 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789777526578307		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8789777526578307 | validation: 0.7775873431346512]
	TIME [epoch: 2.68 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880861402890666		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8880861402890666 | validation: 0.7272534979834195]
	TIME [epoch: 2.68 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886495733977238		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.886495733977238 | validation: 0.7314599385073921]
	TIME [epoch: 2.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629802813530736		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8629802813530736 | validation: 0.7446531163289176]
	TIME [epoch: 2.68 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549601546008944		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8549601546008944 | validation: 0.7151520134401672]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587759853638812		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8587759853638812 | validation: 0.6993124301964904]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642594278919081		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8642594278919081 | validation: 0.7556777835929601]
	TIME [epoch: 2.68 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017309774427466		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9017309774427466 | validation: 0.7736847010904722]
	TIME [epoch: 2.68 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9708555304689672		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.9708555304689672 | validation: 0.7373935553756662]
	TIME [epoch: 2.68 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873374317223968		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8873374317223968 | validation: 0.747619567557759]
	TIME [epoch: 2.68 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750601073468078		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8750601073468078 | validation: 0.7176381658086172]
	TIME [epoch: 2.68 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834714532693221		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8834714532693221 | validation: 0.7026205415307324]
	TIME [epoch: 2.68 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8630796392855578		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8630796392855578 | validation: 0.7418523246597291]
	TIME [epoch: 2.68 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707484674444286		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8707484674444286 | validation: 0.7140955068349822]
	TIME [epoch: 2.68 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688618363117201		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8688618363117201 | validation: 0.7776187343376915]
	TIME [epoch: 2.68 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764319556534237		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8764319556534237 | validation: 0.7166366498001429]
	TIME [epoch: 2.68 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609081290084211		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8609081290084211 | validation: 0.7663430550052177]
	TIME [epoch: 2.68 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873806189269131		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.873806189269131 | validation: 0.7721904524697609]
	TIME [epoch: 2.68 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916840558072587		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.916840558072587 | validation: 0.7233141905008433]
	TIME [epoch: 2.68 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8903108490820592		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8903108490820592 | validation: 0.6931315798747235]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486979122753544		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8486979122753544 | validation: 0.72616241676092]
	TIME [epoch: 2.67 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572644732126634		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8572644732126634 | validation: 0.7044230688957]
	TIME [epoch: 2.68 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486812565503786		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8486812565503786 | validation: 0.7260694587959128]
	TIME [epoch: 2.68 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537835419936449		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8537835419936449 | validation: 0.6929506542181554]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842662038164915		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.842662038164915 | validation: 0.7083125764083658]
	TIME [epoch: 2.68 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419304481730248		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8419304481730248 | validation: 0.7345703729149481]
	TIME [epoch: 2.68 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591459161146816		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8591459161146816 | validation: 0.9972724144328453]
	TIME [epoch: 2.68 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108430389750872		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.108430389750872 | validation: 0.8349455855782509]
	TIME [epoch: 2.68 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9885173549181459		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.9885173549181459 | validation: 0.7276451001831483]
	TIME [epoch: 2.68 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8521970725777791		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8521970725777791 | validation: 0.772448115342969]
	TIME [epoch: 2.68 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778715673131215		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8778715673131215 | validation: 0.686422178353717]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310690642138369		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8310690642138369 | validation: 0.6916856814623021]
	TIME [epoch: 2.68 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8311039959131779		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8311039959131779 | validation: 0.7204539429287801]
	TIME [epoch: 2.68 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349877249036166		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8349877249036166 | validation: 0.7007941352295628]
	TIME [epoch: 2.68 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496545051201357		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8496545051201357 | validation: 0.7031487044879609]
	TIME [epoch: 2.68 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417702667479101		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8417702667479101 | validation: 0.7036005226234702]
	TIME [epoch: 2.67 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273039343557548		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8273039343557548 | validation: 0.6965534299060874]
	TIME [epoch: 2.69 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276167236270575		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8276167236270575 | validation: 0.7088621854080609]
	TIME [epoch: 2.67 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080360288976912		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8080360288976912 | validation: 0.7121901215931032]
	TIME [epoch: 2.68 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240374640326046		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8240374640326046 | validation: 0.963926820837577]
	TIME [epoch: 2.67 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0068690238254723		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0068690238254723 | validation: 1.1430839988211434]
	TIME [epoch: 2.68 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2608135440911918		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2608135440911918 | validation: 0.7232097444794352]
	TIME [epoch: 2.67 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814943080210957		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.814943080210957 | validation: 0.8140638150399799]
	TIME [epoch: 2.68 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232674568447635		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.9232674568447635 | validation: 0.6790361164371892]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066553393535623		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8066553393535623 | validation: 0.7095475139623993]
	TIME [epoch: 2.67 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823969852637222		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.823969852637222 | validation: 0.6622424951362869]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677175256397553		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7677175256397553 | validation: 0.6761316356409941]
	TIME [epoch: 2.67 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660726409849025		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7660726409849025 | validation: 0.6460856411502652]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262807962461619		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7262807962461619 | validation: 0.6509339051082378]
	TIME [epoch: 2.67 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6994076015919649		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6994076015919649 | validation: 0.6803836580571909]
	TIME [epoch: 2.67 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476512136400095		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7476512136400095 | validation: 1.7597604003840701]
	TIME [epoch: 2.67 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.746590555315884		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.746590555315884 | validation: 0.9393130471419724]
	TIME [epoch: 2.67 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0284817484116986		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0284817484116986 | validation: 0.7444426768237289]
	TIME [epoch: 2.67 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843245852974971		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.843245852974971 | validation: 0.6813142070413547]
	TIME [epoch: 2.67 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954174928105031		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7954174928105031 | validation: 0.7053677065120761]
	TIME [epoch: 2.67 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044277961082631		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8044277961082631 | validation: 0.6542603039418196]
	TIME [epoch: 2.67 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619903412286223		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7619903412286223 | validation: 0.6656018244532277]
	TIME [epoch: 2.67 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676684171235684		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7676684171235684 | validation: 0.6503050523496665]
	TIME [epoch: 2.67 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367304124971031		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7367304124971031 | validation: 0.6446373884087564]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246817148367546		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7246817148367546 | validation: 0.649112151755584]
	TIME [epoch: 2.67 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017821570343498		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7017821570343498 | validation: 0.6306604283066383]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869325942015613		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6869325942015613 | validation: 0.6249728834583372]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707333108596686		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6707333108596686 | validation: 0.6271697122625715]
	TIME [epoch: 2.68 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615800766182903		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6615800766182903 | validation: 0.6319488175227608]
	TIME [epoch: 2.67 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556428989693807		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6556428989693807 | validation: 0.6423867255571315]
	TIME [epoch: 2.68 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503023267188869		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6503023267188869 | validation: 0.7343310094559051]
	TIME [epoch: 2.67 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770194200504478		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7770194200504478 | validation: 0.894136144508877]
	TIME [epoch: 2.67 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093109549589472		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9093109549589472 | validation: 0.7753696785471722]
	TIME [epoch: 2.66 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433978198817311		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8433978198817311 | validation: 0.6579585378825228]
	TIME [epoch: 2.67 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7294527302176002		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7294527302176002 | validation: 0.6676000588252811]
	TIME [epoch: 2.67 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492724683855148		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7492724683855148 | validation: 0.5988412284024999]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670618717920161		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.670618717920161 | validation: 0.6030650464382955]
	TIME [epoch: 2.67 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595197477885648		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6595197477885648 | validation: 0.6261777597769259]
	TIME [epoch: 2.67 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600474915625122		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6600474915625122 | validation: 0.6807538638698492]
	TIME [epoch: 2.67 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127289957391792		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7127289957391792 | validation: 0.7447338875962377]
	TIME [epoch: 2.67 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517470958487416		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7517470958487416 | validation: 0.7673060237508935]
	TIME [epoch: 2.68 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158235650414207		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8158235650414207 | validation: 0.5916722254694017]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535590920681686		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6535590920681686 | validation: 0.6418365438531399]
	TIME [epoch: 2.67 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960149148111271		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6960149148111271 | validation: 0.615240181121836]
	TIME [epoch: 2.66 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663848036671146		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.663848036671146 | validation: 0.6001406451115243]
	TIME [epoch: 2.67 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6086935177847503		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6086935177847503 | validation: 0.5719610656141925]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983995095824736		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.5983995095824736 | validation: 0.561529580723929]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936824501406015		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.5936824501406015 | validation: 0.6467711400271172]
	TIME [epoch: 2.67 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359585149616332		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6359585149616332 | validation: 0.7951103195590413]
	TIME [epoch: 2.67 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584202483020468		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8584202483020468 | validation: 0.5619436326525735]
	TIME [epoch: 177 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6126146636641943		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6126146636641943 | validation: 0.6133447496805975]
	TIME [epoch: 5.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488689004932848		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6488689004932848 | validation: 0.6810039604538073]
	TIME [epoch: 5.77 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203251656430817		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7203251656430817 | validation: 0.5820146005095558]
	TIME [epoch: 5.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6094425145736707		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6094425145736707 | validation: 0.5928408085308695]
	TIME [epoch: 5.76 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120687443686993		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6120687443686993 | validation: 0.5836615993524917]
	TIME [epoch: 5.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5921818571359304		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.5921818571359304 | validation: 0.5590886263027993]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595774797819421		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.595774797819421 | validation: 0.5678335095614845]
	TIME [epoch: 5.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947301575584175		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.5947301575584175 | validation: 0.6538977096441788]
	TIME [epoch: 5.77 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940717670564928		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6940717670564928 | validation: 0.5538385061123241]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.584000804492897		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.584000804492897 | validation: 0.5376580007377226]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516470356270793		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5516470356270793 | validation: 0.5267853175571657]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482856762471616		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.5482856762471616 | validation: 0.5869830688152525]
	TIME [epoch: 5.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065611116149277		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6065611116149277 | validation: 0.7386898197061419]
	TIME [epoch: 5.78 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115959831789885		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7115959831789885 | validation: 0.7598883381891535]
	TIME [epoch: 5.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400371166368491		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8400371166368491 | validation: 0.5634096929361501]
	TIME [epoch: 5.78 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384138348313328		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6384138348313328 | validation: 0.6219649241872531]
	TIME [epoch: 5.79 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747420415193597		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6747420415193597 | validation: 0.5106330232220068]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5476863977806687		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.5476863977806687 | validation: 0.4935684023621161]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308085775002525		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.5308085775002525 | validation: 0.6361401272000483]
	TIME [epoch: 5.78 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5916935092963476		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5916935092963476 | validation: 0.7330444592068088]
	TIME [epoch: 5.79 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568406732181328		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7568406732181328 | validation: 0.4992401442141535]
	TIME [epoch: 5.77 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5540163520015006		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5540163520015006 | validation: 0.576787017431604]
	TIME [epoch: 5.78 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6110218442039348		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6110218442039348 | validation: 0.5497442508653522]
	TIME [epoch: 5.77 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833537860469583		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.5833537860469583 | validation: 0.49263827306777447]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094099104117951		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.5094099104117951 | validation: 0.4893863343133548]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4946227475987455		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.4946227475987455 | validation: 0.49068945834340294]
	TIME [epoch: 5.77 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4965108158101353		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4965108158101353 | validation: 0.5222499624895076]
	TIME [epoch: 5.76 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5061488646850895		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.5061488646850895 | validation: 0.599717172351829]
	TIME [epoch: 5.77 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6152645535347288		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6152645535347288 | validation: 0.5459557903764466]
	TIME [epoch: 5.77 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571794751685231		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.5571794751685231 | validation: 0.5763078825461846]
	TIME [epoch: 5.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600204329570046		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.600204329570046 | validation: 0.5075432056519505]
	TIME [epoch: 5.76 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026264888546826		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5026264888546826 | validation: 0.46875986734157477]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4706690038786151		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.4706690038786151 | validation: 0.4879841807055634]
	TIME [epoch: 5.76 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47569017649052625		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.47569017649052625 | validation: 0.476154908205358]
	TIME [epoch: 5.74 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48123787573750376		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.48123787573750376 | validation: 0.5483451199441692]
	TIME [epoch: 5.76 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537045207820083		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.5537045207820083 | validation: 0.6753643449578333]
	TIME [epoch: 5.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129623850795974		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7129623850795974 | validation: 0.4954623856671192]
	TIME [epoch: 5.76 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5321443875634929		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.5321443875634929 | validation: 0.5351023349746449]
	TIME [epoch: 5.77 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523051689706265		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.5523051689706265 | validation: 0.5627039783675573]
	TIME [epoch: 5.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.591317636020283		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.591317636020283 | validation: 0.4558402316463111]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4557914253159345		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.4557914253159345 | validation: 0.4451448438623009]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414980032072932		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.4414980032072932 | validation: 0.4555513134288501]
	TIME [epoch: 5.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4546817062572869		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.4546817062572869 | validation: 0.4621155216502709]
	TIME [epoch: 5.77 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813380718944672		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.4813380718944672 | validation: 0.5173823392910969]
	TIME [epoch: 5.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49118734102485123		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.49118734102485123 | validation: 0.5603924426518215]
	TIME [epoch: 5.77 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817714176280937		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.5817714176280937 | validation: 0.4404322144189447]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44466846004905436		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.44466846004905436 | validation: 0.4758529104154768]
	TIME [epoch: 5.77 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46790182271476277		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.46790182271476277 | validation: 0.6056704323592447]
	TIME [epoch: 5.75 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403616146283401		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6403616146283401 | validation: 0.44757234832508896]
	TIME [epoch: 5.77 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43525997505255587		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.43525997505255587 | validation: 0.39700080573060115]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.410485868346235		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.410485868346235 | validation: 0.3953753225108801]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39628494244174806		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.39628494244174806 | validation: 0.4027277163483913]
	TIME [epoch: 5.9 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3933669430289873		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.3933669430289873 | validation: 0.39176449836618055]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3889941895880605		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3889941895880605 | validation: 0.4175174919290061]
	TIME [epoch: 5.77 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4190602208573489		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.4190602208573489 | validation: 0.6692913825722671]
	TIME [epoch: 5.78 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234305631571666		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6234305631571666 | validation: 0.6615454518500392]
	TIME [epoch: 5.78 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775776817288484		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.775776817288484 | validation: 0.5475490924894365]
	TIME [epoch: 5.78 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416433393064755		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6416433393064755 | validation: 0.49161479102041394]
	TIME [epoch: 5.77 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421664888359895		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5421664888359895 | validation: 0.4106666792948116]
	TIME [epoch: 5.77 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4337044178455278		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.4337044178455278 | validation: 0.4294573956410407]
	TIME [epoch: 5.77 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40168013681005255		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.40168013681005255 | validation: 0.4813439886144221]
	TIME [epoch: 5.77 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41436178798611656		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.41436178798611656 | validation: 0.4158257593616044]
	TIME [epoch: 5.76 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42744081046358956		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.42744081046358956 | validation: 0.4416060785008167]
	TIME [epoch: 5.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4216898220286607		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4216898220286607 | validation: 0.38696657804721957]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3981007167771514		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3981007167771514 | validation: 0.3851472902759362]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3657153250829245		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.3657153250829245 | validation: 0.385353927704415]
	TIME [epoch: 5.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920801394600529		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.3920801394600529 | validation: 0.4905062688324046]
	TIME [epoch: 5.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4367726435790278		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.4367726435790278 | validation: 0.4912526579506697]
	TIME [epoch: 5.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5351750943929686		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.5351750943929686 | validation: 0.35879241294063435]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886282025300059		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3886282025300059 | validation: 0.5064502837717054]
	TIME [epoch: 5.79 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4621464783167346		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.4621464783167346 | validation: 0.4562186785841535]
	TIME [epoch: 5.77 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4789180468271524		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.4789180468271524 | validation: 0.3826400607683682]
	TIME [epoch: 5.79 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355671718856861		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.355671718856861 | validation: 0.42649618760711533]
	TIME [epoch: 5.78 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.423178545057136		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.423178545057136 | validation: 0.4140699244463303]
	TIME [epoch: 5.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4744212159204789		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.4744212159204789 | validation: 0.3766689085314743]
	TIME [epoch: 5.79 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34652267625116706		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.34652267625116706 | validation: 0.41279847552375726]
	TIME [epoch: 5.78 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3732454009858824		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3732454009858824 | validation: 0.40996288069489695]
	TIME [epoch: 5.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44565871242491795		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.44565871242491795 | validation: 0.40715503965528105]
	TIME [epoch: 5.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3449340994259444		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3449340994259444 | validation: 0.31761825653965486]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3054472745403748		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.3054472745403748 | validation: 0.3421382712444615]
	TIME [epoch: 5.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3531476235514374		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.3531476235514374 | validation: 0.4236381429450795]
	TIME [epoch: 5.81 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36714962297834125		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.36714962297834125 | validation: 0.3900437517431496]
	TIME [epoch: 5.79 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40556032616542637		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.40556032616542637 | validation: 0.2927597409406885]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29748600670579084		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.29748600670579084 | validation: 0.32678910500387065]
	TIME [epoch: 5.76 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005633735402711		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3005633735402711 | validation: 0.3653872808107882]
	TIME [epoch: 5.81 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36971209664524274		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.36971209664524274 | validation: 0.38502955108054077]
	TIME [epoch: 5.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33361008716200397		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.33361008716200397 | validation: 0.3636394970333717]
	TIME [epoch: 5.81 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3784743671235583		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3784743671235583 | validation: 0.32290259441538444]
	TIME [epoch: 5.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29291290924923613		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.29291290924923613 | validation: 0.3159075505437728]
	TIME [epoch: 5.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29419441905559107		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.29419441905559107 | validation: 0.40346079958425474]
	TIME [epoch: 5.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39144148476990326		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.39144148476990326 | validation: 0.34230159952868383]
	TIME [epoch: 5.81 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28512584874693025		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.28512584874693025 | validation: 0.2663167993753994]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30601685603690043		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.30601685603690043 | validation: 0.45078481776366514]
	TIME [epoch: 5.77 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35448721755978513		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.35448721755978513 | validation: 0.41835641397649304]
	TIME [epoch: 5.77 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4519559129603982		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.4519559129603982 | validation: 0.2854683290593049]
	TIME [epoch: 5.77 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769914624830155		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.2769914624830155 | validation: 0.4975881181629239]
	TIME [epoch: 5.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4230704768505067		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.4230704768505067 | validation: 0.4990685714492652]
	TIME [epoch: 5.81 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5748577306635587		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.5748577306635587 | validation: 0.36860029285508467]
	TIME [epoch: 5.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4205800450128797		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.4205800450128797 | validation: 0.4561618627640918]
	TIME [epoch: 5.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3867009245473007		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.3867009245473007 | validation: 0.26099681752420656]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26767896014346887		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.26767896014346887 | validation: 0.24394192554018007]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26471553921990526		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.26471553921990526 | validation: 0.31303857280457575]
	TIME [epoch: 5.81 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25459659220264697		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.25459659220264697 | validation: 0.26427665191338007]
	TIME [epoch: 5.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2609920537321982		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.2609920537321982 | validation: 0.35498308654027766]
	TIME [epoch: 5.81 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3150041408128043		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.3150041408128043 | validation: 0.3655110353354996]
	TIME [epoch: 5.81 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40283421441799105		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.40283421441799105 | validation: 0.2776099969505665]
	TIME [epoch: 5.82 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787173526860129		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2787173526860129 | validation: 0.44611497739815925]
	TIME [epoch: 5.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3700882606760116		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.3700882606760116 | validation: 0.36013532801893455]
	TIME [epoch: 5.81 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40445189626948674		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.40445189626948674 | validation: 0.35359420887674764]
	TIME [epoch: 5.81 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312183916027053		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.312183916027053 | validation: 0.3550613830374319]
	TIME [epoch: 5.82 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096353908079313		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.3096353908079313 | validation: 0.2741898732111019]
	TIME [epoch: 5.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33003927667296923		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.33003927667296923 | validation: 0.35371175364604623]
	TIME [epoch: 5.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769953052087194		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.2769953052087194 | validation: 0.2476011821829568]
	TIME [epoch: 5.81 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22545715252517368		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.22545715252517368 | validation: 0.23876403756081396]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2573942447843873		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2573942447843873 | validation: 0.3723443727701195]
	TIME [epoch: 5.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28991978358267484		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.28991978358267484 | validation: 0.3073576493003347]
	TIME [epoch: 5.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290834777226547		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3290834777226547 | validation: 0.22677147654559182]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22625519590433132		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.22625519590433132 | validation: 0.34774376445628247]
	TIME [epoch: 5.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2820282018961045		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2820282018961045 | validation: 0.3180994990534607]
	TIME [epoch: 5.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3670586526993549		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.3670586526993549 | validation: 0.2554074223018383]
	TIME [epoch: 5.81 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24693800152519055		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.24693800152519055 | validation: 0.4198136590796372]
	TIME [epoch: 5.81 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578817765057567		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.3578817765057567 | validation: 0.34009501543384224]
	TIME [epoch: 5.81 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.375705445152266		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.375705445152266 | validation: 0.3559360714917844]
	TIME [epoch: 5.81 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30481959370061623		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.30481959370061623 | validation: 0.3495856908223647]
	TIME [epoch: 5.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29073940068103027		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.29073940068103027 | validation: 0.28231871736339836]
	TIME [epoch: 5.81 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32342459145216834		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.32342459145216834 | validation: 0.28456178987191727]
	TIME [epoch: 5.81 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22480340613740646		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.22480340613740646 | validation: 0.23411439003337886]
	TIME [epoch: 5.81 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21365850662078448		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.21365850662078448 | validation: 0.2208153850712966]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23476907208278333		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.23476907208278333 | validation: 0.3051274369678223]
	TIME [epoch: 5.79 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.241207207387371		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.241207207387371 | validation: 0.24697293653201946]
	TIME [epoch: 5.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889835844050996		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.2889835844050996 | validation: 0.24274582884759616]
	TIME [epoch: 5.81 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20464882008971969		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.20464882008971969 | validation: 0.2526606368032109]
	TIME [epoch: 5.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22752286800830987		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.22752286800830987 | validation: 0.26361097946943174]
	TIME [epoch: 5.79 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31239577518967465		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.31239577518967465 | validation: 0.32619386843838244]
	TIME [epoch: 5.81 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24050351947618012		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.24050351947618012 | validation: 0.20054812484321083]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2166296659261138		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.2166296659261138 | validation: 0.25648925875728007]
	TIME [epoch: 5.81 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2017801309655546		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.2017801309655546 | validation: 0.2064421394809438]
	TIME [epoch: 5.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21204060621924103		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.21204060621924103 | validation: 0.3264909645990881]
	TIME [epoch: 5.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690002268678553		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.2690002268678553 | validation: 0.3465240253130572]
	TIME [epoch: 5.81 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40298469509254664		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.40298469509254664 | validation: 0.3321554807722895]
	TIME [epoch: 5.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28151873428714846		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.28151873428714846 | validation: 0.42547800786698337]
	TIME [epoch: 5.81 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3684077254122303		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.3684077254122303 | validation: 0.32143252316923043]
	TIME [epoch: 5.81 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.377238325004962		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.377238325004962 | validation: 0.3752413571044064]
	TIME [epoch: 5.81 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30471806841046184		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.30471806841046184 | validation: 0.33397873992001276]
	TIME [epoch: 5.81 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25332480717473227		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.25332480717473227 | validation: 0.2502510344282281]
	TIME [epoch: 5.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29729550145987105		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.29729550145987105 | validation: 0.2169038875652098]
	TIME [epoch: 5.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18996113321553773		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.18996113321553773 | validation: 0.2520385217539422]
	TIME [epoch: 5.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1963008483706717		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1963008483706717 | validation: 0.19821499624978123]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20788127582385307		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20788127582385307 | validation: 0.2378924200709522]
	TIME [epoch: 5.8 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20551288392432562		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.20551288392432562 | validation: 0.2121972438388073]
	TIME [epoch: 5.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22201696424584316		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.22201696424584316 | validation: 0.2367740848533325]
	TIME [epoch: 5.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18849340131006415		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.18849340131006415 | validation: 0.1688131653857935]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18461010301940337		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.18461010301940337 | validation: 0.2222610842584908]
	TIME [epoch: 5.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18409912064188796		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.18409912064188796 | validation: 0.18683762040473736]
	TIME [epoch: 5.78 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21346246898924442		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.21346246898924442 | validation: 0.35677289079574925]
	TIME [epoch: 5.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25466855446569236		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.25466855446569236 | validation: 0.3082560967597101]
	TIME [epoch: 5.79 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3579899720170802		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.3579899720170802 | validation: 0.28136732392274294]
	TIME [epoch: 5.81 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25380772434835347		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.25380772434835347 | validation: 0.3728761034227904]
	TIME [epoch: 5.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3133331323848796		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3133331323848796 | validation: 0.2649350804595379]
	TIME [epoch: 5.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036803294433361		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.3036803294433361 | validation: 0.3106026877622423]
	TIME [epoch: 5.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24837993372703074		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.24837993372703074 | validation: 0.2708401745163414]
	TIME [epoch: 5.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22331151184895642		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.22331151184895642 | validation: 0.2439602163700184]
	TIME [epoch: 5.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27182576640972517		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.27182576640972517 | validation: 0.260458666569977]
	TIME [epoch: 5.82 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20253422357913642		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.20253422357913642 | validation: 0.2494882462910385]
	TIME [epoch: 5.79 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2056234491549549		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2056234491549549 | validation: 0.20114095651011815]
	TIME [epoch: 5.82 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25588492762076726		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.25588492762076726 | validation: 0.2847266643370979]
	TIME [epoch: 5.81 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21030958731841018		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.21030958731841018 | validation: 0.170214650768233]
	TIME [epoch: 5.81 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19072803354099335		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.19072803354099335 | validation: 0.2048062490931328]
	TIME [epoch: 5.81 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17834563547688312		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.17834563547688312 | validation: 0.17837425421498174]
	TIME [epoch: 5.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17299233534213385		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.17299233534213385 | validation: 0.20863496944163754]
	TIME [epoch: 5.81 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17129024450031652		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.17129024450031652 | validation: 0.18014470837099184]
	TIME [epoch: 5.81 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17518455519070636		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.17518455519070636 | validation: 0.22364035990170797]
	TIME [epoch: 5.81 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18708583751309568		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.18708583751309568 | validation: 0.21240318912764966]
	TIME [epoch: 5.81 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24156153997552932		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.24156153997552932 | validation: 0.2805945632413754]
	TIME [epoch: 5.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20200417412376717		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.20200417412376717 | validation: 0.17897462918696994]
	TIME [epoch: 5.81 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21920961515581913		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.21920961515581913 | validation: 0.3172604732117806]
	TIME [epoch: 5.81 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2158860410190514		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2158860410190514 | validation: 0.25015057975115973]
	TIME [epoch: 5.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2622183778364945		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.2622183778364945 | validation: 0.18751398954041895]
	TIME [epoch: 5.81 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804127436342532		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.1804127436342532 | validation: 0.21013402443011586]
	TIME [epoch: 5.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16628950464581313		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.16628950464581313 | validation: 0.15566555404081592]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15818241384522522		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.15818241384522522 | validation: 0.20487452285739907]
	TIME [epoch: 5.79 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610540041593212		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.1610540041593212 | validation: 0.19591242723410596]
	TIME [epoch: 5.79 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875589419671502		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1875589419671502 | validation: 0.32833749401919676]
	TIME [epoch: 5.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26963677990560775		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.26963677990560775 | validation: 0.2817562501764841]
	TIME [epoch: 5.79 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3412658101933269		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3412658101933269 | validation: 0.31211938039423576]
	TIME [epoch: 5.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2683832106079927		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.2683832106079927 | validation: 0.5183941116909309]
	TIME [epoch: 5.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417377064970889		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4417377064970889 | validation: 0.27415578028581145]
	TIME [epoch: 5.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3270419071194824		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.3270419071194824 | validation: 0.2625681901482478]
	TIME [epoch: 5.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20363173420945052		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.20363173420945052 | validation: 0.2681087327655572]
	TIME [epoch: 5.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17817186632007434		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.17817186632007434 | validation: 0.16919034564877378]
	TIME [epoch: 5.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20528889487644603		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.20528889487644603 | validation: 0.25112702414934535]
	TIME [epoch: 5.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18680240672844797		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.18680240672844797 | validation: 0.17245434186088904]
	TIME [epoch: 5.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19305328290036555		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.19305328290036555 | validation: 0.19982437521899762]
	TIME [epoch: 5.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652892928414507		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.1652892928414507 | validation: 0.15030284824155726]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14980618842135032		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.14980618842135032 | validation: 0.17405488907001226]
	TIME [epoch: 5.77 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15064468540089132		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.15064468540089132 | validation: 0.16779280961417953]
	TIME [epoch: 5.76 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15096724238594106		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.15096724238594106 | validation: 0.16383671858122956]
	TIME [epoch: 5.77 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15693495985047334		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.15693495985047334 | validation: 0.21336069485946504]
	TIME [epoch: 5.76 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16583336950323563		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.16583336950323563 | validation: 0.18474252929313578]
	TIME [epoch: 5.78 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22683605791404646		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.22683605791404646 | validation: 0.2914371854192369]
	TIME [epoch: 5.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19866342067007453		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.19866342067007453 | validation: 0.1682653965908335]
	TIME [epoch: 5.78 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21554976616215563		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.21554976616215563 | validation: 0.20553239345238516]
	TIME [epoch: 5.77 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16201969248293846		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.16201969248293846 | validation: 0.16392571090020344]
	TIME [epoch: 5.78 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582972006096307		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.1582972006096307 | validation: 0.21123731533422593]
	TIME [epoch: 5.77 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622524814151263		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.1622524814151263 | validation: 0.2397287503163736]
	TIME [epoch: 5.79 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24147752287600974		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.24147752287600974 | validation: 0.304765391045925]
	TIME [epoch: 5.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2240147704060898		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2240147704060898 | validation: 0.17269086349534213]
	TIME [epoch: 5.78 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136940590479854		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.2136940590479854 | validation: 0.2274459098762013]
	TIME [epoch: 5.77 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577326929683607		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.1577326929683607 | validation: 0.18815887176377566]
	TIME [epoch: 5.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16089026973650256		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.16089026973650256 | validation: 0.1565495103135239]
	TIME [epoch: 5.76 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18834312394108785		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.18834312394108785 | validation: 0.25943024350569993]
	TIME [epoch: 5.78 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741877124183879		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.1741877124183879 | validation: 0.1787694304643804]
	TIME [epoch: 5.77 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20795085010437628		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.20795085010437628 | validation: 0.20849894049583018]
	TIME [epoch: 5.78 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14677934682463925		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.14677934682463925 | validation: 0.15441490641867206]
	TIME [epoch: 5.76 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440896765803221		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.1440896765803221 | validation: 0.21496278553513398]
	TIME [epoch: 5.78 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15057469517319846		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.15057469517319846 | validation: 0.15581728863157443]
	TIME [epoch: 5.71 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16627102855464837		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.16627102855464837 | validation: 0.2988642283839936]
	TIME [epoch: 5.78 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19483666934928437		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.19483666934928437 | validation: 0.23477358139547755]
	TIME [epoch: 5.71 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756580666373355		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.2756580666373355 | validation: 0.17541168203758534]
	TIME [epoch: 5.78 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15075727368604575		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.15075727368604575 | validation: 0.36387493477134214]
	TIME [epoch: 5.72 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27563650231033543		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.27563650231033543 | validation: 0.2829776248103599]
	TIME [epoch: 5.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005688762137628		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.3005688762137628 | validation: 0.2699327116299958]
	TIME [epoch: 5.75 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23582915987723352		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.23582915987723352 | validation: 0.3287915384941867]
	TIME [epoch: 5.78 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745165725352746		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2745165725352746 | validation: 0.18806358274527668]
	TIME [epoch: 5.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18981044421861776		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.18981044421861776 | validation: 0.26676120668294995]
	TIME [epoch: 5.78 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19597727572386475		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.19597727572386475 | validation: 0.2062614742121443]
	TIME [epoch: 5.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15363983972132858		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.15363983972132858 | validation: 0.16361443083145513]
	TIME [epoch: 5.77 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935657394948629		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1935657394948629 | validation: 0.24144628953616012]
	TIME [epoch: 5.77 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15460962944224027		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.15460962944224027 | validation: 0.15144923095530005]
	TIME [epoch: 5.78 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14125871481072277		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.14125871481072277 | validation: 0.1762276549951175]
	TIME [epoch: 5.76 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13891993897329635		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.13891993897329635 | validation: 0.1382354807978036]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14123117324921056		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.14123117324921056 | validation: 0.1801840173358382]
	TIME [epoch: 5.79 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13654795913109727		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.13654795913109727 | validation: 0.14830999086608396]
	TIME [epoch: 5.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16045279709598936		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.16045279709598936 | validation: 0.2327234616108121]
	TIME [epoch: 5.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.165959126330335		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.165959126330335 | validation: 0.17764435755131047]
	TIME [epoch: 5.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21753336663338171		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.21753336663338171 | validation: 0.2165691947873456]
	TIME [epoch: 5.79 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501742255186168		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.1501742255186168 | validation: 0.15228400582032942]
	TIME [epoch: 5.79 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14719719200997347		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.14719719200997347 | validation: 0.21175890773405215]
	TIME [epoch: 5.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15456634357570054		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.15456634357570054 | validation: 0.18038553399859023]
	TIME [epoch: 5.79 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15109829471007866		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.15109829471007866 | validation: 0.15308968944815893]
	TIME [epoch: 5.79 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17635590588196054		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.17635590588196054 | validation: 0.3225782114512755]
	TIME [epoch: 5.79 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22256383504534555		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.22256383504534555 | validation: 0.19239307475033207]
	TIME [epoch: 5.79 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24637722011608443		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.24637722011608443 | validation: 0.1752185397399319]
	TIME [epoch: 5.79 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1371916859104608		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1371916859104608 | validation: 0.30147211734553]
	TIME [epoch: 5.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22238654658101698		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.22238654658101698 | validation: 0.22636884166529933]
	TIME [epoch: 5.78 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2692945593885785		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.2692945593885785 | validation: 0.1997786665813046]
	TIME [epoch: 5.81 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17123700702521538		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.17123700702521538 | validation: 0.2950633691844961]
	TIME [epoch: 5.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.232104863328134		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.232104863328134 | validation: 0.18180545142915996]
	TIME [epoch: 5.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18153005020698326		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.18153005020698326 | validation: 0.1680118498444502]
	TIME [epoch: 5.78 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334362946479165		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.14334362946479165 | validation: 0.1980232408958841]
	TIME [epoch: 5.79 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15840889382419993		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.15840889382419993 | validation: 0.15221657034275754]
	TIME [epoch: 5.79 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.165068319120993		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.165068319120993 | validation: 0.20854286715327364]
	TIME [epoch: 5.79 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380528408297768		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.1380528408297768 | validation: 0.1513840152688224]
	TIME [epoch: 5.79 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13429659188187074		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.13429659188187074 | validation: 0.17344924073202847]
	TIME [epoch: 5.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205491322424553		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.13205491322424553 | validation: 0.13288259042408876]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1303432195606997		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.1303432195606997 | validation: 0.15435533155190734]
	TIME [epoch: 5.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12846226007843414		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.12846226007843414 | validation: 0.14003451494507152]
	TIME [epoch: 5.78 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12617344577494205		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.12617344577494205 | validation: 0.18703850680142048]
	TIME [epoch: 5.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13098962894814717		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.13098962894814717 | validation: 0.12380357795533824]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540616888068673		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.1540616888068673 | validation: 0.2989542605762453]
	TIME [epoch: 5.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1929999419144314		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.1929999419144314 | validation: 0.22898575018477071]
	TIME [epoch: 5.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.263060641945323		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.263060641945323 | validation: 0.13277273559868874]
	TIME [epoch: 5.79 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12707316098778343		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.12707316098778343 | validation: 0.31164446631228326]
	TIME [epoch: 5.79 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20488846910795244		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.20488846910795244 | validation: 0.18587449929254982]
	TIME [epoch: 5.78 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2093884843532576		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2093884843532576 | validation: 0.14443366440384128]
	TIME [epoch: 5.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394204831099288		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.12394204831099288 | validation: 0.2493624378581644]
	TIME [epoch: 5.79 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16930750194692784		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.16930750194692784 | validation: 0.20380755934460734]
	TIME [epoch: 5.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21998355751077114		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.21998355751077114 | validation: 0.1375063049629021]
	TIME [epoch: 5.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13527514918067618		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.13527514918067618 | validation: 0.3188404031576684]
	TIME [epoch: 5.79 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130678463236923		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2130678463236923 | validation: 0.2134121063017796]
	TIME [epoch: 5.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2142991698081402		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2142991698081402 | validation: 0.15581702927896562]
	TIME [epoch: 5.79 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15611119172320084		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.15611119172320084 | validation: 0.2953223080199587]
	TIME [epoch: 5.79 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17183473802173618		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.17183473802173618 | validation: 0.1603507363322927]
	TIME [epoch: 5.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1567959855734791		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.1567959855734791 | validation: 0.14533925479731477]
	TIME [epoch: 5.79 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14368719908954794		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.14368719908954794 | validation: 0.20512694140246024]
	TIME [epoch: 5.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15002154551492802		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.15002154551492802 | validation: 0.16230409469125215]
	TIME [epoch: 5.79 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13328873162106952		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.13328873162106952 | validation: 0.13059567571492278]
	TIME [epoch: 5.79 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13544730037079844		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.13544730037079844 | validation: 0.1879387672647472]
	TIME [epoch: 5.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13628032740131418		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.13628032740131418 | validation: 0.12939456296104132]
	TIME [epoch: 5.79 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458281956585755		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.1458281956585755 | validation: 0.23990918059994676]
	TIME [epoch: 5.86 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15704087410040093		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.15704087410040093 | validation: 0.16847276940904926]
	TIME [epoch: 5.79 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1884569030101748		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.1884569030101748 | validation: 0.16336105678563664]
	TIME [epoch: 5.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327897112638099		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.12327897112638099 | validation: 0.19405686634587674]
	TIME [epoch: 5.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12935364452229337		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.12935364452229337 | validation: 0.13030932964879346]
	TIME [epoch: 5.79 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422805941632099		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.1422805941632099 | validation: 0.2118853166774468]
	TIME [epoch: 5.79 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14699727721257264		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.14699727721257264 | validation: 0.18804697824379896]
	TIME [epoch: 5.79 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19237584642044803		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.19237584642044803 | validation: 0.1868156220474135]
	TIME [epoch: 5.79 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15445103450007278		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.15445103450007278 | validation: 0.17819246278856526]
	TIME [epoch: 5.79 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1371546944279814		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.1371546944279814 | validation: 0.12519647681416277]
	TIME [epoch: 5.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12319349344337858		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.12319349344337858 | validation: 0.1618304903828065]
	TIME [epoch: 5.79 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12522603356673617		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.12522603356673617 | validation: 0.13893733791655816]
	TIME [epoch: 5.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377345470231549		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.1377345470231549 | validation: 0.2813331624315579]
	TIME [epoch: 5.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21400732266351402		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.21400732266351402 | validation: 0.1892008580078046]
	TIME [epoch: 5.79 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22832092171401955		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.22832092171401955 | validation: 0.17808584239360556]
	TIME [epoch: 5.79 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14165318492373366		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.14165318492373366 | validation: 0.2706546807960825]
	TIME [epoch: 5.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1924361596428726		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.1924361596428726 | validation: 0.1768631536998749]
	TIME [epoch: 5.79 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19140215503975833		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.19140215503975833 | validation: 0.17849306250335803]
	TIME [epoch: 5.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13943871603259142		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.13943871603259142 | validation: 0.203823213109527]
	TIME [epoch: 5.79 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15212503589458656		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.15212503589458656 | validation: 0.13126913752702463]
	TIME [epoch: 5.79 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513736144679107		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.1513736144679107 | validation: 0.19128481765759797]
	TIME [epoch: 175 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13231960541565696		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.13231960541565696 | validation: 0.14368159786299592]
	TIME [epoch: 12.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12384148208452947		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.12384148208452947 | validation: 0.11569541602827034]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11942859874078793		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.11942859874078793 | validation: 0.1755448364009905]
	TIME [epoch: 12.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12119432515631086		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.12119432515631086 | validation: 0.14016679413635172]
	TIME [epoch: 12.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12412021697829584		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.12412021697829584 | validation: 0.16660407496567686]
	TIME [epoch: 12.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1249031186727092		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.1249031186727092 | validation: 0.12768527317826553]
	TIME [epoch: 12.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13503750965976635		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.13503750965976635 | validation: 0.2250376623147898]
	TIME [epoch: 12.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510583040718614		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1510583040718614 | validation: 0.1701500966256857]
	TIME [epoch: 12.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19335925173141466		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.19335925173141466 | validation: 0.25257363672716854]
	TIME [epoch: 12.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.164387690865144		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.164387690865144 | validation: 0.1225583081582255]
	TIME [epoch: 12.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14052683085937545		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.14052683085937545 | validation: 0.14400933194935633]
	TIME [epoch: 12.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1249246620638126		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.1249246620638126 | validation: 0.16240071249711507]
	TIME [epoch: 12.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11807789010995928		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.11807789010995928 | validation: 0.1278931106712474]
	TIME [epoch: 12.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11798333881218995		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.11798333881218995 | validation: 0.17632276683322048]
	TIME [epoch: 12.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12027683952067086		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.12027683952067086 | validation: 0.11882592060937734]
	TIME [epoch: 12.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11883552191997261		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.11883552191997261 | validation: 0.2269428287062949]
	TIME [epoch: 12.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14198648391274474		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.14198648391274474 | validation: 0.12278902741714202]
	TIME [epoch: 12.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15376371430409555		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.15376371430409555 | validation: 0.21618286636502573]
	TIME [epoch: 12.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13861862275721706		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.13861862275721706 | validation: 0.16371505830112232]
	TIME [epoch: 12.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18004227533889589		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.18004227533889589 | validation: 0.19644540132708327]
	TIME [epoch: 12.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15579417321617392		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.15579417321617392 | validation: 0.12374992531144585]
	TIME [epoch: 12.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12679369346736		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12679369346736 | validation: 0.20316613839657754]
	TIME [epoch: 12.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12297917329186715		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.12297917329186715 | validation: 0.11988041132912902]
	TIME [epoch: 12.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12182181469318268		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.12182181469318268 | validation: 0.16503940705443843]
	TIME [epoch: 12.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11252661756584115		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.11252661756584115 | validation: 0.1311739479534678]
	TIME [epoch: 12.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11863376320700443		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.11863376320700443 | validation: 0.17054223327002702]
	TIME [epoch: 12.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12303895685616277		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.12303895685616277 | validation: 0.14006132830710438]
	TIME [epoch: 12.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518171604464915		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1518171604464915 | validation: 0.1726904418258725]
	TIME [epoch: 12.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13513711756526714		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.13513711756526714 | validation: 0.12753473789654013]
	TIME [epoch: 12.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366597729093114		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.1366597729093114 | validation: 0.24609651239335123]
	TIME [epoch: 12.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430929413257654		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1430929413257654 | validation: 0.1470647198483745]
	TIME [epoch: 12.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15849217269255644		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.15849217269255644 | validation: 0.19992411346207503]
	TIME [epoch: 12.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12083756284192894		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.12083756284192894 | validation: 0.11834984910311311]
	TIME [epoch: 12.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11528180445342764		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.11528180445342764 | validation: 0.19542933999854945]
	TIME [epoch: 12.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11768470389111402		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.11768470389111402 | validation: 0.1322019592835086]
	TIME [epoch: 12.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10819538070174019		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.10819538070174019 | validation: 0.13391404204661875]
	TIME [epoch: 12.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11401995157701938		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.11401995157701938 | validation: 0.17628685323633986]
	TIME [epoch: 12.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11476385497298987		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.11476385497298987 | validation: 0.11321412927541041]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723433028504292		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.13723433028504292 | validation: 0.2859512482912326]
	TIME [epoch: 12.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17673567200854165		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.17673567200854165 | validation: 0.1721942573378079]
	TIME [epoch: 12.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19497350320313225		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.19497350320313225 | validation: 0.1254904877616399]
	TIME [epoch: 12.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144302265070087		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.11144302265070087 | validation: 0.20105570374332654]
	TIME [epoch: 12.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12784640251654922		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.12784640251654922 | validation: 0.119244520274333]
	TIME [epoch: 12.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14143959557161753		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.14143959557161753 | validation: 0.19260129807980272]
	TIME [epoch: 12.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12499640118882889		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.12499640118882889 | validation: 0.1398966641861043]
	TIME [epoch: 12.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253236732964773		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1253236732964773 | validation: 0.14455736469865174]
	TIME [epoch: 12.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234198741533671		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.1234198741533671 | validation: 0.17792744227644014]
	TIME [epoch: 12.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13295015265484078		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.13295015265484078 | validation: 0.17988541082037987]
	TIME [epoch: 12.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13336535879705624		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.13336535879705624 | validation: 0.12795463193820208]
	TIME [epoch: 12.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15850612062911565		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.15850612062911565 | validation: 0.21885052426001034]
	TIME [epoch: 12.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274007088239047		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1274007088239047 | validation: 0.11882966759300316]
	TIME [epoch: 12.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12333503817261378		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.12333503817261378 | validation: 0.1731691521139641]
	TIME [epoch: 12.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11417642000113311		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.11417642000113311 | validation: 0.14489083601254005]
	TIME [epoch: 12.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1158785643032079		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.1158785643032079 | validation: 0.1487207025590407]
	TIME [epoch: 12.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334720981814383		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.14334720981814383 | validation: 0.21129594417013753]
	TIME [epoch: 12.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14498112222658968		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.14498112222658968 | validation: 0.1202540928301171]
	TIME [epoch: 12.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153738406725113		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.153738406725113 | validation: 0.1756224684556676]
	TIME [epoch: 12.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11507482282717124		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.11507482282717124 | validation: 0.14231571641483756]
	TIME [epoch: 12.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011031497962209		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.11011031497962209 | validation: 0.1193703319188352]
	TIME [epoch: 12.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13589236337661592		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.13589236337661592 | validation: 0.2436752534196942]
	TIME [epoch: 12.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13868108157130354		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.13868108157130354 | validation: 0.13741411429332212]
	TIME [epoch: 12.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401274380729678		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1401274380729678 | validation: 0.16129815114952237]
	TIME [epoch: 12.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11595457315854595		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.11595457315854595 | validation: 0.15653373585174354]
	TIME [epoch: 12.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229059323867957		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.12229059323867957 | validation: 0.14218262059442854]
	TIME [epoch: 12.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12460218487557813		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.12460218487557813 | validation: 0.14732880941592805]
	TIME [epoch: 12.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11465456274217198		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.11465456274217198 | validation: 0.18003738299726174]
	TIME [epoch: 12.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10952396515890885		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.10952396515890885 | validation: 0.10531643455279119]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397892800000547		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.12397892800000547 | validation: 0.212615846027027]
	TIME [epoch: 12.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12018285946713031		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.12018285946713031 | validation: 0.1195134561801588]
	TIME [epoch: 12.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12419544803873528		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.12419544803873528 | validation: 0.2231460102070349]
	TIME [epoch: 12.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14204820701812435		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.14204820701812435 | validation: 0.13620959665346546]
	TIME [epoch: 12.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15832061805598682		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.15832061805598682 | validation: 0.16070093017830073]
	TIME [epoch: 12.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669165357932961		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.10669165357932961 | validation: 0.15864680902880002]
	TIME [epoch: 12.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11244454311306874		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.11244454311306874 | validation: 0.11655951428120857]
	TIME [epoch: 12.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536099844335967		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.11536099844335967 | validation: 0.18491896316557396]
	TIME [epoch: 12.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13990760005302613		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.13990760005302613 | validation: 0.130527279525451]
	TIME [epoch: 12.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14160950617335372		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.14160950617335372 | validation: 0.21194776391920592]
	TIME [epoch: 12.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284802082532303		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.1284802082532303 | validation: 0.11655078966125441]
	TIME [epoch: 12.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12195429240707016		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.12195429240707016 | validation: 0.14509879407685974]
	TIME [epoch: 12.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10774017628839348		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.10774017628839348 | validation: 0.13219726081501565]
	TIME [epoch: 12.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10390693966241969		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.10390693966241969 | validation: 0.13626326372825287]
	TIME [epoch: 12.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10632681828126643		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.10632681828126643 | validation: 0.11686535752371907]
	TIME [epoch: 12.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10810036782583125		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.10810036782583125 | validation: 0.17410430511947175]
	TIME [epoch: 12.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10678977407414991		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.10678977407414991 | validation: 0.12811352131838505]
	TIME [epoch: 12.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11365159673524396		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.11365159673524396 | validation: 0.15312165270214523]
	TIME [epoch: 12.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167787304225962		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.1167787304225962 | validation: 0.14516034369044067]
	TIME [epoch: 12.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13030306790709742		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.13030306790709742 | validation: 0.12487514982880397]
	TIME [epoch: 12.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441629318747289		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1441629318747289 | validation: 0.29244463901229184]
	TIME [epoch: 12.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17786904450240382		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.17786904450240382 | validation: 0.11188341408333019]
	TIME [epoch: 12.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14427457205333086		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.14427457205333086 | validation: 0.12962316613220537]
	TIME [epoch: 12.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056789915474694		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.1056789915474694 | validation: 0.221191308912427]
	TIME [epoch: 12.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13132261665955056		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.13132261665955056 | validation: 0.11352897769184832]
	TIME [epoch: 12.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1404000632220643		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.1404000632220643 | validation: 0.14792459578107167]
	TIME [epoch: 12.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09826095519398387		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.09826095519398387 | validation: 0.15576312489499228]
	TIME [epoch: 12.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10601106419465112		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.10601106419465112 | validation: 0.10751630359891261]
	TIME [epoch: 12.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1074488837600019		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1074488837600019 | validation: 0.1630454556002854]
	TIME [epoch: 12.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537806354698476		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.10537806354698476 | validation: 0.10603298821207759]
	TIME [epoch: 12.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10357252956050637		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.10357252956050637 | validation: 0.11983950283873074]
	TIME [epoch: 12.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10589911137538678		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.10589911137538678 | validation: 0.19289379615670088]
	TIME [epoch: 12.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12481798487370493		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.12481798487370493 | validation: 0.14540762513942837]
	TIME [epoch: 12.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12245586482629366		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.12245586482629366 | validation: 0.1340254015887393]
	TIME [epoch: 12.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11904254967844814		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.11904254967844814 | validation: 0.1987786737881563]
	TIME [epoch: 12.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286436283017665		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.12286436283017665 | validation: 0.13284347334651722]
	TIME [epoch: 12.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15161185964054327		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.15161185964054327 | validation: 0.1491713231232333]
	TIME [epoch: 12.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755794443931083		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10755794443931083 | validation: 0.1398200913629237]
	TIME [epoch: 12.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10428480875066512		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.10428480875066512 | validation: 0.10448171113701353]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10521454913309528		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.10521454913309528 | validation: 0.16482549313489445]
	TIME [epoch: 12.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929311393963552		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.10929311393963552 | validation: 0.1041704043839036]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12001794215965894		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.12001794215965894 | validation: 0.19940905775451806]
	TIME [epoch: 12.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13645066696831937		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.13645066696831937 | validation: 0.1396515494414289]
	TIME [epoch: 12.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14428306560385015		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.14428306560385015 | validation: 0.14217518453694425]
	TIME [epoch: 12.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243813308418531		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.10243813308418531 | validation: 0.16309287557704796]
	TIME [epoch: 12.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059024904579555		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1059024904579555 | validation: 0.1083463373575314]
	TIME [epoch: 12.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961126954154414		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.10961126954154414 | validation: 0.1426420613644153]
	TIME [epoch: 12.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12603729040280093		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.12603729040280093 | validation: 0.18213299655750956]
	TIME [epoch: 12.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13780210266922768		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.13780210266922768 | validation: 0.15596592015236257]
	TIME [epoch: 12.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10935173296313835		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.10935173296313835 | validation: 0.10488038663406302]
	TIME [epoch: 12.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10295547265137905		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.10295547265137905 | validation: 0.1491497143114005]
	TIME [epoch: 12.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10809658626253853		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10809658626253853 | validation: 0.12897415328743875]
	TIME [epoch: 12.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10596984832418947		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.10596984832418947 | validation: 0.1123223422933374]
	TIME [epoch: 12.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041882580704004		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1041882580704004 | validation: 0.18548578194080975]
	TIME [epoch: 12.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768982654043617		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.10768982654043617 | validation: 0.11446051460237097]
	TIME [epoch: 12.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13539995007374073		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.13539995007374073 | validation: 0.19888940954595039]
	TIME [epoch: 12.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276896090289559		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.1276896090289559 | validation: 0.12979190280092043]
	TIME [epoch: 12.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14120000936744187		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.14120000936744187 | validation: 0.1455198893965543]
	TIME [epoch: 12.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10374051116276356		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.10374051116276356 | validation: 0.1436894022564856]
	TIME [epoch: 12.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968869986582239		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.0968869986582239 | validation: 0.11175482147945977]
	TIME [epoch: 12.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10979115360674346		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.10979115360674346 | validation: 0.2040754170065161]
	TIME [epoch: 12.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12125247207313844		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.12125247207313844 | validation: 0.11550984806026923]
	TIME [epoch: 12.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13633468671900073		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.13633468671900073 | validation: 0.18357731599254037]
	TIME [epoch: 12.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10367514355443166		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.10367514355443166 | validation: 0.12416355731860818]
	TIME [epoch: 12.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768332650114453		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.09768332650114453 | validation: 0.11495701981643824]
	TIME [epoch: 12.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10195131636205598		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.10195131636205598 | validation: 0.17348595196946023]
	TIME [epoch: 12.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051082684537708		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.1051082684537708 | validation: 0.11082079579756346]
	TIME [epoch: 12.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1105540422692006		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.1105540422692006 | validation: 0.16535796871863814]
	TIME [epoch: 12.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10178598972304904		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.10178598972304904 | validation: 0.10052650189150991]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086088093788409		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.1086088093788409 | validation: 0.16902913769833572]
	TIME [epoch: 12.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10527576407858127		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10527576407858127 | validation: 0.10490489003104386]
	TIME [epoch: 12.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239378646567658		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.10239378646567658 | validation: 0.17978419458296074]
	TIME [epoch: 12.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629985526518489		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.10629985526518489 | validation: 0.1174608887033404]
	TIME [epoch: 12.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12180834316643231		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.12180834316643231 | validation: 0.17676161497837173]
	TIME [epoch: 12.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12983128080778986		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.12983128080778986 | validation: 0.15631586096617583]
	TIME [epoch: 12.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14053066654815705		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.14053066654815705 | validation: 0.3217301244314021]
	TIME [epoch: 12.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24808001135963245		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.24808001135963245 | validation: 0.19462031229519325]
	TIME [epoch: 12.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18479485688792738		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.18479485688792738 | validation: 0.08635084896487018]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1138850553288378		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.1138850553288378 | validation: 0.172582015503019]
	TIME [epoch: 12.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12693093276555772		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.12693093276555772 | validation: 0.1805273504006496]
	TIME [epoch: 12.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11055152363449999		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.11055152363449999 | validation: 0.11877972272453333]
	TIME [epoch: 12.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09837135421118343		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09837135421118343 | validation: 0.11085150562820681]
	TIME [epoch: 12.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503851877950705		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.10503851877950705 | validation: 0.14692612749250908]
	TIME [epoch: 12.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09848835203137819		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.09848835203137819 | validation: 0.13304204116447035]
	TIME [epoch: 12.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09663521143322823		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.09663521143322823 | validation: 0.13960705761994519]
	TIME [epoch: 12.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102667521383969		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.10102667521383969 | validation: 0.11961963558414156]
	TIME [epoch: 12.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788429147833462		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09788429147833462 | validation: 0.1369119730517431]
	TIME [epoch: 12.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09944866808843596		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.09944866808843596 | validation: 0.11447622923059414]
	TIME [epoch: 12.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09857549981580065		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.09857549981580065 | validation: 0.1290260667509382]
	TIME [epoch: 12.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09441119579143038		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.09441119579143038 | validation: 0.12576498396142588]
	TIME [epoch: 12.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09935067854383033		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.09935067854383033 | validation: 0.16003587766305272]
	TIME [epoch: 12.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305854412204107		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.10305854412204107 | validation: 0.12555431146988902]
	TIME [epoch: 12.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10336568888833254		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.10336568888833254 | validation: 0.13058904605705718]
	TIME [epoch: 12.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015433720916556		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.10015433720916556 | validation: 0.1389418885933595]
	TIME [epoch: 12.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469254422141173		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.09469254422141173 | validation: 0.1014924759148437]
	TIME [epoch: 12.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10686532750756012		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.10686532750756012 | validation: 0.17161272265624775]
	TIME [epoch: 12.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566754976035021		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.10566754976035021 | validation: 0.10106580180451999]
	TIME [epoch: 12.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10974485857787729		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.10974485857787729 | validation: 0.212033391039865]
	TIME [epoch: 12.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11282523140090196		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.11282523140090196 | validation: 0.10747596713402191]
	TIME [epoch: 12.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404506689513646		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.10404506689513646 | validation: 0.15274203498699856]
	TIME [epoch: 12.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743023485538015		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.09743023485538015 | validation: 0.1136276239831615]
	TIME [epoch: 12.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10022926143754088		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.10022926143754088 | validation: 0.1288194092404397]
	TIME [epoch: 12.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681680813100524		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.09681680813100524 | validation: 0.14884347842491819]
	TIME [epoch: 12.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09568719383254887		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.09568719383254887 | validation: 0.12251840205297802]
	TIME [epoch: 12.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810598347633526		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.09810598347633526 | validation: 0.13496095326862953]
	TIME [epoch: 12.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09631439129885415		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.09631439129885415 | validation: 0.10733393415442233]
	TIME [epoch: 12.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935109759169362		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.0935109759169362 | validation: 0.14670479087108307]
	TIME [epoch: 12.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09757402669726985		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09757402669726985 | validation: 0.12117093453123262]
	TIME [epoch: 12.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11715278785310423		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.11715278785310423 | validation: 0.253985626608278]
	TIME [epoch: 12.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15007835464950633		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.15007835464950633 | validation: 0.10568385091969679]
	TIME [epoch: 12.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12277738955342526		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.12277738955342526 | validation: 0.15950816626283204]
	TIME [epoch: 12.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135799769944509		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.10135799769944509 | validation: 0.12906450331859062]
	TIME [epoch: 12.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09592307228761256		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.09592307228761256 | validation: 0.12402989386132131]
	TIME [epoch: 12.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916568678836427		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.0916568678836427 | validation: 0.1581939851065198]
	TIME [epoch: 12.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09393991473414014		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.09393991473414014 | validation: 0.14292510733524041]
	TIME [epoch: 12.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10206576292620988		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.10206576292620988 | validation: 0.10121330474844076]
	TIME [epoch: 12.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466519856000524		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10466519856000524 | validation: 0.170863984911376]
	TIME [epoch: 12.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09897841331585909		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.09897841331585909 | validation: 0.11679571298336106]
	TIME [epoch: 12.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093719448613313		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.093719448613313 | validation: 0.13554086940187446]
	TIME [epoch: 12.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09630834903404763		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.09630834903404763 | validation: 0.12322624916719419]
	TIME [epoch: 12.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038667458908868		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.10038667458908868 | validation: 0.12908864334658368]
	TIME [epoch: 12.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532113700848043		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.10532113700848043 | validation: 0.1488036858524937]
	TIME [epoch: 12.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020900094723697		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.1020900094723697 | validation: 0.10803594823344626]
	TIME [epoch: 12.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09217328905838514		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.09217328905838514 | validation: 0.11921177219191155]
	TIME [epoch: 12.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09812689936309704		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.09812689936309704 | validation: 0.1560457447186156]
	TIME [epoch: 12.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169305590677802		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.10169305590677802 | validation: 0.15736507410620126]
	TIME [epoch: 12.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11423983615287768		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.11423983615287768 | validation: 0.1082260816176822]
	TIME [epoch: 12.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12248128994783873		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.12248128994783873 | validation: 0.20291225298699642]
	TIME [epoch: 12.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1076221471005988		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.1076221471005988 | validation: 0.0882351879903557]
	TIME [epoch: 12.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10947248377921233		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.10947248377921233 | validation: 0.14058528155102876]
	TIME [epoch: 12.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09376167896148532		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.09376167896148532 | validation: 0.13380466916753367]
	TIME [epoch: 12.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584973515851195		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.09584973515851195 | validation: 0.1119719616026968]
	TIME [epoch: 12.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09440277465268622		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.09440277465268622 | validation: 0.25943258343355907]
	TIME [epoch: 12.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17160578551888198		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.17160578551888198 | validation: 0.09994249214328627]
	TIME [epoch: 12.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603143909523357		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.11603143909523357 | validation: 0.14499411788351932]
	TIME [epoch: 12.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09956824014285834		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09956824014285834 | validation: 0.1634513597309027]
	TIME [epoch: 12.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09895914750907742		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09895914750907742 | validation: 0.10254160905481907]
	TIME [epoch: 12.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0976520924717828		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.0976520924717828 | validation: 0.1331158508835004]
	TIME [epoch: 12.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511759771948482		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.09511759771948482 | validation: 0.13449009010705007]
	TIME [epoch: 12.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930643168169546		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0930643168169546 | validation: 0.12958578528714204]
	TIME [epoch: 12.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0906220620241783		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.0906220620241783 | validation: 0.1303250908042539]
	TIME [epoch: 12.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09017009153869683		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.09017009153869683 | validation: 0.11264443658422048]
	TIME [epoch: 12.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09594302686540264		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.09594302686540264 | validation: 0.12358211091776612]
	TIME [epoch: 12.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365688914892523		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.09365688914892523 | validation: 0.1461540911514513]
	TIME [epoch: 12.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953910253488342		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.09953910253488342 | validation: 0.10380067122775423]
	TIME [epoch: 12.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10776537593211444		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.10776537593211444 | validation: 0.17314930038401566]
	TIME [epoch: 12.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10609708874902983		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.10609708874902983 | validation: 0.09274000653207658]
	TIME [epoch: 12.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123105818380705		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.11123105818380705 | validation: 0.12823279850426164]
	TIME [epoch: 12.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09130000743770811		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.09130000743770811 | validation: 0.16716359480093326]
	TIME [epoch: 12.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735428842262313		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.09735428842262313 | validation: 0.09659502315815743]
	TIME [epoch: 12.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11297539159464297		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.11297539159464297 | validation: 0.3397532582391558]
	TIME [epoch: 12.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18271107606178547		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.18271107606178547 | validation: 0.14268009312077543]
	TIME [epoch: 12.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09975327878480045		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.09975327878480045 | validation: 0.07779527535401623]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11127467683347261		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.11127467683347261 | validation: 0.11062612262966176]
	TIME [epoch: 12.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09031999479639186		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.09031999479639186 | validation: 0.15787291053675645]
	TIME [epoch: 12.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034390453417487		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.10034390453417487 | validation: 0.11509757460060036]
	TIME [epoch: 12.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572529782200456		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.09572529782200456 | validation: 0.11374566270665548]
	TIME [epoch: 12.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09174398764337838		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.09174398764337838 | validation: 0.12877238295590818]
	TIME [epoch: 12.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09105993893714605		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.09105993893714605 | validation: 0.10902276836004934]
	TIME [epoch: 12.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091011109846473		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.091011109846473 | validation: 0.1471049517664993]
	TIME [epoch: 12.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09266082229862578		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.09266082229862578 | validation: 0.12218064603897641]
	TIME [epoch: 12.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251880498190936		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.09251880498190936 | validation: 0.10632327661389332]
	TIME [epoch: 12.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09417116457199587		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.09417116457199587 | validation: 0.13585462311037408]
	TIME [epoch: 12.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740041637821203		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.09740041637821203 | validation: 0.11499923924383021]
	TIME [epoch: 12.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09926142291292489		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.09926142291292489 | validation: 0.12716596947291658]
	TIME [epoch: 12.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09981521030853535		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.09981521030853535 | validation: 0.1335029397692836]
	TIME [epoch: 12.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09079285124718435		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.09079285124718435 | validation: 0.1307054919319938]
	TIME [epoch: 12.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09614275485300783		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.09614275485300783 | validation: 0.10823179854090191]
	TIME [epoch: 12.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539973123817273		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.09539973123817273 | validation: 0.14103756389937983]
	TIME [epoch: 12.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10508087824027552		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.10508087824027552 | validation: 0.1120274756596321]
	TIME [epoch: 12.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10970138409706244		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.10970138409706244 | validation: 0.1709688437129583]
	TIME [epoch: 12.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908047188390948		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.09908047188390948 | validation: 0.10666326040565735]
	TIME [epoch: 12.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264185255138767		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.09264185255138767 | validation: 0.11628166784462146]
	TIME [epoch: 12.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08488943109796651		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.08488943109796651 | validation: 0.15012138707218423]
	TIME [epoch: 12.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448695718334499		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.09448695718334499 | validation: 0.12026483040784766]
	TIME [epoch: 12.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0945726115481388		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.0945726115481388 | validation: 0.147851941436829]
	TIME [epoch: 12.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09759801520715314		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.09759801520715314 | validation: 0.09556456216548039]
	TIME [epoch: 12.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11330833646668914		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11330833646668914 | validation: 0.18919438154351464]
	TIME [epoch: 12.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11004740070567484		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.11004740070567484 | validation: 0.11173108401227627]
	TIME [epoch: 12.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735422705630803		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.09735422705630803 | validation: 0.10848818881757816]
	TIME [epoch: 12.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09453788853027424		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.09453788853027424 | validation: 0.1988272683843358]
	TIME [epoch: 12.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124207172720789		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.11124207172720789 | validation: 0.08457650967931461]
	TIME [epoch: 12.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564316503601346		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.11564316503601346 | validation: 0.13727720615230324]
	TIME [epoch: 12.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0975642631194949		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.0975642631194949 | validation: 0.13798336229450592]
	TIME [epoch: 12.3 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513359545840136		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.09513359545840136 | validation: 0.1244174642553928]
	TIME [epoch: 12.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09415829338523234		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.09415829338523234 | validation: 0.130075032430888]
	TIME [epoch: 12.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09194867604314227		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.09194867604314227 | validation: 0.13265072871610092]
	TIME [epoch: 12.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879434797307337		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0879434797307337 | validation: 0.11712742682992]
	TIME [epoch: 12.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09011041114794484		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.09011041114794484 | validation: 0.12851292776981524]
	TIME [epoch: 12.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09156724104996986		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.09156724104996986 | validation: 0.11162020798357515]
	TIME [epoch: 12.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084582949005438		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.09084582949005438 | validation: 0.13251675532107834]
	TIME [epoch: 12.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09001649120067266		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.09001649120067266 | validation: 0.11597677403584467]
	TIME [epoch: 12.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09470264429585122		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.09470264429585122 | validation: 0.15294134098848713]
	TIME [epoch: 12.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10208790795426759		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.10208790795426759 | validation: 0.11625869554413276]
	TIME [epoch: 12.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11683214254555857		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.11683214254555857 | validation: 0.2622266172929573]
	TIME [epoch: 12.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288439915611003		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.1288439915611003 | validation: 0.09422461493304422]
	TIME [epoch: 12.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0941421615219556		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.0941421615219556 | validation: 0.10065128867103193]
	TIME [epoch: 12.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09079989289741479		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.09079989289741479 | validation: 0.14860474955335387]
	TIME [epoch: 12.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09241287049775364		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.09241287049775364 | validation: 0.12076686605930403]
	TIME [epoch: 12.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08949461107862003		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.08949461107862003 | validation: 0.10156821910528345]
	TIME [epoch: 12.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08992556898196495		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.08992556898196495 | validation: 0.12598679127694062]
	TIME [epoch: 12.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08971557794048211		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.08971557794048211 | validation: 0.13503073410259334]
	TIME [epoch: 12.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09390280224963488		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.09390280224963488 | validation: 0.1276288617637638]
	TIME [epoch: 12.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896731393804798		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.0896731393804798 | validation: 0.12142187843685043]
	TIME [epoch: 12.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0931303907103899		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.0931303907103899 | validation: 0.12838619398681678]
	TIME [epoch: 12.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056702021516572		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.09056702021516572 | validation: 0.12783966113722012]
	TIME [epoch: 12.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09025530177812105		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.09025530177812105 | validation: 0.1336791138351925]
	TIME [epoch: 12.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09070780833042279		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.09070780833042279 | validation: 0.09122744711943177]
	TIME [epoch: 12.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09930101933476275		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.09930101933476275 | validation: 0.19558709227135962]
	TIME [epoch: 12.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10150597869764022		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.10150597869764022 | validation: 0.1056170498938738]
	TIME [epoch: 12.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09544671707141404		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.09544671707141404 | validation: 0.13199224930420747]
	TIME [epoch: 12.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09319270396516541		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.09319270396516541 | validation: 0.09949174443970182]
	TIME [epoch: 12.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438768183792268		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.09438768183792268 | validation: 0.14617715527824462]
	TIME [epoch: 12.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0899961381601446		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.0899961381601446 | validation: 0.11214566107759627]
	TIME [epoch: 12.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09323411604419862		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.09323411604419862 | validation: 0.13186278311733252]
	TIME [epoch: 12.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918137388477372		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.0918137388477372 | validation: 0.11185694194164585]
	TIME [epoch: 12.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422304607399148		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.09422304607399148 | validation: 0.1156735749807652]
	TIME [epoch: 12.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935809737104911		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.0935809737104911 | validation: 0.10863456919050699]
	TIME [epoch: 12.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237551297227406		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.09237551297227406 | validation: 0.15466575531011936]
	TIME [epoch: 12.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09591234020922067		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.09591234020922067 | validation: 0.11398551158176172]
	TIME [epoch: 12.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09259851086940123		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.09259851086940123 | validation: 0.12014967880904932]
	TIME [epoch: 12.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08723563239804942		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.08723563239804942 | validation: 0.09905556574258151]
	TIME [epoch: 12.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195299424083704		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.09195299424083704 | validation: 0.19294496457402543]
	TIME [epoch: 12.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10022978330542742		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.10022978330542742 | validation: 0.0919374916644141]
	TIME [epoch: 12.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084401796370412		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.09084401796370412 | validation: 0.1573372376619787]
	TIME [epoch: 12.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771497984343185		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.09771497984343185 | validation: 0.10605128576997752]
	TIME [epoch: 12.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08940861168652031		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.08940861168652031 | validation: 0.0925158440950492]
	TIME [epoch: 12.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09192087103068229		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.09192087103068229 | validation: 0.18161003133020942]
	TIME [epoch: 12.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10296339207809997		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.10296339207809997 | validation: 0.09710246222412737]
	TIME [epoch: 12.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096767875566385		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.11096767875566385 | validation: 0.1615428082240888]
	TIME [epoch: 12.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983039909632595		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.09983039909632595 | validation: 0.1342556837682962]
	TIME [epoch: 12.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08751638098709727		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.08751638098709727 | validation: 0.10812035152890327]
	TIME [epoch: 12.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0908779712943219		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.0908779712943219 | validation: 0.12336693021310255]
	TIME [epoch: 12.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471135119572612		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.08471135119572612 | validation: 0.18425389650157067]
	TIME [epoch: 12.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10269381650036838		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.10269381650036838 | validation: 0.08591646426805158]
	TIME [epoch: 12.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10211437070162849		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.10211437070162849 | validation: 0.14320873222706454]
	TIME [epoch: 12.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08968698298085499		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.08968698298085499 | validation: 0.12912799853553308]
	TIME [epoch: 12.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08642769688111258		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08642769688111258 | validation: 0.10529233759761852]
	TIME [epoch: 12.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660668212385414		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.08660668212385414 | validation: 0.12160206350647243]
	TIME [epoch: 12.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886536038734473		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.0886536038734473 | validation: 0.12242817213872491]
	TIME [epoch: 12.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08623103121803449		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.08623103121803449 | validation: 0.12139010241360229]
	TIME [epoch: 12.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220990250254		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09220990250254 | validation: 0.14465860971604536]
	TIME [epoch: 12.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08931980133660851		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.08931980133660851 | validation: 0.14137204541659845]
	TIME [epoch: 12.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08837890484701653		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.08837890484701653 | validation: 0.08892869722778778]
	TIME [epoch: 12.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317504612011991		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.09317504612011991 | validation: 0.1741327555473566]
	TIME [epoch: 12.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993636755006504		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.0993636755006504 | validation: 0.10249311216871496]
	TIME [epoch: 12.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178847672494929		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.09178847672494929 | validation: 0.11194609167558428]
	TIME [epoch: 12.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818399061212444		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.08818399061212444 | validation: 0.16526762955231494]
	TIME [epoch: 12.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181922877358438		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.09181922877358438 | validation: 0.09476980628226245]
	TIME [epoch: 12.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257811476394358		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.09257811476394358 | validation: 0.10978698300200945]
	TIME [epoch: 12.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743355884315017		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.08743355884315017 | validation: 0.1309430815939613]
	TIME [epoch: 12.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862370645409518		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0862370645409518 | validation: 0.12668454354774875]
	TIME [epoch: 12.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813731551258412		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.08813731551258412 | validation: 0.12598940396636957]
	TIME [epoch: 12.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936246685163034		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0936246685163034 | validation: 0.15224825224942554]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_125505/states/model_phi1_4c_v_mmd1_822.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6816.583 seconds.
