Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2613258481

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.3453774437917625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3453774437917625 | validation: 6.769450635264684]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.197899773672877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.197899773672877 | validation: 6.875388661260885]
	TIME [epoch: 0.813 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.153009538428471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.153009538428471 | validation: 6.8813147118826805]
	TIME [epoch: 0.707 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.639975937517958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.639975937517958 | validation: 5.958154924738969]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.641875569081328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.641875569081328 | validation: 6.876676111321402]
	TIME [epoch: 0.714 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.23891071716019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.23891071716019 | validation: 6.567783055887699]
	TIME [epoch: 0.71 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.558533171848834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.558533171848834 | validation: 5.885806076999907]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4799957572504505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4799957572504505 | validation: 5.996112240656659]
	TIME [epoch: 0.709 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.138961203981524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138961203981524 | validation: 6.340228259334747]
	TIME [epoch: 0.707 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.224515037377926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.224515037377926 | validation: 5.978089938752617]
	TIME [epoch: 0.705 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.049763650319455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.049763650319455 | validation: 5.970692170861454]
	TIME [epoch: 0.71 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028371826559972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.028371826559972 | validation: 6.042068531394751]
	TIME [epoch: 0.705 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.983956286648329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.983956286648329 | validation: 5.9004723129532675]
	TIME [epoch: 0.708 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.969624600976474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.969624600976474 | validation: 6.091869631144791]
	TIME [epoch: 0.708 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.019662657655631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.019662657655631 | validation: 5.813211210868385]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.033547204251256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033547204251256 | validation: 6.1137847721978815]
	TIME [epoch: 0.707 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.040963177466589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040963177466589 | validation: 5.86358822860933]
	TIME [epoch: 0.706 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8815416775318092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8815416775318092 | validation: 5.86755269999608]
	TIME [epoch: 0.706 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.862725764021949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.862725764021949 | validation: 5.9554731942263315]
	TIME [epoch: 0.705 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.865030439459032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.865030439459032 | validation: 5.797961136040339]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8628794180350123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8628794180350123 | validation: 5.984599881051963]
	TIME [epoch: 0.708 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8877623999939064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8877623999939064 | validation: 5.772214124349787]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8557149770649017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8557149770649017 | validation: 5.9530558482759295]
	TIME [epoch: 0.709 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.853658913826623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.853658913826623 | validation: 5.765034460072604]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.803870417328829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.803870417328829 | validation: 5.893699611863084]
	TIME [epoch: 0.712 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.790028739244085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.790028739244085 | validation: 5.750548921717083]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.770076744707909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.770076744707909 | validation: 5.870933099739891]
	TIME [epoch: 0.708 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7697766588115447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7697766588115447 | validation: 5.7200294456591685]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7651115495453555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7651115495453555 | validation: 5.888931760554627]
	TIME [epoch: 0.712 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7927248807698244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7927248807698244 | validation: 5.702322731755171]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.746074641441296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.746074641441296 | validation: 5.83389325561223]
	TIME [epoch: 0.711 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.733929265159547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.733929265159547 | validation: 5.689645447500283]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.693231285817278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.693231285817278 | validation: 5.777724772380636]
	TIME [epoch: 0.712 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6792619564635167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6792619564635167 | validation: 5.675525445449255]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6607524840089902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6607524840089902 | validation: 5.752769543605354]
	TIME [epoch: 0.714 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6577653249995636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6577653249995636 | validation: 5.642778250501289]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.651773326159156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.651773326159156 | validation: 5.752725761545612]
	TIME [epoch: 0.71 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6743542796638664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6743542796638664 | validation: 5.611428763176942]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.637829404030909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637829404030909 | validation: 5.715934578181666]
	TIME [epoch: 0.711 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6345573193705274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6345573193705274 | validation: 5.595203582933518]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.589159651023408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.589159651023408 | validation: 5.64857312888725]
	TIME [epoch: 0.708 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.570676314634282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.570676314634282 | validation: 5.567712037263947]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.547286647524966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.547286647524966 | validation: 5.603679708273077]
	TIME [epoch: 0.71 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.539736892162394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539736892162394 | validation: 5.532249887097514]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.527087026864647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527087026864647 | validation: 5.580120444878974]
	TIME [epoch: 0.713 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5306175909733084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5306175909733084 | validation: 5.499283449357757]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5200684273578795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5200684273578795 | validation: 5.532349061537004]
	TIME [epoch: 0.71 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.519069677605598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.519069677605598 | validation: 5.452472382170007]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4659129936257105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4659129936257105 | validation: 5.459632162586502]
	TIME [epoch: 0.713 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4403259106113264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4403259106113264 | validation: 5.408911323682496]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.416459558808091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.416459558808091 | validation: 5.394987225375484]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.394887412525865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.394887412525865 | validation: 5.3685127602655776]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3776575789386665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3776575789386665 | validation: 5.340423331258154]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.367821386984046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.367821386984046 | validation: 5.321355291940846]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.372752571177856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.372752571177856 | validation: 5.296721360675874]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.379253186059567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.379253186059567 | validation: 5.254629976578745]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.31749382058589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.31749382058589 | validation: 5.209898294550833]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2855228991485275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2855228991485275 | validation: 5.1664253892641625]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2594080889308583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2594080889308583 | validation: 5.142294279561529]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.242324475297023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.242324475297023 | validation: 5.113754738255136]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2221879165813245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2221879165813245 | validation: 5.067887591104225]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.202505542514255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202505542514255 | validation: 5.053940432764987]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1850384644065857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1850384644065857 | validation: 5.006993481566135]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.184695819023973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.184695819023973 | validation: 5.025463591489433]
	TIME [epoch: 0.71 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1815389751054326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1815389751054326 | validation: 4.956770898476479]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2149108304923573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2149108304923573 | validation: 4.955467862253891]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1291905186627353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1291905186627353 | validation: 4.904118607430594]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1069911087314113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1069911087314113 | validation: 4.858088363200583]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0989415979895982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0989415979895982 | validation: 4.894805257852179]
	TIME [epoch: 0.714 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.089206067426103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.089206067426103 | validation: 4.8173090189249335]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.081822871060293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.081822871060293 | validation: 4.8272876316878595]
	TIME [epoch: 0.713 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0702343404407904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0702343404407904 | validation: 4.772344193997035]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.059029470119558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.059029470119558 | validation: 4.776580675940994]
	TIME [epoch: 0.71 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.034337724004298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.034337724004298 | validation: 4.720806939653061]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.026760134119588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.026760134119588 | validation: 4.735864111133834]
	TIME [epoch: 0.709 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.010307082630917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.010307082630917 | validation: 4.662458204017088]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0041689823747806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0041689823747806 | validation: 4.712232750830771]
	TIME [epoch: 0.711 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9936820475356978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9936820475356978 | validation: 4.620386684077513]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9889440362373825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9889440362373825 | validation: 4.642482635349923]
	TIME [epoch: 0.711 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9604981799364936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9604981799364936 | validation: 4.5681717296284745]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.948519486195702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.948519486195702 | validation: 4.571161750906728]
	TIME [epoch: 0.712 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9372615026736817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9372615026736817 | validation: 4.523169235205231]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9219599028282413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9219599028282413 | validation: 4.518593659960749]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9037225412786443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9037225412786443 | validation: 4.4443187724660085]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8732533104909885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8732533104909885 | validation: 4.417811316073592]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.836554878088694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.836554878088694 | validation: 4.066602494239709]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6100930366598005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6100930366598005 | validation: 3.1516633086875303]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5371890376042425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5371890376042425 | validation: 3.4745661976947546]
	TIME [epoch: 0.709 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.479017675004987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.479017675004987 | validation: 1.743086937121688]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7671100786395317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7671100786395317 | validation: 1.2165318350703416]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.598045384677508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.598045384677508 | validation: 1.9045357888193941]
	TIME [epoch: 0.708 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4472864344628062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4472864344628062 | validation: 1.1369513064932693]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2546426673132816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2546426673132816 | validation: 1.1721977685750844]
	TIME [epoch: 0.709 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1972157665979841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1972157665979841 | validation: 1.2186631786821849]
	TIME [epoch: 0.706 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160751398104077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.160751398104077 | validation: 1.0505309570918842]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1488547579902335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1488547579902335 | validation: 1.469370963084382]
	TIME [epoch: 0.71 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222530798799064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.222530798799064 | validation: 1.0074782361499701]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2452107944376098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2452107944376098 | validation: 1.5481923108997255]
	TIME [epoch: 0.716 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2166653676213846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2166653676213846 | validation: 0.948962321997382]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0685954592898563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0685954592898563 | validation: 0.953726734838106]
	TIME [epoch: 0.711 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0577274173088838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0577274173088838 | validation: 1.2047447806484368]
	TIME [epoch: 0.712 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0775992551039146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0775992551039146 | validation: 0.8752856147267386]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0536642290041311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0536642290041311 | validation: 1.100201417736843]
	TIME [epoch: 0.712 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0299600560351148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0299600560351148 | validation: 0.8996658920062282]
	TIME [epoch: 0.71 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0039088805118055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0039088805118055 | validation: 1.0158780831304761]
	TIME [epoch: 0.709 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9920089959028549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9920089959028549 | validation: 0.8667752408105502]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9968032487674917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9968032487674917 | validation: 1.1070990535870517]
	TIME [epoch: 0.713 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016842003981252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016842003981252 | validation: 0.8536028766155117]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0236256055241193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0236256055241193 | validation: 1.2107721528105744]
	TIME [epoch: 0.712 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0376517940254626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0376517940254626 | validation: 0.7750381283854378]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976609091377441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9976609091377441 | validation: 0.9629674098195278]
	TIME [epoch: 0.712 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617367482846828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9617367482846828 | validation: 0.8081533010045483]
	TIME [epoch: 0.71 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422800437251876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9422800437251876 | validation: 0.9168882499537663]
	TIME [epoch: 0.71 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9411455191960709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9411455191960709 | validation: 0.8289620088959622]
	TIME [epoch: 0.707 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9461956076452158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9461956076452158 | validation: 0.9364663431871428]
	TIME [epoch: 0.709 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568841702822315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568841702822315 | validation: 0.8854408756096703]
	TIME [epoch: 0.707 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9917225053947216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9917225053947216 | validation: 0.9041506413917887]
	TIME [epoch: 0.707 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444998028118949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9444998028118949 | validation: 0.9415452141834679]
	TIME [epoch: 0.707 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287539514217074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9287539514217074 | validation: 0.7876781964058508]
	TIME [epoch: 0.709 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945279225178695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945279225178695 | validation: 1.3315961913659846]
	TIME [epoch: 0.707 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082484752630706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.082484752630706 | validation: 0.7003025989606825]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0585621922070778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0585621922070778 | validation: 0.9852857249594509]
	TIME [epoch: 0.709 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349776268671832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9349776268671832 | validation: 0.7752994749530397]
	TIME [epoch: 0.708 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937305580905727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937305580905727 | validation: 0.7795974093274955]
	TIME [epoch: 0.706 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9125858342462569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9125858342462569 | validation: 0.9519937577677038]
	TIME [epoch: 0.708 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215222239131918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9215222239131918 | validation: 0.813406720404535]
	TIME [epoch: 0.709 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295296752491163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9295296752491163 | validation: 0.9475364797203606]
	TIME [epoch: 0.708 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504712853036728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9504712853036728 | validation: 0.8254513619360392]
	TIME [epoch: 0.707 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9217296809714004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9217296809714004 | validation: 0.8405584371445114]
	TIME [epoch: 0.708 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163618500186391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9163618500186391 | validation: 0.8453034134638076]
	TIME [epoch: 0.709 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960194547264081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960194547264081 | validation: 0.7641881458522595]
	TIME [epoch: 0.708 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016899294866602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016899294866602 | validation: 0.9280306620179313]
	TIME [epoch: 0.707 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908077879670206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908077879670206 | validation: 0.7012409344512093]
	TIME [epoch: 0.71 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9414809330962539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9414809330962539 | validation: 0.9975127429294516]
	TIME [epoch: 0.71 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9352889348760828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9352889348760828 | validation: 0.7096446692738007]
	TIME [epoch: 0.707 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0825008565227787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0825008565227787 | validation: 1.1667768020321694]
	TIME [epoch: 0.706 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016236956047233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016236956047233 | validation: 0.7752439466785036]
	TIME [epoch: 0.709 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.888498500323964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.888498500323964 | validation: 0.6623189282091001]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972010260988446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8972010260988446 | validation: 0.9110031845425489]
	TIME [epoch: 0.708 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926011026700513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926011026700513 | validation: 0.7287950879471039]
	TIME [epoch: 0.707 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743930701641098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743930701641098 | validation: 0.8618299638636508]
	TIME [epoch: 0.705 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804098407510657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8804098407510657 | validation: 0.7605681123166033]
	TIME [epoch: 0.704 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026884541773859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026884541773859 | validation: 0.948031045558124]
	TIME [epoch: 0.704 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347262864306721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9347262864306721 | validation: 0.780348606467856]
	TIME [epoch: 0.705 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142623184809057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9142623184809057 | validation: 0.825251204102992]
	TIME [epoch: 0.704 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911611215299283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911611215299283 | validation: 0.805735858981655]
	TIME [epoch: 0.703 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793935282827474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8793935282827474 | validation: 0.7688525138459541]
	TIME [epoch: 0.703 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775584338886603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775584338886603 | validation: 0.8106104534364134]
	TIME [epoch: 0.703 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674020744794381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674020744794381 | validation: 0.797062217573083]
	TIME [epoch: 0.704 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834236266201062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834236266201062 | validation: 0.7976315984154383]
	TIME [epoch: 0.702 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880726081416088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880726081416088 | validation: 0.8578434457071717]
	TIME [epoch: 0.703 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921515764772249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.921515764772249 | validation: 0.7241476802149891]
	TIME [epoch: 0.705 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041555522232654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9041555522232654 | validation: 1.0776304236635619]
	TIME [epoch: 0.705 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451569359244388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451569359244388 | validation: 0.5968987244310677]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0259648314820613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0259648314820613 | validation: 1.0140397506306917]
	TIME [epoch: 0.714 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9544220053761313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9544220053761313 | validation: 0.6678113777243505]
	TIME [epoch: 0.713 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095743779854532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9095743779854532 | validation: 0.7433800512492947]
	TIME [epoch: 0.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599894703205192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599894703205192 | validation: 0.894742604298778]
	TIME [epoch: 0.707 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681176417394576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681176417394576 | validation: 0.6434142855030516]
	TIME [epoch: 0.711 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803474624917431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803474624917431 | validation: 0.884636122533859]
	TIME [epoch: 0.71 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870873807036723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.870873807036723 | validation: 0.6828908703940675]
	TIME [epoch: 0.71 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488175943116306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8488175943116306 | validation: 0.7983329822360625]
	TIME [epoch: 0.71 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572564506020848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572564506020848 | validation: 0.7405203973618552]
	TIME [epoch: 0.713 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701544686209884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8701544686209884 | validation: 0.9038590949787854]
	TIME [epoch: 0.712 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202995263381104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202995263381104 | validation: 0.793938569171157]
	TIME [epoch: 0.712 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009319141386517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009319141386517 | validation: 0.7178888288832085]
	TIME [epoch: 0.711 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960279983377992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960279983377992 | validation: 0.8370739892925194]
	TIME [epoch: 0.711 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546660595776825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8546660595776825 | validation: 0.6246509606115224]
	TIME [epoch: 0.712 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864764507611892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864764507611892 | validation: 0.971244310091716]
	TIME [epoch: 0.716 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993317285316877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8993317285316877 | validation: 0.6791375784557065]
	TIME [epoch: 0.709 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9469309881406054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9469309881406054 | validation: 1.025827309917897]
	TIME [epoch: 0.708 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232204473793243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9232204473793243 | validation: 0.6832416446232209]
	TIME [epoch: 0.708 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270029352466272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270029352466272 | validation: 0.6556945015133586]
	TIME [epoch: 0.708 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408424059740093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408424059740093 | validation: 0.8640835708772037]
	TIME [epoch: 0.708 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640938750888953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8640938750888953 | validation: 0.6270463346169631]
	TIME [epoch: 0.712 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9128931777917633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9128931777917633 | validation: 0.797328999362645]
	TIME [epoch: 0.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234792769100635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234792769100635 | validation: 0.7440241347005464]
	TIME [epoch: 0.707 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205132530610467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8205132530610467 | validation: 0.6995494579461259]
	TIME [epoch: 0.707 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860579776309847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860579776309847 | validation: 1.1278019395189733]
	TIME [epoch: 0.707 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9900225594694189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9900225594694189 | validation: 0.6732660205913135]
	TIME [epoch: 0.707 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644780977533969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644780977533969 | validation: 0.753757682484512]
	TIME [epoch: 0.708 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231920952731346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231920952731346 | validation: 0.6708159937970924]
	TIME [epoch: 0.707 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119760128485664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119760128485664 | validation: 0.7082296851969848]
	TIME [epoch: 0.708 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8217416417468456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8217416417468456 | validation: 0.830066527347372]
	TIME [epoch: 0.708 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795204400339267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8795204400339267 | validation: 0.7256710019693315]
	TIME [epoch: 0.708 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9622962699540082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622962699540082 | validation: 0.739660483053858]
	TIME [epoch: 0.709 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8143791460309782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8143791460309782 | validation: 0.7690353500847876]
	TIME [epoch: 0.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099437601591118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099437601591118 | validation: 0.618715083158281]
	TIME [epoch: 0.707 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569954522968842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8569954522968842 | validation: 1.1211743427402632]
	TIME [epoch: 0.708 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585857629065462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9585857629065462 | validation: 0.5962576634334263]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708977861980808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8708977861980808 | validation: 0.7691369544505364]
	TIME [epoch: 0.716 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036151061350575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036151061350575 | validation: 0.672948351207455]
	TIME [epoch: 0.711 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764539183362327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764539183362327 | validation: 0.6683530955992786]
	TIME [epoch: 0.711 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773471742330688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7773471742330688 | validation: 0.6945411838365433]
	TIME [epoch: 0.708 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640445777430934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7640445777430934 | validation: 0.653689833556629]
	TIME [epoch: 0.708 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942836224244587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942836224244587 | validation: 0.9051898673296542]
	TIME [epoch: 0.708 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9909746350729023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9909746350729023 | validation: 0.8914997973042391]
	TIME [epoch: 0.709 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035402170137746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.035402170137746 | validation: 0.6373354353471886]
	TIME [epoch: 0.709 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035183407048164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8035183407048164 | validation: 0.9661523419919971]
	TIME [epoch: 0.708 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991566934961268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991566934961268 | validation: 0.5662549958207009]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926714402142976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926714402142976 | validation: 0.7513264709081566]
	TIME [epoch: 175 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735885323150905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7735885323150905 | validation: 0.704403729898367]
	TIME [epoch: 1.51 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694438995802265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694438995802265 | validation: 0.665579712936346]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004505462818079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8004505462818079 | validation: 0.7380813682375311]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963142721671891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963142721671891 | validation: 0.7960354167863731]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151548546551024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151548546551024 | validation: 0.6958275880728566]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8535242101098045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8535242101098045 | validation: 0.9237452059618069]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532623084863699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532623084863699 | validation: 0.6203786242117012]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871842980830845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7871842980830845 | validation: 0.69718485346958]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502899228117684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502899228117684 | validation: 0.6965987130177818]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564125221947857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564125221947857 | validation: 0.6737624055653648]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036416565744987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036416565744987 | validation: 0.7412427913857282]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8441139008790074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8441139008790074 | validation: 0.7563931795266821]
	TIME [epoch: 1.39 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791540167779618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791540167779618 | validation: 0.5717083722534343]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756179802278324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756179802278324 | validation: 0.7970995347972459]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575528602133486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7575528602133486 | validation: 0.5463070103561191]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082072949684614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082072949684614 | validation: 0.8264140826953668]
	TIME [epoch: 1.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645714585717036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645714585717036 | validation: 0.6671417028195374]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471504358587201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471504358587201 | validation: 0.6223446345139153]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751017523335338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751017523335338 | validation: 0.6873754774995361]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526460559255486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526460559255486 | validation: 0.5984768368623866]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045044610792504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045044610792504 | validation: 0.6152433727846152]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685022644115878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6685022644115878 | validation: 0.67207349434434]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830398525881909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830398525881909 | validation: 0.5820556086924313]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961457359457316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961457359457316 | validation: 0.8963009767214424]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192789559068423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192789559068423 | validation: 0.5318687406102176]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182044620070747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182044620070747 | validation: 0.637069950285392]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501981127174458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501981127174458 | validation: 0.5881083283467726]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403147297102717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403147297102717 | validation: 0.5946242104253732]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847552735649558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6847552735649558 | validation: 0.7434929731257699]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379201985491576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379201985491576 | validation: 0.637690262840761]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494933237069621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494933237069621 | validation: 0.5595444030555224]
	TIME [epoch: 1.39 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441599462322637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441599462322637 | validation: 0.6025267881350697]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376373926159199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376373926159199 | validation: 0.5767612361590212]
	TIME [epoch: 1.39 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549634018469678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549634018469678 | validation: 0.6752690426760642]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547719703027873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547719703027873 | validation: 0.6946891620861337]
	TIME [epoch: 1.39 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501618967822313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7501618967822313 | validation: 0.49274149252289307]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508363610343606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6508363610343606 | validation: 0.8727346759674566]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393262505155173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393262505155173 | validation: 0.501778439678065]
	TIME [epoch: 1.39 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316071802783194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7316071802783194 | validation: 0.733191250885265]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674669933300204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674669933300204 | validation: 0.7713709558451682]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464344649218236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464344649218236 | validation: 0.5582516219829027]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755970054704333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6755970054704333 | validation: 0.525611042629967]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6052808128120698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6052808128120698 | validation: 0.6182267028059392]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160049705519691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6160049705519691 | validation: 0.49499520631795335]
	TIME [epoch: 1.39 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933194987492499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933194987492499 | validation: 0.5538172076584903]
	TIME [epoch: 1.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5927388967669688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5927388967669688 | validation: 0.6137641721690423]
	TIME [epoch: 1.39 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531238317616984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531238317616984 | validation: 0.7423622520147296]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9079151404403075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9079151404403075 | validation: 0.7398252023766543]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196904680101482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196904680101482 | validation: 0.5105380465781012]
	TIME [epoch: 1.39 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315217147464105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6315217147464105 | validation: 0.5961328200390302]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303929512235501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303929512235501 | validation: 0.7153306051534266]
	TIME [epoch: 1.39 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622136851652629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7622136851652629 | validation: 0.6115583194783647]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279719200283663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279719200283663 | validation: 0.7271663099565043]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705501099182521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.705501099182521 | validation: 0.5334150365789802]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300310834680066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6300310834680066 | validation: 0.5480273420025873]
	TIME [epoch: 1.39 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167610566948973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6167610566948973 | validation: 0.7217252204100545]
	TIME [epoch: 1.39 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120873499328534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7120873499328534 | validation: 0.5681030979544366]
	TIME [epoch: 1.39 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413153175031232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7413153175031232 | validation: 0.6301237279382685]
	TIME [epoch: 1.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232487579947997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6232487579947997 | validation: 0.5203381233329595]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5742545111538653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5742545111538653 | validation: 0.4944888280066498]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710319617272033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710319617272033 | validation: 0.6077301815926293]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935101552702956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5935101552702956 | validation: 0.5675091136161254]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873640058211881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6873640058211881 | validation: 0.6929446463205547]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937014588843053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6937014588843053 | validation: 0.556528136212101]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621662611325314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621662611325314 | validation: 0.5103743291643582]
	TIME [epoch: 1.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5766029869113508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5766029869113508 | validation: 0.5991476154836031]
	TIME [epoch: 1.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5718742004259523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5718742004259523 | validation: 0.4734579623581688]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673907967016399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673907967016399 | validation: 0.778829882032783]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384604611103469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6384604611103469 | validation: 0.5554689316925358]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5465161736708755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5465161736708755 | validation: 0.48041285341774453]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542961309790972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542961309790972 | validation: 0.5520049683036267]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5455762204139482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5455762204139482 | validation: 0.5292324768330327]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279811087308467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5279811087308467 | validation: 0.46283344172594615]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5227531167582756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5227531167582756 | validation: 0.5779839758647141]
	TIME [epoch: 1.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5201647899026628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5201647899026628 | validation: 0.4791397803039843]
	TIME [epoch: 1.39 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6119961510159644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6119961510159644 | validation: 1.2744290990052918]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2558947982638473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2558947982638473 | validation: 0.44678617306639223]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5309243241939011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5309243241939011 | validation: 0.49420858993897704]
	TIME [epoch: 1.39 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399608895075273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5399608895075273 | validation: 0.6922698531382645]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335557011273796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7335557011273796 | validation: 0.6092443258002004]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175346961644368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7175346961644368 | validation: 0.5788962463414364]
	TIME [epoch: 1.39 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5434472857478979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5434472857478979 | validation: 0.5014116407422561]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589472387761377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.589472387761377 | validation: 0.7781592279768325]
	TIME [epoch: 1.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376222160928127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376222160928127 | validation: 0.5384266446771128]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631744408494582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5631744408494582 | validation: 0.4733404154388418]
	TIME [epoch: 1.39 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5699333859518345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5699333859518345 | validation: 0.7819616314818445]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208407253634894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6208407253634894 | validation: 0.5862150008375061]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112963253801683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112963253801683 | validation: 0.49513123276098325]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305321494100158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5305321494100158 | validation: 0.46697971230224555]
	TIME [epoch: 1.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4845992367326893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4845992367326893 | validation: 0.6532427617694059]
	TIME [epoch: 1.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5090859568486666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5090859568486666 | validation: 0.5126310247024721]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6068366473774364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6068366473774364 | validation: 0.7259262104645673]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725796540012717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6725796540012717 | validation: 0.4684000563717149]
	TIME [epoch: 1.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793096980450538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793096980450538 | validation: 0.5102008880429848]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4980743150938171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4980743150938171 | validation: 0.4754438875900025]
	TIME [epoch: 1.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4648918177275002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4648918177275002 | validation: 0.5332272164748636]
	TIME [epoch: 1.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5330354793665396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5330354793665396 | validation: 0.6736895243328371]
	TIME [epoch: 1.39 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371801823985171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371801823985171 | validation: 0.4370289301149519]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478027300471777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478027300471777 | validation: 0.845153791817824]
	TIME [epoch: 1.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650982527910114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650982527910114 | validation: 0.6012683655868388]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182737307462834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6182737307462834 | validation: 0.569054499019989]
	TIME [epoch: 1.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856206400261222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4856206400261222 | validation: 0.6722637731340246]
	TIME [epoch: 1.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128421056478277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5128421056478277 | validation: 0.5587147232301023]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368729748180355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6368729748180355 | validation: 0.570551576734679]
	TIME [epoch: 1.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624926705310156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5624926705310156 | validation: 0.470458878600281]
	TIME [epoch: 1.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47363350841292073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47363350841292073 | validation: 0.4976065775310441]
	TIME [epoch: 1.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43996859802170063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43996859802170063 | validation: 0.48526055145365277]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4663175238033732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4663175238033732 | validation: 0.6502709808664715]
	TIME [epoch: 1.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883932027098628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5883932027098628 | validation: 0.4795427407726324]
	TIME [epoch: 1.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951705118838453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951705118838453 | validation: 0.5154990331422399]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5147661976176082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5147661976176082 | validation: 0.4746696064044434]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4686178535462184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4686178535462184 | validation: 0.5186384205350475]
	TIME [epoch: 1.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48432063162711786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48432063162711786 | validation: 0.5831435833710527]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5666468409619038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5666468409619038 | validation: 0.3903926929719543]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981948770062172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5981948770062172 | validation: 0.8510560599694511]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288238964720748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6288238964720748 | validation: 0.6104168698352552]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5746398221664503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5746398221664503 | validation: 0.5241200718245927]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4550454028633606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4550454028633606 | validation: 0.6217537395394326]
	TIME [epoch: 1.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5032728912383932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5032728912383932 | validation: 0.5398383971738632]
	TIME [epoch: 1.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540598746664152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6540598746664152 | validation: 0.45884418381668146]
	TIME [epoch: 1.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47014795431899814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47014795431899814 | validation: 0.5149627412155626]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40646272323076227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40646272323076227 | validation: 0.4567769704237534]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.421140929767424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.421140929767424 | validation: 0.5788440425265997]
	TIME [epoch: 1.39 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175551117260464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5175551117260464 | validation: 0.5072584106611764]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679581960332283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679581960332283 | validation: 0.45951222175266776]
	TIME [epoch: 1.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45796627294161324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45796627294161324 | validation: 0.4582716751296143]
	TIME [epoch: 1.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40827370774166566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40827370774166566 | validation: 0.5244225692172949]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4538835064353458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4538835064353458 | validation: 0.5486098850632576]
	TIME [epoch: 1.43 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967534245233871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5967534245233871 | validation: 0.30036676348390956]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5868523094427481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5868523094427481 | validation: 1.1854460053724374]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083477947780671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083477947780671 | validation: 0.7147265406565534]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710858260840682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710858260840682 | validation: 0.5089279229721714]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186183759109733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186183759109733 | validation: 0.47748864350638326]
	TIME [epoch: 1.39 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3908488986449828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3908488986449828 | validation: 0.4820856493398049]
	TIME [epoch: 1.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37300410593907074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37300410593907074 | validation: 0.39760654134615625]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3599834843171059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3599834843171059 | validation: 0.40515070978904943]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3785748424659925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3785748424659925 | validation: 0.5324202849768759]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4597949399434597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4597949399434597 | validation: 0.45810445435513714]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6089292581369417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6089292581369417 | validation: 0.4804323555589868]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3836825896821631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3836825896821631 | validation: 0.43571378373004443]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3628342499315605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3628342499315605 | validation: 0.46609459091282335]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4862513658261851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4862513658261851 | validation: 0.3898417842402868]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619903744839327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4619903744839327 | validation: 0.3188644681150645]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151347838147229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4151347838147229 | validation: 0.873086765468257]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595571407975443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5595571407975443 | validation: 0.5234521142352876]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46568172988424095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46568172988424095 | validation: 0.38981574081879833]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33743848669208626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33743848669208626 | validation: 0.5454253061895447]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41263169915039544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41263169915039544 | validation: 0.37236897030777893]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46189349811614827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46189349811614827 | validation: 0.33346233174042245]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39501749078922116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39501749078922116 | validation: 0.5891239013242772]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41224522325520085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41224522325520085 | validation: 0.3375009829174775]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4094700253987054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4094700253987054 | validation: 0.38677593865874554]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625127475508094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3625127475508094 | validation: 0.4061870387395152]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3805390652890597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3805390652890597 | validation: 0.579991073341923]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174790537551704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5174790537551704 | validation: 0.3582546417720119]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40288513104866164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40288513104866164 | validation: 0.33305167875413516]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025498213497515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3025498213497515 | validation: 0.3854190088462847]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32158829125202376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32158829125202376 | validation: 0.5243350924628655]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35639554100117865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35639554100117865 | validation: 0.47634177960716517]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813679711301356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4813679711301356 | validation: 0.6232947821491642]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42699448416050845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42699448416050845 | validation: 0.3258960914356273]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038323059897158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3038323059897158 | validation: 0.30893083361443424]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3640121537374886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3640121537374886 | validation: 0.4693917248340877]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4024865519531538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4024865519531538 | validation: 0.3577085225746973]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41792566125900804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41792566125900804 | validation: 0.2989769519415981]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37989340331684307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37989340331684307 | validation: 0.6822899523438413]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45464251596831434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45464251596831434 | validation: 0.4686991123009152]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37326609418114326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37326609418114326 | validation: 0.26267644634693393]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2974521051624351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2974521051624351 | validation: 0.5191993734270722]
	TIME [epoch: 1.4 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34420767190314183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34420767190314183 | validation: 0.42099013445975475]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37525558591104796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37525558591104796 | validation: 0.33562360241546757]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2550591470827445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2550591470827445 | validation: 0.3065130221745631]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27377802415263547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27377802415263547 | validation: 0.3791660365359679]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4349459110239539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4349459110239539 | validation: 0.6198884524481517]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341441629188937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5341441629188937 | validation: 0.293167763117231]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31798425943934916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31798425943934916 | validation: 0.2887255781130993]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24172628479254407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24172628479254407 | validation: 0.3161401692116854]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24921852104050027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24921852104050027 | validation: 0.2714611740998008]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31989172429338547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31989172429338547 | validation: 0.37778639185318014]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36850031659593574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36850031659593574 | validation: 0.3666652112508175]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922667652706642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3922667652706642 | validation: 0.524248767845848]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34596518584410774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34596518584410774 | validation: 0.4989368568595278]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756609673824645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4756609673824645 | validation: 0.5379032005909352]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3674982114451608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3674982114451608 | validation: 0.25456966545541093]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.267013405617526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.267013405617526 | validation: 0.23845754623359958]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23892366185107553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23892366185107553 | validation: 0.3598160123944704]
	TIME [epoch: 1.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25996566491584494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25996566491584494 | validation: 0.23424826034330595]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28715996686111406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28715996686111406 | validation: 0.3163910938099998]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778419455503258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778419455503258 | validation: 0.3316880418106994]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3385778300399011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3385778300399011 | validation: 0.2784588723592217]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659923456649351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3659923456649351 | validation: 0.38925562526254065]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749843980913207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749843980913207 | validation: 0.37796543734898314]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35413421242847276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35413421242847276 | validation: 0.7798586124639448]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4920889940057474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4920889940057474 | validation: 0.36109243252259865]
	TIME [epoch: 1.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871008016164255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2871008016164255 | validation: 0.1961356889075483]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2942112980675399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2942112980675399 | validation: 0.44208698900402743]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337913601369303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3337913601369303 | validation: 0.25017369038137255]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29082043389818185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29082043389818185 | validation: 0.26459479212635534]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2343800569505696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2343800569505696 | validation: 0.41177836168444354]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30742073218107513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30742073218107513 | validation: 0.2616377201484735]
	TIME [epoch: 1.39 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2701858525242976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2701858525242976 | validation: 0.27037518493608625]
	TIME [epoch: 1.39 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.224640335460672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.224640335460672 | validation: 0.2663236457461124]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23747702200166854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23747702200166854 | validation: 0.31687370765811135]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2928367931696652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2928367931696652 | validation: 0.4116001148229579]
	TIME [epoch: 1.39 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3623624196824147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3623624196824147 | validation: 0.3259948018845475]
	TIME [epoch: 1.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127926964530409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3127926964530409 | validation: 0.22500061063129062]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2653606728009237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2653606728009237 | validation: 0.7559832164501903]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5377994857246688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5377994857246688 | validation: 0.34643990142131115]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3154492094336567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3154492094336567 | validation: 0.22716018591912782]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22641107980067815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22641107980067815 | validation: 0.26205306170750414]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21148316902743333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21148316902743333 | validation: 0.2104870530382944]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19161310292802622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19161310292802622 | validation: 0.3214676832792773]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24584402769195063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24584402769195063 | validation: 0.47898171423950214]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3554316961979496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3554316961979496 | validation: 0.2500557216369825]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27202033011522436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27202033011522436 | validation: 0.28779121491681886]
	TIME [epoch: 1.39 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24634020156032765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24634020156032765 | validation: 0.24819199625798918]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3181337161039694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3181337161039694 | validation: 0.34909394815558714]
	TIME [epoch: 1.39 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23745608167382312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23745608167382312 | validation: 0.23554742259915998]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2375189081608114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2375189081608114 | validation: 0.3301718364698455]
	TIME [epoch: 1.39 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2340803120800696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2340803120800696 | validation: 0.15659429636917122]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24218416084532535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24218416084532535 | validation: 0.29395020967166025]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22064245656235298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22064245656235298 | validation: 0.17074042280363133]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22391695535446368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22391695535446368 | validation: 0.2535888802648589]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24132022084963475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24132022084963475 | validation: 0.3007026514707383]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018498783456857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3018498783456857 | validation: 0.20514600469923502]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2470628013785838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2470628013785838 | validation: 0.2589412964574532]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21654143910799306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21654143910799306 | validation: 0.2725057908995212]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21956500834187995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21956500834187995 | validation: 0.39558575167620524]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37242130186425326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37242130186425326 | validation: 0.44136285484045423]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2976920555149075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2976920555149075 | validation: 0.24596900577306197]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3251582206468915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3251582206468915 | validation: 0.3246751463832725]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23368185462286825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23368185462286825 | validation: 0.15273814400335728]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17221986684203594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17221986684203594 | validation: 0.2161510919752102]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17343542764091627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17343542764091627 | validation: 0.24198970286345445]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26005004612468013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26005004612468013 | validation: 0.30015143561989605]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34319829103279437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34319829103279437 | validation: 0.35419532973106543]
	TIME [epoch: 1.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23697713520036792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23697713520036792 | validation: 0.29090157808000766]
	TIME [epoch: 1.39 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21291242985210787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21291242985210787 | validation: 0.19769800166975995]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15892610961016032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15892610961016032 | validation: 0.2063211904522315]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15448673477904085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15448673477904085 | validation: 0.14042520556995167]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828245766436327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1828245766436327 | validation: 0.31777625895500106]
	TIME [epoch: 1.39 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847830593857995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2847830593857995 | validation: 0.19765240239245108]
	TIME [epoch: 1.39 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32275348418571476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32275348418571476 | validation: 0.29608136699557697]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24529463773719926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24529463773719926 | validation: 0.9378134841182825]
	TIME [epoch: 1.39 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282575622358366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282575622358366 | validation: 0.6698005054199464]
	TIME [epoch: 1.39 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49977777419514596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49977777419514596 | validation: 0.4363858269007446]
	TIME [epoch: 1.39 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3867277773365818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3867277773365818 | validation: 0.2671214216237196]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173412715496715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2173412715496715 | validation: 0.22241350357329515]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18571246741404437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18571246741404437 | validation: 0.13511135076921452]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22664228196249397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22664228196249397 | validation: 0.19715271902526796]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16913057280665417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16913057280665417 | validation: 0.12277080263245131]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16768376902937507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16768376902937507 | validation: 0.2789539076094125]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2171862077582132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2171862077582132 | validation: 0.4883404977699932]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4060486431250177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4060486431250177 | validation: 0.369904382769578]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24319621625801988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24319621625801988 | validation: 0.16551681842211752]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15628113622192463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15628113622192463 | validation: 0.17794284301244084]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328326517212289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1328326517212289 | validation: 0.13406362145699804]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545959064535835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1545959064535835 | validation: 0.2555556220315193]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24269619195913414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24269619195913414 | validation: 0.24730121754457457]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3189926908110675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3189926908110675 | validation: 0.21199164380236182]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16257295243286504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16257295243286504 | validation: 0.17387799321914954]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14268644475992406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14268644475992406 | validation: 0.20660721510777008]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15594859726944013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15594859726944013 | validation: 0.1930235451953594]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18563316590458243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18563316590458243 | validation: 0.269006677675278]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21977187694313036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21977187694313036 | validation: 0.2213578927493617]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23391951741176178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23391951741176178 | validation: 0.25367812028564546]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859364824324281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2859364824324281 | validation: 0.2347745969061232]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18523290819867086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18523290819867086 | validation: 0.12647872144198]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18474049132470696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18474049132470696 | validation: 0.20747045654711782]
	TIME [epoch: 1.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17230099488813103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17230099488813103 | validation: 0.1198262019933551]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18097717683904072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18097717683904072 | validation: 0.19255995494782407]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711408932627349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711408932627349 | validation: 0.17948317658662707]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22303071000823657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22303071000823657 | validation: 0.39378920340758006]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31479562638344627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31479562638344627 | validation: 0.7369530986939165]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42518305445000354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42518305445000354 | validation: 0.16607883033176127]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16597200701257414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16597200701257414 | validation: 0.24178817072145373]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1961874395835923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1961874395835923 | validation: 0.19897622665730968]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24872481047275163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24872481047275163 | validation: 0.1691999015062076]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14007597693212467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14007597693212467 | validation: 0.1372831522419796]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12538142504358024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12538142504358024 | validation: 0.19839098131968896]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13824679702509182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13824679702509182 | validation: 0.1232962082099129]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501976888488133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1501976888488133 | validation: 0.21488208116182783]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750517956637308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1750517956637308 | validation: 0.14979084338373103]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.247401578500087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.247401578500087 | validation: 0.24866420254596677]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17974685935576173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17974685935576173 | validation: 0.33002058246938604]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27244376644248336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27244376644248336 | validation: 0.31860184676756687]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080555614848457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3080555614848457 | validation: 0.20862302146665349]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20008802078272303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20008802078272303 | validation: 0.16757175624009088]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12373162670508897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12373162670508897 | validation: 0.14838599792956458]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14394331796299525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14394331796299525 | validation: 0.24243609204813713]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2078953981348169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2078953981348169 | validation: 0.2437293542710105]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23121663466682968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23121663466682968 | validation: 0.22131501761246986]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16201466947316795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16201466947316795 | validation: 0.1687177024998102]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13582609995304967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13582609995304967 | validation: 0.16815062828837216]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1323524695669252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1323524695669252 | validation: 0.1202643593103653]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15287596450348295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15287596450348295 | validation: 0.24905021741271538]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20560368033609763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20560368033609763 | validation: 0.16053682173273806]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271989095819906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271989095819906 | validation: 0.35885602797569466]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2280205716323139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2280205716323139 | validation: 0.2197256231204513]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17006044506354934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17006044506354934 | validation: 0.19047886673309275]
	TIME [epoch: 179 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163777852734266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.163777852734266 | validation: 0.23366668629497328]
	TIME [epoch: 2.75 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2931996533019617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2931996533019617 | validation: 0.15698440174072953]
	TIME [epoch: 2.75 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13990744772146585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13990744772146585 | validation: 0.18483375816352396]
	TIME [epoch: 2.74 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12268072390715162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12268072390715162 | validation: 0.11571753694834115]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691667404680986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10691667404680986 | validation: 0.14766175819099295]
	TIME [epoch: 2.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260972014856115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260972014856115 | validation: 0.10328401186962904]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564193874364466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10564193874364466 | validation: 0.17007625237582266]
	TIME [epoch: 2.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14148300398758135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14148300398758135 | validation: 0.15165115291436304]
	TIME [epoch: 2.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20415886460599125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20415886460599125 | validation: 0.2669350730442163]
	TIME [epoch: 2.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2770546137105271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2770546137105271 | validation: 0.20821839338281]
	TIME [epoch: 2.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575017196099672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575017196099672 | validation: 0.24458587525184622]
	TIME [epoch: 2.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20362359223784154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20362359223784154 | validation: 0.49202219810717507]
	TIME [epoch: 2.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3521720167688567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3521720167688567 | validation: 0.16375520097163557]
	TIME [epoch: 2.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831501279592118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1831501279592118 | validation: 0.2031473145091189]
	TIME [epoch: 2.75 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13202990051382105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13202990051382105 | validation: 0.14929351091051868]
	TIME [epoch: 2.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17158592336117554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17158592336117554 | validation: 0.2544460695696541]
	TIME [epoch: 2.74 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727400569205825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1727400569205825 | validation: 0.13334484124504975]
	TIME [epoch: 2.75 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15026530620991904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15026530620991904 | validation: 0.1832183090456594]
	TIME [epoch: 2.74 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11906119462504808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11906119462504808 | validation: 0.11357542270084885]
	TIME [epoch: 2.74 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13934710327672215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13934710327672215 | validation: 0.17505513078319904]
	TIME [epoch: 2.75 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14108059088315603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14108059088315603 | validation: 0.13132357411726675]
	TIME [epoch: 2.74 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16996492511782457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16996492511782457 | validation: 0.14224486556517213]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10355571833133446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10355571833133446 | validation: 0.11409533019491458]
	TIME [epoch: 2.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11973581111957175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11973581111957175 | validation: 0.15641304805984746]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994322106457414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11994322106457414 | validation: 0.1963179562321205]
	TIME [epoch: 2.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19593094620067641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19593094620067641 | validation: 0.4674795941133771]
	TIME [epoch: 2.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35393473656082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35393473656082 | validation: 0.2492555664234537]
	TIME [epoch: 2.74 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2315156957509081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2315156957509081 | validation: 0.15574604960396377]
	TIME [epoch: 2.74 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862649828257973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10862649828257973 | validation: 0.14022954227044054]
	TIME [epoch: 2.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11026525125919424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11026525125919424 | validation: 0.19831524295936404]
	TIME [epoch: 2.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18531312775657616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18531312775657616 | validation: 0.15339670056237212]
	TIME [epoch: 2.74 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11964495114245739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11964495114245739 | validation: 0.11982504418180369]
	TIME [epoch: 2.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817153914144743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10817153914144743 | validation: 0.14208387135129016]
	TIME [epoch: 2.74 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142731497618817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09142731497618817 | validation: 0.10698963223279097]
	TIME [epoch: 2.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12354832683101402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12354832683101402 | validation: 0.22399311795653418]
	TIME [epoch: 2.74 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16490907850648348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16490907850648348 | validation: 0.2280549318552398]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28055719745505664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28055719745505664 | validation: 0.37689958528710116]
	TIME [epoch: 2.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2388486881016272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2388486881016272 | validation: 0.14834859446459359]
	TIME [epoch: 2.74 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13266757683545272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13266757683545272 | validation: 0.22050746249393882]
	TIME [epoch: 2.73 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12863511942515088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12863511942515088 | validation: 0.1758358509197877]
	TIME [epoch: 2.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16496593388257172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16496593388257172 | validation: 0.1640878403778735]
	TIME [epoch: 2.74 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17459655545855937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17459655545855937 | validation: 0.20423157895736097]
	TIME [epoch: 2.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13719963638789487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13719963638789487 | validation: 0.17860743414445135]
	TIME [epoch: 2.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12424426682707929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12424426682707929 | validation: 0.14924485451055086]
	TIME [epoch: 2.74 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11071546083651627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11071546083651627 | validation: 0.14187935692853065]
	TIME [epoch: 2.74 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10675387978658638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10675387978658638 | validation: 0.12718197475255072]
	TIME [epoch: 2.73 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11203964450785005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11203964450785005 | validation: 0.16427173117701407]
	TIME [epoch: 2.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12267549706121438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12267549706121438 | validation: 0.13732306270852096]
	TIME [epoch: 2.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339014574671918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1339014574671918 | validation: 0.16435620297407422]
	TIME [epoch: 2.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15730832517617818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15730832517617818 | validation: 0.2306476872959911]
	TIME [epoch: 2.74 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15620905395183649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15620905395183649 | validation: 0.24861471779071398]
	TIME [epoch: 2.74 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21635304280540577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21635304280540577 | validation: 0.36214596572432595]
	TIME [epoch: 2.74 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23084950955237055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23084950955237055 | validation: 0.1488279854618488]
	TIME [epoch: 2.74 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1964223204036893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1964223204036893 | validation: 0.19410618329149487]
	TIME [epoch: 2.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12473184169041038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12473184169041038 | validation: 0.18289061638500956]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494661831469358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1494661831469358 | validation: 0.22295721281720826]
	TIME [epoch: 2.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16998329384578587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16998329384578587 | validation: 0.2030164988396022]
	TIME [epoch: 2.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15232433415921093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15232433415921093 | validation: 0.15474552687786003]
	TIME [epoch: 2.74 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08750638514232818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08750638514232818 | validation: 0.10924402460149008]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579720968174937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579720968174937 | validation: 0.14830580619208453]
	TIME [epoch: 2.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07857857731405243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07857857731405243 | validation: 0.10469915231422987]
	TIME [epoch: 2.74 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10510768001851448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10510768001851448 | validation: 0.1862490263817494]
	TIME [epoch: 2.74 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14699179245647548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14699179245647548 | validation: 0.11028780529651533]
	TIME [epoch: 2.74 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1788057957149871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1788057957149871 | validation: 0.1703975922272566]
	TIME [epoch: 2.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630135683085405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11630135683085405 | validation: 0.15062798665520313]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12872551644743419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12872551644743419 | validation: 0.1914906082548811]
	TIME [epoch: 2.74 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15787730045797096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15787730045797096 | validation: 0.2649829779448508]
	TIME [epoch: 2.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23437719798763287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23437719798763287 | validation: 0.2907935234669097]
	TIME [epoch: 2.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1969626516747066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1969626516747066 | validation: 0.2707850687343161]
	TIME [epoch: 2.74 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16360531021018507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16360531021018507 | validation: 0.22980819978180636]
	TIME [epoch: 2.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13039120566931525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13039120566931525 | validation: 0.1546055973053716]
	TIME [epoch: 2.74 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816483946171028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08816483946171028 | validation: 0.11605537212494843]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645678975335132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08645678975335132 | validation: 0.15757811629262986]
	TIME [epoch: 2.74 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08993795923457074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08993795923457074 | validation: 0.12313560893842915]
	TIME [epoch: 2.74 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255845474314193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255845474314193 | validation: 0.23718699936367005]
	TIME [epoch: 2.74 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16789532340061672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16789532340061672 | validation: 0.13044926593026654]
	TIME [epoch: 2.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18018913042227233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18018913042227233 | validation: 0.17819310831708407]
	TIME [epoch: 2.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09637330328854539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09637330328854539 | validation: 0.12679780300729188]
	TIME [epoch: 2.74 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07222718141045459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07222718141045459 | validation: 0.17504844718436585]
	TIME [epoch: 2.74 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07559499896695912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07559499896695912 | validation: 0.18782731897449875]
	TIME [epoch: 2.74 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10485736145751812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10485736145751812 | validation: 0.2889393042498282]
	TIME [epoch: 2.74 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16159925438394793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16159925438394793 | validation: 0.2237337388573459]
	TIME [epoch: 2.74 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16514802242944343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16514802242944343 | validation: 0.15883864845203385]
	TIME [epoch: 2.74 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20913636667890736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20913636667890736 | validation: 0.16598172518039497]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10354229595512003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10354229595512003 | validation: 0.12294496463951099]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091548845086819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09091548845086819 | validation: 0.1475735438156543]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08447048032033892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08447048032033892 | validation: 0.11192849692939318]
	TIME [epoch: 2.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10219016745235768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10219016745235768 | validation: 0.1872196016778469]
	TIME [epoch: 2.74 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12925213102075545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12925213102075545 | validation: 0.22351886127447962]
	TIME [epoch: 2.74 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19335225777683995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19335225777683995 | validation: 0.26610493216068226]
	TIME [epoch: 2.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038737483380961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2038737483380961 | validation: 0.24828827037759027]
	TIME [epoch: 2.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18981484368582027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18981484368582027 | validation: 0.16094341255700667]
	TIME [epoch: 2.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12988572729218284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12988572729218284 | validation: 0.1670457252765268]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.089008255842053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.089008255842053 | validation: 0.12519271457438658]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686659871180545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686659871180545 | validation: 0.1456511403564503]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08056744677026524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08056744677026524 | validation: 0.11465565702578834]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714370734719651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08714370734719651 | validation: 0.14648209105237317]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08093218830911596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093218830911596 | validation: 0.12030438563780775]
	TIME [epoch: 2.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09138852044385035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09138852044385035 | validation: 0.16442539839466527]
	TIME [epoch: 2.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341286800601452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341286800601452 | validation: 0.15502880947861866]
	TIME [epoch: 2.74 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127490558529387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.127490558529387 | validation: 0.2772594682970189]
	TIME [epoch: 2.74 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16169977779743833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16169977779743833 | validation: 0.29646548003444545]
	TIME [epoch: 2.74 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17159037827545653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17159037827545653 | validation: 0.18198814772106708]
	TIME [epoch: 2.74 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10939102724840792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10939102724840792 | validation: 0.19021813067782523]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382826498286055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1382826498286055 | validation: 0.15419179623653825]
	TIME [epoch: 2.73 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23098161672099474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23098161672099474 | validation: 0.16531618833322628]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015217048398462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1015217048398462 | validation: 0.11188544306994129]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_161041/states/model_phi1_4a_v_mmd2_608.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1444.756 seconds.
