Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/basic/data_phi1_3a/training', validation_data='data/training_data/basic/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2021028128

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.004379785015892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.004379785015892 | validation: 5.604481122576496]
	TIME [epoch: 277 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.445551121305027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.445551121305027 | validation: 4.5106103157886395]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.107677173022593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.107677173022593 | validation: 5.348837393025587]
	TIME [epoch: 0.656 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.308485408136374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.308485408136374 | validation: 4.7622654828060105]
	TIME [epoch: 0.656 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.185567979632074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.185567979632074 | validation: 4.3732849353029835]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.888803411625489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.888803411625489 | validation: 4.675238711128877]
	TIME [epoch: 0.652 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6602112283469648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6602112283469648 | validation: 4.62748347726805]
	TIME [epoch: 0.649 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.622193873681444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.622193873681444 | validation: 4.38019395852946]
	TIME [epoch: 0.646 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.584081706024808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.584081706024808 | validation: 4.466771532596042]
	TIME [epoch: 0.649 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.539484877485687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539484877485687 | validation: 4.391348139759207]
	TIME [epoch: 0.648 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5039771824484394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5039771824484394 | validation: 4.401738825323655]
	TIME [epoch: 0.644 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.485327715771345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485327715771345 | validation: 4.276848780765553]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5079843179437615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5079843179437615 | validation: 4.530610611512013]
	TIME [epoch: 0.653 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6532206215073604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6532206215073604 | validation: 4.2349324586138595]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.467703264455163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.467703264455163 | validation: 4.326599163304931]
	TIME [epoch: 0.655 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4302291061777828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4302291061777828 | validation: 4.197770165333196]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3960590445847822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3960590445847822 | validation: 4.281329807432796]
	TIME [epoch: 0.648 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3958995685982107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3958995685982107 | validation: 4.166067134029557]
	TIME [epoch: 0.646 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3968332997324353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3968332997324353 | validation: 4.295951388920428]
	TIME [epoch: 0.655 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4350686749033335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4350686749033335 | validation: 4.14243097245664]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3758677318692385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3758677318692385 | validation: 4.221162649722408]
	TIME [epoch: 0.659 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3638156402219836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3638156402219836 | validation: 4.102215184483613]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.316235919594508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.316235919594508 | validation: 4.163241313378248]
	TIME [epoch: 0.649 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.297778950017097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.297778950017097 | validation: 4.085189236142944]
	TIME [epoch: 0.645 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2824419361131025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2824419361131025 | validation: 4.127988612455927]
	TIME [epoch: 0.652 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.284683838504974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.284683838504974 | validation: 4.055222609297139]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2749034237695436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2749034237695436 | validation: 4.13546288396854]
	TIME [epoch: 0.652 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.296702741455207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296702741455207 | validation: 4.032661318685692]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.261023889877831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.261023889877831 | validation: 4.087644932692631]
	TIME [epoch: 0.648 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.254751622946393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.254751622946393 | validation: 3.9956527561406676]
	TIME [epoch: 0.646 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.213455305366291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.213455305366291 | validation: 4.024546018969554]
	TIME [epoch: 0.651 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.191826269543219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.191826269543219 | validation: 3.963909866898793]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1767879511374804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1767879511374804 | validation: 4.005611743130996]
	TIME [epoch: 0.649 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.176665109182386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.176665109182386 | validation: 3.918508670324565]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.173157956580656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.173157956580656 | validation: 4.035023202007284]
	TIME [epoch: 0.651 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1836833561440323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1836833561440323 | validation: 3.878386480958227]
	TIME [epoch: 0.647 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1473304611777455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1473304611777455 | validation: 4.021127835325607]
	TIME [epoch: 0.648 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1431751066421625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1431751066421625 | validation: 3.8421380358049904]
	TIME [epoch: 0.646 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1102919673379628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1102919673379628 | validation: 3.930867092833863]
	TIME [epoch: 0.649 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.088715147401091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.088715147401091 | validation: 3.828677719211031]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0831313065503054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0831313065503054 | validation: 3.90555773775088]
	TIME [epoch: 0.65 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0935164547939453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0935164547939453 | validation: 3.850750618792107]
	TIME [epoch: 0.647 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1228929727762846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1228929727762846 | validation: 3.8296519112281295]
	TIME [epoch: 0.647 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.073952165895784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.073952165895784 | validation: 3.8460485508801927]
	TIME [epoch: 0.647 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0547610668736946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0547610668736946 | validation: 3.7489083491434787]
	TIME [epoch: 0.646 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.010018728647077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.010018728647077 | validation: 3.8244623960444417]
	TIME [epoch: 0.647 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0028216398682197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0028216398682197 | validation: 3.718963470539272]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.989156778718497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.989156778718497 | validation: 3.7948956689042634]
	TIME [epoch: 0.649 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.988278049317655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.988278049317655 | validation: 3.6906589420339353]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.970887140172623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.970887140172623 | validation: 3.7613003354545684]
	TIME [epoch: 0.659 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9613973248353886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9613973248353886 | validation: 3.6568187555228224]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.945239804825783		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.945239804825783 | validation: 3.721961903410791]
	TIME [epoch: 0.656 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9417883628389223		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.9417883628389223 | validation: 3.635410200788172]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9221227880251774		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.9221227880251774 | validation: 3.714032460486854]
	TIME [epoch: 0.653 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9280630180548166		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.9280630180548166 | validation: 3.5864270767601156]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.913426522578206		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.913426522578206 | validation: 3.708547753282649]
	TIME [epoch: 0.65 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9219498014248035		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.9219498014248035 | validation: 3.543866344351368]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.915598168761136		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.915598168761136 | validation: 3.6498026873576026]
	TIME [epoch: 0.65 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8799313834456415		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.8799313834456415 | validation: 3.537428161480463]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.850830768766096		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.850830768766096 | validation: 3.5465732431946813]
	TIME [epoch: 0.65 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.831788309281348		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.831788309281348 | validation: 3.5849294324091803]
	TIME [epoch: 0.65 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8245597310926134		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.8245597310926134 | validation: 3.477141724113615]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8220129051590495		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.8220129051590495 | validation: 3.5382495574444413]
	TIME [epoch: 0.651 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8147402783396878		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8147402783396878 | validation: 3.4574073289986766]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8109984755768886		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.8109984755768886 | validation: 3.5477766749448585]
	TIME [epoch: 0.655 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.817773110194997		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.817773110194997 | validation: 3.439486112018311]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8003710856019053		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.8003710856019053 | validation: 3.5267389534855553]
	TIME [epoch: 0.656 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.810437390490439		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.810437390490439 | validation: 3.4073317597003143]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7841591104778876		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.7841591104778876 | validation: 3.4669678116626486]
	TIME [epoch: 0.652 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7636340848034155		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.7636340848034155 | validation: 3.3832584663594263]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.738610338943218		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.738610338943218 | validation: 3.398070816359752]
	TIME [epoch: 0.646 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7293439982832894		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.7293439982832894 | validation: 3.355279980078273]
	TIME [epoch: 0.645 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.717229559521811		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.717229559521811 | validation: 3.3705703477375693]
	TIME [epoch: 0.655 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.710178332970204		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.710178332970204 | validation: 3.3103810328050205]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.705573763678661		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.705573763678661 | validation: 3.4508981257837377]
	TIME [epoch: 0.653 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7609483021575363		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.7609483021575363 | validation: 3.3001718324317495]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7994196338414317		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.7994196338414317 | validation: 3.4105724468061824]
	TIME [epoch: 0.653 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.743783421010799		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.743783421010799 | validation: 3.290677823103898]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.65989297232751		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.65989297232751 | validation: 3.2541759623321216]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.688893905085992		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.688893905085992 | validation: 3.2666637869230932]
	TIME [epoch: 0.649 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.655333738022639		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.655333738022639 | validation: 3.2140537942898777]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.607219127884303		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.607219127884303 | validation: 3.1424712672113366]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.572089720608224		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.572089720608224 | validation: 2.984917414723087]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4145633128996487		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.4145633128996487 | validation: 2.5891471355782327]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.180260497365253		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.180260497365253 | validation: 2.357229877403762]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9800459420666066		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.9800459420666066 | validation: 2.1049104213394654]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0645383804626953		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.0645383804626953 | validation: 1.8161655739746174]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6325588794045347		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6325588794045347 | validation: 1.27530310443051]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4128773708695799		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.4128773708695799 | validation: 1.1784995278536863]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194129739285439		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.194129739285439 | validation: 1.2530805619509904]
	TIME [epoch: 0.653 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5487601852832638		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.5487601852832638 | validation: 1.343514986165165]
	TIME [epoch: 0.654 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2093152604500623		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2093152604500623 | validation: 0.9752031708505484]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9818971973211922		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9818971973211922 | validation: 0.890460694845082]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0226881835637573		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.0226881835637573 | validation: 1.0839561782858138]
	TIME [epoch: 0.652 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0173900183315874		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0173900183315874 | validation: 0.8860564155489931]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9403005290244125		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9403005290244125 | validation: 0.9140871250680477]
	TIME [epoch: 0.653 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8988404528282502		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8988404528282502 | validation: 0.8697996433676138]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8968060453769243		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8968060453769243 | validation: 0.8438042361462869]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871945818370265		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8871945818370265 | validation: 0.920024909014823]
	TIME [epoch: 0.652 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900500713389794		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8900500713389794 | validation: 0.8564951199445293]
	TIME [epoch: 0.651 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892738539260767		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8892738539260767 | validation: 1.0227685227396286]
	TIME [epoch: 0.651 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349085910098088		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.9349085910098088 | validation: 0.8945977806139571]
	TIME [epoch: 0.65 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9726440563349144		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9726440563349144 | validation: 1.0957987876798614]
	TIME [epoch: 0.648 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9876747913027595		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9876747913027595 | validation: 0.8281948377607509]
	TIME [epoch: 0.647 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951739707965995		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8951739707965995 | validation: 0.8670316723805347]
	TIME [epoch: 0.652 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8618159090153015		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8618159090153015 | validation: 0.8201834992906492]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500589921438362		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8500589921438362 | validation: 0.8511682783996222]
	TIME [epoch: 0.654 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380552669945498		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8380552669945498 | validation: 0.8908879213884049]
	TIME [epoch: 0.65 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487977640162222		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8487977640162222 | validation: 0.8338825433130271]
	TIME [epoch: 0.649 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474925737233941		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8474925737233941 | validation: 0.9374792972737354]
	TIME [epoch: 0.649 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8792111225559629		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8792111225559629 | validation: 0.8970305110590747]
	TIME [epoch: 0.648 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9517129559501817		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9517129559501817 | validation: 1.094003547904914]
	TIME [epoch: 0.648 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993035062115595		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9993035062115595 | validation: 0.8513413019507926]
	TIME [epoch: 0.65 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827615958803304		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8827615958803304 | validation: 0.8243568424697356]
	TIME [epoch: 0.649 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272743701038031		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8272743701038031 | validation: 0.8425398375225829]
	TIME [epoch: 0.647 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313146506365994		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8313146506365994 | validation: 0.8286199860051838]
	TIME [epoch: 0.65 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445884561336116		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8445884561336116 | validation: 0.8593628924418781]
	TIME [epoch: 0.649 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334783355844636		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8334783355844636 | validation: 0.84760624366945]
	TIME [epoch: 0.648 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380666112150891		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8380666112150891 | validation: 0.8685436559790474]
	TIME [epoch: 0.648 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287666765806655		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8287666765806655 | validation: 0.8569408897552844]
	TIME [epoch: 0.65 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473375074623806		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8473375074623806 | validation: 0.9376513958115992]
	TIME [epoch: 0.649 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873469667885576		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8873469667885576 | validation: 0.9012908481855222]
	TIME [epoch: 0.648 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223643138895173		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.9223643138895173 | validation: 1.0069465013664827]
	TIME [epoch: 0.649 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9108924947390531		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9108924947390531 | validation: 0.82969390351044]
	TIME [epoch: 0.649 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320554700981362		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8320554700981362 | validation: 0.8260884388217757]
	TIME [epoch: 0.648 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025518254040809		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8025518254040809 | validation: 0.8390125454442251]
	TIME [epoch: 0.648 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135620893403217		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8135620893403217 | validation: 0.8322238868098357]
	TIME [epoch: 0.648 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158303256879771		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8158303256879771 | validation: 0.8470587503970844]
	TIME [epoch: 0.648 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084676888874331		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8084676888874331 | validation: 0.8218008094082413]
	TIME [epoch: 0.65 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244088455251823		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8244088455251823 | validation: 0.8816563237133732]
	TIME [epoch: 0.648 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499006966410164		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8499006966410164 | validation: 0.8292350201811989]
	TIME [epoch: 0.647 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8410528354105558		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8410528354105558 | validation: 0.836758055302494]
	TIME [epoch: 0.648 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500202224219544		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8500202224219544 | validation: 0.9134766098685632]
	TIME [epoch: 0.652 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631691788841838		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8631691788841838 | validation: 0.9139968811007808]
	TIME [epoch: 0.647 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295870889972564		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9295870889972564 | validation: 1.0159530176844722]
	TIME [epoch: 0.647 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232447650004346		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9232447650004346 | validation: 0.815496788819528]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026560501905476		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8026560501905476 | validation: 0.8093551330805415]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164772530394736		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8164772530394736 | validation: 0.8727479795230384]
	TIME [epoch: 0.649 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155128761934681		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8155128761934681 | validation: 0.8423144954510948]
	TIME [epoch: 0.648 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018784039227108		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8018784039227108 | validation: 0.8321903336281071]
	TIME [epoch: 0.647 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008826898037495		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8008826898037495 | validation: 0.8516790147370108]
	TIME [epoch: 0.649 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8238211819135893		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8238211819135893 | validation: 0.8628296770885303]
	TIME [epoch: 0.648 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499704043326983		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8499704043326983 | validation: 0.9304074997871852]
	TIME [epoch: 0.648 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945421844254198		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8945421844254198 | validation: 0.865862645344169]
	TIME [epoch: 0.647 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551161003822463		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8551161003822463 | validation: 0.848144122820473]
	TIME [epoch: 0.647 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050533486846166		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8050533486846166 | validation: 0.8223003754378205]
	TIME [epoch: 0.646 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935851127300874		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7935851127300874 | validation: 0.8226728274639602]
	TIME [epoch: 0.646 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966765698036244		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7966765698036244 | validation: 0.829181673831813]
	TIME [epoch: 0.649 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035033921853474		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8035033921853474 | validation: 0.8505745144943623]
	TIME [epoch: 0.648 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244005454188085		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8244005454188085 | validation: 0.8253433140205493]
	TIME [epoch: 0.648 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167290017079231		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8167290017079231 | validation: 0.8363254080629882]
	TIME [epoch: 0.647 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142381571253221		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8142381571253221 | validation: 0.848700739428163]
	TIME [epoch: 0.649 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923843929287142		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7923843929287142 | validation: 0.827288086766464]
	TIME [epoch: 0.649 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082570113673336		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8082570113673336 | validation: 0.9430152514665825]
	TIME [epoch: 0.649 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606295633161923		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8606295633161923 | validation: 0.8751924798498604]
	TIME [epoch: 0.65 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891804642080825		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.891804642080825 | validation: 0.8723753041230751]
	TIME [epoch: 0.65 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300695660382603		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8300695660382603 | validation: 0.8160842747217159]
	TIME [epoch: 0.647 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884919948314019		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7884919948314019 | validation: 0.7797494047676221]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885029454078216		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7885029454078216 | validation: 0.8408010099451314]
	TIME [epoch: 0.652 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797268218491838		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.797268218491838 | validation: 0.8044296567441027]
	TIME [epoch: 0.651 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836817414559423		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7836817414559423 | validation: 0.7959028871126349]
	TIME [epoch: 0.652 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780919499996926		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7780919499996926 | validation: 0.7944682265660573]
	TIME [epoch: 0.652 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7733575165129496		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7733575165129496 | validation: 0.7933966766945311]
	TIME [epoch: 0.649 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917215594794961		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7917215594794961 | validation: 0.8459776162130709]
	TIME [epoch: 0.651 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198035479586591		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8198035479586591 | validation: 0.8846403728678589]
	TIME [epoch: 0.652 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9069894636389974		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.9069894636389974 | validation: 0.8946942818860422]
	TIME [epoch: 0.649 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447679225862784		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8447679225862784 | validation: 0.7987737811567255]
	TIME [epoch: 0.646 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739099955157608		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7739099955157608 | validation: 0.7998810291007218]
	TIME [epoch: 0.65 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691124524700191		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.7691124524700191 | validation: 0.784368417199961]
	TIME [epoch: 0.645 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748629200739925		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7748629200739925 | validation: 0.7940246265727221]
	TIME [epoch: 0.648 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914905765838391		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7914905765838391 | validation: 0.8035819490010992]
	TIME [epoch: 0.646 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817549021564778		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7817549021564778 | validation: 0.8025748558218221]
	TIME [epoch: 0.65 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774453104450094		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7774453104450094 | validation: 0.7731118057544051]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732126145179089		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7732126145179089 | validation: 0.8412448783416799]
	TIME [epoch: 0.653 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960462816699518		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7960462816699518 | validation: 0.815823350878183]
	TIME [epoch: 0.651 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234894377441453		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8234894377441453 | validation: 0.8214445776149785]
	TIME [epoch: 0.651 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887958297481862		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7887958297481862 | validation: 0.7720118769877958]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758419369970301		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.758419369970301 | validation: 0.7659779019431423]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622525058147737		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7622525058147737 | validation: 0.8137440312464019]
	TIME [epoch: 0.653 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807392959523918		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7807392959523918 | validation: 0.793746252022964]
	TIME [epoch: 0.654 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927804022860488		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7927804022860488 | validation: 0.7761372839549845]
	TIME [epoch: 0.65 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657224311551721		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7657224311551721 | validation: 0.7551775742836793]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572848998472211		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7572848998472211 | validation: 0.7464580646380443]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506863173401763		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7506863173401763 | validation: 0.7712145584844521]
	TIME [epoch: 0.654 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554805816109382		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7554805816109382 | validation: 0.7467852854323658]
	TIME [epoch: 0.652 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524598029297175		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7524598029297175 | validation: 0.7856937701122984]
	TIME [epoch: 0.649 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774414870408121		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.774414870408121 | validation: 0.7451473770751225]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735670367627083		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7735670367627083 | validation: 0.7725590537467089]
	TIME [epoch: 0.654 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699572633495371		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7699572633495371 | validation: 0.7429193394563031]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499577684360554		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7499577684360554 | validation: 0.7522812291872122]
	TIME [epoch: 0.654 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398941389888342		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7398941389888342 | validation: 0.7351388680008003]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337633976713077		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7337633976713077 | validation: 0.7574809820176185]
	TIME [epoch: 0.655 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333231269192116		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7333231269192116 | validation: 0.719509356906432]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517680013805069		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7517680013805069 | validation: 0.7716246569298724]
	TIME [epoch: 0.655 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760452231216992		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.760452231216992 | validation: 0.7321723574962944]
	TIME [epoch: 0.655 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626202094551054		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7626202094551054 | validation: 0.7240013958163628]
	TIME [epoch: 0.656 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459994395369721		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7459994395369721 | validation: 0.7603814451784787]
	TIME [epoch: 0.653 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711190344187441		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7711190344187441 | validation: 0.7425262054514667]
	TIME [epoch: 0.655 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608673974555944		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7608673974555944 | validation: 0.7358846917189594]
	TIME [epoch: 0.655 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282335200096824		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7282335200096824 | validation: 0.6980403944294327]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7090105610852715		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7090105610852715 | validation: 0.6952162486657774]
	TIME [epoch: 288 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094230511779273		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7094230511779273 | validation: 0.6906552849471689]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043653883246203		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7043653883246203 | validation: 0.6834461882837026]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135613214331088		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7135613214331088 | validation: 0.6988276128539259]
	TIME [epoch: 1.28 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093604150718653		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7093604150718653 | validation: 0.7087557763187614]
	TIME [epoch: 1.28 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160649341216646		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7160649341216646 | validation: 0.7359257937183614]
	TIME [epoch: 1.28 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373565067316062		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7373565067316062 | validation: 0.69291504370725]
	TIME [epoch: 1.28 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265888485588089		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7265888485588089 | validation: 0.681196596891452]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048851401514847		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7048851401514847 | validation: 0.6738197017071546]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836800190799551		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6836800190799551 | validation: 0.6680619497915012]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825282756856029		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6825282756856029 | validation: 0.6482074265603434]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763744400836578		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6763744400836578 | validation: 0.6637160106105334]
	TIME [epoch: 1.28 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722789043903356		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6722789043903356 | validation: 0.6472462719779742]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668041828810704		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.668041828810704 | validation: 0.6864473927385281]
	TIME [epoch: 1.27 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805209997120599		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6805209997120599 | validation: 0.6410775605305976]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999630623261558		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6999630623261558 | validation: 0.6732800029408252]
	TIME [epoch: 1.27 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739648389650915		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6739648389650915 | validation: 0.6286622309173432]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633048472747731		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6633048472747731 | validation: 0.6743301365252136]
	TIME [epoch: 1.27 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7036311034298568		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7036311034298568 | validation: 0.7632100807956855]
	TIME [epoch: 1.27 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802713194790463		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.802713194790463 | validation: 0.6599232966022204]
	TIME [epoch: 1.27 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698331147434847		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.698331147434847 | validation: 0.6314121640267758]
	TIME [epoch: 1.27 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6452331320981634		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6452331320981634 | validation: 0.639714817835806]
	TIME [epoch: 1.27 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581902635597608		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6581902635597608 | validation: 0.6279656520086979]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533543390394979		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6533543390394979 | validation: 0.6097947808016619]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342628735447583		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6342628735447583 | validation: 0.6070009902274416]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234585162692171		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6234585162692171 | validation: 0.59528573397255]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350820908780691		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6350820908780691 | validation: 0.6692049911070088]
	TIME [epoch: 1.27 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585866652772552		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6585866652772552 | validation: 0.603543414352659]
	TIME [epoch: 1.27 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395330130488935		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6395330130488935 | validation: 0.6111831600458375]
	TIME [epoch: 1.27 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247218329411541		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6247218329411541 | validation: 0.5861060866990936]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149204224739037		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6149204224739037 | validation: 0.5695589435264801]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121573782714742		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6121573782714742 | validation: 0.6091327768691647]
	TIME [epoch: 1.27 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201917985236972		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6201917985236972 | validation: 0.604138842209084]
	TIME [epoch: 1.28 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581623545007713		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6581623545007713 | validation: 0.6250747971258145]
	TIME [epoch: 1.28 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6164033804923765		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6164033804923765 | validation: 0.5764372761063908]
	TIME [epoch: 1.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6031296403199324		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6031296403199324 | validation: 0.5402775061143595]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5943265303684371		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.5943265303684371 | validation: 0.5871521573161873]
	TIME [epoch: 1.28 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970732398163635		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.5970732398163635 | validation: 0.5647719432792874]
	TIME [epoch: 1.28 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246992434404359		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6246992434404359 | validation: 0.6094909806700303]
	TIME [epoch: 1.29 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974919319196813		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.5974919319196813 | validation: 0.5332487087358748]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687918803358311		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.5687918803358311 | validation: 0.5465261973664944]
	TIME [epoch: 1.28 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5607263242129481		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.5607263242129481 | validation: 0.5105680763811759]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595063202437449		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.5595063202437449 | validation: 0.5585858979807822]
	TIME [epoch: 1.27 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5623140209461306		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5623140209461306 | validation: 0.5130374598524909]
	TIME [epoch: 1.27 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983131447336111		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.5983131447336111 | validation: 0.6387529803727379]
	TIME [epoch: 1.27 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618940518301432		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.618940518301432 | validation: 0.5141184719780144]
	TIME [epoch: 1.28 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570235002998747		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.570235002998747 | validation: 0.508825847728999]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390325088524448		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.5390325088524448 | validation: 0.4816172090246246]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478519216886941		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5478519216886941 | validation: 0.4905980087483025]
	TIME [epoch: 1.27 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5370642242870858		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.5370642242870858 | validation: 0.5101559984871373]
	TIME [epoch: 1.27 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400177223461585		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.5400177223461585 | validation: 0.4628877286476403]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442966282509114		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.5442966282509114 | validation: 0.6067006494492384]
	TIME [epoch: 1.27 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.581120346429553		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.581120346429553 | validation: 0.45964535205947393]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508929940730902		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5508929940730902 | validation: 0.5056896092329567]
	TIME [epoch: 1.27 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5156960453639817		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5156960453639817 | validation: 0.4600111672456967]
	TIME [epoch: 1.27 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021225299716977		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5021225299716977 | validation: 0.45422127679129826]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4887802138939024		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.4887802138939024 | validation: 0.4406389103501056]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4991446792803398		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.4991446792803398 | validation: 0.49455431222452795]
	TIME [epoch: 1.27 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4940803977103167		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.4940803977103167 | validation: 0.45860024349229794]
	TIME [epoch: 1.27 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5498378183118707		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5498378183118707 | validation: 0.5930211300873774]
	TIME [epoch: 1.27 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506386075405753		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5506386075405753 | validation: 0.4333999046753778]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4897555755916772		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.4897555755916772 | validation: 0.42262417277807307]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4639232600133566		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.4639232600133566 | validation: 0.457147610121244]
	TIME [epoch: 1.28 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46507270809854917		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.46507270809854917 | validation: 0.40459635546250833]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4611833850589291		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4611833850589291 | validation: 0.46944101198293836]
	TIME [epoch: 1.31 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4552548434831777		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4552548434831777 | validation: 0.4118347645885927]
	TIME [epoch: 1.27 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46417058541805556		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.46417058541805556 | validation: 0.5171384777737086]
	TIME [epoch: 1.27 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48339831510792375		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.48339831510792375 | validation: 0.42384935064120877]
	TIME [epoch: 1.27 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48018196486299575		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.48018196486299575 | validation: 0.4734708280701918]
	TIME [epoch: 1.27 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44739398684710757		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.44739398684710757 | validation: 0.40256822084532495]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44093428391811956		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.44093428391811956 | validation: 0.5740494367616341]
	TIME [epoch: 1.27 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5121483910064173		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5121483910064173 | validation: 0.39427059791654456]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42699121542361296		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.42699121542361296 | validation: 0.3869158145709728]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44185811736601466		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.44185811736601466 | validation: 0.4961104517377446]
	TIME [epoch: 1.28 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4589039355098642		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.4589039355098642 | validation: 0.3895753400040971]
	TIME [epoch: 1.27 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4156098233535238		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.4156098233535238 | validation: 0.382838583392237]
	TIME [epoch: 1.26 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3840337255550867		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.3840337255550867 | validation: 0.36119468708395613]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3844815467756482		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3844815467756482 | validation: 0.42512835656097514]
	TIME [epoch: 1.27 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3899848396166793		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.3899848396166793 | validation: 0.39546681982364956]
	TIME [epoch: 1.27 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4226221932641718		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.4226221932641718 | validation: 0.6650269491605161]
	TIME [epoch: 1.27 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5602417364255331		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5602417364255331 | validation: 0.4674198805283128]
	TIME [epoch: 1.27 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.405291261150105		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.405291261150105 | validation: 0.5311239064911595]
	TIME [epoch: 1.27 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5589592484286177		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5589592484286177 | validation: 0.4660892011710378]
	TIME [epoch: 1.27 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4357552916609473		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.4357552916609473 | validation: 0.7093250606831325]
	TIME [epoch: 1.27 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202159602169937		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6202159602169937 | validation: 0.44561193898726653]
	TIME [epoch: 1.27 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.402870889036535		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.402870889036535 | validation: 0.438753985540071]
	TIME [epoch: 1.27 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4528129861269435		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4528129861269435 | validation: 0.3813174859155514]
	TIME [epoch: 1.27 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3601872975114276		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3601872975114276 | validation: 0.42984278311803426]
	TIME [epoch: 1.27 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380816580949337		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.380816580949337 | validation: 0.3670580339951344]
	TIME [epoch: 1.27 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35989071866070005		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.35989071866070005 | validation: 0.4022328063146889]
	TIME [epoch: 1.27 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3561464625731481		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.3561464625731481 | validation: 0.37087863976419844]
	TIME [epoch: 1.27 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.360142287293433		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.360142287293433 | validation: 0.4443893616074783]
	TIME [epoch: 1.27 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36340727881119433		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.36340727881119433 | validation: 0.3719286076530483]
	TIME [epoch: 1.27 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675378682180417		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.3675378682180417 | validation: 0.42385950536737255]
	TIME [epoch: 1.27 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36420669613707946		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.36420669613707946 | validation: 0.358201202844905]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3410043135337912		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3410043135337912 | validation: 0.36869932191748983]
	TIME [epoch: 1.27 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259854997799478		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.3259854997799478 | validation: 0.35601295597466065]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32122035648470365		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.32122035648470365 | validation: 0.38885889074909863]
	TIME [epoch: 1.27 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116307545471459		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.3116307545471459 | validation: 0.4284455736352724]
	TIME [epoch: 1.27 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39786442387543575		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.39786442387543575 | validation: 0.6724625320346299]
	TIME [epoch: 1.27 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570576248273067		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.570576248273067 | validation: 0.6074430282200223]
	TIME [epoch: 1.27 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217343178595001		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5217343178595001 | validation: 0.3815573763236482]
	TIME [epoch: 1.27 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34700859786004146		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.34700859786004146 | validation: 0.46498577827363685]
	TIME [epoch: 1.27 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3779525743791825		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.3779525743791825 | validation: 0.41047493399176993]
	TIME [epoch: 1.26 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40977073434344674		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.40977073434344674 | validation: 0.39597029768879277]
	TIME [epoch: 1.27 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34382181405771506		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.34382181405771506 | validation: 0.3879363299571804]
	TIME [epoch: 1.26 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155302054908989		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.3155302054908989 | validation: 0.37403504877752985]
	TIME [epoch: 1.27 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463989754551325		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.3463989754551325 | validation: 0.3998267459146069]
	TIME [epoch: 1.27 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31826637843286293		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.31826637843286293 | validation: 0.34727637622573015]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149884493211475		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3149884493211475 | validation: 0.37694055767016005]
	TIME [epoch: 1.27 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3160159410017436		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.3160159410017436 | validation: 0.3453510029858186]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3067936924731804		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.3067936924731804 | validation: 0.37926060974877096]
	TIME [epoch: 1.27 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044040997841401		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.3044040997841401 | validation: 0.36930364875919786]
	TIME [epoch: 1.27 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323894159410436		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.3323894159410436 | validation: 0.4797673490907691]
	TIME [epoch: 1.27 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35894206927382927		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.35894206927382927 | validation: 0.3581271402210967]
	TIME [epoch: 1.27 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30551517476556184		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.30551517476556184 | validation: 0.3491833222667948]
	TIME [epoch: 1.27 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742805803225628		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.2742805803225628 | validation: 0.3366461741113693]
	TIME [epoch: 1.26 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2738517196546739		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.2738517196546739 | validation: 0.34607035567830613]
	TIME [epoch: 1.27 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26665661361452286		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.26665661361452286 | validation: 0.3472132529622585]
	TIME [epoch: 1.26 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27351053014349375		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.27351053014349375 | validation: 0.4373931938992591]
	TIME [epoch: 1.27 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31901269144855354		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.31901269144855354 | validation: 0.4784412352091645]
	TIME [epoch: 1.26 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.434591506405806		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.434591506405806 | validation: 0.5801416753854861]
	TIME [epoch: 1.27 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4438800642174433		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4438800642174433 | validation: 0.3465958559013365]
	TIME [epoch: 1.26 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28467903182510196		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.28467903182510196 | validation: 0.450195665334902]
	TIME [epoch: 1.27 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3921319208055723		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.3921319208055723 | validation: 0.5442987248750402]
	TIME [epoch: 1.27 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41088643646625883		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.41088643646625883 | validation: 0.37259868677788743]
	TIME [epoch: 1.27 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3329809939998593		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3329809939998593 | validation: 0.3720292110660278]
	TIME [epoch: 1.27 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31710625859285574		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.31710625859285574 | validation: 0.44712277486308843]
	TIME [epoch: 1.27 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3374342117850337		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.3374342117850337 | validation: 0.32763755083111995]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756204495938957		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.2756204495938957 | validation: 0.3280924938073062]
	TIME [epoch: 1.27 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26491134450095016		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.26491134450095016 | validation: 0.36351840382158707]
	TIME [epoch: 1.27 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2638494567835812		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.2638494567835812 | validation: 0.3290858913554988]
	TIME [epoch: 1.27 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262490006226185		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.262490006226185 | validation: 0.34297464009913625]
	TIME [epoch: 1.27 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2597816511174988		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.2597816511174988 | validation: 0.3071291063614089]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258274128778105		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.258274128778105 | validation: 0.3686612838559238]
	TIME [epoch: 1.27 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2617020803782809		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.2617020803782809 | validation: 0.31528757266840135]
	TIME [epoch: 1.27 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26508003267430974		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.26508003267430974 | validation: 0.47314574378931573]
	TIME [epoch: 1.26 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339196072376905		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3339196072376905 | validation: 0.3703445360717921]
	TIME [epoch: 1.27 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34437989253434637		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.34437989253434637 | validation: 0.31752608708911095]
	TIME [epoch: 1.26 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27385934165108483		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.27385934165108483 | validation: 0.394801755734278]
	TIME [epoch: 1.27 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3146777594449483		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.3146777594449483 | validation: 0.31129904302778155]
	TIME [epoch: 1.27 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25622547386390165		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.25622547386390165 | validation: 0.30861942355762184]
	TIME [epoch: 1.27 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24646718841062498		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.24646718841062498 | validation: 0.3320532803458941]
	TIME [epoch: 1.27 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25080120625995567		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.25080120625995567 | validation: 0.4132162279055964]
	TIME [epoch: 1.27 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2952217584872739		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2952217584872739 | validation: 0.3998329002890168]
	TIME [epoch: 1.26 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473608692816565		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.3473608692816565 | validation: 0.42867200163015406]
	TIME [epoch: 1.26 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31656209288015363		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.31656209288015363 | validation: 0.2989667286948838]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23316411500284012		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.23316411500284012 | validation: 0.33758847398974434]
	TIME [epoch: 1.27 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700684776372183		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.2700684776372183 | validation: 0.39341489721311035]
	TIME [epoch: 1.27 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30371009459999276		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.30371009459999276 | validation: 0.29749252465773107]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2393716311361861		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2393716311361861 | validation: 0.3161760620056325]
	TIME [epoch: 1.27 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2581225524930735		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2581225524930735 | validation: 0.43988842599044436]
	TIME [epoch: 1.27 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31717433135249107		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.31717433135249107 | validation: 0.30811373916998736]
	TIME [epoch: 1.27 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26344822652286964		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.26344822652286964 | validation: 0.2811612502223436]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347392234153807		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.2347392234153807 | validation: 0.32323004099413305]
	TIME [epoch: 1.27 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23029726026554817		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.23029726026554817 | validation: 0.2650144476096516]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22137892392131406		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.22137892392131406 | validation: 0.3091160728463124]
	TIME [epoch: 1.28 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2275091927104814		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.2275091927104814 | validation: 0.29506948854627074]
	TIME [epoch: 1.28 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24381763157115027		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.24381763157115027 | validation: 0.36084072315920324]
	TIME [epoch: 1.27 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2642041596191406		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.2642041596191406 | validation: 0.8437744144758702]
	TIME [epoch: 1.27 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650367861810682		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.7650367861810682 | validation: 0.7888468290694259]
	TIME [epoch: 1.27 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475426829351451		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.7475426829351451 | validation: 0.5762089671987031]
	TIME [epoch: 1.27 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571010553877805		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5571010553877805 | validation: 0.3640894287197478]
	TIME [epoch: 1.27 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32427298780157116		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.32427298780157116 | validation: 0.38118414721942245]
	TIME [epoch: 1.27 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2982441946143141		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.2982441946143141 | validation: 0.3415106143564794]
	TIME [epoch: 1.27 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27422851532170833		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.27422851532170833 | validation: 0.3045992196263481]
	TIME [epoch: 1.27 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23195642995327284		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.23195642995327284 | validation: 0.30670336880862487]
	TIME [epoch: 1.27 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357692330147412		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2357692330147412 | validation: 0.28150388863788817]
	TIME [epoch: 1.27 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23050034673813788		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.23050034673813788 | validation: 0.2845014761363718]
	TIME [epoch: 1.27 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21945107347674914		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.21945107347674914 | validation: 0.2927889791827633]
	TIME [epoch: 1.27 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2153161794490795		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.2153161794490795 | validation: 0.2760515604108517]
	TIME [epoch: 1.27 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21112045971120832		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.21112045971120832 | validation: 0.2713684621481027]
	TIME [epoch: 1.27 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20903930800922282		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.20903930800922282 | validation: 0.2666363761571542]
	TIME [epoch: 1.27 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20815422624801092		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.20815422624801092 | validation: 0.2867834241530902]
	TIME [epoch: 1.27 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22933287912680644		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.22933287912680644 | validation: 0.34328522191923394]
	TIME [epoch: 1.27 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34420751329847465		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.34420751329847465 | validation: 0.28749123127328435]
	TIME [epoch: 1.27 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23020951079816235		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.23020951079816235 | validation: 0.27509102503448263]
	TIME [epoch: 1.27 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20259789165942657		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.20259789165942657 | validation: 0.25088661185002464]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21567358177305604		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.21567358177305604 | validation: 0.33146810017974954]
	TIME [epoch: 1.27 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2253008899227006		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.2253008899227006 | validation: 0.28411275037646805]
	TIME [epoch: 1.27 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26393629041253946		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.26393629041253946 | validation: 0.3404277584462532]
	TIME [epoch: 1.27 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2524210156747488		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.2524210156747488 | validation: 0.3061651411047559]
	TIME [epoch: 1.28 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24188133553067379		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.24188133553067379 | validation: 0.3433569631391553]
	TIME [epoch: 1.27 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2748995567057009		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.2748995567057009 | validation: 0.2854140177380905]
	TIME [epoch: 1.27 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24083071882386548		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.24083071882386548 | validation: 0.2865562952604882]
	TIME [epoch: 1.27 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2222721730320133		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.2222721730320133 | validation: 0.26061980857792455]
	TIME [epoch: 1.27 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2141668553671523		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.2141668553671523 | validation: 0.3335672123817689]
	TIME [epoch: 1.27 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23395804986831276		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.23395804986831276 | validation: 0.27388090570754875]
	TIME [epoch: 1.27 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2290102139742845		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.2290102139742845 | validation: 0.2712887031834508]
	TIME [epoch: 1.27 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22055497245648795		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.22055497245648795 | validation: 0.3443322229170997]
	TIME [epoch: 1.27 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26721004485072203		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.26721004485072203 | validation: 0.30414579878657133]
	TIME [epoch: 1.27 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435908533883527		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2435908533883527 | validation: 0.244711233249928]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21184392773269856		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.21184392773269856 | validation: 0.2805368359185543]
	TIME [epoch: 1.27 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20008775780003185		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.20008775780003185 | validation: 0.24695279966677158]
	TIME [epoch: 1.27 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2056586945285834		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.2056586945285834 | validation: 0.27106426385821547]
	TIME [epoch: 1.27 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19660338519880205		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.19660338519880205 | validation: 0.27450739952913156]
	TIME [epoch: 1.27 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20761806667514204		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.20761806667514204 | validation: 0.3628476507326081]
	TIME [epoch: 1.27 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2695667808224513		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2695667808224513 | validation: 0.25525851800306854]
	TIME [epoch: 1.27 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19639786119883887		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.19639786119883887 | validation: 0.2747934300774429]
	TIME [epoch: 1.27 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20446105577130888		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.20446105577130888 | validation: 0.24012248517120494]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20272041367045127		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.20272041367045127 | validation: 0.29004786230754875]
	TIME [epoch: 1.27 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22252441704101367		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.22252441704101367 | validation: 0.30272953200670155]
	TIME [epoch: 1.27 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22553575424056438		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.22553575424056438 | validation: 0.3041776135076356]
	TIME [epoch: 1.27 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2467845092155953		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2467845092155953 | validation: 0.24537635733726734]
	TIME [epoch: 1.27 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21567433019251694		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.21567433019251694 | validation: 0.28574259793000517]
	TIME [epoch: 1.27 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20539507274166327		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.20539507274166327 | validation: 0.25721397167488835]
	TIME [epoch: 1.27 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2315600712898808		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.2315600712898808 | validation: 0.26263054818749715]
	TIME [epoch: 1.27 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018130895304929		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2018130895304929 | validation: 0.23323936838627982]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18557198933393426		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.18557198933393426 | validation: 0.22715313215819888]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18309463548480565		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.18309463548480565 | validation: 0.25728514048629225]
	TIME [epoch: 1.27 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792791411587693		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.1792791411587693 | validation: 0.2182153231564617]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17650721268578268		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.17650721268578268 | validation: 0.2586663417666908]
	TIME [epoch: 1.28 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19086339143530445		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.19086339143530445 | validation: 0.2109469771813308]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19717173875883262		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.19717173875883262 | validation: 0.24495319474035382]
	TIME [epoch: 1.28 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18086715504096604		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.18086715504096604 | validation: 0.21137417197376163]
	TIME [epoch: 1.27 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19506448487722344		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.19506448487722344 | validation: 0.38929426031773473]
	TIME [epoch: 1.27 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2761713492029285		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2761713492029285 | validation: 0.3873222006537986]
	TIME [epoch: 1.28 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35639375660182226		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.35639375660182226 | validation: 0.22503428424111938]
	TIME [epoch: 1.27 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2055231265183085		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.2055231265183085 | validation: 0.27569691798445944]
	TIME [epoch: 1.27 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885599847984115		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.1885599847984115 | validation: 0.26833496032500354]
	TIME [epoch: 1.27 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20205024830539148		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.20205024830539148 | validation: 0.23599083306204813]
	TIME [epoch: 1.27 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19372404527455434		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.19372404527455434 | validation: 0.2428777251771316]
	TIME [epoch: 1.27 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17095601856105944		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.17095601856105944 | validation: 0.20844041479783526]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16480989339149768		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.16480989339149768 | validation: 0.2129746732735882]
	TIME [epoch: 1.28 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1668209466892148		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.1668209466892148 | validation: 0.25273617764095724]
	TIME [epoch: 1.27 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18335489351677292		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.18335489351677292 | validation: 0.285691072824642]
	TIME [epoch: 1.28 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23324166542465374		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.23324166542465374 | validation: 0.2749435129430639]
	TIME [epoch: 1.28 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22943719904151508		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.22943719904151508 | validation: 0.3301999563763403]
	TIME [epoch: 1.27 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24503423660528803		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.24503423660528803 | validation: 0.23500139429341527]
	TIME [epoch: 1.27 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19616577947003686		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.19616577947003686 | validation: 0.2865508425601854]
	TIME [epoch: 1.27 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2056618506804923		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.2056618506804923 | validation: 0.20326380779997688]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15609511957957473		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.15609511957957473 | validation: 0.20165720822821545]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1667910914067063		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.1667910914067063 | validation: 0.2194230876305766]
	TIME [epoch: 1.27 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620416004235284		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1620416004235284 | validation: 0.21778914918871448]
	TIME [epoch: 1.27 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15546265343157775		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.15546265343157775 | validation: 0.2163780301019611]
	TIME [epoch: 1.27 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15611818439011182		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.15611818439011182 | validation: 0.19593665595424561]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15075648892208002		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.15075648892208002 | validation: 0.19048019361472515]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15479640257876034		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.15479640257876034 | validation: 0.18770853554447067]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15624969181949605		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.15624969181949605 | validation: 0.2474801818754603]
	TIME [epoch: 1.27 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17962865822781574		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.17962865822781574 | validation: 0.38469300666095896]
	TIME [epoch: 1.27 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3075921889526554		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.3075921889526554 | validation: 0.26842993405013177]
	TIME [epoch: 1.27 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435967181954027		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2435967181954027 | validation: 0.22082973462568445]
	TIME [epoch: 1.28 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15179453306427598		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.15179453306427598 | validation: 0.1900851012909175]
	TIME [epoch: 1.28 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14945272418806232		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.14945272418806232 | validation: 0.18169848944131303]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15443775588775954		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.15443775588775954 | validation: 0.19807937877997375]
	TIME [epoch: 1.28 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1526215763710346		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.1526215763710346 | validation: 0.19859846012221474]
	TIME [epoch: 1.27 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16057918463808263		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.16057918463808263 | validation: 0.24178777721378078]
	TIME [epoch: 1.28 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19599093027802006		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.19599093027802006 | validation: 0.24946335501188185]
	TIME [epoch: 1.27 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19083581362572571		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.19083581362572571 | validation: 0.2812126450378773]
	TIME [epoch: 1.28 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206886166014354		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.2206886166014354 | validation: 0.2015820987354957]
	TIME [epoch: 1.28 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15965007837479744		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.15965007837479744 | validation: 0.19441423366448704]
	TIME [epoch: 1.28 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15329663895600718		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.15329663895600718 | validation: 0.18893304870479305]
	TIME [epoch: 1.28 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14747853529760155		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.14747853529760155 | validation: 0.1813155292000568]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14274046158338013		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.14274046158338013 | validation: 0.17833697403057]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13773897443925806		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.13773897443925806 | validation: 0.17762446229441434]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122174392468467		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.14122174392468467 | validation: 0.19039016057280497]
	TIME [epoch: 1.28 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145129708180653		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.145129708180653 | validation: 0.22264161684054062]
	TIME [epoch: 1.28 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16986131689910156		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.16986131689910156 | validation: 0.4776014334947637]
	TIME [epoch: 1.28 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4735023905229462		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4735023905229462 | validation: 0.20052773617100175]
	TIME [epoch: 1.28 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13942906906907582		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.13942906906907582 | validation: 0.20879357169485024]
	TIME [epoch: 1.28 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20396867584061817		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.20396867584061817 | validation: 0.20053766646500099]
	TIME [epoch: 1.28 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524098467808298		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.1524098467808298 | validation: 0.20272195518091696]
	TIME [epoch: 1.28 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14005325462563986		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.14005325462563986 | validation: 0.17048970303429561]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14356773789848948		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.14356773789848948 | validation: 0.20003705306171585]
	TIME [epoch: 1.28 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16662353174858063		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.16662353174858063 | validation: 0.25873850266095266]
	TIME [epoch: 1.28 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031594227322168		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2031594227322168 | validation: 0.19970305683182418]
	TIME [epoch: 1.28 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15858578383978134		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.15858578383978134 | validation: 0.1679191615876012]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802409315976225		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.13802409315976225 | validation: 0.19930496486184068]
	TIME [epoch: 1.28 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14481903340497443		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.14481903340497443 | validation: 0.17182463941186585]
	TIME [epoch: 1.28 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13593256520630181		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.13593256520630181 | validation: 0.1636127194580399]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13227523655990578		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.13227523655990578 | validation: 0.17437917770306277]
	TIME [epoch: 1.28 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338393694067248		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.1338393694067248 | validation: 0.1908204307847186]
	TIME [epoch: 1.28 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15320261032292404		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.15320261032292404 | validation: 0.30338277735290675]
	TIME [epoch: 1.27 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23348951740570698		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.23348951740570698 | validation: 0.22417309710163963]
	TIME [epoch: 1.28 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15864090166779313		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.15864090166779313 | validation: 0.16612150944521376]
	TIME [epoch: 1.28 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1361036651767368		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.1361036651767368 | validation: 0.165735440550347]
	TIME [epoch: 1.28 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14048611867683491		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.14048611867683491 | validation: 0.1733365126761265]
	TIME [epoch: 1.28 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14221799495570717		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.14221799495570717 | validation: 0.1683134874860858]
	TIME [epoch: 1.28 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14052546731237442		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.14052546731237442 | validation: 0.19029335634787684]
	TIME [epoch: 1.27 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366868451865449		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1366868451865449 | validation: 0.17583573948816103]
	TIME [epoch: 1.28 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14859094103296927		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.14859094103296927 | validation: 0.2067404217882856]
	TIME [epoch: 1.28 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725847718423616		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.15725847718423616 | validation: 0.23264931894233803]
	TIME [epoch: 1.28 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20062505699514702		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.20062505699514702 | validation: 0.21727424638030418]
	TIME [epoch: 1.28 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1906992545766557		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.1906992545766557 | validation: 0.1657985039328047]
	TIME [epoch: 1.27 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12904722573187877		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.12904722573187877 | validation: 0.17496474256805952]
	TIME [epoch: 1.28 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1235475870739607		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.1235475870739607 | validation: 0.15761231309285667]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1265510841613195		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.1265510841613195 | validation: 0.19142185121883853]
	TIME [epoch: 1.28 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13294935548283193		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.13294935548283193 | validation: 0.16425250099717437]
	TIME [epoch: 1.28 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14245316217243745		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.14245316217243745 | validation: 0.18369023842342522]
	TIME [epoch: 1.28 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12346570193518525		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.12346570193518525 | validation: 0.1870519580860176]
	TIME [epoch: 1.28 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14609294944691467		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.14609294944691467 | validation: 0.2564832834600437]
	TIME [epoch: 1.28 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21947360067265842		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.21947360067265842 | validation: 0.35195603257030417]
	TIME [epoch: 1.28 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4561989847248296		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.4561989847248296 | validation: 0.32579198663683984]
	TIME [epoch: 1.28 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36006630465397776		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.36006630465397776 | validation: 0.2048012774620263]
	TIME [epoch: 1.28 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14510505924640543		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.14510505924640543 | validation: 0.1947039313384724]
	TIME [epoch: 1.28 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18026587092565385		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.18026587092565385 | validation: 0.17275042758734754]
	TIME [epoch: 1.28 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14506061321094374		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.14506061321094374 | validation: 0.18625311148060564]
	TIME [epoch: 1.28 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1291632792625556		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.1291632792625556 | validation: 0.17279089130832048]
	TIME [epoch: 1.28 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234585274118506		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.1234585274118506 | validation: 0.15750631179777141]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12062577064769109		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.12062577064769109 | validation: 0.15202661236587256]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11794759538483701		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.11794759538483701 | validation: 0.16686725275426204]
	TIME [epoch: 291 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12002113161884566		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.12002113161884566 | validation: 0.16201721717725748]
	TIME [epoch: 2.55 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1198619577411019		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1198619577411019 | validation: 0.15743645259357253]
	TIME [epoch: 2.53 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440355184354296		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.12440355184354296 | validation: 0.145904151231513]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11939371462682949		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.11939371462682949 | validation: 0.15171298368295033]
	TIME [epoch: 2.53 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11834168538406008		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.11834168538406008 | validation: 0.1488893023297578]
	TIME [epoch: 2.54 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11769838979716785		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.11769838979716785 | validation: 0.13332419500451698]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581818442077048		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.11581818442077048 | validation: 0.1693378582250781]
	TIME [epoch: 2.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12371780377636833		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.12371780377636833 | validation: 0.20944936922437823]
	TIME [epoch: 2.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16803643197353516		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.16803643197353516 | validation: 0.21104774676043236]
	TIME [epoch: 2.53 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812060992045179		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.1812060992045179 | validation: 0.16250158266505113]
	TIME [epoch: 2.53 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13902160609190145		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.13902160609190145 | validation: 0.14387587125264062]
	TIME [epoch: 2.53 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448350467975588		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.11448350467975588 | validation: 0.13677986955224672]
	TIME [epoch: 2.52 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1106356197904616		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1106356197904616 | validation: 0.14002172573846355]
	TIME [epoch: 2.53 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10941574402974372		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.10941574402974372 | validation: 0.15214284396919148]
	TIME [epoch: 2.53 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11794199921104519		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.11794199921104519 | validation: 0.15840086026868183]
	TIME [epoch: 2.53 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064258181404337		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.13064258181404337 | validation: 0.22626639758705283]
	TIME [epoch: 2.53 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2059328200499373		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.2059328200499373 | validation: 0.13662312791156914]
	TIME [epoch: 2.53 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1142121050777386		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.1142121050777386 | validation: 0.20205412700184128]
	TIME [epoch: 2.52 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13517502628555858		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.13517502628555858 | validation: 0.14993718213524612]
	TIME [epoch: 2.53 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12207092417274108		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.12207092417274108 | validation: 0.15472228710223562]
	TIME [epoch: 2.53 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13911904330599092		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.13911904330599092 | validation: 0.16588939772428593]
	TIME [epoch: 2.53 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12406858640715906		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12406858640715906 | validation: 0.14244823950631208]
	TIME [epoch: 2.52 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11172825811083388		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.11172825811083388 | validation: 0.12723284724975145]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10881097896120155		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.10881097896120155 | validation: 0.13578325638603111]
	TIME [epoch: 2.52 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10826247064946692		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.10826247064946692 | validation: 0.1362020925824378]
	TIME [epoch: 2.53 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002373493616771		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.11002373493616771 | validation: 0.14808501906413057]
	TIME [epoch: 2.53 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11854818868358745		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.11854818868358745 | validation: 0.18993241017565876]
	TIME [epoch: 2.53 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15311592363521637		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.15311592363521637 | validation: 0.192824577064033]
	TIME [epoch: 2.53 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15607709752620066		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.15607709752620066 | validation: 0.15789942728012962]
	TIME [epoch: 2.53 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13387877862147599		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.13387877862147599 | validation: 0.14114674037986785]
	TIME [epoch: 2.53 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12156570457463273		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.12156570457463273 | validation: 0.13774896420603516]
	TIME [epoch: 2.53 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788980458992768		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.10788980458992768 | validation: 0.12748445414591833]
	TIME [epoch: 2.53 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192918187500714		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.10192918187500714 | validation: 0.12184388270691522]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10453000662463705		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.10453000662463705 | validation: 0.1252681282310763]
	TIME [epoch: 2.54 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10580002647728193		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.10580002647728193 | validation: 0.14600082036852804]
	TIME [epoch: 2.54 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11430949465668373		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.11430949465668373 | validation: 0.13706428531776746]
	TIME [epoch: 2.53 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12068284690979575		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.12068284690979575 | validation: 0.15137947145183794]
	TIME [epoch: 2.53 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12810247865349506		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.12810247865349506 | validation: 0.21911843068769193]
	TIME [epoch: 2.53 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.204851972909065		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.204851972909065 | validation: 0.15698009959451736]
	TIME [epoch: 2.54 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13947376416451274		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.13947376416451274 | validation: 0.18308474118277698]
	TIME [epoch: 2.53 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402003186035672		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.1402003186035672 | validation: 0.13461385787220712]
	TIME [epoch: 2.54 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976612152131846		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.09976612152131846 | validation: 0.11560887114444213]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515541533275256		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.10515541533275256 | validation: 0.12941706278676587]
	TIME [epoch: 2.52 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09465252622248854		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.09465252622248854 | validation: 0.13863217153228377]
	TIME [epoch: 2.52 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10315081837602401		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.10315081837602401 | validation: 0.10909433427053244]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09809909421120291		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.09809909421120291 | validation: 0.12354252239827451]
	TIME [epoch: 2.51 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10423557882878758		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.10423557882878758 | validation: 0.12914584479492172]
	TIME [epoch: 2.51 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10593837923387828		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.10593837923387828 | validation: 0.16297112417453455]
	TIME [epoch: 2.51 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1136769186265452		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.1136769186265452 | validation: 0.12172621990582011]
	TIME [epoch: 2.51 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1200533070799567		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.1200533070799567 | validation: 0.14605091771956827]
	TIME [epoch: 2.51 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11804791627677524		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.11804791627677524 | validation: 0.1405216956626167]
	TIME [epoch: 2.51 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.107967797056598		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.107967797056598 | validation: 0.13685267534808732]
	TIME [epoch: 2.51 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11458196318518457		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.11458196318518457 | validation: 0.14657828338906928]
	TIME [epoch: 2.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.116745297290624		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.116745297290624 | validation: 0.16521334972591994]
	TIME [epoch: 2.51 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12040638275722028		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.12040638275722028 | validation: 0.12420025249480587]
	TIME [epoch: 2.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1061410447830555		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.1061410447830555 | validation: 0.11831618983712906]
	TIME [epoch: 2.51 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102062916379531		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.10102062916379531 | validation: 0.11808856218728554]
	TIME [epoch: 2.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750147368217235		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.09750147368217235 | validation: 0.11106790436907553]
	TIME [epoch: 2.51 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254673698646766		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.10254673698646766 | validation: 0.14808618049785488]
	TIME [epoch: 2.51 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10575084462238465		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.10575084462238465 | validation: 0.11791114158164838]
	TIME [epoch: 2.51 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10957373245683252		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.10957373245683252 | validation: 0.14406965633433522]
	TIME [epoch: 2.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231492662601223		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1231492662601223 | validation: 0.1509755351090336]
	TIME [epoch: 2.51 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302117188240726		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1302117188240726 | validation: 0.12963697286394069]
	TIME [epoch: 2.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11070684479086093		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.11070684479086093 | validation: 0.13313962214532676]
	TIME [epoch: 2.51 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09745833251767926		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.09745833251767926 | validation: 0.1120619920914494]
	TIME [epoch: 2.51 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09088448468222807		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.09088448468222807 | validation: 0.10712678219617229]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833192939903127		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.08833192939903127 | validation: 0.10758668923704354]
	TIME [epoch: 2.53 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08854516168509326		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.08854516168509326 | validation: 0.10542361275114882]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894905701289954		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.0894905701289954 | validation: 0.10023517833003336]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09184120108053627		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.09184120108053627 | validation: 0.13156476019944965]
	TIME [epoch: 2.51 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10228196160545906		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.10228196160545906 | validation: 0.13825820428233204]
	TIME [epoch: 2.52 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1216744340474384		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.1216744340474384 | validation: 0.17543623775042275]
	TIME [epoch: 2.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659123854934828		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.1659123854934828 | validation: 0.12812943048329695]
	TIME [epoch: 2.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11462051042031376		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.11462051042031376 | validation: 0.11195935559441722]
	TIME [epoch: 2.51 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08742359239920104		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.08742359239920104 | validation: 0.10254023047905081]
	TIME [epoch: 2.51 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08740407547550634		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.08740407547550634 | validation: 0.10369314863827124]
	TIME [epoch: 2.51 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890607300430819		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.0890607300430819 | validation: 0.11831165956240353]
	TIME [epoch: 2.51 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08980938440901154		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.08980938440901154 | validation: 0.09833744132329869]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294161255962351		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.09294161255962351 | validation: 0.13182951456607786]
	TIME [epoch: 2.52 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657290317316576		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.09657290317316576 | validation: 0.10845273771292603]
	TIME [epoch: 2.52 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09426259574597796		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.09426259574597796 | validation: 0.11529684421562905]
	TIME [epoch: 2.51 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10266562109511755		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.10266562109511755 | validation: 0.1587202258713205]
	TIME [epoch: 2.52 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379249300814293		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1379249300814293 | validation: 0.12038968078628531]
	TIME [epoch: 2.51 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11570617107795168		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.11570617107795168 | validation: 0.12078522977949846]
	TIME [epoch: 2.52 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144967946162755		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.11144967946162755 | validation: 0.12868641808669232]
	TIME [epoch: 2.51 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029670186600033		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.10029670186600033 | validation: 0.09498237093200679]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883328226834741		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.0883328226834741 | validation: 0.1235562892204297]
	TIME [epoch: 2.51 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826098638169816		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.08826098638169816 | validation: 0.0981552509654236]
	TIME [epoch: 2.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671986822268178		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.09671986822268178 | validation: 0.11825892726205806]
	TIME [epoch: 2.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10157853744178245		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.10157853744178245 | validation: 0.10173657063145436]
	TIME [epoch: 2.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08854526247185579		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.08854526247185579 | validation: 0.10030584787137849]
	TIME [epoch: 2.51 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08528036830236581		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.08528036830236581 | validation: 0.09312589050984971]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08428224778398319		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.08428224778398319 | validation: 0.11005912380945415]
	TIME [epoch: 2.51 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785876538386882		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.08785876538386882 | validation: 0.09852446803240784]
	TIME [epoch: 2.53 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0875578299955393		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.0875578299955393 | validation: 0.1014241874674958]
	TIME [epoch: 2.51 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08379320484328638		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.08379320484328638 | validation: 0.08652350218493046]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980319948740541		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.07980319948740541 | validation: 0.09619866465148064]
	TIME [epoch: 2.51 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07880040434890144		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.07880040434890144 | validation: 0.09103782636908715]
	TIME [epoch: 2.51 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08249956264594849		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.08249956264594849 | validation: 0.1023581195784043]
	TIME [epoch: 2.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151552763288962		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.09151552763288962 | validation: 0.1573462560496222]
	TIME [epoch: 2.51 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270984295870736		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.1270984295870736 | validation: 0.16704741787207028]
	TIME [epoch: 2.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14634961475973426		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.14634961475973426 | validation: 0.12183254609599173]
	TIME [epoch: 2.51 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1124752177440326		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1124752177440326 | validation: 0.13211354618884585]
	TIME [epoch: 2.51 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056247999456691		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.1056247999456691 | validation: 0.0980361757570464]
	TIME [epoch: 2.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744703694360294		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08744703694360294 | validation: 0.08571283326531001]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08172379378309003		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.08172379378309003 | validation: 0.09958531323138749]
	TIME [epoch: 2.51 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08208763159263631		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.08208763159263631 | validation: 0.09507833337642067]
	TIME [epoch: 2.51 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995068001479803		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.07995068001479803 | validation: 0.08758421412516221]
	TIME [epoch: 2.51 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543460551803202		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.07543460551803202 | validation: 0.12371766540364917]
	TIME [epoch: 2.51 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608239956345483		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.08608239956345483 | validation: 0.09745688158839932]
	TIME [epoch: 2.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768830342022529		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.09768830342022529 | validation: 0.08662965320142707]
	TIME [epoch: 2.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789467693347139		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.0789467693347139 | validation: 0.13476599677771964]
	TIME [epoch: 2.51 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08702124127492444		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.08702124127492444 | validation: 0.0846671780860145]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744107273494095		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.08744107273494095 | validation: 0.10020753541678605]
	TIME [epoch: 2.51 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09012066608493491		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.09012066608493491 | validation: 0.13002491650147746]
	TIME [epoch: 2.51 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167207764601869		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.11167207764601869 | validation: 0.13349100417210205]
	TIME [epoch: 2.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11872652985979809		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11872652985979809 | validation: 0.0810807021732223]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08366377717513009		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.08366377717513009 | validation: 0.09086288102780815]
	TIME [epoch: 2.51 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07672009186514449		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.07672009186514449 | validation: 0.11183876253305239]
	TIME [epoch: 2.51 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07864904858961183		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.07864904858961183 | validation: 0.08512443530825262]
	TIME [epoch: 2.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188235554863026		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.08188235554863026 | validation: 0.11119854336364457]
	TIME [epoch: 2.51 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806663554280451		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.0806663554280451 | validation: 0.09001732530697026]
	TIME [epoch: 2.51 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801160713957107		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.0801160713957107 | validation: 0.09555505474416982]
	TIME [epoch: 2.51 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849816724359203		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.0849816724359203 | validation: 0.09270190284326299]
	TIME [epoch: 2.51 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092621187316958		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.08092621187316958 | validation: 0.0974170145590744]
	TIME [epoch: 2.51 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592592899504457		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.08592592899504457 | validation: 0.09734213990314096]
	TIME [epoch: 2.51 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955583366116578		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.08955583366116578 | validation: 0.09736559102098873]
	TIME [epoch: 2.51 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08706619999153699		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.08706619999153699 | validation: 0.09675038029527383]
	TIME [epoch: 2.51 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144234298580445		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.08144234298580445 | validation: 0.09643219429492661]
	TIME [epoch: 2.52 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08374923562811716		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.08374923562811716 | validation: 0.0868171308552286]
	TIME [epoch: 2.51 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203044345375336		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.08203044345375336 | validation: 0.09724411108830978]
	TIME [epoch: 2.51 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08090313511404365		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.08090313511404365 | validation: 0.08101601781877302]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0770865443659915		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.0770865443659915 | validation: 0.08801691529947134]
	TIME [epoch: 2.51 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779910871160638		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.07779910871160638 | validation: 0.08492349074419225]
	TIME [epoch: 2.51 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791583917531271		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.0791583917531271 | validation: 0.08976398613392783]
	TIME [epoch: 2.52 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531848764874477		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.07531848764874477 | validation: 0.08580071898642368]
	TIME [epoch: 2.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0773230185844301		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.0773230185844301 | validation: 0.08468476921465645]
	TIME [epoch: 2.51 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764872486919565		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.0764872486919565 | validation: 0.10339823806650757]
	TIME [epoch: 2.52 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0823055970556258		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.0823055970556258 | validation: 0.10117012858413058]
	TIME [epoch: 2.51 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031314573434781		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.10031314573434781 | validation: 0.09464876557479854]
	TIME [epoch: 2.51 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788669740717232		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08788669740717232 | validation: 0.10057205281596847]
	TIME [epoch: 2.51 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0814160534008644		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.0814160534008644 | validation: 0.07547201564708186]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266990646440151		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07266990646440151 | validation: 0.07942404559903077]
	TIME [epoch: 2.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751014365149997		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.0751014365149997 | validation: 0.08940219999347024]
	TIME [epoch: 2.53 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07303219425195315		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.07303219425195315 | validation: 0.08139422745534504]
	TIME [epoch: 2.53 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07081621927598414		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.07081621927598414 | validation: 0.07584784381811355]
	TIME [epoch: 2.53 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06844819831682987		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06844819831682987 | validation: 0.09134809984709916]
	TIME [epoch: 2.53 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07176989768596871		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.07176989768596871 | validation: 0.08260463564042411]
	TIME [epoch: 2.53 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08025977349819793		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.08025977349819793 | validation: 0.09505058451852588]
	TIME [epoch: 2.53 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543029403321386		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.07543029403321386 | validation: 0.07300059145147912]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833478084727293		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06833478084727293 | validation: 0.07541424866823238]
	TIME [epoch: 2.51 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016902956274156		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.07016902956274156 | validation: 0.11602939806646125]
	TIME [epoch: 2.51 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09943528550446341		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.09943528550446341 | validation: 0.12395996049110158]
	TIME [epoch: 2.51 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1277860548361429		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.1277860548361429 | validation: 0.08685307475872049]
	TIME [epoch: 2.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050211824376458		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.09050211824376458 | validation: 0.088285048700252]
	TIME [epoch: 2.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07465224054839016		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.07465224054839016 | validation: 0.07697488782417945]
	TIME [epoch: 2.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177395990025198		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07177395990025198 | validation: 0.07943831063789827]
	TIME [epoch: 2.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508391437961458		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.07508391437961458 | validation: 0.1001527551096314]
	TIME [epoch: 2.51 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0757817932341897		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.0757817932341897 | validation: 0.07212697464436715]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06656362227632483		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.06656362227632483 | validation: 0.07507963774336285]
	TIME [epoch: 2.52 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700578918476337		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.06700578918476337 | validation: 0.08411178889172032]
	TIME [epoch: 2.53 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685264053467994		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06685264053467994 | validation: 0.06853482206950516]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06482647645566418		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06482647645566418 | validation: 0.07734251484327476]
	TIME [epoch: 2.53 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483306437656777		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06483306437656777 | validation: 0.07367460583663192]
	TIME [epoch: 2.53 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689448022495982		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06689448022495982 | validation: 0.07027481367328862]
	TIME [epoch: 2.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06828680637615142		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.06828680637615142 | validation: 0.0824047723028325]
	TIME [epoch: 2.53 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07156974321090742		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.07156974321090742 | validation: 0.10354704035011238]
	TIME [epoch: 2.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970677860123622		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.0970677860123622 | validation: 0.11471812689152719]
	TIME [epoch: 2.53 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11626423544160755		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.11626423544160755 | validation: 0.0906583801086261]
	TIME [epoch: 2.53 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468854119800674		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.08468854119800674 | validation: 0.06461902414893238]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261756128811595		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06261756128811595 | validation: 0.07859750200783236]
	TIME [epoch: 2.53 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705482940236207		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.06705482940236207 | validation: 0.07766874517930009]
	TIME [epoch: 2.52 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06586138323932726		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.06586138323932726 | validation: 0.06635905433359016]
	TIME [epoch: 2.52 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495129637708942		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.06495129637708942 | validation: 0.09068160859345402]
	TIME [epoch: 2.52 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07379876471033849		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.07379876471033849 | validation: 0.0671427896178252]
	TIME [epoch: 2.52 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06849536684903988		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06849536684903988 | validation: 0.06941344710277579]
	TIME [epoch: 2.53 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243738625665619		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.06243738625665619 | validation: 0.06777159077047466]
	TIME [epoch: 2.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0629755229543487		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.0629755229543487 | validation: 0.06666188584768888]
	TIME [epoch: 2.53 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383216988527245		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06383216988527245 | validation: 0.06556407615340222]
	TIME [epoch: 2.52 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06305015590842596		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06305015590842596 | validation: 0.06697506756575071]
	TIME [epoch: 2.53 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06127771073465436		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06127771073465436 | validation: 0.0705082659526598]
	TIME [epoch: 2.52 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061174091663139515		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.061174091663139515 | validation: 0.06738013268963637]
	TIME [epoch: 2.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564829900872927		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06564829900872927 | validation: 0.07319777626583206]
	TIME [epoch: 2.52 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063425637450393		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07063425637450393 | validation: 0.06666926150570777]
	TIME [epoch: 2.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145162509647171		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07145162509647171 | validation: 0.10017873793273113]
	TIME [epoch: 2.52 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08548284560852278		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.08548284560852278 | validation: 0.10325858945240557]
	TIME [epoch: 2.52 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09361486576925195		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.09361486576925195 | validation: 0.0960827851228287]
	TIME [epoch: 2.52 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590722591900658		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.09590722591900658 | validation: 0.07562058420103684]
	TIME [epoch: 2.52 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06692431485648831		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06692431485648831 | validation: 0.0708638341829644]
	TIME [epoch: 2.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674730593534353		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.0674730593534353 | validation: 0.06683827538886145]
	TIME [epoch: 2.52 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06562919718697378		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06562919718697378 | validation: 0.06830069285232819]
	TIME [epoch: 2.52 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06543708788818661		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06543708788818661 | validation: 0.06919944872893533]
	TIME [epoch: 2.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060298781508564804		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.060298781508564804 | validation: 0.06050962580266966]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05938742141445626		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.05938742141445626 | validation: 0.06310210767164619]
	TIME [epoch: 2.53 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057036253465551426		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.057036253465551426 | validation: 0.06875875536228464]
	TIME [epoch: 2.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058085133517001265		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.058085133517001265 | validation: 0.061308169986588015]
	TIME [epoch: 2.52 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06179910258603153		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06179910258603153 | validation: 0.06580296251138085]
	TIME [epoch: 2.52 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06563129301499412		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.06563129301499412 | validation: 0.08839204363489306]
	TIME [epoch: 2.52 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07296218073179611		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.07296218073179611 | validation: 0.07358989567506938]
	TIME [epoch: 2.53 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014061589004001		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.08014061589004001 | validation: 0.08161589490945567]
	TIME [epoch: 2.52 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07791740774953097		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.07791740774953097 | validation: 0.0722312729914624]
	TIME [epoch: 2.52 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690938935361654		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.0690938935361654 | validation: 0.06335029861636424]
	TIME [epoch: 2.54 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056884716573720386		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.056884716573720386 | validation: 0.05485383658313139]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05695997510819803		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.05695997510819803 | validation: 0.0664951671872773]
	TIME [epoch: 2.52 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05698354202057415		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05698354202057415 | validation: 0.06758477400725148]
	TIME [epoch: 2.51 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06580057478801114		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06580057478801114 | validation: 0.08321851957543679]
	TIME [epoch: 2.52 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148407815182249		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.07148407815182249 | validation: 0.08206070066621407]
	TIME [epoch: 2.52 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703813739063419		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.0703813739063419 | validation: 0.061638016075736546]
	TIME [epoch: 2.52 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06207100727129001		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06207100727129001 | validation: 0.06769189079455727]
	TIME [epoch: 2.52 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0629547828360777		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.0629547828360777 | validation: 0.06484104975753668]
	TIME [epoch: 2.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901909024884562		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05901909024884562 | validation: 0.06113783029812661]
	TIME [epoch: 2.51 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567947211953473		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.0567947211953473 | validation: 0.05633221796610569]
	TIME [epoch: 2.52 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057481763167079016		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.057481763167079016 | validation: 0.067275488486582]
	TIME [epoch: 2.51 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05906211841067403		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.05906211841067403 | validation: 0.057657461364888196]
	TIME [epoch: 2.51 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05492596115740324		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05492596115740324 | validation: 0.059821690855153924]
	TIME [epoch: 2.52 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056560986276053826		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.056560986276053826 | validation: 0.07741568073391243]
	TIME [epoch: 2.52 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060523453793219474		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.060523453793219474 | validation: 0.052104805569496565]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732775225728494		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.05732775225728494 | validation: 0.0590161335294088]
	TIME [epoch: 2.53 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055387252382313366		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.055387252382313366 | validation: 0.07474321398052788]
	TIME [epoch: 2.53 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06620477752778983		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06620477752778983 | validation: 0.0796644397368543]
	TIME [epoch: 2.53 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872946987546253		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.07872946987546253 | validation: 0.08829160147474785]
	TIME [epoch: 2.53 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09624445782975162		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.09624445782975162 | validation: 0.07840135748596443]
	TIME [epoch: 2.53 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980621405320891		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06980621405320891 | validation: 0.05646797207711605]
	TIME [epoch: 2.53 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05478846154176039		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.05478846154176039 | validation: 0.05008626902727378]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055247621903106316		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.055247621903106316 | validation: 0.06422593721913009]
	TIME [epoch: 2.55 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05923980556248767		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.05923980556248767 | validation: 0.05899396427939444]
	TIME [epoch: 2.53 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739672215346601		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.05739672215346601 | validation: 0.057175887965510944]
	TIME [epoch: 2.53 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0587841152473263		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.0587841152473263 | validation: 0.06338346380020358]
	TIME [epoch: 2.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704256565742522		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.05704256565742522 | validation: 0.060914382772753545]
	TIME [epoch: 2.53 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05786578402024661		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.05786578402024661 | validation: 0.052572203078676764]
	TIME [epoch: 2.52 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529434211282332		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05529434211282332 | validation: 0.061949987516843046]
	TIME [epoch: 2.54 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567190402154654		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.05567190402154654 | validation: 0.05552681619895159]
	TIME [epoch: 2.53 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053224495959007745		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.053224495959007745 | validation: 0.05764280216256117]
	TIME [epoch: 2.53 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053607576762549076		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.053607576762549076 | validation: 0.06032703969025158]
	TIME [epoch: 2.53 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05580147175295718		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.05580147175295718 | validation: 0.06378617105303447]
	TIME [epoch: 2.53 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060059136914677395		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.060059136914677395 | validation: 0.07633664461569467]
	TIME [epoch: 2.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06344820921735753		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.06344820921735753 | validation: 0.055211530700025095]
	TIME [epoch: 2.53 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076899544469949		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.06076899544469949 | validation: 0.06762022742103409]
	TIME [epoch: 2.53 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06380182749952822		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06380182749952822 | validation: 0.06734632169619839]
	TIME [epoch: 2.53 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703076599601471		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.06703076599601471 | validation: 0.06138459826400971]
	TIME [epoch: 2.56 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067040367111992		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.06067040367111992 | validation: 0.05397497764260626]
	TIME [epoch: 2.53 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05553722402315686		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.05553722402315686 | validation: 0.05112835168277372]
	TIME [epoch: 2.52 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357076843450373		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.05357076843450373 | validation: 0.05365791189586369]
	TIME [epoch: 2.53 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05019338836249826		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05019338836249826 | validation: 0.06683856811319595]
	TIME [epoch: 2.53 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05983558185549307		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05983558185549307 | validation: 0.0611323264784927]
	TIME [epoch: 2.53 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272725953463222		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06272725953463222 | validation: 0.05380047385685487]
	TIME [epoch: 2.53 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392501207654099		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05392501207654099 | validation: 0.057642781183493044]
	TIME [epoch: 2.53 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05448328131776537		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.05448328131776537 | validation: 0.053422938668144584]
	TIME [epoch: 2.52 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057380901058635844		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.057380901058635844 | validation: 0.06359877989448291]
	TIME [epoch: 2.53 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05742105273590238		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05742105273590238 | validation: 0.062321096153444624]
	TIME [epoch: 2.53 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06166582805520739		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06166582805520739 | validation: 0.06088832658168747]
	TIME [epoch: 2.53 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057648038526918785		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.057648038526918785 | validation: 0.05807439651586197]
	TIME [epoch: 2.53 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051188772205701935		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.051188772205701935 | validation: 0.054526553219414624]
	TIME [epoch: 2.53 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05299375715276982		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.05299375715276982 | validation: 0.04804287408146581]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05204948323126672		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.05204948323126672 | validation: 0.054264202043351584]
	TIME [epoch: 2.53 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496315040559421		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.0496315040559421 | validation: 0.045339916551922654]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495419689293485		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.0495419689293485 | validation: 0.055047299370632886]
	TIME [epoch: 2.53 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049137442952133806		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.049137442952133806 | validation: 0.04693624181934125]
	TIME [epoch: 2.53 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049493788376808436		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.049493788376808436 | validation: 0.05817180855759996]
	TIME [epoch: 2.53 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303484525722878		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05303484525722878 | validation: 0.051270125741204486]
	TIME [epoch: 2.53 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05451127748467556		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05451127748467556 | validation: 0.05489261383590736]
	TIME [epoch: 2.53 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0564412941779959		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.0564412941779959 | validation: 0.06967356820894162]
	TIME [epoch: 2.53 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871157156529481		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.06871157156529481 | validation: 0.07214726883954753]
	TIME [epoch: 2.53 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.077806913653227		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.077806913653227 | validation: 0.059760235661867056]
	TIME [epoch: 2.52 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0596098342894957		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.0596098342894957 | validation: 0.05268298864637869]
	TIME [epoch: 2.53 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887144698738238		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.04887144698738238 | validation: 0.049761981575507985]
	TIME [epoch: 2.53 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051856179836851306		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.051856179836851306 | validation: 0.056944188760110676]
	TIME [epoch: 2.52 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030625623693258		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05030625623693258 | validation: 0.04674171643579395]
	TIME [epoch: 2.53 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0494617946195352		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0494617946195352 | validation: 0.047812611763128135]
	TIME [epoch: 2.53 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047510085420896464		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.047510085420896464 | validation: 0.04915304983433469]
	TIME [epoch: 2.53 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674673451750347		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04674673451750347 | validation: 0.05199537083970116]
	TIME [epoch: 2.53 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05285797365457017		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.05285797365457017 | validation: 0.059154603366496]
	TIME [epoch: 2.53 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058774918970288116		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.058774918970288116 | validation: 0.06326328211402239]
	TIME [epoch: 2.52 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521030837417238		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.06521030837417238 | validation: 0.05099054676598689]
	TIME [epoch: 2.53 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292942322005361		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.05292942322005361 | validation: 0.05171376943888204]
	TIME [epoch: 2.53 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048092024221713284		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.048092024221713284 | validation: 0.05121276193261786]
	TIME [epoch: 2.52 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04927940557956584		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04927940557956584 | validation: 0.044284705766060015]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04807688631045795		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.04807688631045795 | validation: 0.04769334251513448]
	TIME [epoch: 2.53 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05173488392813965		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.05173488392813965 | validation: 0.05563126273171706]
	TIME [epoch: 2.53 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054897346645804646		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.054897346645804646 | validation: 0.05918800671541258]
	TIME [epoch: 2.53 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055837977674777405		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.055837977674777405 | validation: 0.04788178975132578]
	TIME [epoch: 2.52 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0514425979387362		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.0514425979387362 | validation: 0.053987909921723067]
	TIME [epoch: 2.53 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051231570778997516		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.051231570778997516 | validation: 0.04176981377581516]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046707654760044605		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.046707654760044605 | validation: 0.04460475151234523]
	TIME [epoch: 2.53 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628017186327494		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.04628017186327494 | validation: 0.0456342085966433]
	TIME [epoch: 2.53 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045780392099841125		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.045780392099841125 | validation: 0.04359280502360049]
	TIME [epoch: 2.53 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04737190344407871		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.04737190344407871 | validation: 0.053511256916841554]
	TIME [epoch: 2.53 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04690051133687753		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.04690051133687753 | validation: 0.037572360472709364]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046961868471029455		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.046961868471029455 | validation: 0.053685468533101126]
	TIME [epoch: 2.53 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047546518019962306		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.047546518019962306 | validation: 0.05320817635129285]
	TIME [epoch: 2.53 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056620119244448354		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.056620119244448354 | validation: 0.07271171325549956]
	TIME [epoch: 2.53 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791151158392988		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.0791151158392988 | validation: 0.054832761366836104]
	TIME [epoch: 2.53 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057986059311288446		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.057986059311288446 | validation: 0.04838432818131456]
	TIME [epoch: 2.53 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692128057063278		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04692128057063278 | validation: 0.050134340356873364]
	TIME [epoch: 2.53 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051984161159923443		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.051984161159923443 | validation: 0.04642715177830834]
	TIME [epoch: 2.53 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507261948462299		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.0507261948462299 | validation: 0.05142205454029896]
	TIME [epoch: 2.53 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768842923794976		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.04768842923794976 | validation: 0.0435114977387907]
	TIME [epoch: 2.53 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046135189935675475		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.046135189935675475 | validation: 0.042392729378114224]
	TIME [epoch: 2.52 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045930010344761324		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.045930010344761324 | validation: 0.04918066508862487]
	TIME [epoch: 2.53 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04960082691861006		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.04960082691861006 | validation: 0.04656083734045164]
	TIME [epoch: 2.54 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0480065185317128		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.0480065185317128 | validation: 0.04495331615354383]
	TIME [epoch: 2.53 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735088341469629		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.04735088341469629 | validation: 0.06220402103015983]
	TIME [epoch: 2.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425345299667627		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.05425345299667627 | validation: 0.05570051041438047]
	TIME [epoch: 2.53 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486585768149716		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.05486585768149716 | validation: 0.05269870699721022]
	TIME [epoch: 2.53 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05060900083826901		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.05060900083826901 | validation: 0.04800809941179787]
	TIME [epoch: 2.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049633798790863166		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.049633798790863166 | validation: 0.03883899793004271]
	TIME [epoch: 2.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046707485230432964		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.046707485230432964 | validation: 0.04537469283610401]
	TIME [epoch: 2.53 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642951123390727		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.04642951123390727 | validation: 0.04793799359126703]
	TIME [epoch: 2.53 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045825339334370004		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.045825339334370004 | validation: 0.039207615195709346]
	TIME [epoch: 2.53 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045833948155757094		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.045833948155757094 | validation: 0.045298816222050564]
	TIME [epoch: 2.53 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422947149053751		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.0422947149053751 | validation: 0.04315861832959905]
	TIME [epoch: 2.53 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04273418671369272		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04273418671369272 | validation: 0.04225041640857826]
	TIME [epoch: 2.52 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045405718729648686		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.045405718729648686 | validation: 0.055413123180621526]
	TIME [epoch: 2.52 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497421219834123		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.0497421219834123 | validation: 0.058723418392480055]
	TIME [epoch: 2.53 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05699086221997506		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05699086221997506 | validation: 0.04721801874319695]
	TIME [epoch: 2.53 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011900662524193		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.06011900662524193 | validation: 0.0627544573492919]
	TIME [epoch: 2.53 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05409237813427886		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.05409237813427886 | validation: 0.04540741160303373]
	TIME [epoch: 2.53 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045910389034875286		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.045910389034875286 | validation: 0.04166780378128559]
	TIME [epoch: 2.52 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575248031054282		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.04575248031054282 | validation: 0.05049557412407364]
	TIME [epoch: 2.53 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905493004307736		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.04905493004307736 | validation: 0.036103697556317184]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440519498647965		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0440519498647965 | validation: 0.042713782265682135]
	TIME [epoch: 2.53 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04081443804561472		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.04081443804561472 | validation: 0.03887288484734671]
	TIME [epoch: 2.53 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04113090612754721		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.04113090612754721 | validation: 0.041354701279687295]
	TIME [epoch: 2.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042991580038036846		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.042991580038036846 | validation: 0.039428638708885216]
	TIME [epoch: 2.52 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233310944455231		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.04233310944455231 | validation: 0.044603686715986676]
	TIME [epoch: 2.53 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04229868733098389		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.04229868733098389 | validation: 0.04427151314040103]
	TIME [epoch: 2.53 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616703500872715		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04616703500872715 | validation: 0.05068300229524317]
	TIME [epoch: 2.53 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663704274989621		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05663704274989621 | validation: 0.0579278724105605]
	TIME [epoch: 2.53 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050283986314422845		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.050283986314422845 | validation: 0.03853207818772156]
	TIME [epoch: 2.53 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04975239528720615		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.04975239528720615 | validation: 0.04425906095169095]
	TIME [epoch: 2.53 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046694904730382486		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.046694904730382486 | validation: 0.04557901986290223]
	TIME [epoch: 2.53 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04531577507614109		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.04531577507614109 | validation: 0.0418675984271197]
	TIME [epoch: 2.53 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04402783712981436		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04402783712981436 | validation: 0.04137305060274252]
	TIME [epoch: 2.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041868707339876694		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.041868707339876694 | validation: 0.04882450190138002]
	TIME [epoch: 2.53 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534182952960329		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.04534182952960329 | validation: 0.046439148484136245]
	TIME [epoch: 2.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047784358713645364		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.047784358713645364 | validation: 0.039490141305041616]
	TIME [epoch: 2.53 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045818799133621815		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.045818799133621815 | validation: 0.051706140379730474]
	TIME [epoch: 2.52 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05204105595082861		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.05204105595082861 | validation: 0.04380550606607277]
	TIME [epoch: 2.52 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0490616349632931		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.0490616349632931 | validation: 0.041108340336059986]
	TIME [epoch: 2.53 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04167708639378862		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.04167708639378862 | validation: 0.04337603857445533]
	TIME [epoch: 2.53 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04170830264029318		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04170830264029318 | validation: 0.03635110519718978]
	TIME [epoch: 2.53 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041885794498996834		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.041885794498996834 | validation: 0.04460294049593499]
	TIME [epoch: 2.53 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054079313766033		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.04054079313766033 | validation: 0.0384515055996619]
	TIME [epoch: 2.52 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202532101558568		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.04202532101558568 | validation: 0.039362294388782644]
	TIME [epoch: 2.53 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042928206171217		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.042928206171217 | validation: 0.0475097016404301]
	TIME [epoch: 2.53 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115271266800329		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.04115271266800329 | validation: 0.03885667841728871]
	TIME [epoch: 2.53 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0408623352942066		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.0408623352942066 | validation: 0.037797107610514726]
	TIME [epoch: 2.53 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041033699544448514		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.041033699544448514 | validation: 0.04359966866199033]
	TIME [epoch: 2.53 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043786739111230086		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.043786739111230086 | validation: 0.04087457039474706]
	TIME [epoch: 2.53 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04515785508155287		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.04515785508155287 | validation: 0.05199418023199407]
	TIME [epoch: 2.53 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04990147424681069		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04990147424681069 | validation: 0.043180537147601006]
	TIME [epoch: 2.53 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05323638636734691		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.05323638636734691 | validation: 0.05004847924070413]
	TIME [epoch: 2.53 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04850850505891483		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.04850850505891483 | validation: 0.04446783683399621]
	TIME [epoch: 2.53 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04250405176615745		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.04250405176615745 | validation: 0.04019427244906473]
	TIME [epoch: 2.53 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03908168335952072		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.03908168335952072 | validation: 0.03575261583017048]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384861402214004		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.0384861402214004 | validation: 0.03917763211932705]
	TIME [epoch: 2.53 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03875934682205382		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03875934682205382 | validation: 0.0431102353573252]
	TIME [epoch: 2.53 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04112201317472737		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.04112201317472737 | validation: 0.03770994134746612]
	TIME [epoch: 2.55 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04009607942662962		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.04009607942662962 | validation: 0.04044483093600713]
	TIME [epoch: 2.53 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0402428171870908		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.0402428171870908 | validation: 0.03926762605075784]
	TIME [epoch: 2.52 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148023230624867		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.04148023230624867 | validation: 0.04591438371636529]
	TIME [epoch: 2.53 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04761069049557153		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.04761069049557153 | validation: 0.05631697834679738]
	TIME [epoch: 2.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04846241470200685		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.04846241470200685 | validation: 0.036987595072996815]
	TIME [epoch: 2.52 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04456029990800843		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.04456029990800843 | validation: 0.04312927661368845]
	TIME [epoch: 2.53 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475172972771312		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.04475172972771312 | validation: 0.043519913808533706]
	TIME [epoch: 2.53 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04174503217762494		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.04174503217762494 | validation: 0.03794333031572329]
	TIME [epoch: 2.53 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045489824543668116		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.045489824543668116 | validation: 0.039602076885327304]
	TIME [epoch: 2.53 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04090483348006841		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.04090483348006841 | validation: 0.036904417300268576]
	TIME [epoch: 2.53 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041572304824259804		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.041572304824259804 | validation: 0.04083475130664527]
	TIME [epoch: 2.53 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179911014724202		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04179911014724202 | validation: 0.03563557540139371]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0376214711288121		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0376214711288121 | validation: 0.039210307011522644]
	TIME [epoch: 2.53 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038046538275792574		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.038046538275792574 | validation: 0.03488422359067564]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041096025863210726		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.041096025863210726 | validation: 0.037879776846055695]
	TIME [epoch: 2.53 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04078896130884809		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.04078896130884809 | validation: 0.036958281506588135]
	TIME [epoch: 2.53 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04194174848302705		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.04194174848302705 | validation: 0.03478979293025022]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_876.pth
	Model improved!!!
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04297180907783887		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.04297180907783887 | validation: 0.044431368938768166]
	TIME [epoch: 2.53 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04176453648391245		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.04176453648391245 | validation: 0.03701561493502403]
	TIME [epoch: 2.53 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389985082414474		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.0389985082414474 | validation: 0.03602969579264123]
	TIME [epoch: 2.52 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038207981774931114		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.038207981774931114 | validation: 0.03538508655036695]
	TIME [epoch: 2.53 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03848894100567321		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.03848894100567321 | validation: 0.03442684357613664]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039478613728056645		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.039478613728056645 | validation: 0.04197088209194495]
	TIME [epoch: 2.53 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043136429847223205		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.043136429847223205 | validation: 0.047991674455101774]
	TIME [epoch: 2.52 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044034160625354625		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.044034160625354625 | validation: 0.035585047495103626]
	TIME [epoch: 2.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04400446493236453		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.04400446493236453 | validation: 0.0448060870137685]
	TIME [epoch: 2.53 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04116653365195944		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.04116653365195944 | validation: 0.035015532293076045]
	TIME [epoch: 2.52 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993778054059608		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.03993778054059608 | validation: 0.037937387786644496]
	TIME [epoch: 2.53 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913895459368345		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03913895459368345 | validation: 0.030311642326264283]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_888.pth
	Model improved!!!
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03781066360078308		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.03781066360078308 | validation: 0.036237755400857434]
	TIME [epoch: 2.53 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829264401929647		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.03829264401929647 | validation: 0.03716978032552854]
	TIME [epoch: 2.53 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024094565781732		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04024094565781732 | validation: 0.04069186480392969]
	TIME [epoch: 2.52 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046354574858979467		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.046354574858979467 | validation: 0.05430217552154724]
	TIME [epoch: 2.53 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814207572104742		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.04814207572104742 | validation: 0.03366474176612911]
	TIME [epoch: 2.52 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264602703307762		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.04264602703307762 | validation: 0.035887218556944114]
	TIME [epoch: 2.53 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039216604031644516		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.039216604031644516 | validation: 0.03918081850293714]
	TIME [epoch: 2.52 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040134588597420084		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.040134588597420084 | validation: 0.035121892768482744]
	TIME [epoch: 2.53 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983066793097186		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.03983066793097186 | validation: 0.037067725219010215]
	TIME [epoch: 2.52 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03969892391361857		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.03969892391361857 | validation: 0.03788410495393399]
	TIME [epoch: 2.53 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041089270548586666		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.041089270548586666 | validation: 0.03393544261687595]
	TIME [epoch: 2.52 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637362650029712		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03637362650029712 | validation: 0.033921941847079036]
	TIME [epoch: 2.53 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03733006491964306		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.03733006491964306 | validation: 0.032492147287722026]
	TIME [epoch: 2.51 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03779587392756981		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03779587392756981 | validation: 0.034017638150839594]
	TIME [epoch: 2.51 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03605854341953492		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.03605854341953492 | validation: 0.029845012535724632]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037802887709113726		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.037802887709113726 | validation: 0.03789731567892446]
	TIME [epoch: 2.51 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03797399120844328		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.03797399120844328 | validation: 0.03260774614020067]
	TIME [epoch: 2.51 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040210321461492685		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.040210321461492685 | validation: 0.03912081427224421]
	TIME [epoch: 2.51 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04000339030823173		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04000339030823173 | validation: 0.03110903367481992]
	TIME [epoch: 2.51 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03904404895252641		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03904404895252641 | validation: 0.040460676728764355]
	TIME [epoch: 2.51 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753885022414004		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03753885022414004 | validation: 0.04615001611919335]
	TIME [epoch: 2.51 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05039658806776222		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05039658806776222 | validation: 0.049164967737044575]
	TIME [epoch: 2.51 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04128075303451813		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.04128075303451813 | validation: 0.035038384511637925]
	TIME [epoch: 2.51 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989928096811933		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03989928096811933 | validation: 0.03600593840155453]
	TIME [epoch: 2.51 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038095192318665476		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.038095192318665476 | validation: 0.041011401750465463]
	TIME [epoch: 2.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03787958381142059		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03787958381142059 | validation: 0.03743606921903129]
	TIME [epoch: 2.51 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0363479117429641		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.0363479117429641 | validation: 0.030594429381166438]
	TIME [epoch: 2.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782046242502848		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03782046242502848 | validation: 0.03507700392646973]
	TIME [epoch: 2.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581045107656569		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.03581045107656569 | validation: 0.03195637500864349]
	TIME [epoch: 2.51 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03742681741686719		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03742681741686719 | validation: 0.033613417200187366]
	TIME [epoch: 2.51 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035902731702225237		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.035902731702225237 | validation: 0.029512308085999063]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035741729543036775		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.035741729543036775 | validation: 0.03302828465506032]
	TIME [epoch: 2.53 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03494527250921254		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.03494527250921254 | validation: 0.03205046338343256]
	TIME [epoch: 2.53 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036559187997730676		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.036559187997730676 | validation: 0.03380458946045326]
	TIME [epoch: 2.53 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03901309160858725		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03901309160858725 | validation: 0.03126951636524228]
	TIME [epoch: 2.52 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041092061803400005		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.041092061803400005 | validation: 0.03857300540810634]
	TIME [epoch: 2.53 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04021828433306591		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.04021828433306591 | validation: 0.030094779477620855]
	TIME [epoch: 2.52 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03889365043154157		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03889365043154157 | validation: 0.0397952891287765]
	TIME [epoch: 2.53 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03772155024690237		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.03772155024690237 | validation: 0.029209085518684303]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037615787551394116		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.037615787551394116 | validation: 0.0348830793977155]
	TIME [epoch: 2.53 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712577466923567		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.03712577466923567 | validation: 0.031729013313274226]
	TIME [epoch: 2.52 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346983543232169		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0346983543232169 | validation: 0.0316454244379318]
	TIME [epoch: 2.53 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037111489528853216		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.037111489528853216 | validation: 0.038586683221719376]
	TIME [epoch: 2.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03546017498645362		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03546017498645362 | validation: 0.0314831765610323]
	TIME [epoch: 2.52 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552904717337943		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.03552904717337943 | validation: 0.0367716450870381]
	TIME [epoch: 2.53 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702501175547673		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03702501175547673 | validation: 0.03642175095951651]
	TIME [epoch: 2.53 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03887292450807266		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03887292450807266 | validation: 0.036011937430134866]
	TIME [epoch: 2.52 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442189710283407		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0442189710283407 | validation: 0.03203812058764525]
	TIME [epoch: 2.53 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662970917467631		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03662970917467631 | validation: 0.03141648662608032]
	TIME [epoch: 2.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03547952786425901		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.03547952786425901 | validation: 0.035271213234608315]
	TIME [epoch: 2.53 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036178480897610533		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.036178480897610533 | validation: 0.03354998313599633]
	TIME [epoch: 2.52 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540238045001091		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.03540238045001091 | validation: 0.03804736364141612]
	TIME [epoch: 2.53 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03395242078033109		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.03395242078033109 | validation: 0.02891703685166738]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384257909132299		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03384257909132299 | validation: 0.027419242003379286]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03313654486838738		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03313654486838738 | validation: 0.028514758594667122]
	TIME [epoch: 2.53 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033034851807662066		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.033034851807662066 | validation: 0.02971724109939691]
	TIME [epoch: 2.53 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034384991781025914		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.034384991781025914 | validation: 0.037304565256141155]
	TIME [epoch: 2.53 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03458786142096287		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.03458786142096287 | validation: 0.026906728621892435]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0357930609447327		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.0357930609447327 | validation: 0.03953649762787065]
	TIME [epoch: 2.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037015307183621		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.04037015307183621 | validation: 0.03572690111368836]
	TIME [epoch: 2.52 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04466377477836849		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04466377477836849 | validation: 0.03464136214153784]
	TIME [epoch: 2.52 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039288585108647946		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.039288585108647946 | validation: 0.03848793013686097]
	TIME [epoch: 2.52 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03261950944610764		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.03261950944610764 | validation: 0.028490680867878666]
	TIME [epoch: 2.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448237488831382		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.03448237488831382 | validation: 0.026732928206757097]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034599484356579384		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.034599484356579384 | validation: 0.0324636269243416]
	TIME [epoch: 2.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035703655645990255		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.035703655645990255 | validation: 0.03066234531137504]
	TIME [epoch: 2.52 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253070683129377		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.03253070683129377 | validation: 0.02900653593941807]
	TIME [epoch: 2.53 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033093512053381874		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.033093512053381874 | validation: 0.03267672725195019]
	TIME [epoch: 2.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03415914487738639		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.03415914487738639 | validation: 0.026546092239975835]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393752447728412		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03393752447728412 | validation: 0.027679203010679995]
	TIME [epoch: 2.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03232052144531723		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.03232052144531723 | validation: 0.030654730867114333]
	TIME [epoch: 2.52 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03258030495003388		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.03258030495003388 | validation: 0.03670447171225645]
	TIME [epoch: 2.52 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03442667633180722		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.03442667633180722 | validation: 0.03671386020761465]
	TIME [epoch: 2.52 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035800481112501836		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.035800481112501836 | validation: 0.03305645163817442]
	TIME [epoch: 2.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062116916263813		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.04062116916263813 | validation: 0.04092916916804887]
	TIME [epoch: 2.52 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905534957047904		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03905534957047904 | validation: 0.030705104407641327]
	TIME [epoch: 2.52 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298250408430333		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.03298250408430333 | validation: 0.029010066406834525]
	TIME [epoch: 2.52 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342607772371583		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0342607772371583 | validation: 0.03275903826163475]
	TIME [epoch: 2.52 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034344696138133755		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.034344696138133755 | validation: 0.026781604573438825]
	TIME [epoch: 2.52 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03302009596411456		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03302009596411456 | validation: 0.0277203644972305]
	TIME [epoch: 2.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434111209624916		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.03434111209624916 | validation: 0.028910233807401237]
	TIME [epoch: 2.52 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491983893768769		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.03491983893768769 | validation: 0.03209140143908646]
	TIME [epoch: 2.52 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762763570680736		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.03762763570680736 | validation: 0.029134772731699784]
	TIME [epoch: 2.52 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036675375296694526		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.036675375296694526 | validation: 0.03022271987669044]
	TIME [epoch: 2.52 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03378855174187193		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.03378855174187193 | validation: 0.032639120203756325]
	TIME [epoch: 2.52 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034626091307484994		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.034626091307484994 | validation: 0.034055084166884846]
	TIME [epoch: 2.52 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03310659526059696		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.03310659526059696 | validation: 0.029967782438874775]
	TIME [epoch: 2.52 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220332318450005		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03220332318450005 | validation: 0.03241431933466459]
	TIME [epoch: 2.52 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035195497008641274		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.035195497008641274 | validation: 0.02903270332148285]
	TIME [epoch: 2.52 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216538236241675		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03216538236241675 | validation: 0.029164501998441467]
	TIME [epoch: 2.52 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034234712543145		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.034234712543145 | validation: 0.025853212421206896]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203431190477467		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.03203431190477467 | validation: 0.0316800432886619]
	TIME [epoch: 2.53 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033086550050421645		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.033086550050421645 | validation: 0.03071813664299221]
	TIME [epoch: 2.53 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422001534476387		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03422001534476387 | validation: 0.032646560785313965]
	TIME [epoch: 2.52 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035772517510327013		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.035772517510327013 | validation: 0.031081343495079978]
	TIME [epoch: 2.53 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03664234390456472		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03664234390456472 | validation: 0.035480914788110725]
	TIME [epoch: 2.53 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03476032167739539		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.03476032167739539 | validation: 0.024934388797983976]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03374078536189338		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03374078536189338 | validation: 0.03514257929215978]
	TIME [epoch: 2.53 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039111060122164726		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.039111060122164726 | validation: 0.032810167825972486]
	TIME [epoch: 2.52 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03573566501730681		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03573566501730681 | validation: 0.02522257872945753]
	TIME [epoch: 2.53 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032356811996236495		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.032356811996236495 | validation: 0.02846819139630254]
	TIME [epoch: 2.52 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410570970627228		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03410570970627228 | validation: 0.03437732934886395]
	TIME [epoch: 2.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03442603319299343		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.03442603319299343 | validation: 0.02599833198124933]
	TIME [epoch: 2.52 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03164018059029387		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03164018059029387 | validation: 0.02824145015103484]
	TIME [epoch: 2.52 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031396852092908496		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.031396852092908496 | validation: 0.029279117928323842]
	TIME [epoch: 2.52 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032016823563196746		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.032016823563196746 | validation: 0.02311498375948235]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032495993477990405		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.032495993477990405 | validation: 0.027215798319743908]
	TIME [epoch: 2.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031469784494414406		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.031469784494414406 | validation: 0.028207948029011654]
	TIME [epoch: 2.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03252699047992187		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.03252699047992187 | validation: 0.02815392850065501]
	TIME [epoch: 2.52 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034053568274914216		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.034053568274914216 | validation: 0.033276339358340026]
	TIME [epoch: 2.52 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298128387809349		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.03298128387809349 | validation: 0.026816656497792152]
	TIME [epoch: 2.52 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0336438508832547		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0336438508832547 | validation: 0.03033039407187095]
	TIME [epoch: 2.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03553979408552592		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.03553979408552592 | validation: 0.024760668329990528]
	TIME [epoch: 297 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429883699552971		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.03429883699552971 | validation: 0.030817647082146744]
	TIME [epoch: 5.44 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035605437910564766		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.035605437910564766 | validation: 0.02610421234400075]
	TIME [epoch: 5.43 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031332841061263175		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.031332841061263175 | validation: 0.02856440025076042]
	TIME [epoch: 5.43 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03247384248317988		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.03247384248317988 | validation: 0.026248452773294552]
	TIME [epoch: 5.44 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03167776795557398		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.03167776795557398 | validation: 0.02797001729556955]
	TIME [epoch: 5.43 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03211227607610823		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.03211227607610823 | validation: 0.02602470551012015]
	TIME [epoch: 5.44 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030888913528668963		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.030888913528668963 | validation: 0.028588005784130996]
	TIME [epoch: 5.43 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03121294938270034		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.03121294938270034 | validation: 0.024517059969268198]
	TIME [epoch: 5.44 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03174249466309052		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.03174249466309052 | validation: 0.026000639476916117]
	TIME [epoch: 5.43 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03244868997378295		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.03244868997378295 | validation: 0.026069114836812325]
	TIME [epoch: 5.43 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03215281529145128		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03215281529145128 | validation: 0.022713263407351393]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220045447431771		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.03220045447431771 | validation: 0.026742282310059363]
	TIME [epoch: 5.44 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462103562745521		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03462103562745521 | validation: 0.03944530168607377]
	TIME [epoch: 5.42 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03592559532775626		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.03592559532775626 | validation: 0.028388202194074365]
	TIME [epoch: 5.43 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037002333762863604		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.037002333762863604 | validation: 0.031152284486791228]
	TIME [epoch: 5.43 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031391550880259855		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.031391550880259855 | validation: 0.03103727497742812]
	TIME [epoch: 5.43 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03139835968372262		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03139835968372262 | validation: 0.027429969933181503]
	TIME [epoch: 5.44 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156561802702741		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.03156561802702741 | validation: 0.027663831628060298]
	TIME [epoch: 5.42 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03144619620227772		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03144619620227772 | validation: 0.03144771995759905]
	TIME [epoch: 5.43 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031747856739092334		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.031747856739092334 | validation: 0.026974523036427556]
	TIME [epoch: 5.43 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03299089337101421		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03299089337101421 | validation: 0.029697816602867563]
	TIME [epoch: 5.43 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03092109373519988		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.03092109373519988 | validation: 0.027473119357750427]
	TIME [epoch: 5.42 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031848728876569136		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.031848728876569136 | validation: 0.028313732403083443]
	TIME [epoch: 5.43 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02986596992682049		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.02986596992682049 | validation: 0.028119832025391145]
	TIME [epoch: 5.42 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030123559700534767		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.030123559700534767 | validation: 0.02442680605836652]
	TIME [epoch: 5.43 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03045260584808191		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.03045260584808191 | validation: 0.024969872220929563]
	TIME [epoch: 5.42 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03107447633035468		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.03107447633035468 | validation: 0.021928634544768906]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030285312051896343		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.030285312051896343 | validation: 0.024401536713298352]
	TIME [epoch: 5.42 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03305487166372252		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03305487166372252 | validation: 0.02992489948908804]
	TIME [epoch: 5.42 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03212136301728322		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.03212136301728322 | validation: 0.02568732223763888]
	TIME [epoch: 5.43 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520930040339034		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.03520930040339034 | validation: 0.02726708784782438]
	TIME [epoch: 5.41 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03264739471458843		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.03264739471458843 | validation: 0.02632902919248888]
	TIME [epoch: 5.42 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03271513389287573		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03271513389287573 | validation: 0.029606651003559022]
	TIME [epoch: 5.43 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151572368388862		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.03151572368388862 | validation: 0.02298278184114441]
	TIME [epoch: 5.43 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0317107734007821		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0317107734007821 | validation: 0.02655903427703321]
	TIME [epoch: 5.44 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031075937247185475		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.031075937247185475 | validation: 0.024935619284997775]
	TIME [epoch: 5.42 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0302161611317745		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.0302161611317745 | validation: 0.02642034465105559]
	TIME [epoch: 5.44 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030700375481833905		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.030700375481833905 | validation: 0.027548675494883212]
	TIME [epoch: 5.43 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0333077256408452		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.0333077256408452 | validation: 0.028105635086352318]
	TIME [epoch: 5.43 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031769414302475256		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.031769414302475256 | validation: 0.025993377414390836]
	TIME [epoch: 5.43 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032129457217136295		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.032129457217136295 | validation: 0.028349258371180304]
	TIME [epoch: 5.42 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03153399544071506		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.03153399544071506 | validation: 0.025002059915235138]
	TIME [epoch: 5.47 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322795961417504		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.0322795961417504 | validation: 0.029360323166854413]
	TIME [epoch: 5.43 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031524883227975364		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.031524883227975364 | validation: 0.029466331199764318]
	TIME [epoch: 5.43 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230350002912618		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03230350002912618 | validation: 0.025653733643171972]
	TIME [epoch: 5.43 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316305946604036		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.0316305946604036 | validation: 0.03323387013971268]
	TIME [epoch: 5.43 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516879629334487		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.03516879629334487 | validation: 0.02600982711599564]
	TIME [epoch: 5.42 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02995801089453829		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.02995801089453829 | validation: 0.022895676629610857]
	TIME [epoch: 5.43 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02971544380703771		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.02971544380703771 | validation: 0.026027654017087162]
	TIME [epoch: 5.42 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02883892233998627		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.02883892233998627 | validation: 0.032254528830871966]
	TIME [epoch: 5.44 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030464924326589867		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.030464924326589867 | validation: 0.025752873279300953]
	TIME [epoch: 5.44 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02977692323434277		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.02977692323434277 | validation: 0.02489363827743592]
	TIME [epoch: 5.44 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031773847435123		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.031773847435123 | validation: 0.02491863985499366]
	TIME [epoch: 5.43 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030723263882110326		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.030723263882110326 | validation: 0.02781156497673709]
	TIME [epoch: 5.44 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03197542163675293		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.03197542163675293 | validation: 0.02587949010513961]
	TIME [epoch: 5.42 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03116953639066825		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.03116953639066825 | validation: 0.028998244444878904]
	TIME [epoch: 5.44 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03046482539560491		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03046482539560491 | validation: 0.025180581410991495]
	TIME [epoch: 5.44 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029324712453788253		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.029324712453788253 | validation: 0.028977999247550165]
	TIME [epoch: 5.44 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03167602758242499		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.03167602758242499 | validation: 0.03108136458155172]
	TIME [epoch: 5.42 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032290581184990126		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.032290581184990126 | validation: 0.021433925048111147]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1061.pth
	Model improved!!!
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030288980165820628		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.030288980165820628 | validation: 0.02670587116517399]
	TIME [epoch: 5.42 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028464422284693428		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.028464422284693428 | validation: 0.025209313176840664]
	TIME [epoch: 5.42 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030786337939981753		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.030786337939981753 | validation: 0.023725906754263806]
	TIME [epoch: 5.41 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030495433935248985		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.030495433935248985 | validation: 0.023722465133089666]
	TIME [epoch: 5.43 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03037022745348405		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.03037022745348405 | validation: 0.02493400330700368]
	TIME [epoch: 5.43 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028761101370334958		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.028761101370334958 | validation: 0.029762629594856472]
	TIME [epoch: 5.44 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02999265998722141		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.02999265998722141 | validation: 0.02415420090554257]
	TIME [epoch: 5.42 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030741174391935452		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.030741174391935452 | validation: 0.026348147547335027]
	TIME [epoch: 5.43 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029563647736956425		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.029563647736956425 | validation: 0.02578715598722536]
	TIME [epoch: 5.41 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029661427934555017		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.029661427934555017 | validation: 0.025342837029296114]
	TIME [epoch: 5.44 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02860740358832782		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.02860740358832782 | validation: 0.025577260750907427]
	TIME [epoch: 5.42 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030202040027835633		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.030202040027835633 | validation: 0.027082011963465952]
	TIME [epoch: 5.43 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029668658815092033		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.029668658815092033 | validation: 0.022739306100002073]
	TIME [epoch: 5.42 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028790299199734654		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.028790299199734654 | validation: 0.021637918858321288]
	TIME [epoch: 5.43 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031764746423847544		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.031764746423847544 | validation: 0.03270005902115191]
	TIME [epoch: 5.43 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031762141159905596		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.031762141159905596 | validation: 0.027749988845702872]
	TIME [epoch: 5.44 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03427023164522397		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03427023164522397 | validation: 0.024684175794012594]
	TIME [epoch: 5.43 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031485830378234485		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.031485830378234485 | validation: 0.025330666975582006]
	TIME [epoch: 5.44 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029294441391423502		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.029294441391423502 | validation: 0.030086690822378904]
	TIME [epoch: 5.43 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030735921366696325		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.030735921366696325 | validation: 0.029672054769948386]
	TIME [epoch: 5.44 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02926026382281987		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02926026382281987 | validation: 0.023211096789788017]
	TIME [epoch: 5.44 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811974267049117		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.02811974267049117 | validation: 0.02286497285788859]
	TIME [epoch: 5.43 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02931292891387882		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.02931292891387882 | validation: 0.027319187484756948]
	TIME [epoch: 5.43 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02841958662748545		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.02841958662748545 | validation: 0.02266295216306658]
	TIME [epoch: 5.44 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027612195114616416		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.027612195114616416 | validation: 0.02461312576309248]
	TIME [epoch: 5.43 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030979188575542042		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.030979188575542042 | validation: 0.02212011826213181]
	TIME [epoch: 5.44 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030168890590507452		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.030168890590507452 | validation: 0.023870229172830805]
	TIME [epoch: 5.43 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031054894432909465		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.031054894432909465 | validation: 0.025502797803882257]
	TIME [epoch: 5.43 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02951421305062021		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.02951421305062021 | validation: 0.025276803537120526]
	TIME [epoch: 5.43 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02911637148882008		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.02911637148882008 | validation: 0.022941988642426678]
	TIME [epoch: 5.43 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028259132542818706		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.028259132542818706 | validation: 0.02299469900892963]
	TIME [epoch: 5.43 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031244770039105214		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.031244770039105214 | validation: 0.0270549478501386]
	TIME [epoch: 5.44 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03213629959614625		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.03213629959614625 | validation: 0.02730026345542863]
	TIME [epoch: 5.43 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030195130478041704		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.030195130478041704 | validation: 0.025160610376528742]
	TIME [epoch: 5.44 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028837875871621368		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.028837875871621368 | validation: 0.025356643906872745]
	TIME [epoch: 5.44 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027505439219401743		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.027505439219401743 | validation: 0.0254886939150978]
	TIME [epoch: 5.44 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0272421820717251		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.0272421820717251 | validation: 0.023403672968212486]
	TIME [epoch: 5.44 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028910920336100956		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.028910920336100956 | validation: 0.022643003369607846]
	TIME [epoch: 5.43 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029824581128694724		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.029824581128694724 | validation: 0.019383795572269685]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1100.pth
	Model improved!!!
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02781128570204697		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.02781128570204697 | validation: 0.0238498921143388]
	TIME [epoch: 5.44 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030057759362926508		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.030057759362926508 | validation: 0.02131808494173834]
	TIME [epoch: 5.43 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02988729148922269		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.02988729148922269 | validation: 0.02733890971111319]
	TIME [epoch: 5.46 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027988753931477407		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.027988753931477407 | validation: 0.030387175970324698]
	TIME [epoch: 5.44 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029017415759922695		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.029017415759922695 | validation: 0.02048721027614163]
	TIME [epoch: 5.44 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029664218875663045		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.029664218875663045 | validation: 0.021698019823639475]
	TIME [epoch: 5.43 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026174442888043617		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.026174442888043617 | validation: 0.02988858448604728]
	TIME [epoch: 5.44 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03247052611907474		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.03247052611907474 | validation: 0.021651671485055847]
	TIME [epoch: 5.43 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030056251017093923		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.030056251017093923 | validation: 0.02480064566524841]
	TIME [epoch: 5.44 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03015675325043018		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.03015675325043018 | validation: 0.023344999678721223]
	TIME [epoch: 5.44 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921877854084552		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.02921877854084552 | validation: 0.024920469168111678]
	TIME [epoch: 5.44 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02815999995209359		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.02815999995209359 | validation: 0.02001200041249245]
	TIME [epoch: 5.44 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753033766516634		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.02753033766516634 | validation: 0.02476350802985802]
	TIME [epoch: 5.44 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02696199186717195		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.02696199186717195 | validation: 0.022924118333935064]
	TIME [epoch: 5.44 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02827579479119391		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.02827579479119391 | validation: 0.021904944909413873]
	TIME [epoch: 5.44 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027791800419124577		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.027791800419124577 | validation: 0.024459074967088847]
	TIME [epoch: 5.44 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027994983141525775		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.027994983141525775 | validation: 0.02463969575975299]
	TIME [epoch: 5.43 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028366143280369842		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.028366143280369842 | validation: 0.02555939546921998]
	TIME [epoch: 5.43 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030331749024176975		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.030331749024176975 | validation: 0.02642037198919178]
	TIME [epoch: 5.44 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030138545945100282		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.030138545945100282 | validation: 0.024161593486398084]
	TIME [epoch: 5.43 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026472648662109445		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.026472648662109445 | validation: 0.025978095082469368]
	TIME [epoch: 5.47 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02854993974911058		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.02854993974911058 | validation: 0.02525798013774714]
	TIME [epoch: 5.44 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029413145373099525		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.029413145373099525 | validation: 0.021966255596072317]
	TIME [epoch: 5.43 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02809547808124801		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.02809547808124801 | validation: 0.02387365687167897]
	TIME [epoch: 5.44 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029048408543262773		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.029048408543262773 | validation: 0.024686392736887136]
	TIME [epoch: 5.44 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028661859874435425		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.028661859874435425 | validation: 0.022872391972122576]
	TIME [epoch: 5.44 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750578475077106		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.02750578475077106 | validation: 0.021834329241487118]
	TIME [epoch: 5.44 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028481503007716676		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.028481503007716676 | validation: 0.020590796500116304]
	TIME [epoch: 5.44 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028137987134234096		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.028137987134234096 | validation: 0.02128906504348252]
	TIME [epoch: 5.43 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027333297113612414		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.027333297113612414 | validation: 0.024770356475666458]
	TIME [epoch: 5.46 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647310492947111		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02647310492947111 | validation: 0.025677364242021196]
	TIME [epoch: 5.43 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030599532416930062		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.030599532416930062 | validation: 0.02527085171376872]
	TIME [epoch: 5.43 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02806509365986649		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.02806509365986649 | validation: 0.021646643490941565]
	TIME [epoch: 5.44 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027425858396378802		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.027425858396378802 | validation: 0.02326062944628054]
	TIME [epoch: 5.43 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027557317192552763		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.027557317192552763 | validation: 0.02530048800976488]
	TIME [epoch: 5.44 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027259902687668616		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.027259902687668616 | validation: 0.020532272547912747]
	TIME [epoch: 5.43 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029287965484488393		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.029287965484488393 | validation: 0.024974919765048554]
	TIME [epoch: 5.43 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028559575095932682		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.028559575095932682 | validation: 0.022665286908629304]
	TIME [epoch: 5.43 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02770174095573085		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.02770174095573085 | validation: 0.023015056169692707]
	TIME [epoch: 5.44 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02877754943737086		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.02877754943737086 | validation: 0.030759604152594728]
	TIME [epoch: 5.46 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030876936905866046		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.030876936905866046 | validation: 0.0217967291169492]
	TIME [epoch: 5.44 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030470804418314047		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.030470804418314047 | validation: 0.02347379747157847]
	TIME [epoch: 5.43 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02863241613189418		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.02863241613189418 | validation: 0.02787114626170578]
	TIME [epoch: 5.43 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026150228329899626		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.026150228329899626 | validation: 0.020308135510474404]
	TIME [epoch: 5.44 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029614391976078132		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.029614391976078132 | validation: 0.024438881615957683]
	TIME [epoch: 5.43 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026305382320740395		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.026305382320740395 | validation: 0.023083440719456217]
	TIME [epoch: 5.43 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02838121518417439		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.02838121518417439 | validation: 0.021317151412603826]
	TIME [epoch: 5.44 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02895703484555245		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02895703484555245 | validation: 0.023338311359955056]
	TIME [epoch: 5.43 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02761164839686728		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02761164839686728 | validation: 0.023422404914401035]
	TIME [epoch: 5.43 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026950315560243483		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.026950315560243483 | validation: 0.019955018862094383]
	TIME [epoch: 5.45 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02789353082029332		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.02789353082029332 | validation: 0.028262733485212972]
	TIME [epoch: 5.43 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028539818324311742		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.028539818324311742 | validation: 0.022933256440041263]
	TIME [epoch: 5.43 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028057612510424212		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.028057612510424212 | validation: 0.021185070824229588]
	TIME [epoch: 5.43 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027739453901042013		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.027739453901042013 | validation: 0.025313525995589573]
	TIME [epoch: 5.44 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026847782495382097		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.026847782495382097 | validation: 0.02224180023378778]
	TIME [epoch: 5.43 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026122928026594806		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.026122928026594806 | validation: 0.019533563280303312]
	TIME [epoch: 5.43 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027365874633988447		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.027365874633988447 | validation: 0.024029934680192655]
	TIME [epoch: 5.44 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028391832114848067		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.028391832114848067 | validation: 0.019576217300958307]
	TIME [epoch: 5.43 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028331483640681956		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.028331483640681956 | validation: 0.022945443304552723]
	TIME [epoch: 5.43 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026970240259416627		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.026970240259416627 | validation: 0.025421888985660036]
	TIME [epoch: 5.48 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02677218913043903		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.02677218913043903 | validation: 0.021613525243520365]
	TIME [epoch: 5.43 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028188948221728304		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.028188948221728304 | validation: 0.02446483733679763]
	TIME [epoch: 5.44 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02755094425768928		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.02755094425768928 | validation: 0.018509972013543215]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026682199645127424		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.026682199645127424 | validation: 0.02262383738245985]
	TIME [epoch: 5.44 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02600983436175229		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.02600983436175229 | validation: 0.022496887768936154]
	TIME [epoch: 5.43 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02656496007971102		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.02656496007971102 | validation: 0.026343978001577298]
	TIME [epoch: 5.43 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027222474561000302		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.027222474561000302 | validation: 0.023370142507922326]
	TIME [epoch: 5.43 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02767424308000666		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.02767424308000666 | validation: 0.02475315743089684]
	TIME [epoch: 5.46 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02735550948772012		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.02735550948772012 | validation: 0.02402664960798643]
	TIME [epoch: 5.44 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027239109335782503		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.027239109335782503 | validation: 0.02013501704671374]
	TIME [epoch: 5.44 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028265424522480164		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.028265424522480164 | validation: 0.0250822198484438]
	TIME [epoch: 5.44 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025835263513355154		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.025835263513355154 | validation: 0.023568597776074604]
	TIME [epoch: 5.44 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0254761760454035		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.0254761760454035 | validation: 0.026820727240796323]
	TIME [epoch: 5.43 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027856198144096317		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.027856198144096317 | validation: 0.02040133822524669]
	TIME [epoch: 5.44 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029526431719428637		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.029526431719428637 | validation: 0.021407836045730173]
	TIME [epoch: 5.43 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028181120810781623		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.028181120810781623 | validation: 0.02269741927286939]
	TIME [epoch: 5.43 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026660828168976698		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.026660828168976698 | validation: 0.021818738259485326]
	TIME [epoch: 5.44 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027518219102071452		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.027518219102071452 | validation: 0.025108860403781037]
	TIME [epoch: 5.43 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027300740764427723		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.027300740764427723 | validation: 0.025032670505934912]
	TIME [epoch: 5.46 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027824612438233295		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.027824612438233295 | validation: 0.02039940733185741]
	TIME [epoch: 5.43 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026806970224152016		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.026806970224152016 | validation: 0.023668842488762578]
	TIME [epoch: 5.44 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025951318225544005		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.025951318225544005 | validation: 0.021156762479704834]
	TIME [epoch: 5.43 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026837811074097752		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.026837811074097752 | validation: 0.022311239705304405]
	TIME [epoch: 5.44 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024678205250885876		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.024678205250885876 | validation: 0.023570912509653864]
	TIME [epoch: 5.43 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027582488723014414		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.027582488723014414 | validation: 0.024414067911495058]
	TIME [epoch: 5.43 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026346785059092395		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.026346785059092395 | validation: 0.021922178685264672]
	TIME [epoch: 5.43 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026332635911180426		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.026332635911180426 | validation: 0.02005378961206632]
	TIME [epoch: 5.44 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02711660228896487		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02711660228896487 | validation: 0.019827372782988042]
	TIME [epoch: 5.42 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024500550793777436		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.024500550793777436 | validation: 0.02005270550345394]
	TIME [epoch: 5.43 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02724678575299977		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.02724678575299977 | validation: 0.022089179147089313]
	TIME [epoch: 5.43 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025402271048294966		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.025402271048294966 | validation: 0.021575398566134177]
	TIME [epoch: 5.43 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026951386176441484		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.026951386176441484 | validation: 0.02323266693278995]
	TIME [epoch: 5.44 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026307946854977705		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.026307946854977705 | validation: 0.0223704415044972]
	TIME [epoch: 5.43 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027473450986780862		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.027473450986780862 | validation: 0.020458978503236327]
	TIME [epoch: 5.43 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025787895416578534		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.025787895416578534 | validation: 0.01980589524960477]
	TIME [epoch: 5.44 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028773907299261756		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.028773907299261756 | validation: 0.01934217872338815]
	TIME [epoch: 5.44 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0266319207329959		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.0266319207329959 | validation: 0.024691838827213927]
	TIME [epoch: 5.43 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027181076219765		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.027181076219765 | validation: 0.021317412625478795]
	TIME [epoch: 5.44 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027143386404416107		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.027143386404416107 | validation: 0.024189932917234452]
	TIME [epoch: 5.45 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026805034003044757		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.026805034003044757 | validation: 0.021695854667133415]
	TIME [epoch: 5.44 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02609466152155041		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.02609466152155041 | validation: 0.01900420739687683]
	TIME [epoch: 5.44 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028372190213342287		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.028372190213342287 | validation: 0.023227414646178824]
	TIME [epoch: 5.44 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025378873359132425		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.025378873359132425 | validation: 0.020159449571002182]
	TIME [epoch: 5.44 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02630389231035341		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.02630389231035341 | validation: 0.01866501731857097]
	TIME [epoch: 5.44 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026290005006746133		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.026290005006746133 | validation: 0.02088569723630937]
	TIME [epoch: 5.44 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028234654051437685		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.028234654051437685 | validation: 0.019351364917449133]
	TIME [epoch: 5.44 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02527963411834617		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.02527963411834617 | validation: 0.018792336035477926]
	TIME [epoch: 5.44 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02699848000773624		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.02699848000773624 | validation: 0.022462310760405724]
	TIME [epoch: 5.44 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025508515773317995		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.025508515773317995 | validation: 0.019413180636609375]
	TIME [epoch: 5.43 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025730459555290627		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.025730459555290627 | validation: 0.020373870696259982]
	TIME [epoch: 5.42 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026989001899679774		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.026989001899679774 | validation: 0.020275276402475447]
	TIME [epoch: 5.43 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025511340152161166		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.025511340152161166 | validation: 0.021874938306170096]
	TIME [epoch: 5.42 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025840464959644784		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.025840464959644784 | validation: 0.02001651165987388]
	TIME [epoch: 5.43 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02703071923156208		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.02703071923156208 | validation: 0.019488440398464587]
	TIME [epoch: 5.42 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025164770206943278		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.025164770206943278 | validation: 0.023387749813301762]
	TIME [epoch: 5.43 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025211614447678738		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.025211614447678738 | validation: 0.024291834642181445]
	TIME [epoch: 5.42 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025253560423169445		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.025253560423169445 | validation: 0.017729437109473813]
	TIME [epoch: 5.44 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1217.pth
	Model improved!!!
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025407619378917376		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.025407619378917376 | validation: 0.022416560823859778]
	TIME [epoch: 5.42 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027672395442013113		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.027672395442013113 | validation: 0.017823503799993198]
	TIME [epoch: 5.44 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027426636693797253		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.027426636693797253 | validation: 0.019588661899766942]
	TIME [epoch: 5.42 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025918977987722185		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.025918977987722185 | validation: 0.015910848741424583]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1221.pth
	Model improved!!!
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026183685829054337		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.026183685829054337 | validation: 0.01856881132316604]
	TIME [epoch: 5.43 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027066772375229097		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.027066772375229097 | validation: 0.024339756247857808]
	TIME [epoch: 5.42 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023944511650949272		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.023944511650949272 | validation: 0.018831645436157635]
	TIME [epoch: 5.44 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026535001272995212		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.026535001272995212 | validation: 0.021731129958877323]
	TIME [epoch: 5.47 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027263852140646583		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.027263852140646583 | validation: 0.019486603955234494]
	TIME [epoch: 5.44 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02497684712073002		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.02497684712073002 | validation: 0.021630771489530905]
	TIME [epoch: 5.43 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025885427350594164		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.025885427350594164 | validation: 0.018102847834569013]
	TIME [epoch: 5.43 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025361702304875444		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.025361702304875444 | validation: 0.019494902583842694]
	TIME [epoch: 5.43 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026202724663214872		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.026202724663214872 | validation: 0.01715627581491087]
	TIME [epoch: 5.43 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024927185539749337		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.024927185539749337 | validation: 0.018558533435869284]
	TIME [epoch: 5.44 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026666748538475723		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.026666748538475723 | validation: 0.019481944385832502]
	TIME [epoch: 5.43 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025944836880978962		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.025944836880978962 | validation: 0.021463653751483472]
	TIME [epoch: 5.43 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02499818087414294		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.02499818087414294 | validation: 0.018555620812254026]
	TIME [epoch: 5.43 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026770541328350116		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.026770541328350116 | validation: 0.018768164162708764]
	TIME [epoch: 5.44 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02475623964115275		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02475623964115275 | validation: 0.02113091580191341]
	TIME [epoch: 5.44 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025680107520190527		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.025680107520190527 | validation: 0.019238844129937777]
	TIME [epoch: 5.43 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023341978507359477		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.023341978507359477 | validation: 0.023349811945600787]
	TIME [epoch: 5.43 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026208317006897894		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.026208317006897894 | validation: 0.02172123224543748]
	TIME [epoch: 5.45 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023789715701737508		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.023789715701737508 | validation: 0.0190319984084166]
	TIME [epoch: 5.44 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026654031905127395		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.026654031905127395 | validation: 0.024962674608631892]
	TIME [epoch: 5.43 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02639667574823716		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02639667574823716 | validation: 0.01970323854002772]
	TIME [epoch: 5.44 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026151601116543945		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.026151601116543945 | validation: 0.019111457977686458]
	TIME [epoch: 5.44 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025505473629536687		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.025505473629536687 | validation: 0.02243799162072778]
	TIME [epoch: 5.44 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02465037805102048		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02465037805102048 | validation: 0.02016149820700564]
	TIME [epoch: 5.44 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02613148072526375		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.02613148072526375 | validation: 0.01953719904011518]
	TIME [epoch: 5.44 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025413134670199566		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.025413134670199566 | validation: 0.02046286346993468]
	TIME [epoch: 5.44 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025710623936259697		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.025710623936259697 | validation: 0.020341730689155225]
	TIME [epoch: 5.45 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520781579355992		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.02520781579355992 | validation: 0.019925151534755048]
	TIME [epoch: 5.43 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025998911202119786		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.025998911202119786 | validation: 0.02094950731724651]
	TIME [epoch: 5.45 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026286651106174005		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.026286651106174005 | validation: 0.02213137960370797]
	TIME [epoch: 5.44 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025737941032767532		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.025737941032767532 | validation: 0.02084772400641447]
	TIME [epoch: 5.45 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026706542178114707		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.026706542178114707 | validation: 0.019016362663297693]
	TIME [epoch: 5.43 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02517804026344919		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.02517804026344919 | validation: 0.02402905097572991]
	TIME [epoch: 5.44 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024650980333174075		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.024650980333174075 | validation: 0.022097345996922525]
	TIME [epoch: 5.46 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02615048124670502		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.02615048124670502 | validation: 0.020475787193995287]
	TIME [epoch: 5.44 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02579600813971094		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.02579600813971094 | validation: 0.01984564152074091]
	TIME [epoch: 5.44 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024751072448125116		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.024751072448125116 | validation: 0.023135725733271897]
	TIME [epoch: 5.44 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02590380707235979		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.02590380707235979 | validation: 0.019185870007836187]
	TIME [epoch: 5.44 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023777285100688977		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.023777285100688977 | validation: 0.016963204188052496]
	TIME [epoch: 5.44 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024781421363553904		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.024781421363553904 | validation: 0.018142801542666048]
	TIME [epoch: 5.43 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025882783380847717		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.025882783380847717 | validation: 0.02271142942157788]
	TIME [epoch: 5.44 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026142650328505313		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.026142650328505313 | validation: 0.022032477065556184]
	TIME [epoch: 5.44 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520771581245941		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.02520771581245941 | validation: 0.0228042857651527]
	TIME [epoch: 5.43 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024199945074028284		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.024199945074028284 | validation: 0.022307272609209196]
	TIME [epoch: 5.43 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02518146538122347		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.02518146538122347 | validation: 0.019384046880550367]
	TIME [epoch: 5.44 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02547096570302574		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.02547096570302574 | validation: 0.018340822384739802]
	TIME [epoch: 5.43 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024212527594303382		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.024212527594303382 | validation: 0.019104551994910714]
	TIME [epoch: 5.44 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024054953740828253		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.024054953740828253 | validation: 0.0176114449930633]
	TIME [epoch: 5.43 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540206427364189		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.02540206427364189 | validation: 0.019259152807639623]
	TIME [epoch: 5.44 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023780901306811382		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.023780901306811382 | validation: 0.017700247583862922]
	TIME [epoch: 5.43 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024736838227733378		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.024736838227733378 | validation: 0.023373126247707543]
	TIME [epoch: 5.44 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02486901951488731		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.02486901951488731 | validation: 0.01908583185511281]
	TIME [epoch: 5.43 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024606728624690773		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.024606728624690773 | validation: 0.02194348880524867]
	TIME [epoch: 5.44 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025294606991392705		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.025294606991392705 | validation: 0.018113988341323817]
	TIME [epoch: 5.44 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025563349564895272		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.025563349564895272 | validation: 0.018501896825210914]
	TIME [epoch: 5.44 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028017370269167518		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.028017370269167518 | validation: 0.023714878018800758]
	TIME [epoch: 5.44 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027551435585941936		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.027551435585941936 | validation: 0.018486153525329375]
	TIME [epoch: 5.44 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026436276598715506		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.026436276598715506 | validation: 0.01729799505086926]
	TIME [epoch: 5.43 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02420716821856204		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02420716821856204 | validation: 0.02349104920547205]
	TIME [epoch: 5.44 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024062053278991514		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.024062053278991514 | validation: 0.020206759287201833]
	TIME [epoch: 5.43 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520951882215684		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.02520951882215684 | validation: 0.019125339204761945]
	TIME [epoch: 5.44 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02346334650901509		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.02346334650901509 | validation: 0.021041360389307418]
	TIME [epoch: 5.42 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02361136310404711		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.02361136310404711 | validation: 0.019531724135668112]
	TIME [epoch: 5.44 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02305902081419158		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.02305902081419158 | validation: 0.019295300609641925]
	TIME [epoch: 5.43 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026392869387612865		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.026392869387612865 | validation: 0.019853289703221724]
	TIME [epoch: 5.44 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024358907370343647		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.024358907370343647 | validation: 0.020592119224582747]
	TIME [epoch: 5.43 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025153697213158882		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.025153697213158882 | validation: 0.022736999858617338]
	TIME [epoch: 5.44 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023928512842892578		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.023928512842892578 | validation: 0.017767975622101157]
	TIME [epoch: 5.43 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0240534715609623		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.0240534715609623 | validation: 0.01848207458794798]
	TIME [epoch: 5.44 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02395726995626746		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02395726995626746 | validation: 0.0194588864722899]
	TIME [epoch: 5.43 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023311683280353025		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.023311683280353025 | validation: 0.019677482033363197]
	TIME [epoch: 5.45 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02561432015698809		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.02561432015698809 | validation: 0.01859255035736797]
	TIME [epoch: 5.43 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02414513608803727		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.02414513608803727 | validation: 0.02073030797129923]
	TIME [epoch: 5.43 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023940915398876623		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.023940915398876623 | validation: 0.017601211194021943]
	TIME [epoch: 5.42 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025163211514805307		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.025163211514805307 | validation: 0.016652091635555283]
	TIME [epoch: 5.45 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393388813291681		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.02393388813291681 | validation: 0.018567881609365827]
	TIME [epoch: 5.42 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025245478908583353		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.025245478908583353 | validation: 0.017461430884880287]
	TIME [epoch: 5.45 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025257411687754618		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.025257411687754618 | validation: 0.02029388510627361]
	TIME [epoch: 5.43 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025110647834862113		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.025110647834862113 | validation: 0.021027234715801486]
	TIME [epoch: 5.43 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022860483857949726		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.022860483857949726 | validation: 0.019766772256669347]
	TIME [epoch: 5.44 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02546688861019952		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.02546688861019952 | validation: 0.016278707887306603]
	TIME [epoch: 5.44 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023843887325508288		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.023843887325508288 | validation: 0.027588308955776582]
	TIME [epoch: 5.45 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02959208784851651		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.02959208784851651 | validation: 0.022278904081604652]
	TIME [epoch: 5.45 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026749159531882713		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.026749159531882713 | validation: 0.020382256007104978]
	TIME [epoch: 5.45 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024889493436256488		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.024889493436256488 | validation: 0.014482154520195273]
	TIME [epoch: 5.44 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1306.pth
	Model improved!!!
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348693043070445		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.02348693043070445 | validation: 0.016988451652657444]
	TIME [epoch: 5.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0235590179049536		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0235590179049536 | validation: 0.018724220656748326]
	TIME [epoch: 5.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02320636569699976		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.02320636569699976 | validation: 0.02006057800542362]
	TIME [epoch: 5.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024159593786088385		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.024159593786088385 | validation: 0.01824418887249374]
	TIME [epoch: 5.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025812791917272353		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.025812791917272353 | validation: 0.0209830021858285]
	TIME [epoch: 5.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023301021805638877		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.023301021805638877 | validation: 0.018712465594868388]
	TIME [epoch: 5.41 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02578724165850128		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02578724165850128 | validation: 0.02088656305485333]
	TIME [epoch: 5.45 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023737002339802585		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.023737002339802585 | validation: 0.01792216640632583]
	TIME [epoch: 5.43 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02440732997154317		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.02440732997154317 | validation: 0.023210440004226198]
	TIME [epoch: 5.45 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02430517259432891		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.02430517259432891 | validation: 0.019897177908131294]
	TIME [epoch: 5.43 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023917995603606884		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.023917995603606884 | validation: 0.016095569626132258]
	TIME [epoch: 5.45 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022667999235074447		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.022667999235074447 | validation: 0.02282524101550561]
	TIME [epoch: 5.43 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02546761907777105		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.02546761907777105 | validation: 0.01794210647844659]
	TIME [epoch: 5.45 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02469741773229992		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02469741773229992 | validation: 0.020716698229794158]
	TIME [epoch: 5.45 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024072570838414048		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.024072570838414048 | validation: 0.02091007751781307]
	TIME [epoch: 5.46 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024543310447373744		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.024543310447373744 | validation: 0.01769638299917958]
	TIME [epoch: 5.45 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02434053258673825		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02434053258673825 | validation: 0.0166788658782981]
	TIME [epoch: 5.44 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023601419537378536		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.023601419537378536 | validation: 0.015594766643822889]
	TIME [epoch: 5.43 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02554148501335013		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.02554148501335013 | validation: 0.0172003268193103]
	TIME [epoch: 5.45 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02334593458431952		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.02334593458431952 | validation: 0.01701440871751436]
	TIME [epoch: 5.45 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439488873522752		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.02439488873522752 | validation: 0.019607393446190086]
	TIME [epoch: 5.46 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023255696619063772		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.023255696619063772 | validation: 0.02010550437275116]
	TIME [epoch: 5.44 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024763109226121696		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.024763109226121696 | validation: 0.020050589040964874]
	TIME [epoch: 5.46 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023565021713946074		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.023565021713946074 | validation: 0.01929658653979398]
	TIME [epoch: 5.46 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024439275136407212		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.024439275136407212 | validation: 0.01904769425659211]
	TIME [epoch: 5.46 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02414586822464056		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.02414586822464056 | validation: 0.01809492117432859]
	TIME [epoch: 5.43 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023090372577482247		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.023090372577482247 | validation: 0.019387095393587052]
	TIME [epoch: 5.45 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02344468988618834		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02344468988618834 | validation: 0.01678677725929737]
	TIME [epoch: 5.44 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025715471998250982		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.025715471998250982 | validation: 0.015919240022751132]
	TIME [epoch: 5.45 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025027622162605664		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.025027622162605664 | validation: 0.018176028890579978]
	TIME [epoch: 5.44 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02476605649250967		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.02476605649250967 | validation: 0.018929123779229368]
	TIME [epoch: 5.46 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02407012412579875		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.02407012412579875 | validation: 0.019091990692125346]
	TIME [epoch: 5.44 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027377315750494388		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.027377315750494388 | validation: 0.01857522001812133]
	TIME [epoch: 5.45 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023725255577144912		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.023725255577144912 | validation: 0.019231712739267583]
	TIME [epoch: 5.44 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02335890666452416		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.02335890666452416 | validation: 0.015170355480197118]
	TIME [epoch: 5.45 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02282675987622909		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.02282675987622909 | validation: 0.014540565168063341]
	TIME [epoch: 5.44 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024232714742844205		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.024232714742844205 | validation: 0.017548266091966327]
	TIME [epoch: 5.44 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023552120030290283		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.023552120030290283 | validation: 0.018668914798562038]
	TIME [epoch: 5.45 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024379649316386174		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.024379649316386174 | validation: 0.018376607903987402]
	TIME [epoch: 5.45 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02428304596081813		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.02428304596081813 | validation: 0.020885095228991152]
	TIME [epoch: 5.45 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022426393873943264		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.022426393873943264 | validation: 0.01821883836024239]
	TIME [epoch: 5.45 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024502913610709248		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.024502913610709248 | validation: 0.017355131339549613]
	TIME [epoch: 5.45 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02327531730454448		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.02327531730454448 | validation: 0.020237102945585696]
	TIME [epoch: 5.45 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024143555544274938		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.024143555544274938 | validation: 0.019946090920884174]
	TIME [epoch: 5.44 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024473504388092763		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.024473504388092763 | validation: 0.017620526983950446]
	TIME [epoch: 5.45 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024850812794107578		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.024850812794107578 | validation: 0.020337275630539654]
	TIME [epoch: 5.45 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023738205494876756		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.023738205494876756 | validation: 0.017601072356722237]
	TIME [epoch: 5.44 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023973505272908088		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.023973505272908088 | validation: 0.018701388915618133]
	TIME [epoch: 5.45 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022743589307596998		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.022743589307596998 | validation: 0.018118902679104206]
	TIME [epoch: 5.45 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024307236622344998		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.024307236622344998 | validation: 0.01633691011721185]
	TIME [epoch: 5.46 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022610008859632585		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.022610008859632585 | validation: 0.017175753097716785]
	TIME [epoch: 5.45 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022488814401805373		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.022488814401805373 | validation: 0.02030758829436792]
	TIME [epoch: 5.44 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023778821757955117		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.023778821757955117 | validation: 0.018897151268906287]
	TIME [epoch: 5.44 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024470568384039968		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.024470568384039968 | validation: 0.02188978174836503]
	TIME [epoch: 5.44 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348728125384813		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.02348728125384813 | validation: 0.020253740139679843]
	TIME [epoch: 5.45 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024520819808348065		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.024520819808348065 | validation: 0.017276802484370524]
	TIME [epoch: 5.42 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021796838865902862		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.021796838865902862 | validation: 0.01717726773241751]
	TIME [epoch: 5.45 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023146309581438672		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.023146309581438672 | validation: 0.016461556366001634]
	TIME [epoch: 5.43 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022484039343893522		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.022484039343893522 | validation: 0.023027719333008612]
	TIME [epoch: 5.44 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02305011609872162		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02305011609872162 | validation: 0.01863407495907812]
	TIME [epoch: 5.43 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02427518243983271		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.02427518243983271 | validation: 0.01678730773203091]
	TIME [epoch: 5.44 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02462494238297982		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.02462494238297982 | validation: 0.021137587114659323]
	TIME [epoch: 5.48 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02304385290981732		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.02304385290981732 | validation: 0.01726820242774666]
	TIME [epoch: 5.45 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024071806781469342		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.024071806781469342 | validation: 0.020598612308119726]
	TIME [epoch: 5.46 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022449930541999272		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.022449930541999272 | validation: 0.016016467433083527]
	TIME [epoch: 5.45 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023247511287337562		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.023247511287337562 | validation: 0.017291900562182168]
	TIME [epoch: 5.46 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02368627651083203		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.02368627651083203 | validation: 0.01774631223224754]
	TIME [epoch: 5.44 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02422000025043377		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.02422000025043377 | validation: 0.020994771043396112]
	TIME [epoch: 5.44 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025181481733259112		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.025181481733259112 | validation: 0.017884472089857755]
	TIME [epoch: 5.44 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022973763022627353		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.022973763022627353 | validation: 0.019801313301566304]
	TIME [epoch: 5.42 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023683742587358073		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.023683742587358073 | validation: 0.02066329990840723]
	TIME [epoch: 5.44 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023488465975631083		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.023488465975631083 | validation: 0.019829596742190665]
	TIME [epoch: 5.42 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024272887056124053		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.024272887056124053 | validation: 0.014168242764392287]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1379.pth
	Model improved!!!
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023645436447959987		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.023645436447959987 | validation: 0.018552645530992385]
	TIME [epoch: 5.43 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022783163816846135		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.022783163816846135 | validation: 0.016931076749180134]
	TIME [epoch: 5.43 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022990041556892337		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.022990041556892337 | validation: 0.01590884646318857]
	TIME [epoch: 5.41 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022631860312538905		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.022631860312538905 | validation: 0.019930303607505796]
	TIME [epoch: 5.42 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026152600528332882		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.026152600528332882 | validation: 0.016972849519150537]
	TIME [epoch: 5.41 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023919768305261952		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.023919768305261952 | validation: 0.016880215078898596]
	TIME [epoch: 5.41 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0229789011843791		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.0229789011843791 | validation: 0.020828605295200482]
	TIME [epoch: 5.41 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023801492475223826		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.023801492475223826 | validation: 0.01600878427384489]
	TIME [epoch: 5.41 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022331878067320076		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.022331878067320076 | validation: 0.021561329518062822]
	TIME [epoch: 5.41 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023912153712312537		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.023912153712312537 | validation: 0.018357598846352886]
	TIME [epoch: 5.42 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02202869728597666		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.02202869728597666 | validation: 0.01794493915088735]
	TIME [epoch: 5.44 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023143886156837646		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.023143886156837646 | validation: 0.01819981884379845]
	TIME [epoch: 5.41 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021889626083443647		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.021889626083443647 | validation: 0.021042708956638796]
	TIME [epoch: 5.42 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025130837192683574		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.025130837192683574 | validation: 0.02026227095143921]
	TIME [epoch: 5.44 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023495162903504802		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.023495162903504802 | validation: 0.020585201642892098]
	TIME [epoch: 5.42 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024254075179975652		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.024254075179975652 | validation: 0.017345925810202136]
	TIME [epoch: 5.43 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024378608407170307		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.024378608407170307 | validation: 0.02035397861968842]
	TIME [epoch: 5.43 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023541340746121433		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.023541340746121433 | validation: 0.018694879057731918]
	TIME [epoch: 5.44 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02308481798429459		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.02308481798429459 | validation: 0.01720072535641746]
	TIME [epoch: 5.42 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023252945867089272		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.023252945867089272 | validation: 0.012880014412467167]
	TIME [epoch: 5.44 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1399.pth
	Model improved!!!
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024694468368489274		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.024694468368489274 | validation: 0.020854814903800263]
	TIME [epoch: 5.43 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02297138767589625		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.02297138767589625 | validation: 0.01999467333254809]
	TIME [epoch: 5.44 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348118820086347		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02348118820086347 | validation: 0.01730315895933154]
	TIME [epoch: 5.44 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022658233775472648		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.022658233775472648 | validation: 0.017577964343339532]
	TIME [epoch: 5.44 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02135998688544411		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.02135998688544411 | validation: 0.0180561852162485]
	TIME [epoch: 5.45 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02260867874893976		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.02260867874893976 | validation: 0.018291614561519415]
	TIME [epoch: 5.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022808403043715418		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.022808403043715418 | validation: 0.015247399763474634]
	TIME [epoch: 5.41 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02377714358622548		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.02377714358622548 | validation: 0.019914741168683858]
	TIME [epoch: 5.41 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022554550946293325		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.022554550946293325 | validation: 0.020409434951112904]
	TIME [epoch: 5.41 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024090298080345747		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.024090298080345747 | validation: 0.017931150363469435]
	TIME [epoch: 5.4 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02323213529701483		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.02323213529701483 | validation: 0.017305419471635244]
	TIME [epoch: 5.4 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023244003146718557		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.023244003146718557 | validation: 0.01732751976166875]
	TIME [epoch: 5.41 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022445296989485827		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.022445296989485827 | validation: 0.01856602703934312]
	TIME [epoch: 5.39 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263172860281469		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.02263172860281469 | validation: 0.017430946614124605]
	TIME [epoch: 5.41 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022931139524092235		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.022931139524092235 | validation: 0.021772305444805504]
	TIME [epoch: 5.39 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02344470443411044		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.02344470443411044 | validation: 0.01862373114526297]
	TIME [epoch: 5.41 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02220839242491022		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.02220839242491022 | validation: 0.018348017950903694]
	TIME [epoch: 5.39 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02275747125164532		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.02275747125164532 | validation: 0.023278517519142478]
	TIME [epoch: 5.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02292361120962778		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.02292361120962778 | validation: 0.015915710734553175]
	TIME [epoch: 5.39 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022685064560624166		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.022685064560624166 | validation: 0.021976849332778832]
	TIME [epoch: 5.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022223919838275093		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.022223919838275093 | validation: 0.016314329147035816]
	TIME [epoch: 5.39 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022908291161266674		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.022908291161266674 | validation: 0.021728272607453837]
	TIME [epoch: 5.4 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02303032883186437		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.02303032883186437 | validation: 0.018020960610867814]
	TIME [epoch: 5.4 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021172202722932836		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.021172202722932836 | validation: 0.022026927786351444]
	TIME [epoch: 5.42 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022041828610874267		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.022041828610874267 | validation: 0.021327353092477894]
	TIME [epoch: 5.39 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024143256834980833		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.024143256834980833 | validation: 0.01803826779832073]
	TIME [epoch: 5.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02241700597691879		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.02241700597691879 | validation: 0.016677359347490327]
	TIME [epoch: 5.39 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023271252313293626		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.023271252313293626 | validation: 0.019469606918458445]
	TIME [epoch: 5.4 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023771531175885593		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.023771531175885593 | validation: 0.014543136161609871]
	TIME [epoch: 5.42 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025324943907811974		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.025324943907811974 | validation: 0.018331554841648]
	TIME [epoch: 5.43 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02380355136090711		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.02380355136090711 | validation: 0.017506217671516007]
	TIME [epoch: 5.43 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022110967632884497		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.022110967632884497 | validation: 0.019229633967468646]
	TIME [epoch: 5.44 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02319877156126271		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.02319877156126271 | validation: 0.020393367799962578]
	TIME [epoch: 5.43 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022814544437308137		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.022814544437308137 | validation: 0.015830937770226566]
	TIME [epoch: 5.44 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0236579993331599		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.0236579993331599 | validation: 0.01563205587666332]
	TIME [epoch: 5.41 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021056991432566487		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.021056991432566487 | validation: 0.01770935390126165]
	TIME [epoch: 5.43 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022493011451686886		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.022493011451686886 | validation: 0.017317877943108434]
	TIME [epoch: 5.41 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02332500769424236		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.02332500769424236 | validation: 0.01752834273162267]
	TIME [epoch: 5.44 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022081459728160748		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.022081459728160748 | validation: 0.01673878162684826]
	TIME [epoch: 5.43 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022210955835953533		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.022210955835953533 | validation: 0.017947035805264806]
	TIME [epoch: 5.42 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02315934746828204		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.02315934746828204 | validation: 0.016421956977344488]
	TIME [epoch: 5.44 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02245840739359003		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.02245840739359003 | validation: 0.019092849866187346]
	TIME [epoch: 5.43 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02302283168660651		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.02302283168660651 | validation: 0.016521448532763938]
	TIME [epoch: 5.42 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021684186070679852		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.021684186070679852 | validation: 0.01606423051112721]
	TIME [epoch: 5.44 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02362363513645959		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.02362363513645959 | validation: 0.018858663584859443]
	TIME [epoch: 5.43 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022312460528284662		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.022312460528284662 | validation: 0.02241173118323019]
	TIME [epoch: 5.44 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02268998665027089		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.02268998665027089 | validation: 0.018671921650320257]
	TIME [epoch: 5.43 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022005547926058196		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.022005547926058196 | validation: 0.016364355399505426]
	TIME [epoch: 5.42 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022498557617869803		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.022498557617869803 | validation: 0.021837456793464396]
	TIME [epoch: 5.43 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021495711083202487		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.021495711083202487 | validation: 0.0188789459611992]
	TIME [epoch: 5.43 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02226944981125919		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.02226944981125919 | validation: 0.017465806963721586]
	TIME [epoch: 5.43 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021915776357388648		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.021915776357388648 | validation: 0.016019518024713058]
	TIME [epoch: 5.44 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02299447121319055		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.02299447121319055 | validation: 0.016866067054297978]
	TIME [epoch: 5.44 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02174642729585305		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.02174642729585305 | validation: 0.016552056379386348]
	TIME [epoch: 5.43 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022546747669586833		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.022546747669586833 | validation: 0.01920910922381948]
	TIME [epoch: 5.44 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023244882297277822		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.023244882297277822 | validation: 0.013968120580368004]
	TIME [epoch: 5.43 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023069436468146264		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.023069436468146264 | validation: 0.0184032842401895]
	TIME [epoch: 5.43 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023186050076237325		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.023186050076237325 | validation: 0.01854909277754252]
	TIME [epoch: 5.43 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02253466452240052		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.02253466452240052 | validation: 0.01781662288736855]
	TIME [epoch: 5.42 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022907884884046475		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.022907884884046475 | validation: 0.019521237273818888]
	TIME [epoch: 5.43 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022797863558203738		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.022797863558203738 | validation: 0.016480856018018637]
	TIME [epoch: 5.43 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02317175310794484		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.02317175310794484 | validation: 0.01860505205532556]
	TIME [epoch: 5.41 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022425262863713203		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.022425262863713203 | validation: 0.017255648030388187]
	TIME [epoch: 5.43 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022397726037584113		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.022397726037584113 | validation: 0.016390478263218496]
	TIME [epoch: 5.41 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022721399890586073		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.022721399890586073 | validation: 0.016137764919300412]
	TIME [epoch: 5.44 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02064720666660496		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.02064720666660496 | validation: 0.014390752227680964]
	TIME [epoch: 5.41 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021851233504670272		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.021851233504670272 | validation: 0.0168292521108053]
	TIME [epoch: 5.45 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02167284312509208		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.02167284312509208 | validation: 0.015962802497772828]
	TIME [epoch: 5.41 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022366826892572637		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.022366826892572637 | validation: 0.017403786812823296]
	TIME [epoch: 5.49 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02338325698882071		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.02338325698882071 | validation: 0.018420819054460576]
	TIME [epoch: 5.41 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022931692857301338		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.022931692857301338 | validation: 0.01730541965042817]
	TIME [epoch: 5.45 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02293441988975867		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.02293441988975867 | validation: 0.018800867723325915]
	TIME [epoch: 5.41 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022939461040556743		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.022939461040556743 | validation: 0.016316621057799657]
	TIME [epoch: 5.44 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02302211878953838		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.02302211878953838 | validation: 0.016117992107827918]
	TIME [epoch: 5.41 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02127125934802576		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.02127125934802576 | validation: 0.018674858673950728]
	TIME [epoch: 5.44 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021065451496383832		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.021065451496383832 | validation: 0.015068329346889975]
	TIME [epoch: 5.41 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023286142746772365		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.023286142746772365 | validation: 0.01662814218747467]
	TIME [epoch: 5.43 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022026605103414348		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.022026605103414348 | validation: 0.014957952075599468]
	TIME [epoch: 5.43 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021846168658408133		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.021846168658408133 | validation: 0.016545865193115274]
	TIME [epoch: 5.46 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02249650587353166		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.02249650587353166 | validation: 0.02133136215415801]
	TIME [epoch: 5.44 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021317651313066826		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.021317651313066826 | validation: 0.01727496812156596]
	TIME [epoch: 5.46 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02242054187103559		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.02242054187103559 | validation: 0.01853561318616148]
	TIME [epoch: 5.45 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022235145393961462		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.022235145393961462 | validation: 0.015170698296238634]
	TIME [epoch: 5.42 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02222243358638599		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.02222243358638599 | validation: 0.015483933640149605]
	TIME [epoch: 5.42 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02325421359251526		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.02325421359251526 | validation: 0.01684522433962035]
	TIME [epoch: 5.48 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022148465875275893		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.022148465875275893 | validation: 0.016944527009120466]
	TIME [epoch: 5.42 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021261514422495527		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.021261514422495527 | validation: 0.01821590841686817]
	TIME [epoch: 5.42 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020743425426654163		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.020743425426654163 | validation: 0.01755760874003098]
	TIME [epoch: 5.42 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021265632934002982		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.021265632934002982 | validation: 0.01609683385123328]
	TIME [epoch: 5.42 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023050465073349624		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.023050465073349624 | validation: 0.019082633244484355]
	TIME [epoch: 5.42 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021313031768145637		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.021313031768145637 | validation: 0.015847239540138575]
	TIME [epoch: 5.42 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022185184811522856		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.022185184811522856 | validation: 0.016215333307034115]
	TIME [epoch: 5.42 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023203382470425794		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.023203382470425794 | validation: 0.01508150045718848]
	TIME [epoch: 5.42 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02224924462925749		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.02224924462925749 | validation: 0.01946721292506829]
	TIME [epoch: 5.41 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021775439252334047		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.021775439252334047 | validation: 0.014357133186470573]
	TIME [epoch: 5.42 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023773258697184274		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.023773258697184274 | validation: 0.016182996876130628]
	TIME [epoch: 5.42 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021880080531551577		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.021880080531551577 | validation: 0.019112857491095606]
	TIME [epoch: 5.42 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02044400532560629		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.02044400532560629 | validation: 0.014846106352830008]
	TIME [epoch: 5.45 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02229664384292584		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.02229664384292584 | validation: 0.015438479773934111]
	TIME [epoch: 5.42 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022213231354725048		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.022213231354725048 | validation: 0.01575534568794645]
	TIME [epoch: 5.42 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02116687851257109		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.02116687851257109 | validation: 0.015894641785999874]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_155337/states/model_phi1_3a_v_mmd1_1500.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5819.494 seconds.
