Args:
Namespace(name='model_phi1_2a_v_mmd1', outdir='out/model_training/model_phi1_2a_v_mmd1', training_data='data/training_data/basic/data_phi1_2a/training', validation_data='data/training_data/basic/data_phi1_2a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4215285130

Training model...

Saving initial model state to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.335228912162766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.335228912162766 | validation: 5.497079337866372]
	TIME [epoch: 105 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.124948708081784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.124948708081784 | validation: 4.743430322904376]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.698417758245764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.698417758245764 | validation: 4.504885648018892]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.435458311526126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435458311526126 | validation: 4.0532780559147765]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.105661249176276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.105661249176276 | validation: 3.794824117455447]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.035545555092471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.035545555092471 | validation: 4.093474764298732]
	TIME [epoch: 1.65 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.821443093454899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821443093454899 | validation: 3.6308373183098546]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.583561616627425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.583561616627425 | validation: 3.9181083012889633]
	TIME [epoch: 1.66 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.956447227356696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.956447227356696 | validation: 4.12197046380322]
	TIME [epoch: 1.64 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.71982906755615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.71982906755615 | validation: 3.409501343133897]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.371514026347844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.371514026347844 | validation: 3.30456170795369]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.2460602081674095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2460602081674095 | validation: 3.209679864605297]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.1146182858302085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1146182858302085 | validation: 3.088796660991614]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.00428007648678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.00428007648678 | validation: 3.0842315580314126]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9245598652761147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9245598652761147 | validation: 2.9806292670668633]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.894771566129463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.894771566129463 | validation: 2.975787925973906]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.946555967017077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.946555967017077 | validation: 2.9322549181573656]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.843541646213949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.843541646213949 | validation: 3.042623835218304]
	TIME [epoch: 1.63 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8249288468183185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8249288468183185 | validation: 2.8138429531069518]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6796706668119095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6796706668119095 | validation: 2.796571379901439]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.599758151958599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.599758151958599 | validation: 2.828400571034464]
	TIME [epoch: 1.63 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6866618850440913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6866618850440913 | validation: 2.721338241534476]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6459676874725377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6459676874725377 | validation: 2.691064407898205]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5209244233609196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5209244233609196 | validation: 2.5828619768888403]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4706610982537356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4706610982537356 | validation: 2.5827449274201038]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4141080161602586		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.4141080161602586 | validation: 2.413361420184066]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.337509186952411		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.337509186952411 | validation: 2.5097085692172274]
	TIME [epoch: 1.64 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2563506352570686		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.2563506352570686 | validation: 2.4378366425915896]
	TIME [epoch: 1.64 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3340193963230202		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.3340193963230202 | validation: 2.4366600637907023]
	TIME [epoch: 1.63 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.226969332711372		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.226969332711372 | validation: 2.175904860692603]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.074958078654072		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.074958078654072 | validation: 2.2044272480177076]
	TIME [epoch: 1.64 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.004821091008731		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.004821091008731 | validation: 2.026568487327393]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.064726218509327		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.064726218509327 | validation: 2.159132374673153]
	TIME [epoch: 1.63 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.978240086924655		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.978240086924655 | validation: 1.9895187874065934]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9160063351940306		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.9160063351940306 | validation: 1.7532798416913662]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.755444606173834		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.755444606173834 | validation: 1.7289748660155047]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7289513938091168		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7289513938091168 | validation: 1.7440665382240468]
	TIME [epoch: 1.63 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7199787194741003		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7199787194741003 | validation: 1.7137931156840969]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6627913717021223		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.6627913717021223 | validation: 1.5894117771518124]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5904922580346124		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.5904922580346124 | validation: 1.5340973903821564]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.490754858995861		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.490754858995861 | validation: 1.4926243875384435]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4903629654711081		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4903629654711081 | validation: 1.5348144619965867]
	TIME [epoch: 1.65 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.458894885303644		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.458894885303644 | validation: 1.596582366073355]
	TIME [epoch: 1.65 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5937106103089742		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.5937106103089742 | validation: 1.6750116775462793]
	TIME [epoch: 1.66 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.584531774715583		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.584531774715583 | validation: 1.4505457028685542]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5045372557273229		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.5045372557273229 | validation: 1.5401781382742223]
	TIME [epoch: 1.64 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5368995360416509		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.5368995360416509 | validation: 1.5940272405844615]
	TIME [epoch: 1.64 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4828158357437504		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.4828158357437504 | validation: 1.368896695642487]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.355167129450467		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.355167129450467 | validation: 1.3146702996223754]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3509927735148946		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.3509927735148946 | validation: 1.4262595263974074]
	TIME [epoch: 1.64 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.351303205824782		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.351303205824782 | validation: 1.3886026514486465]
	TIME [epoch: 1.63 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3559941758600111		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3559941758600111 | validation: 1.302453071985897]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4238353308121292		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.4238353308121292 | validation: 1.2528260328470264]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2775674401283617		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2775674401283617 | validation: 1.1884742713435403]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2340166391381202		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.2340166391381202 | validation: 1.158863283507005]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2326074492243286		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2326074492243286 | validation: 1.2102244769206716]
	TIME [epoch: 1.64 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.282666028620027		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.282666028620027 | validation: 1.1150396375508267]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1915005965777987		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1915005965777987 | validation: 1.2760927741886423]
	TIME [epoch: 1.64 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3266954078022088		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3266954078022088 | validation: 1.1943472160671174]
	TIME [epoch: 1.64 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2002623001277932		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2002623001277932 | validation: 1.11799593133347]
	TIME [epoch: 1.65 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2521396318457056		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2521396318457056 | validation: 1.0758287077394701]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.170099079847088		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.170099079847088 | validation: 1.1589912632409198]
	TIME [epoch: 1.66 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1597420795378528		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.1597420795378528 | validation: 1.0895795533138481]
	TIME [epoch: 1.64 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1510143931365695		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.1510143931365695 | validation: 1.1520779795517804]
	TIME [epoch: 1.64 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1381752010254003		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1381752010254003 | validation: 1.0026760747882608]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0471855599330668		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.0471855599330668 | validation: 1.0185303403980672]
	TIME [epoch: 1.64 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2703811327555263		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2703811327555263 | validation: 1.2106452118991182]
	TIME [epoch: 1.64 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2043670989815625		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2043670989815625 | validation: 1.2206072633060572]
	TIME [epoch: 1.64 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2017801142697873		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2017801142697873 | validation: 1.001157290366633]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0449311207262348		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.0449311207262348 | validation: 0.9675575115109908]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1467639832245995		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1467639832245995 | validation: 0.9569052869815153]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0265985244915394		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0265985244915394 | validation: 0.9184472226935786]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0030073633018817		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.0030073633018817 | validation: 0.9160026793226719]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0149916603154714		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0149916603154714 | validation: 0.9778301539026801]
	TIME [epoch: 1.63 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0615654854284542		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0615654854284542 | validation: 0.8992023002970146]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9811760483206609		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.9811760483206609 | validation: 0.9184709463854901]
	TIME [epoch: 1.64 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9968697047095094		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9968697047095094 | validation: 0.8166461716482385]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8909754939246844		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8909754939246844 | validation: 0.797985330779634]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9036055313145026		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.9036055313145026 | validation: 1.0860378368377006]
	TIME [epoch: 1.66 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0548773669924147		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.0548773669924147 | validation: 0.7808167445866794]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9004847597877377		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.9004847597877377 | validation: 1.1025659473259928]
	TIME [epoch: 1.64 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0928722646871598		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0928722646871598 | validation: 0.994360926540268]
	TIME [epoch: 1.64 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9886639249068929		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.9886639249068929 | validation: 0.7599662886197016]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8177852353394561		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8177852353394561 | validation: 0.7502081631626383]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7916857463836745		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7916857463836745 | validation: 0.7641492252388998]
	TIME [epoch: 1.63 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9195658809494641		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9195658809494641 | validation: 0.7637567141357597]
	TIME [epoch: 1.63 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9102775179665152		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9102775179665152 | validation: 0.8740304512915991]
	TIME [epoch: 1.64 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.949496859314163		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.949496859314163 | validation: 0.7417934946427418]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8383734938854228		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8383734938854228 | validation: 0.6937617211447722]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7548203785770149		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7548203785770149 | validation: 0.7173158660996943]
	TIME [epoch: 1.64 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8323986896559893		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8323986896559893 | validation: 0.7356790171360988]
	TIME [epoch: 1.64 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7634758268213824		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7634758268213824 | validation: 0.8091254511506165]
	TIME [epoch: 1.64 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.856359697942169		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.856359697942169 | validation: 0.6345889131133715]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6716132504186846		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6716132504186846 | validation: 0.6329598542654375]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9030304170702881		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9030304170702881 | validation: 0.6506003716376684]
	TIME [epoch: 1.65 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8465895526301854		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.8465895526301854 | validation: 0.6990301371345213]
	TIME [epoch: 1.65 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7917353512347451		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7917353512347451 | validation: 0.5918087774997515]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6255058031531411		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6255058031531411 | validation: 0.5560209824435289]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6107299033017636		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6107299033017636 | validation: 0.6826497452092605]
	TIME [epoch: 1.64 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7516097797467909		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7516097797467909 | validation: 0.7252142885155691]
	TIME [epoch: 1.64 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6895339642311968		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6895339642311968 | validation: 0.5425081431231918]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6115763289638594		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6115763289638594 | validation: 0.6078152398499799]
	TIME [epoch: 1.64 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6787136647098851		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6787136647098851 | validation: 0.48599999327489274]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5682793144062954		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5682793144062954 | validation: 0.6209515549805416]
	TIME [epoch: 1.63 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5756965403635056		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5756965403635056 | validation: 0.460600818150354]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5069224175976481		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5069224175976481 | validation: 0.7313792712867826]
	TIME [epoch: 1.63 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8232677585196189		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.8232677585196189 | validation: 0.6755813334570758]
	TIME [epoch: 1.63 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6329245984693233		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6329245984693233 | validation: 0.4602011521752397]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4825939288781137		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.4825939288781137 | validation: 0.4373002797226855]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45370541233546224		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.45370541233546224 | validation: 0.4729493516407246]
	TIME [epoch: 1.64 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5862572448322929		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.5862572448322929 | validation: 0.5732977130552263]
	TIME [epoch: 1.64 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6842945260809956		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6842945260809956 | validation: 0.4846681964420942]
	TIME [epoch: 1.64 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5484923824294186		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.5484923824294186 | validation: 0.48568927543172785]
	TIME [epoch: 1.67 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5371038745934528		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.5371038745934528 | validation: 0.4897071902142533]
	TIME [epoch: 1.64 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5026793505171776		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.5026793505171776 | validation: 0.3795067271424814]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40681430998517787		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.40681430998517787 | validation: 0.3714610403785047]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5369690515636477		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5369690515636477 | validation: 0.5524723712379913]
	TIME [epoch: 1.64 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.520374765369994		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.520374765369994 | validation: 0.4009972328162638]
	TIME [epoch: 1.63 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4263684822575027		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.4263684822575027 | validation: 0.47383029231146756]
	TIME [epoch: 1.64 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4713122614649936		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4713122614649936 | validation: 0.5577398974717535]
	TIME [epoch: 1.63 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5696288196098039		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.5696288196098039 | validation: 0.6437313713910374]
	TIME [epoch: 1.63 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5218089633121206		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5218089633121206 | validation: 0.371755078749999]
	TIME [epoch: 1.64 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36463088961865336		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.36463088961865336 | validation: 0.34658269905139755]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38815891577167255		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.38815891577167255 | validation: 0.38788195891178234]
	TIME [epoch: 1.64 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4015605959986267		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4015605959986267 | validation: 0.419156558258156]
	TIME [epoch: 1.64 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41724992701753644		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.41724992701753644 | validation: 0.3274777805415331]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42228049417073266		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.42228049417073266 | validation: 0.3295110594558773]
	TIME [epoch: 1.64 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3395098105201533		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.3395098105201533 | validation: 0.3243027152531175]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3207386009645914		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.3207386009645914 | validation: 0.30972734329300255]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4060645530672279		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4060645530672279 | validation: 0.6525487780221697]
	TIME [epoch: 1.63 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2098792413895587		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.2098792413895587 | validation: 1.703538095869756]
	TIME [epoch: 1.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5977680848541342		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.5977680848541342 | validation: 0.9856811321977651]
	TIME [epoch: 1.64 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9926355975586165		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9926355975586165 | validation: 0.6146746549640839]
	TIME [epoch: 1.65 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5816446994615163		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5816446994615163 | validation: 0.5365184019325486]
	TIME [epoch: 1.64 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4525190791206418		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.4525190791206418 | validation: 0.43181617139726003]
	TIME [epoch: 1.63 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3859990301384027		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3859990301384027 | validation: 0.4232208746165686]
	TIME [epoch: 1.63 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3838835208434784		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.3838835208434784 | validation: 0.37921530849030965]
	TIME [epoch: 1.63 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35246807764634647		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.35246807764634647 | validation: 0.3561734840137085]
	TIME [epoch: 1.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.321593868055484		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.321593868055484 | validation: 0.3613339257713808]
	TIME [epoch: 1.63 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3591444011710884		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3591444011710884 | validation: 0.43384726028147985]
	TIME [epoch: 1.63 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39673306341542613		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.39673306341542613 | validation: 0.3839817339645135]
	TIME [epoch: 1.63 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35730904175076406		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.35730904175076406 | validation: 0.331865974029536]
	TIME [epoch: 1.63 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3184369947685606		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3184369947685606 | validation: 0.37057711891813694]
	TIME [epoch: 1.64 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40074930254099905		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.40074930254099905 | validation: 0.6817244685373165]
	TIME [epoch: 1.63 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8762990145379685		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8762990145379685 | validation: 0.3593119852331741]
	TIME [epoch: 1.63 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4643115994248338		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.4643115994248338 | validation: 0.321588407657116]
	TIME [epoch: 1.63 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3392257336427976		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.3392257336427976 | validation: 0.31550375548578047]
	TIME [epoch: 1.63 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3037718428754329		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3037718428754329 | validation: 0.29777594168685984]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29556626287274135		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.29556626287274135 | validation: 0.29441032622643437]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3224453768790113		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.3224453768790113 | validation: 0.3329052979318263]
	TIME [epoch: 1.64 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3422811863330778		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3422811863330778 | validation: 0.32158872137766176]
	TIME [epoch: 1.64 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3356018916481625		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.3356018916481625 | validation: 0.37008645270981194]
	TIME [epoch: 1.66 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31312575671532417		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.31312575671532417 | validation: 0.3218275815693108]
	TIME [epoch: 1.64 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33938926808732606		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.33938926808732606 | validation: 0.36624977046051277]
	TIME [epoch: 1.65 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3293257018030748		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3293257018030748 | validation: 0.2856041689897681]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28449342700699654		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.28449342700699654 | validation: 0.30077229662797045]
	TIME [epoch: 1.64 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42940704970793353		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.42940704970793353 | validation: 0.2729645352245242]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26683579785272304		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.26683579785272304 | validation: 0.28565869568886504]
	TIME [epoch: 1.64 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26056486972935333		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.26056486972935333 | validation: 0.23381325451808413]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25202341788230326		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.25202341788230326 | validation: 0.2704350408976765]
	TIME [epoch: 1.64 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3389594195154231		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3389594195154231 | validation: 0.5932878153336526]
	TIME [epoch: 1.64 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4243787772361509		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4243787772361509 | validation: 0.34552392748620525]
	TIME [epoch: 1.64 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38173430005914577		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.38173430005914577 | validation: 0.277012361619783]
	TIME [epoch: 1.64 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2774407549531548		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2774407549531548 | validation: 0.290104190029672]
	TIME [epoch: 1.65 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29627680516642235		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.29627680516642235 | validation: 0.4350427789876136]
	TIME [epoch: 1.64 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3081887347534375		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3081887347534375 | validation: 0.31816435103095486]
	TIME [epoch: 1.64 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4147029014067175		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4147029014067175 | validation: 0.22628303368527428]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23726440688413458		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.23726440688413458 | validation: 0.2429495959652166]
	TIME [epoch: 1.64 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25742632445535896		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.25742632445535896 | validation: 0.3688054109578558]
	TIME [epoch: 1.64 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43003552891501906		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.43003552891501906 | validation: 0.25028420201965645]
	TIME [epoch: 1.64 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2750272476318135		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2750272476318135 | validation: 0.2447889340117441]
	TIME [epoch: 1.64 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23290771340953986		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.23290771340953986 | validation: 0.27791225401282244]
	TIME [epoch: 1.64 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2522227056611171		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.2522227056611171 | validation: 0.22041420106140983]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2390438415336696		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.2390438415336696 | validation: 0.23369870804240828]
	TIME [epoch: 1.65 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23382609850157537		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.23382609850157537 | validation: 0.3058354796298015]
	TIME [epoch: 1.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2869821090039809		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2869821090039809 | validation: 0.46645122644652554]
	TIME [epoch: 1.64 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35503797157097444		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.35503797157097444 | validation: 0.25992117616178306]
	TIME [epoch: 1.64 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2612743331370039		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.2612743331370039 | validation: 0.24436474375661754]
	TIME [epoch: 1.64 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25942745744088025		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.25942745744088025 | validation: 0.4283124911170342]
	TIME [epoch: 1.64 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3360614094772443		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3360614094772443 | validation: 0.2275858233575029]
	TIME [epoch: 1.64 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24614410109417295		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.24614410109417295 | validation: 0.24816749632949187]
	TIME [epoch: 1.64 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23699063858724212		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.23699063858724212 | validation: 0.22753612961631442]
	TIME [epoch: 1.64 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22422652701122536		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.22422652701122536 | validation: 0.30594099929356605]
	TIME [epoch: 1.64 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27423012999697505		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.27423012999697505 | validation: 0.256409985446343]
	TIME [epoch: 1.64 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23100852215535117		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.23100852215535117 | validation: 0.22496006803710708]
	TIME [epoch: 1.63 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24254490955855085		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.24254490955855085 | validation: 0.25303647272356544]
	TIME [epoch: 1.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2763262713821695		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.2763262713821695 | validation: 0.3612286821481123]
	TIME [epoch: 1.64 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24877739089857917		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.24877739089857917 | validation: 0.2761936360909633]
	TIME [epoch: 1.64 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2685190486173923		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2685190486173923 | validation: 0.20104521075884918]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25663310943109713		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.25663310943109713 | validation: 0.27377039323776997]
	TIME [epoch: 1.63 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22323180284252445		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.22323180284252445 | validation: 0.2579823844942248]
	TIME [epoch: 1.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2726246666983899		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.2726246666983899 | validation: 0.2230914106178779]
	TIME [epoch: 1.63 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25888750398189986		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.25888750398189986 | validation: 0.2400904119471405]
	TIME [epoch: 1.63 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2397042726032843		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.2397042726032843 | validation: 0.20010200188653116]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2129510627126059		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.2129510627126059 | validation: 0.2863176891361707]
	TIME [epoch: 1.66 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25042535270617666		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.25042535270617666 | validation: 0.20966749675100155]
	TIME [epoch: 1.65 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21245740031089055		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.21245740031089055 | validation: 0.23167237906911128]
	TIME [epoch: 1.64 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25847375335981926		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.25847375335981926 | validation: 0.22706161472031133]
	TIME [epoch: 1.64 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24455256569136358		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.24455256569136358 | validation: 0.20191427945786927]
	TIME [epoch: 1.64 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2034814853082566		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.2034814853082566 | validation: 0.19992806924953865]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2091515511754476		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.2091515511754476 | validation: 0.2135439878565126]
	TIME [epoch: 105 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23913795966910267		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.23913795966910267 | validation: 0.19782600262404407]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19059990605568633		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.19059990605568633 | validation: 0.20473621245921883]
	TIME [epoch: 3.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2220566959566164		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2220566959566164 | validation: 0.20342895842412362]
	TIME [epoch: 3.17 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20706481477668376		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.20706481477668376 | validation: 0.24354052948753432]
	TIME [epoch: 3.18 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24427292528377076		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.24427292528377076 | validation: 0.1759546214397122]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17883856548194738		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.17883856548194738 | validation: 0.21333768927594388]
	TIME [epoch: 3.19 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3496655880932304		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.3496655880932304 | validation: 0.4034047810233534]
	TIME [epoch: 3.19 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31728591250733296		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.31728591250733296 | validation: 0.18748268305151167]
	TIME [epoch: 3.21 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18917258079263		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.18917258079263 | validation: 0.1703988804098282]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17295985638380298		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.17295985638380298 | validation: 0.17826277318819211]
	TIME [epoch: 3.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18234682767046723		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.18234682767046723 | validation: 0.20705535721245988]
	TIME [epoch: 3.18 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22889786878552337		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.22889786878552337 | validation: 0.17869626759546092]
	TIME [epoch: 3.18 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18169038918751743		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.18169038918751743 | validation: 0.22635265182425926]
	TIME [epoch: 3.18 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18884297344328688		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.18884297344328688 | validation: 0.20794614835633687]
	TIME [epoch: 3.18 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2295285813705562		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2295285813705562 | validation: 0.2259064784351078]
	TIME [epoch: 3.17 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24225888454830968		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.24225888454830968 | validation: 0.1667978568890247]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17063528722333132		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.17063528722333132 | validation: 0.1931557128049526]
	TIME [epoch: 3.17 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18433537122249966		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.18433537122249966 | validation: 0.1859618304934561]
	TIME [epoch: 3.18 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19278664985275323		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.19278664985275323 | validation: 0.359380175545375]
	TIME [epoch: 3.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25085529048891037		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.25085529048891037 | validation: 0.1640016535327732]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17305743183316355		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.17305743183316355 | validation: 0.17945894231889703]
	TIME [epoch: 3.18 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1786816263872098		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.1786816263872098 | validation: 0.16291331410025814]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16240149979217394		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.16240149979217394 | validation: 0.16032091491668254]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16274594527281164		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.16274594527281164 | validation: 0.25969350162738625]
	TIME [epoch: 3.18 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21329511357175707		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.21329511357175707 | validation: 0.2883777095786104]
	TIME [epoch: 3.18 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21836373337876894		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.21836373337876894 | validation: 0.1679139395325806]
	TIME [epoch: 3.18 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19627577103078442		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.19627577103078442 | validation: 0.1607327065711072]
	TIME [epoch: 3.17 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15405306590101384		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.15405306590101384 | validation: 0.16760301257295646]
	TIME [epoch: 3.18 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1812191547190808		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.1812191547190808 | validation: 0.2008142748577445]
	TIME [epoch: 3.18 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21868014235639333		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.21868014235639333 | validation: 0.19481812671145332]
	TIME [epoch: 3.19 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18392919361197804		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.18392919361197804 | validation: 0.1580535789745753]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15786118382292913		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.15786118382292913 | validation: 0.17411683137068953]
	TIME [epoch: 3.19 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15820232265289128		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.15820232265289128 | validation: 0.15433290271979405]
	TIME [epoch: 3.17 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16670368679309674		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.16670368679309674 | validation: 0.19539675794560676]
	TIME [epoch: 3.19 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16955871946016304		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.16955871946016304 | validation: 0.18147278620856377]
	TIME [epoch: 3.18 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1878592022783635		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1878592022783635 | validation: 0.20768848982925633]
	TIME [epoch: 3.18 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2098650201321492		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2098650201321492 | validation: 0.1458250332701117]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1497869332763428		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1497869332763428 | validation: 0.18510770674576255]
	TIME [epoch: 3.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19786400481450145		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.19786400481450145 | validation: 0.2078417866128045]
	TIME [epoch: 3.18 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21562092072058958		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.21562092072058958 | validation: 0.1558811255301581]
	TIME [epoch: 3.18 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15603874601806472		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.15603874601806472 | validation: 0.16944317423398747]
	TIME [epoch: 3.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1760638553992968		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.1760638553992968 | validation: 0.15732538007980312]
	TIME [epoch: 3.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14879715385292733		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.14879715385292733 | validation: 0.15316341091926888]
	TIME [epoch: 3.18 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15447093320359676		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.15447093320359676 | validation: 0.14485930612953637]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1592410132536477		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1592410132536477 | validation: 0.16277191336036467]
	TIME [epoch: 3.19 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1596280156648969		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1596280156648969 | validation: 0.1511557063645479]
	TIME [epoch: 3.18 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14544494232276223		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.14544494232276223 | validation: 0.16250945849674656]
	TIME [epoch: 3.18 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1723252727552616		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.1723252727552616 | validation: 0.185402875126997]
	TIME [epoch: 3.18 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16424044195810578		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.16424044195810578 | validation: 0.15879767976759965]
	TIME [epoch: 3.19 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1570817711019591		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.1570817711019591 | validation: 0.15518257928307988]
	TIME [epoch: 3.18 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1736375840906605		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1736375840906605 | validation: 0.24141521877697267]
	TIME [epoch: 3.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18615379832343937		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.18615379832343937 | validation: 0.14009692002915972]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14253470719096825		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.14253470719096825 | validation: 0.14158157722887096]
	TIME [epoch: 3.22 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13815182711688362		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.13815182711688362 | validation: 0.19202791550870707]
	TIME [epoch: 3.19 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21137523382060408		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.21137523382060408 | validation: 0.1453712706708977]
	TIME [epoch: 3.19 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13897558332163867		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.13897558332163867 | validation: 0.1716186158468319]
	TIME [epoch: 3.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14600044093732562		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.14600044093732562 | validation: 0.28961751058171553]
	TIME [epoch: 3.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20107476229582993		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.20107476229582993 | validation: 0.13626701099546076]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14498064951786926		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.14498064951786926 | validation: 0.16439945687862312]
	TIME [epoch: 3.18 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1426113338893548		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1426113338893548 | validation: 0.16078185425919608]
	TIME [epoch: 3.19 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14115966937785335		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.14115966937785335 | validation: 0.13515358295430482]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12933456384536024		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.12933456384536024 | validation: 0.1623822800728298]
	TIME [epoch: 3.19 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.152855986374857		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.152855986374857 | validation: 0.16249655394627316]
	TIME [epoch: 3.19 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14088560141482015		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.14088560141482015 | validation: 0.1500834111165534]
	TIME [epoch: 3.21 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1393848567181899		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1393848567181899 | validation: 0.1454228842394108]
	TIME [epoch: 3.19 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1321577764369935		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1321577764369935 | validation: 0.12677694101883694]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1303328271489685		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1303328271489685 | validation: 0.15306896074116272]
	TIME [epoch: 3.21 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16805519248118733		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.16805519248118733 | validation: 0.16627910257889794]
	TIME [epoch: 3.19 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15502371583461425		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.15502371583461425 | validation: 0.1686809066092208]
	TIME [epoch: 3.21 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14360294541224267		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.14360294541224267 | validation: 0.1688092603193539]
	TIME [epoch: 3.19 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1584359395393769		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.1584359395393769 | validation: 0.13812213649113428]
	TIME [epoch: 3.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12085219593177632		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.12085219593177632 | validation: 0.1310936533116851]
	TIME [epoch: 3.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12477524621163795		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.12477524621163795 | validation: 0.1263784268471413]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12756419802702085		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.12756419802702085 | validation: 0.1691660653613105]
	TIME [epoch: 3.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15014378763300826		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.15014378763300826 | validation: 0.1516782255941726]
	TIME [epoch: 3.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13632518838050442		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.13632518838050442 | validation: 0.14921650985308746]
	TIME [epoch: 3.23 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18021533393133843		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.18021533393133843 | validation: 0.14930335664507355]
	TIME [epoch: 3.21 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4697620156259153		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.4697620156259153 | validation: 0.25332831581284654]
	TIME [epoch: 3.21 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2552503393588521		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.2552503393588521 | validation: 0.18204669840348853]
	TIME [epoch: 3.19 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1587801280663959		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1587801280663959 | validation: 0.17602892589614844]
	TIME [epoch: 3.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13252656840078292		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.13252656840078292 | validation: 0.1357006149137405]
	TIME [epoch: 3.19 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11689181417916855		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.11689181417916855 | validation: 0.12832798031147497]
	TIME [epoch: 3.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11637645513141817		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.11637645513141817 | validation: 0.17831229979006757]
	TIME [epoch: 3.19 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1390357122370389		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1390357122370389 | validation: 0.1326410274592461]
	TIME [epoch: 3.21 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12336685168000269		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.12336685168000269 | validation: 0.12369999281687812]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11763651711942041		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.11763651711942041 | validation: 0.12320331443122662]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11532944980352552		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.11532944980352552 | validation: 0.14574202728947974]
	TIME [epoch: 3.21 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1317798750452973		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1317798750452973 | validation: 0.12973150448442125]
	TIME [epoch: 3.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1156635661592136		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1156635661592136 | validation: 0.23617814272142923]
	TIME [epoch: 3.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1764156114408639		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1764156114408639 | validation: 0.16053012570352346]
	TIME [epoch: 3.18 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15779621431972352		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.15779621431972352 | validation: 0.11769774298576952]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12002041705067353		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.12002041705067353 | validation: 0.12296544255661108]
	TIME [epoch: 3.19 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.125353096956814		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.125353096956814 | validation: 0.12389941781270064]
	TIME [epoch: 3.19 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1082481123865648		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.1082481123865648 | validation: 0.12066140444263006]
	TIME [epoch: 3.18 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12710908824970996		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.12710908824970996 | validation: 0.15593619243340617]
	TIME [epoch: 3.18 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15872153205683934		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.15872153205683934 | validation: 0.11571843607570097]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1299242275960562		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.1299242275960562 | validation: 0.19194550655322554]
	TIME [epoch: 3.18 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13778714341935583		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.13778714341935583 | validation: 0.149041841427942]
	TIME [epoch: 3.19 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1310354710237116		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1310354710237116 | validation: 0.14054600017698468]
	TIME [epoch: 3.21 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1309746608566022		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.1309746608566022 | validation: 0.12327706072284231]
	TIME [epoch: 3.19 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1120071573166233		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1120071573166233 | validation: 0.16912212706946345]
	TIME [epoch: 3.18 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12636242915896223		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.12636242915896223 | validation: 0.12121040194572577]
	TIME [epoch: 3.17 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10733017811242494		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.10733017811242494 | validation: 0.11364431680950325]
	TIME [epoch: 3.17 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11217194378272641		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.11217194378272641 | validation: 0.11993668305956302]
	TIME [epoch: 3.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10869525792923435		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10869525792923435 | validation: 0.10756316348046689]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10832401040202275		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.10832401040202275 | validation: 0.13629181113148484]
	TIME [epoch: 3.19 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14027543804865983		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.14027543804865983 | validation: 0.117870326316016]
	TIME [epoch: 3.19 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10915653644969223		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.10915653644969223 | validation: 0.1192707272624646]
	TIME [epoch: 3.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10783706257043296		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10783706257043296 | validation: 0.1295565533310575]
	TIME [epoch: 3.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12201012369951125		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.12201012369951125 | validation: 0.11630259225473001]
	TIME [epoch: 3.21 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12515098457007137		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.12515098457007137 | validation: 0.1542053799265426]
	TIME [epoch: 3.19 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12008380778380962		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.12008380778380962 | validation: 0.11495140889477673]
	TIME [epoch: 3.19 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10913440772649968		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10913440772649968 | validation: 0.11444367117931908]
	TIME [epoch: 3.18 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1141830500252092		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.1141830500252092 | validation: 0.108162738640638]
	TIME [epoch: 3.19 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10102567284592417		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.10102567284592417 | validation: 0.10823855720438705]
	TIME [epoch: 3.19 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10350548321731795		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.10350548321731795 | validation: 0.10935922187842446]
	TIME [epoch: 3.19 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10653792228952993		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.10653792228952993 | validation: 0.11275262956419015]
	TIME [epoch: 3.18 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1337435067018263		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.1337435067018263 | validation: 0.12229172958453778]
	TIME [epoch: 3.19 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11041252922936512		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.11041252922936512 | validation: 0.10454969702522186]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10184193030060773		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.10184193030060773 | validation: 0.11768895360854437]
	TIME [epoch: 3.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11865955832256284		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.11865955832256284 | validation: 0.11099065026486672]
	TIME [epoch: 3.19 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10512421216227573		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.10512421216227573 | validation: 0.11358162937508152]
	TIME [epoch: 3.19 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09912551427302277		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.09912551427302277 | validation: 0.12618185569753113]
	TIME [epoch: 3.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12582798774456924		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.12582798774456924 | validation: 0.16602043766519523]
	TIME [epoch: 3.18 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14205292540031839		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.14205292540031839 | validation: 0.1326559513033094]
	TIME [epoch: 3.18 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11716303184842894		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.11716303184842894 | validation: 0.12518867386737856]
	TIME [epoch: 3.18 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10494496814182566		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.10494496814182566 | validation: 0.1110536556952221]
	TIME [epoch: 3.19 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12792536806808213		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.12792536806808213 | validation: 0.10727265719400547]
	TIME [epoch: 3.18 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10578635854321113		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.10578635854321113 | validation: 0.10767003993658111]
	TIME [epoch: 3.19 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10147239870604215		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.10147239870604215 | validation: 0.1160523441905525]
	TIME [epoch: 3.18 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09771907238532318		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.09771907238532318 | validation: 0.10276039911662097]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10119584890689115		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.10119584890689115 | validation: 0.11927537174317933]
	TIME [epoch: 3.19 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10246457326009833		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.10246457326009833 | validation: 0.12760224478928553]
	TIME [epoch: 3.19 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10723227767625926		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.10723227767625926 | validation: 0.10961938944115027]
	TIME [epoch: 3.21 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10063011026955543		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.10063011026955543 | validation: 0.10553131841351107]
	TIME [epoch: 3.21 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09965734539092107		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.09965734539092107 | validation: 0.1208347892091513]
	TIME [epoch: 3.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11382535035231464		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.11382535035231464 | validation: 0.11697317710735658]
	TIME [epoch: 3.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0983364609981724		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.0983364609981724 | validation: 0.10299612020959602]
	TIME [epoch: 3.18 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09774120919571024		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.09774120919571024 | validation: 0.10433525844217342]
	TIME [epoch: 3.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09870019976286389		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09870019976286389 | validation: 0.09915658719368059]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09592863244368094		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.09592863244368094 | validation: 0.10507829309222315]
	TIME [epoch: 3.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10356575679936719		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.10356575679936719 | validation: 0.11010807905356157]
	TIME [epoch: 3.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11383740731885203		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.11383740731885203 | validation: 0.12352631797763139]
	TIME [epoch: 3.19 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11202184676134562		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.11202184676134562 | validation: 0.09769993153159938]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09186121873400266		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.09186121873400266 | validation: 0.1549148080490868]
	TIME [epoch: 3.22 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10825039599262773		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.10825039599262773 | validation: 0.11081533324298265]
	TIME [epoch: 3.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09222871887208436		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.09222871887208436 | validation: 0.10068827832522348]
	TIME [epoch: 3.19 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09006747499663562		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.09006747499663562 | validation: 0.14822022312844416]
	TIME [epoch: 3.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10329336315852697		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.10329336315852697 | validation: 0.11154647349564457]
	TIME [epoch: 3.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1142296755608381		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.1142296755608381 | validation: 0.11484383594309155]
	TIME [epoch: 3.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10675800671239657		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.10675800671239657 | validation: 0.10810355247772097]
	TIME [epoch: 3.19 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09685052805180208		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.09685052805180208 | validation: 0.1126140154574597]
	TIME [epoch: 3.19 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0980622044127886		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0980622044127886 | validation: 0.10165379552701384]
	TIME [epoch: 3.19 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09124961818054726		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.09124961818054726 | validation: 0.0977993103742246]
	TIME [epoch: 3.19 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09310974177999046		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.09310974177999046 | validation: 0.09659897160997942]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09164299080325805		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.09164299080325805 | validation: 0.13177211868686353]
	TIME [epoch: 3.19 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10873313757406515		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.10873313757406515 | validation: 0.09818956716238283]
	TIME [epoch: 3.23 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08915509533247953		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.08915509533247953 | validation: 0.09851746581635618]
	TIME [epoch: 3.21 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09390708182905513		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.09390708182905513 | validation: 0.10869667702223851]
	TIME [epoch: 3.21 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09646974515725312		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.09646974515725312 | validation: 0.08795753623970742]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09832001708447202		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.09832001708447202 | validation: 0.10881600715999465]
	TIME [epoch: 3.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09425247833127726		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.09425247833127726 | validation: 0.11529316156065846]
	TIME [epoch: 3.19 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0993300070028279		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.0993300070028279 | validation: 0.09833165931328892]
	TIME [epoch: 3.19 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08990849320387342		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08990849320387342 | validation: 0.09780670593200996]
	TIME [epoch: 3.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09145500990292119		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.09145500990292119 | validation: 0.12454114520211534]
	TIME [epoch: 3.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10354418024070125		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.10354418024070125 | validation: 0.11979318681771159]
	TIME [epoch: 3.19 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09474114728614172		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.09474114728614172 | validation: 0.10283668089530401]
	TIME [epoch: 3.19 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09737271766735073		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.09737271766735073 | validation: 0.1101339117339904]
	TIME [epoch: 3.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09195207407779295		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.09195207407779295 | validation: 0.09797881179063331]
	TIME [epoch: 3.21 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0885595806856963		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0885595806856963 | validation: 0.09146906135702335]
	TIME [epoch: 3.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09566785069584446		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.09566785069584446 | validation: 0.09479403886251431]
	TIME [epoch: 3.19 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08444057135009567		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.08444057135009567 | validation: 0.12852363216471674]
	TIME [epoch: 3.19 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10108403131639195		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.10108403131639195 | validation: 0.09422083206447739]
	TIME [epoch: 3.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08469013368337593		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.08469013368337593 | validation: 0.09461930890617336]
	TIME [epoch: 3.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08973043089719018		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.08973043089719018 | validation: 0.11230330472677275]
	TIME [epoch: 3.19 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0918817919134246		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.0918817919134246 | validation: 0.1051856056927278]
	TIME [epoch: 3.19 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08773441553361253		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.08773441553361253 | validation: 0.25841733456499166]
	TIME [epoch: 3.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21842194257437378		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.21842194257437378 | validation: 0.11700062475334363]
	TIME [epoch: 3.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11095490570289879		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.11095490570289879 | validation: 0.10598132954830981]
	TIME [epoch: 3.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09490559830224068		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.09490559830224068 | validation: 0.10395481379615762]
	TIME [epoch: 3.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08590916384124286		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.08590916384124286 | validation: 0.10795320706354815]
	TIME [epoch: 3.23 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08747342667771724		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.08747342667771724 | validation: 0.09237032278437178]
	TIME [epoch: 3.21 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08066325919738176		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.08066325919738176 | validation: 0.08722928660881571]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08192498999316225		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.08192498999316225 | validation: 0.12994681110567088]
	TIME [epoch: 3.19 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09419774698187905		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.09419774698187905 | validation: 0.09090240800928655]
	TIME [epoch: 3.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08786498973224291		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.08786498973224291 | validation: 0.09316848574762673]
	TIME [epoch: 3.17 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08171574670862389		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.08171574670862389 | validation: 0.10540963781815177]
	TIME [epoch: 3.18 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08641937123939186		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08641937123939186 | validation: 0.0953802113651529]
	TIME [epoch: 3.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08451370962252278		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.08451370962252278 | validation: 0.09375812547955889]
	TIME [epoch: 3.19 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08228444732632065		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.08228444732632065 | validation: 0.1020744400715648]
	TIME [epoch: 3.19 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08781673254191716		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.08781673254191716 | validation: 0.09230449967508987]
	TIME [epoch: 3.19 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08642492672863483		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.08642492672863483 | validation: 0.09489841369021372]
	TIME [epoch: 3.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07923715729837963		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.07923715729837963 | validation: 0.10292959800619794]
	TIME [epoch: 3.21 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10159479639373414		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.10159479639373414 | validation: 0.09111319221262118]
	TIME [epoch: 3.22 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09389799748529709		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.09389799748529709 | validation: 0.0922088599455074]
	TIME [epoch: 3.72 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08346636314761495		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.08346636314761495 | validation: 0.09552954289115112]
	TIME [epoch: 3.18 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08509204787554212		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.08509204787554212 | validation: 0.09059789933174585]
	TIME [epoch: 3.18 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08841021378098571		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.08841021378098571 | validation: 0.0931472102458838]
	TIME [epoch: 3.18 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08675065602282057		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.08675065602282057 | validation: 0.09284247685874694]
	TIME [epoch: 3.18 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0823803988765966		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.0823803988765966 | validation: 0.09422398035848503]
	TIME [epoch: 3.18 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08059235595589614		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08059235595589614 | validation: 0.08494891490606021]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0799997427211642		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.0799997427211642 | validation: 0.08872139805249862]
	TIME [epoch: 3.19 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07992527520487432		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.07992527520487432 | validation: 0.08338101712656598]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08114695600575195		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.08114695600575195 | validation: 0.09927678025791575]
	TIME [epoch: 3.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08939433795898828		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.08939433795898828 | validation: 0.08934168390670379]
	TIME [epoch: 3.19 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08159602256313427		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.08159602256313427 | validation: 0.08960297548591134]
	TIME [epoch: 3.19 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08359144360847065		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.08359144360847065 | validation: 0.19371116620224504]
	TIME [epoch: 3.18 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14376214141426755		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.14376214141426755 | validation: 0.11391843672946317]
	TIME [epoch: 3.18 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09232801498517898		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.09232801498517898 | validation: 0.09014367580376376]
	TIME [epoch: 3.18 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08253206674643793		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.08253206674643793 | validation: 0.0889349435509093]
	TIME [epoch: 3.19 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07891009930191098		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.07891009930191098 | validation: 0.08162195885123365]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07556311965631898		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.07556311965631898 | validation: 0.08368209919684635]
	TIME [epoch: 3.18 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07584755571439106		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.07584755571439106 | validation: 0.09034018516320796]
	TIME [epoch: 3.18 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08294178142347744		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.08294178142347744 | validation: 0.08718815616082472]
	TIME [epoch: 3.17 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07722560473123646		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.07722560473123646 | validation: 0.09310851972996051]
	TIME [epoch: 3.19 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0793709092432495		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.0793709092432495 | validation: 0.09079736180485017]
	TIME [epoch: 3.19 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08034120169308973		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08034120169308973 | validation: 0.09314548948596402]
	TIME [epoch: 3.19 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07738806159496203		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.07738806159496203 | validation: 0.09402281543144304]
	TIME [epoch: 3.19 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07516071202777462		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.07516071202777462 | validation: 0.08644105044786515]
	TIME [epoch: 3.18 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07621658401037613		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07621658401037613 | validation: 0.087019478871959]
	TIME [epoch: 3.18 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08057844405869834		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08057844405869834 | validation: 0.09012835009258253]
	TIME [epoch: 3.18 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07940470054836007		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.07940470054836007 | validation: 0.08255153785472165]
	TIME [epoch: 3.19 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07463225625955858		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.07463225625955858 | validation: 0.08598713632094268]
	TIME [epoch: 3.18 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07590037943042549		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07590037943042549 | validation: 0.09625273522389347]
	TIME [epoch: 3.18 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07686744220726029		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.07686744220726029 | validation: 0.09372345183550648]
	TIME [epoch: 3.18 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08318895136547547		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.08318895136547547 | validation: 0.09236387480502105]
	TIME [epoch: 3.19 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08656540637303621		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.08656540637303621 | validation: 0.08773801451928556]
	TIME [epoch: 3.18 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07907863522073973		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.07907863522073973 | validation: 0.09730591515682085]
	TIME [epoch: 3.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08070073257946947		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.08070073257946947 | validation: 0.08256017101508484]
	TIME [epoch: 3.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07615968832089734		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07615968832089734 | validation: 0.08133322168126018]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0737801924085696		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0737801924085696 | validation: 0.0842257425441022]
	TIME [epoch: 3.18 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07613072419116909		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07613072419116909 | validation: 0.08527994619403828]
	TIME [epoch: 3.18 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07217428061396283		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.07217428061396283 | validation: 0.09569861673929153]
	TIME [epoch: 3.18 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07709317727350812		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.07709317727350812 | validation: 0.08564489415898993]
	TIME [epoch: 3.18 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07583574779970004		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.07583574779970004 | validation: 0.08232597609001929]
	TIME [epoch: 3.19 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07426453800852452		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.07426453800852452 | validation: 0.0806944182886327]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07258652723683148		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.07258652723683148 | validation: 0.08325359832248394]
	TIME [epoch: 3.19 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07028704972269711		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.07028704972269711 | validation: 0.08222860089454714]
	TIME [epoch: 3.19 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0730911755556378		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.0730911755556378 | validation: 0.08113074889092742]
	TIME [epoch: 3.19 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07183159135891576		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.07183159135891576 | validation: 0.09132573534084089]
	TIME [epoch: 3.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07646235145817967		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.07646235145817967 | validation: 0.08834996295845886]
	TIME [epoch: 3.19 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0769452926738573		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.0769452926738573 | validation: 0.08884702464266564]
	TIME [epoch: 3.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08349296289054312		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.08349296289054312 | validation: 0.0776591656247772]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.070897965745114		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.070897965745114 | validation: 0.08255914417457653]
	TIME [epoch: 3.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08128175263710832		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.08128175263710832 | validation: 0.07871835598617594]
	TIME [epoch: 3.18 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07213483742229956		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.07213483742229956 | validation: 0.08207644771837537]
	TIME [epoch: 3.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06972870096543017		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.06972870096543017 | validation: 0.07426496554987615]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07457185590789386		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.07457185590789386 | validation: 0.07746670959091356]
	TIME [epoch: 3.19 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07276950082645445		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.07276950082645445 | validation: 0.08136755363304055]
	TIME [epoch: 3.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07319591686118665		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.07319591686118665 | validation: 0.07895610803913188]
	TIME [epoch: 3.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07233206101638148		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.07233206101638148 | validation: 0.0841181941052112]
	TIME [epoch: 3.22 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07221344775192758		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.07221344775192758 | validation: 0.07828809220443032]
	TIME [epoch: 3.21 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06959786323531601		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.06959786323531601 | validation: 0.12526460421482816]
	TIME [epoch: 3.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0963482038592229		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.0963482038592229 | validation: 0.0895996025210321]
	TIME [epoch: 3.19 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07240791698083135		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.07240791698083135 | validation: 0.08115317635157644]
	TIME [epoch: 3.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07347480349155874		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.07347480349155874 | validation: 0.08022359826001511]
	TIME [epoch: 3.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07133448759419764		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.07133448759419764 | validation: 0.08783240736132025]
	TIME [epoch: 3.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07212037872041574		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.07212037872041574 | validation: 0.0781334773004749]
	TIME [epoch: 3.19 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07234612929387682		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.07234612929387682 | validation: 0.07759983059276164]
	TIME [epoch: 3.19 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0688956050331823		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.0688956050331823 | validation: 0.08698814321374279]
	TIME [epoch: 3.19 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07310457441754153		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.07310457441754153 | validation: 0.07734375373648664]
	TIME [epoch: 3.19 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07030567527835388		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.07030567527835388 | validation: 0.07835984740138965]
	TIME [epoch: 3.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06836613173350409		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.06836613173350409 | validation: 0.07680648795274414]
	TIME [epoch: 3.21 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06733701114209106		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.06733701114209106 | validation: 0.08114640620078442]
	TIME [epoch: 3.22 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07272200958036627		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.07272200958036627 | validation: 0.07630771528960296]
	TIME [epoch: 3.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06618037915927158		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.06618037915927158 | validation: 0.0741976840058215]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.068017963080716		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.068017963080716 | validation: 0.08166175946514523]
	TIME [epoch: 3.19 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06807007505721938		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.06807007505721938 | validation: 0.07758372775518807]
	TIME [epoch: 3.19 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07357937383443441		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.07357937383443441 | validation: 0.08153738659800039]
	TIME [epoch: 3.19 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06840662131559114		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.06840662131559114 | validation: 0.07792353317822939]
	TIME [epoch: 3.19 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06633885379520083		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.06633885379520083 | validation: 0.07841529975016365]
	TIME [epoch: 3.19 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07733887931919589		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.07733887931919589 | validation: 0.07364978556165942]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06574004621875668		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.06574004621875668 | validation: 0.07919916524052661]
	TIME [epoch: 3.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0788641236107527		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.0788641236107527 | validation: 0.0771074058189245]
	TIME [epoch: 3.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06644742869398661		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.06644742869398661 | validation: 0.07851454009089093]
	TIME [epoch: 3.22 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0655362658719546		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.0655362658719546 | validation: 0.08247127708003994]
	TIME [epoch: 3.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06716973384195704		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.06716973384195704 | validation: 0.07652205269039902]
	TIME [epoch: 3.21 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06899202438795639		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.06899202438795639 | validation: 0.08017798644517744]
	TIME [epoch: 3.19 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0668252621335976		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.0668252621335976 | validation: 0.07347661570790012]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06800807864506976		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.06800807864506976 | validation: 0.11232409805536543]
	TIME [epoch: 3.18 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09066922885895723		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.09066922885895723 | validation: 0.08022269153786817]
	TIME [epoch: 3.19 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06903984326253021		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.06903984326253021 | validation: 0.07977126485776748]
	TIME [epoch: 3.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06893775475757238		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.06893775475757238 | validation: 0.07389425864441257]
	TIME [epoch: 3.18 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06820313589520331		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.06820313589520331 | validation: 0.078777196373722]
	TIME [epoch: 3.21 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06846539630578959		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.06846539630578959 | validation: 0.07830166777255415]
	TIME [epoch: 3.19 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0660069494260743		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.0660069494260743 | validation: 0.07764299548723633]
	TIME [epoch: 3.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06842160503812497		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.06842160503812497 | validation: 0.07645002881394726]
	TIME [epoch: 3.21 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06877994187323327		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.06877994187323327 | validation: 0.07594642710419373]
	TIME [epoch: 3.21 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06879647809387225		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.06879647809387225 | validation: 0.07793735257135866]
	TIME [epoch: 3.19 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06451937866904711		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.06451937866904711 | validation: 0.10357498441884726]
	TIME [epoch: 3.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09877670893147841		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.09877670893147841 | validation: 0.08041449997200951]
	TIME [epoch: 3.19 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07186412127908323		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07186412127908323 | validation: 0.07909399559439045]
	TIME [epoch: 3.19 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06743493811865252		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06743493811865252 | validation: 0.07718735780494419]
	TIME [epoch: 3.19 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06376327911135429		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06376327911135429 | validation: 0.07476119550498088]
	TIME [epoch: 3.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06510190291360818		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.06510190291360818 | validation: 0.07924539057359921]
	TIME [epoch: 3.19 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0661119314357281		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.0661119314357281 | validation: 0.07718956802684339]
	TIME [epoch: 3.19 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06528864758130071		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06528864758130071 | validation: 0.07916737678783461]
	TIME [epoch: 3.19 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06943376040751198		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.06943376040751198 | validation: 0.07552151468369717]
	TIME [epoch: 3.19 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06698132229038833		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06698132229038833 | validation: 0.07364113880068793]
	TIME [epoch: 3.21 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06631994096153658		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06631994096153658 | validation: 0.07538701754124379]
	TIME [epoch: 109 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06530365707567587		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.06530365707567587 | validation: 0.08360925165194884]
	TIME [epoch: 6.33 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06905916133551032		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06905916133551032 | validation: 0.07517946419334653]
	TIME [epoch: 6.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06953858355815536		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.06953858355815536 | validation: 0.07437874677869696]
	TIME [epoch: 6.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06833267974441401		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06833267974441401 | validation: 0.0739549181037411]
	TIME [epoch: 6.28 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06805352840774727		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.06805352840774727 | validation: 0.07207835412131285]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06518689303777311		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06518689303777311 | validation: 0.07658803176431223]
	TIME [epoch: 6.28 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06516383469796774		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06516383469796774 | validation: 0.07567259731666831]
	TIME [epoch: 6.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0644141619863002		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.0644141619863002 | validation: 0.07481212961028798]
	TIME [epoch: 6.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06768318057675121		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.06768318057675121 | validation: 0.07489695705798873]
	TIME [epoch: 6.27 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06460954403630417		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.06460954403630417 | validation: 0.08713814043508442]
	TIME [epoch: 6.29 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06534291064043264		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.06534291064043264 | validation: 0.07888301809247154]
	TIME [epoch: 6.29 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07182377759930028		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.07182377759930028 | validation: 0.07189891560717625]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0642306930581373		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.0642306930581373 | validation: 0.08789742958921173]
	TIME [epoch: 6.31 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07501381444714111		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.07501381444714111 | validation: 0.07263698223138708]
	TIME [epoch: 6.28 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06458623175248288		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06458623175248288 | validation: 0.07589260292309126]
	TIME [epoch: 6.29 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06592602086877022		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.06592602086877022 | validation: 0.06945308893669914]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0683092234398318		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0683092234398318 | validation: 0.08643845105945741]
	TIME [epoch: 6.26 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06794839520950023		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06794839520950023 | validation: 0.07472716245522693]
	TIME [epoch: 6.27 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06518862254622977		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06518862254622977 | validation: 0.0719270490663736]
	TIME [epoch: 6.28 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060057412244386		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.060057412244386 | validation: 0.07225937791684292]
	TIME [epoch: 6.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06137985701253247		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.06137985701253247 | validation: 0.07086160872275825]
	TIME [epoch: 6.29 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07016079139649195		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.07016079139649195 | validation: 0.08199105861974117]
	TIME [epoch: 6.29 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06527452702778186		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06527452702778186 | validation: 0.07646724347303596]
	TIME [epoch: 6.27 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06690856446037835		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06690856446037835 | validation: 0.07047370507729903]
	TIME [epoch: 6.28 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06114134270728343		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.06114134270728343 | validation: 0.06960070482246399]
	TIME [epoch: 6.28 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06479040366041738		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.06479040366041738 | validation: 0.07160990656500009]
	TIME [epoch: 6.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061307270020755936		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.061307270020755936 | validation: 0.07294287128901546]
	TIME [epoch: 6.28 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05983483779803576		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.05983483779803576 | validation: 0.0726199407532162]
	TIME [epoch: 6.27 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06620780011081556		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.06620780011081556 | validation: 0.07246005957564212]
	TIME [epoch: 6.28 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0645311492587262		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.0645311492587262 | validation: 0.07337661952023507]
	TIME [epoch: 6.28 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061514129865763184		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.061514129865763184 | validation: 0.07213952084078608]
	TIME [epoch: 6.28 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06294022924974474		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.06294022924974474 | validation: 0.07337848784080649]
	TIME [epoch: 6.29 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06638579352669144		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06638579352669144 | validation: 0.0726038168352608]
	TIME [epoch: 6.29 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06697224969440793		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.06697224969440793 | validation: 0.06975660045923328]
	TIME [epoch: 6.29 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06232202878407664		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.06232202878407664 | validation: 0.07395312354606552]
	TIME [epoch: 6.27 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06183260088421573		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.06183260088421573 | validation: 0.07360119803339846]
	TIME [epoch: 6.28 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061927468918827284		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.061927468918827284 | validation: 0.07258696540161935]
	TIME [epoch: 6.27 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06350893899013119		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06350893899013119 | validation: 0.06925141982592374]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06320971615741361		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06320971615741361 | validation: 0.07613558215240682]
	TIME [epoch: 6.31 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06123508054000378		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06123508054000378 | validation: 0.06767529179042835]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05775854820982232		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.05775854820982232 | validation: 0.1191083680687489]
	TIME [epoch: 6.27 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09225706595249757		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.09225706595249757 | validation: 0.0866471801199318]
	TIME [epoch: 6.27 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0690166929928544		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.0690166929928544 | validation: 0.07781208652852345]
	TIME [epoch: 6.28 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0630464101241664		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0630464101241664 | validation: 0.0741755241861351]
	TIME [epoch: 6.29 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06426302599751307		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06426302599751307 | validation: 0.07324950857543314]
	TIME [epoch: 6.31 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06213516702383032		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.06213516702383032 | validation: 0.0715172561990068]
	TIME [epoch: 6.29 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06070460965287865		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.06070460965287865 | validation: 0.06910916466361396]
	TIME [epoch: 6.29 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05936306897820513		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05936306897820513 | validation: 0.07082233491589918]
	TIME [epoch: 6.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05808327051895468		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.05808327051895468 | validation: 0.07046280786647947]
	TIME [epoch: 6.27 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06319117337389595		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.06319117337389595 | validation: 0.072672667954723]
	TIME [epoch: 6.28 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06210621299123275		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.06210621299123275 | validation: 0.06813783837278146]
	TIME [epoch: 6.29 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06192948670383484		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06192948670383484 | validation: 0.06600582896340122]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06030194427664948		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06030194427664948 | validation: 0.06930598548055722]
	TIME [epoch: 6.28 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06372315108861651		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.06372315108861651 | validation: 0.07085193130088875]
	TIME [epoch: 6.28 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06333435922705351		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.06333435922705351 | validation: 0.06993494439170715]
	TIME [epoch: 6.29 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05836810474990478		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.05836810474990478 | validation: 0.06983958803271433]
	TIME [epoch: 6.28 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05901666423557952		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.05901666423557952 | validation: 0.06827011351244053]
	TIME [epoch: 6.29 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060244844482344975		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.060244844482344975 | validation: 0.06838641745127826]
	TIME [epoch: 6.34 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0608380583600136		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.0608380583600136 | validation: 0.07302709973234389]
	TIME [epoch: 6.28 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06071562932770347		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.06071562932770347 | validation: 0.06781321174054511]
	TIME [epoch: 6.31 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060425182678902276		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.060425182678902276 | validation: 0.07022948404076988]
	TIME [epoch: 6.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060719260129455124		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.060719260129455124 | validation: 0.06988413655991901]
	TIME [epoch: 6.29 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0591604640675924		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0591604640675924 | validation: 0.0651060357250195]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05713980371140727		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05713980371140727 | validation: 0.07192506007910744]
	TIME [epoch: 6.31 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060388066082665066		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.060388066082665066 | validation: 0.06664715119005614]
	TIME [epoch: 6.28 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05933785689011485		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.05933785689011485 | validation: 0.07039899642105465]
	TIME [epoch: 6.28 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059999263069801786		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.059999263069801786 | validation: 0.06970457476933627]
	TIME [epoch: 6.31 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0605609286384582		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.0605609286384582 | validation: 0.07077344251050445]
	TIME [epoch: 6.31 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05879752133015331		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.05879752133015331 | validation: 0.0703238717549028]
	TIME [epoch: 6.29 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05945401014125031		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.05945401014125031 | validation: 0.07489772131022326]
	TIME [epoch: 6.32 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06440325388070425		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.06440325388070425 | validation: 0.06746723447109847]
	TIME [epoch: 6.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059738211158424176		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.059738211158424176 | validation: 0.06910408602277629]
	TIME [epoch: 6.28 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058670832640667164		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.058670832640667164 | validation: 0.06638622865500877]
	TIME [epoch: 6.31 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05950681548340889		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05950681548340889 | validation: 0.0697939539484551]
	TIME [epoch: 6.31 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06218363785243809		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.06218363785243809 | validation: 0.06778571880226107]
	TIME [epoch: 6.31 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059708711464589795		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.059708711464589795 | validation: 0.06501085586919413]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0611797647639399		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.0611797647639399 | validation: 0.07334444411050761]
	TIME [epoch: 6.34 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0604973759131882		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.0604973759131882 | validation: 0.06627167915293444]
	TIME [epoch: 6.27 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06220763255175453		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.06220763255175453 | validation: 0.06686687514773153]
	TIME [epoch: 6.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06107996804742171		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.06107996804742171 | validation: 0.06789401685729483]
	TIME [epoch: 6.31 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057202367755481626		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.057202367755481626 | validation: 0.06780863658325786]
	TIME [epoch: 6.33 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05785733033747878		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.05785733033747878 | validation: 0.07075275114970882]
	TIME [epoch: 6.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06060854003255928		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.06060854003255928 | validation: 0.06439403182926866]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05804316258438559		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.05804316258438559 | validation: 0.06495264758600054]
	TIME [epoch: 6.32 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058921061250700124		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.058921061250700124 | validation: 0.07116892590217283]
	TIME [epoch: 6.28 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061536563295067986		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.061536563295067986 | validation: 0.06457948186423183]
	TIME [epoch: 6.29 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05878341189841431		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.05878341189841431 | validation: 0.06830703943398815]
	TIME [epoch: 6.29 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05839763359339774		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.05839763359339774 | validation: 0.06931343653492861]
	TIME [epoch: 6.32 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05932262573388476		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.05932262573388476 | validation: 0.0647752708116744]
	TIME [epoch: 6.33 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05589313698958345		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.05589313698958345 | validation: 0.06442519713429094]
	TIME [epoch: 6.31 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06715018853299125		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.06715018853299125 | validation: 0.08620928105094174]
	TIME [epoch: 6.31 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07880208538351038		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.07880208538351038 | validation: 0.08472349737716106]
	TIME [epoch: 6.31 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07026094718819098		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.07026094718819098 | validation: 0.07543315923486633]
	TIME [epoch: 6.28 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06268594865203517		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.06268594865203517 | validation: 0.0733704047108381]
	TIME [epoch: 6.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06071135094373925		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.06071135094373925 | validation: 0.06734124638489311]
	TIME [epoch: 6.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06021404555528514		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.06021404555528514 | validation: 0.06802321922998791]
	TIME [epoch: 6.32 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057006972817967294		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.057006972817967294 | validation: 0.06615633473258062]
	TIME [epoch: 6.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056267349140463066		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.056267349140463066 | validation: 0.06484533367330728]
	TIME [epoch: 6.29 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05588304124470067		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.05588304124470067 | validation: 0.06616140736092328]
	TIME [epoch: 6.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05399611673192417		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.05399611673192417 | validation: 0.06451513324183167]
	TIME [epoch: 6.29 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05730968145633522		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.05730968145633522 | validation: 0.06345141374701262]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058501638916566776		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.058501638916566776 | validation: 0.06525155236540563]
	TIME [epoch: 6.32 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05577285588399179		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.05577285588399179 | validation: 0.06771536838927476]
	TIME [epoch: 6.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057960595810044264		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.057960595810044264 | validation: 0.06702297818170372]
	TIME [epoch: 6.27 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058218615639473364		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.058218615639473364 | validation: 0.06424462982140687]
	TIME [epoch: 6.28 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055302788245379264		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.055302788245379264 | validation: 0.06852078792948972]
	TIME [epoch: 6.27 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057042918573649924		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.057042918573649924 | validation: 0.06431047103257939]
	TIME [epoch: 6.27 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05701002275682446		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.05701002275682446 | validation: 0.06570641744215117]
	TIME [epoch: 6.34 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05620275868413688		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.05620275868413688 | validation: 0.06679042833233642]
	TIME [epoch: 6.28 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055071836833170726		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.055071836833170726 | validation: 0.06690700131596553]
	TIME [epoch: 6.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05834408811827349		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05834408811827349 | validation: 0.06584524251527352]
	TIME [epoch: 6.29 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05797799542431387		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.05797799542431387 | validation: 0.06420886381855387]
	TIME [epoch: 6.29 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058568405114281304		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.058568405114281304 | validation: 0.06532604412659541]
	TIME [epoch: 6.28 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057561209099697856		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.057561209099697856 | validation: 0.06381947493651956]
	TIME [epoch: 6.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05577425980812636		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.05577425980812636 | validation: 0.06360229759113802]
	TIME [epoch: 6.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056741329156568085		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.056741329156568085 | validation: 0.06976738152537423]
	TIME [epoch: 6.29 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05522937901712659		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.05522937901712659 | validation: 0.07032319756291963]
	TIME [epoch: 6.29 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05911583349549911		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.05911583349549911 | validation: 0.06836919894516087]
	TIME [epoch: 6.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05747226713868629		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.05747226713868629 | validation: 0.06485459714978124]
	TIME [epoch: 6.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056568081844419804		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.056568081844419804 | validation: 0.06737727690487907]
	TIME [epoch: 6.28 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0564486751603684		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0564486751603684 | validation: 0.06510812869270244]
	TIME [epoch: 6.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05676481364710276		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05676481364710276 | validation: 0.0690954501372771]
	TIME [epoch: 6.29 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05862589951723965		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05862589951723965 | validation: 0.06508733857855686]
	TIME [epoch: 6.29 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058771955879898516		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.058771955879898516 | validation: 0.06344070609360677]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05885673159600392		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.05885673159600392 | validation: 0.06503701939522889]
	TIME [epoch: 6.27 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056846837168363716		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.056846837168363716 | validation: 0.06617226587790562]
	TIME [epoch: 6.28 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057439526625272984		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.057439526625272984 | validation: 0.06299676595898632]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05635895784379488		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05635895784379488 | validation: 0.0666778392582903]
	TIME [epoch: 6.31 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05654654047507942		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05654654047507942 | validation: 0.06305417896703892]
	TIME [epoch: 6.29 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055285171081797896		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.055285171081797896 | validation: 0.0625810524684317]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05398150294369056		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05398150294369056 | validation: 0.06562840516686612]
	TIME [epoch: 6.28 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0563390256335552		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.0563390256335552 | validation: 0.06745846386820953]
	TIME [epoch: 6.27 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054441118977654676		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.054441118977654676 | validation: 0.06688714960954241]
	TIME [epoch: 6.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0546214436906519		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.0546214436906519 | validation: 0.06634500729689378]
	TIME [epoch: 6.29 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05372674935018186		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.05372674935018186 | validation: 0.06379885169222507]
	TIME [epoch: 6.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05417451710117983		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.05417451710117983 | validation: 0.06820524132740047]
	TIME [epoch: 6.27 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05718083407691597		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.05718083407691597 | validation: 0.06179657568155672]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052838018906442547		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.052838018906442547 | validation: 0.06589764163330496]
	TIME [epoch: 6.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05604113511625792		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.05604113511625792 | validation: 0.0661748642401193]
	TIME [epoch: 6.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05642009849879705		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.05642009849879705 | validation: 0.06416759249829372]
	TIME [epoch: 6.33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05579820483803727		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.05579820483803727 | validation: 0.06352058897680539]
	TIME [epoch: 6.28 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05599182961279775		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.05599182961279775 | validation: 0.06546575409081153]
	TIME [epoch: 6.29 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05492185815424827		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.05492185815424827 | validation: 0.06388498224621075]
	TIME [epoch: 6.29 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054549345016948234		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.054549345016948234 | validation: 0.06573567184528165]
	TIME [epoch: 6.31 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057689665452391875		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.057689665452391875 | validation: 0.06907363959653262]
	TIME [epoch: 6.28 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05794605460907069		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.05794605460907069 | validation: 0.06353647831928387]
	TIME [epoch: 6.33 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05631695777969015		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.05631695777969015 | validation: 0.0644486034131857]
	TIME [epoch: 6.29 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0541263127596054		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.0541263127596054 | validation: 0.070562033595633]
	TIME [epoch: 6.31 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05348868198743158		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.05348868198743158 | validation: 0.06224509492224737]
	TIME [epoch: 6.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054175456150035446		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.054175456150035446 | validation: 0.06422745540484323]
	TIME [epoch: 6.31 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052287087836318566		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.052287087836318566 | validation: 0.06226881177815409]
	TIME [epoch: 6.29 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054258744225454805		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.054258744225454805 | validation: 0.06343573501238518]
	TIME [epoch: 6.33 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0532097389132752		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0532097389132752 | validation: 0.06308674331334123]
	TIME [epoch: 6.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05552029705084682		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.05552029705084682 | validation: 0.06406582511528496]
	TIME [epoch: 6.29 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053512982949108404		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.053512982949108404 | validation: 0.06051276023914082]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05442148741059574		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.05442148741059574 | validation: 0.0653722953847611]
	TIME [epoch: 6.28 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05607332177337265		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.05607332177337265 | validation: 0.06383176567803764]
	TIME [epoch: 6.28 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052883261958912894		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.052883261958912894 | validation: 0.06341385802341543]
	TIME [epoch: 6.28 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052251700855186455		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.052251700855186455 | validation: 0.06349586718393459]
	TIME [epoch: 6.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05518566395498572		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.05518566395498572 | validation: 0.06064605262048305]
	TIME [epoch: 6.33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05601055831119118		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.05601055831119118 | validation: 0.06539506795726753]
	TIME [epoch: 6.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05501433803804922		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.05501433803804922 | validation: 0.062197909028290005]
	TIME [epoch: 6.33 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05583198170534569		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.05583198170534569 | validation: 0.06507368215405511]
	TIME [epoch: 6.28 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05342691021939179		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.05342691021939179 | validation: 0.061623712557965483]
	TIME [epoch: 6.31 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05320615957720509		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.05320615957720509 | validation: 0.05980194286092336]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05283441098791367		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.05283441098791367 | validation: 0.0669776823520553]
	TIME [epoch: 6.32 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056769022138709976		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.056769022138709976 | validation: 0.06368591315816957]
	TIME [epoch: 6.32 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05603085815665793		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.05603085815665793 | validation: 0.05997734719140374]
	TIME [epoch: 6.29 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053248372578351875		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.053248372578351875 | validation: 0.07371697407485985]
	TIME [epoch: 6.29 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056230445892268666		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.056230445892268666 | validation: 0.06599356548258402]
	TIME [epoch: 6.29 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0546748230713566		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.0546748230713566 | validation: 0.0611208501374499]
	TIME [epoch: 6.32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0559568615118677		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.0559568615118677 | validation: 0.06396220998071019]
	TIME [epoch: 6.32 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05176187215295426		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.05176187215295426 | validation: 0.06239937155861855]
	TIME [epoch: 6.31 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05268268050254582		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.05268268050254582 | validation: 0.06729450950033546]
	TIME [epoch: 6.29 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05371642942365171		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.05371642942365171 | validation: 0.06286717813906864]
	TIME [epoch: 6.28 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0518013552034997		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.0518013552034997 | validation: 0.06320637839074086]
	TIME [epoch: 6.32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054067040021536686		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.054067040021536686 | validation: 0.06587288410785623]
	TIME [epoch: 6.29 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05461832962258084		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05461832962258084 | validation: 0.06456006133304158]
	TIME [epoch: 6.33 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054152033031333646		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.054152033031333646 | validation: 0.06189507466264787]
	TIME [epoch: 6.32 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05286612513687319		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.05286612513687319 | validation: 0.06271815288490186]
	TIME [epoch: 6.29 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0540121956245391		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.0540121956245391 | validation: 0.059925039231947114]
	TIME [epoch: 6.31 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05261463011703429		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.05261463011703429 | validation: 0.05940706509467584]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05395643608554958		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.05395643608554958 | validation: 0.06560592255218618]
	TIME [epoch: 6.29 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053200871614347556		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.053200871614347556 | validation: 0.061354079106160135]
	TIME [epoch: 6.33 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053233456538055654		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.053233456538055654 | validation: 0.059274434831773685]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0536486381828528		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.0536486381828528 | validation: 0.06335685503566403]
	TIME [epoch: 6.29 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05323691870978428		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.05323691870978428 | validation: 0.05913553936598457]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054651218515584035		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.054651218515584035 | validation: 0.06410349847202344]
	TIME [epoch: 6.29 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056369738483012855		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.056369738483012855 | validation: 0.06168103427833125]
	TIME [epoch: 6.32 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05490332817987234		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.05490332817987234 | validation: 0.06562008401008071]
	TIME [epoch: 6.34 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051976747788777644		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.051976747788777644 | validation: 0.058876888910914804]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053244403878842814		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.053244403878842814 | validation: 0.06345523159719064]
	TIME [epoch: 6.32 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05476325586007166		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.05476325586007166 | validation: 0.06384736660171522]
	TIME [epoch: 6.29 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051679614289702365		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.051679614289702365 | validation: 0.061493380878932614]
	TIME [epoch: 6.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05315412092192858		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.05315412092192858 | validation: 0.06200914744966057]
	TIME [epoch: 6.29 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05261155271657339		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.05261155271657339 | validation: 0.05815437497582761]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05661219236395553		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.05661219236395553 | validation: 0.06467631594970842]
	TIME [epoch: 6.31 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05376370709179679		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.05376370709179679 | validation: 0.05893283364915311]
	TIME [epoch: 6.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05327601867967602		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.05327601867967602 | validation: 0.06088350097406275]
	TIME [epoch: 6.32 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052205339502108844		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.052205339502108844 | validation: 0.06607528775620419]
	TIME [epoch: 6.29 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05269528812959204		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.05269528812959204 | validation: 0.06433734575939509]
	TIME [epoch: 6.29 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05264207345063256		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.05264207345063256 | validation: 0.06594016574796414]
	TIME [epoch: 6.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05507647921819315		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.05507647921819315 | validation: 0.06528648497961449]
	TIME [epoch: 6.34 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052760274417380834		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.052760274417380834 | validation: 0.06239531419513303]
	TIME [epoch: 6.34 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05434108336427999		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.05434108336427999 | validation: 0.06064130060893134]
	TIME [epoch: 6.31 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052086346755308224		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.052086346755308224 | validation: 0.062188029066441254]
	TIME [epoch: 6.33 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052135913970884365		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.052135913970884365 | validation: 0.064238780299438]
	TIME [epoch: 6.29 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05104024537879036		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.05104024537879036 | validation: 0.06219399902728824]
	TIME [epoch: 6.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05138788399066026		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.05138788399066026 | validation: 0.06197818584189561]
	TIME [epoch: 6.33 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05329086132186185		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.05329086132186185 | validation: 0.06293967836155218]
	TIME [epoch: 6.29 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05135706559123557		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.05135706559123557 | validation: 0.0619411403137699]
	TIME [epoch: 6.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05077829229270436		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.05077829229270436 | validation: 0.06405647831411124]
	TIME [epoch: 6.32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05174985574657015		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.05174985574657015 | validation: 0.06029225621884256]
	TIME [epoch: 6.31 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05315772233826791		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.05315772233826791 | validation: 0.06065133326460014]
	TIME [epoch: 6.29 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05269570544422269		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.05269570544422269 | validation: 0.060145659335419245]
	TIME [epoch: 6.34 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05148501371402153		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.05148501371402153 | validation: 0.060336398952757136]
	TIME [epoch: 6.34 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053198765629859604		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.053198765629859604 | validation: 0.058974563479010735]
	TIME [epoch: 6.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05353047399912192		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.05353047399912192 | validation: 0.05996403928247465]
	TIME [epoch: 6.29 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05154306235574126		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.05154306235574126 | validation: 0.05974791714081003]
	TIME [epoch: 6.32 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05180113271918209		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.05180113271918209 | validation: 0.06491013347138637]
	TIME [epoch: 6.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05320006870302817		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.05320006870302817 | validation: 0.061711678328778254]
	TIME [epoch: 6.31 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05016876188967748		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.05016876188967748 | validation: 0.061344045159617194]
	TIME [epoch: 6.34 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0531619900525508		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.0531619900525508 | validation: 0.06138046095774988]
	TIME [epoch: 6.31 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05313509941793734		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.05313509941793734 | validation: 0.05864862090708636]
	TIME [epoch: 6.32 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05443315196056787		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.05443315196056787 | validation: 0.060102454130430694]
	TIME [epoch: 6.31 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050572784439506		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.050572784439506 | validation: 0.05972493090289484]
	TIME [epoch: 6.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050947388460041015		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.050947388460041015 | validation: 0.06245792436307496]
	TIME [epoch: 6.31 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05147967258925652		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.05147967258925652 | validation: 0.06119097206934626]
	TIME [epoch: 6.35 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05288520120894591		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.05288520120894591 | validation: 0.05826859456889286]
	TIME [epoch: 6.35 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05245831222187133		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.05245831222187133 | validation: 0.059722469250423266]
	TIME [epoch: 6.32 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05372128125094424		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.05372128125094424 | validation: 0.05921937391461716]
	TIME [epoch: 6.31 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05279744609830294		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.05279744609830294 | validation: 0.061981629966620405]
	TIME [epoch: 6.29 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05475922332273084		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.05475922332273084 | validation: 0.05810837097764712]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05277552927100061		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.05277552927100061 | validation: 0.05996039476925319]
	TIME [epoch: 6.34 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050250031232075144		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.050250031232075144 | validation: 0.06076456291869943]
	TIME [epoch: 6.33 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05294376371164951		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.05294376371164951 | validation: 0.05900447235851728]
	TIME [epoch: 6.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051812853176357865		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.051812853176357865 | validation: 0.05905505045750852]
	TIME [epoch: 6.34 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051009083443844486		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.051009083443844486 | validation: 0.05848260479924963]
	TIME [epoch: 6.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05097034609884637		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.05097034609884637 | validation: 0.06074108032702589]
	TIME [epoch: 6.33 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050305119090694864		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.050305119090694864 | validation: 0.062482700395618346]
	TIME [epoch: 6.32 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04995568652465689		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.04995568652465689 | validation: 0.060862817708464335]
	TIME [epoch: 6.35 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05115779498448275		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.05115779498448275 | validation: 0.057870826932926]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05089614754481746		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.05089614754481746 | validation: 0.06200646756312618]
	TIME [epoch: 6.29 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050041374242080994		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.050041374242080994 | validation: 0.0612752220600648]
	TIME [epoch: 6.32 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05166601015252573		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.05166601015252573 | validation: 0.059530100674399004]
	TIME [epoch: 6.29 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05056258034824506		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.05056258034824506 | validation: 0.060388493326534315]
	TIME [epoch: 6.33 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050167899322616484		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.050167899322616484 | validation: 0.06367508364151178]
	TIME [epoch: 6.33 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05114345143035708		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.05114345143035708 | validation: 0.05746247729297112]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05190046592244966		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.05190046592244966 | validation: 0.06257423667788911]
	TIME [epoch: 6.32 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04961844924539296		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.04961844924539296 | validation: 0.05725049428035794]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050385457071737874		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.050385457071737874 | validation: 0.05613632609643931]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05182232895292149		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.05182232895292149 | validation: 0.058997981020772965]
	TIME [epoch: 6.29 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05284060901872914		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.05284060901872914 | validation: 0.05813442133558319]
	TIME [epoch: 6.32 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05330506365664428		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.05330506365664428 | validation: 0.059636898076655925]
	TIME [epoch: 6.31 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050893949031231775		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.050893949031231775 | validation: 0.059186044592635434]
	TIME [epoch: 6.29 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049560138610226956		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.049560138610226956 | validation: 0.06069964607261964]
	TIME [epoch: 6.29 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05153806277338267		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.05153806277338267 | validation: 0.06082349835837615]
	TIME [epoch: 6.27 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052748960379621276		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.052748960379621276 | validation: 0.06099971858109874]
	TIME [epoch: 6.29 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05110016831898287		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.05110016831898287 | validation: 0.05975004597001499]
	TIME [epoch: 6.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049703746291887216		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.049703746291887216 | validation: 0.06047206247990822]
	TIME [epoch: 6.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05175802591124215		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.05175802591124215 | validation: 0.06524132247844308]
	TIME [epoch: 6.29 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05086205769389035		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.05086205769389035 | validation: 0.06079186494462342]
	TIME [epoch: 6.28 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05144264015970249		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.05144264015970249 | validation: 0.05764388496458487]
	TIME [epoch: 6.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05048946887608835		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.05048946887608835 | validation: 0.06100777334256438]
	TIME [epoch: 6.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0521136415206308		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.0521136415206308 | validation: 0.05831951774216442]
	TIME [epoch: 6.28 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04906797112660291		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.04906797112660291 | validation: 0.06107944255660724]
	TIME [epoch: 6.32 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051282510373331376		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.051282510373331376 | validation: 0.06558119840937926]
	TIME [epoch: 6.27 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05150246547202139		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.05150246547202139 | validation: 0.05987524876391658]
	TIME [epoch: 6.28 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04944115462064238		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.04944115462064238 | validation: 0.05679525752987335]
	TIME [epoch: 6.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0494652822644342		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.0494652822644342 | validation: 0.059473826321580284]
	TIME [epoch: 6.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04934861995771951		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.04934861995771951 | validation: 0.05907473602486166]
	TIME [epoch: 6.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051506297109410536		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.051506297109410536 | validation: 0.05920356041690829]
	TIME [epoch: 6.32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049205472322624635		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.049205472322624635 | validation: 0.058257468075953646]
	TIME [epoch: 6.28 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04999043043631917		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.04999043043631917 | validation: 0.05808824331043758]
	TIME [epoch: 6.28 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0523223701279447		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.0523223701279447 | validation: 0.06212479251238017]
	TIME [epoch: 6.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0503898031812158		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.0503898031812158 | validation: 0.05815593008730327]
	TIME [epoch: 6.29 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051659736191491		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.051659736191491 | validation: 0.058237092860871156]
	TIME [epoch: 6.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05331559778557644		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.05331559778557644 | validation: 0.055775662942985194]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05125504027229373		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.05125504027229373 | validation: 0.05922583706073406]
	TIME [epoch: 6.31 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051988965751693046		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.051988965751693046 | validation: 0.06007172690384937]
	TIME [epoch: 6.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05052394248108506		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.05052394248108506 | validation: 0.059239468901172265]
	TIME [epoch: 6.27 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05023334304766865		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.05023334304766865 | validation: 0.057218259713175845]
	TIME [epoch: 6.27 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0494412622393561		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.0494412622393561 | validation: 0.060753399188299965]
	TIME [epoch: 6.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050781746414497564		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.050781746414497564 | validation: 0.05857188432756555]
	TIME [epoch: 6.28 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05034354526035992		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.05034354526035992 | validation: 0.059631235689201645]
	TIME [epoch: 6.32 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04977310893015252		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.04977310893015252 | validation: 0.05786952693247769]
	TIME [epoch: 6.29 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04884415621122949		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.04884415621122949 | validation: 0.05848389121073756]
	TIME [epoch: 6.27 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05060011745794306		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.05060011745794306 | validation: 0.05730646947831766]
	TIME [epoch: 6.27 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04859562894901524		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.04859562894901524 | validation: 0.062246996967201575]
	TIME [epoch: 6.28 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05177182026146458		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.05177182026146458 | validation: 0.062443725480943485]
	TIME [epoch: 6.29 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052124285157239804		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.052124285157239804 | validation: 0.060687095786113116]
	TIME [epoch: 6.34 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049195792511971606		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.049195792511971606 | validation: 0.05909260221626864]
	TIME [epoch: 6.31 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04915152308956737		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.04915152308956737 | validation: 0.055887710563016516]
	TIME [epoch: 6.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04935815849342565		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.04935815849342565 | validation: 0.05891903576872773]
	TIME [epoch: 6.31 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05052559808851276		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.05052559808851276 | validation: 0.05655455006118363]
	TIME [epoch: 6.29 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051187962033580364		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.051187962033580364 | validation: 0.05931904770880375]
	TIME [epoch: 6.29 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052183761315124574		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.052183761315124574 | validation: 0.0607429999774731]
	TIME [epoch: 6.29 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04880684342418792		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.04880684342418792 | validation: 0.05898689335802553]
	TIME [epoch: 6.31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05086031483320067		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.05086031483320067 | validation: 0.06136627603041017]
	TIME [epoch: 6.29 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048899139448529666		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.048899139448529666 | validation: 0.05825395826521671]
	TIME [epoch: 6.31 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04760032843636682		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.04760032843636682 | validation: 0.05944493379519792]
	TIME [epoch: 6.27 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05027741893615294		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.05027741893615294 | validation: 0.06111846755139221]
	TIME [epoch: 6.28 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050804481058180695		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.050804481058180695 | validation: 0.05964808661109629]
	TIME [epoch: 6.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05153598452545602		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.05153598452545602 | validation: 0.05993331063030499]
	TIME [epoch: 6.31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04998659333822611		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.04998659333822611 | validation: 0.06243030408429015]
	TIME [epoch: 6.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05054471127623228		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.05054471127623228 | validation: 0.06153790925173496]
	TIME [epoch: 6.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05009344775394334		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.05009344775394334 | validation: 0.05768816962868513]
	TIME [epoch: 6.29 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049339203359733366		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.049339203359733366 | validation: 0.057370469495190084]
	TIME [epoch: 6.28 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04846844133625543		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.04846844133625543 | validation: 0.05776666507365616]
	TIME [epoch: 6.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050446447159422855		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.050446447159422855 | validation: 0.05958577518684053]
	TIME [epoch: 6.33 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048994266808423365		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.048994266808423365 | validation: 0.06180487113312092]
	TIME [epoch: 6.29 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047723152553187716		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.047723152553187716 | validation: 0.058586891592734607]
	TIME [epoch: 6.28 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04908490319295544		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.04908490319295544 | validation: 0.06277265505370723]
	TIME [epoch: 6.29 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04792523023616496		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.04792523023616496 | validation: 0.057945341034137246]
	TIME [epoch: 6.29 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04816833920504031		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.04816833920504031 | validation: 0.05855566190427715]
	TIME [epoch: 6.27 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04948655780435671		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.04948655780435671 | validation: 0.05612606085878816]
	TIME [epoch: 6.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04940113261097637		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.04940113261097637 | validation: 0.06081876211545755]
	TIME [epoch: 6.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05043418987989857		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.05043418987989857 | validation: 0.06396771624112767]
	TIME [epoch: 6.27 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04951287923930525		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.04951287923930525 | validation: 0.057781455274158657]
	TIME [epoch: 6.29 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049406639715577086		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.049406639715577086 | validation: 0.05639617142079453]
	TIME [epoch: 6.28 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047985846254603726		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.047985846254603726 | validation: 0.0587288293116893]
	TIME [epoch: 6.28 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048922339564877085		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.048922339564877085 | validation: 0.054951010532331646]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05056814742339462		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.05056814742339462 | validation: 0.05794421763626764]
	TIME [epoch: 6.32 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049309573740478146		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.049309573740478146 | validation: 0.058809004996656936]
	TIME [epoch: 6.31 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05003908205235971		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.05003908205235971 | validation: 0.057803580909790146]
	TIME [epoch: 6.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052116300361061706		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.052116300361061706 | validation: 0.056702196612768055]
	TIME [epoch: 6.29 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04708008075270474		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.04708008075270474 | validation: 0.05841755463097218]
	TIME [epoch: 6.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05000835830120227		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.05000835830120227 | validation: 0.062072046575075535]
	TIME [epoch: 6.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04797592787273093		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.04797592787273093 | validation: 0.0573858031413062]
	TIME [epoch: 6.32 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04978924173746389		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.04978924173746389 | validation: 0.057965083384697594]
	TIME [epoch: 6.31 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0488985312850997		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.0488985312850997 | validation: 0.058029629495657445]
	TIME [epoch: 6.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04999340652690104		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.04999340652690104 | validation: 0.0579289186573915]
	TIME [epoch: 6.28 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050446968281760166		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.050446968281760166 | validation: 0.0517112745848682]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04949335327614241		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.04949335327614241 | validation: 0.05578196258598146]
	TIME [epoch: 6.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05003855259246255		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.05003855259246255 | validation: 0.05787091463283725]
	TIME [epoch: 6.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049684083773902155		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.049684083773902155 | validation: 0.058364916497942045]
	TIME [epoch: 6.32 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04876620772109562		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.04876620772109562 | validation: 0.05966564467252812]
	TIME [epoch: 6.31 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0492600242033772		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.0492600242033772 | validation: 0.05541626799269034]
	TIME [epoch: 6.31 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05049709026574771		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.05049709026574771 | validation: 0.05963536438041648]
	TIME [epoch: 6.31 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04844325463594025		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.04844325463594025 | validation: 0.05879847680866195]
	TIME [epoch: 6.28 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048748367239016474		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.048748367239016474 | validation: 0.054091241833018056]
	TIME [epoch: 6.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04994566015239173		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.04994566015239173 | validation: 0.06044634760233028]
	TIME [epoch: 6.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0482752365231617		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.0482752365231617 | validation: 0.05825865757010699]
	TIME [epoch: 6.31 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0478893625631196		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.0478893625631196 | validation: 0.05734371040683942]
	TIME [epoch: 6.31 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050459894720526456		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.050459894720526456 | validation: 0.056181341795396356]
	TIME [epoch: 6.29 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04918456691103303		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.04918456691103303 | validation: 0.0561370476835819]
	TIME [epoch: 6.28 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04978837660971741		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.04978837660971741 | validation: 0.057248079271623835]
	TIME [epoch: 6.31 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04960008665986462		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.04960008665986462 | validation: 0.058870759913535325]
	TIME [epoch: 6.34 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04848564784371179		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.04848564784371179 | validation: 0.05705671968360664]
	TIME [epoch: 6.32 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04887195032862078		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.04887195032862078 | validation: 0.05855078663331579]
	TIME [epoch: 6.28 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049462573741917794		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.049462573741917794 | validation: 0.06212327837476794]
	TIME [epoch: 6.28 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0483549148910343		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.0483549148910343 | validation: 0.05687686532165535]
	TIME [epoch: 6.27 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048086838483320925		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.048086838483320925 | validation: 0.05684672961872477]
	TIME [epoch: 6.28 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04578797026195202		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.04578797026195202 | validation: 0.056732175903246786]
	TIME [epoch: 6.31 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04983047603048195		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.04983047603048195 | validation: 0.05423977517296735]
	TIME [epoch: 6.33 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04834804796787405		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.04834804796787405 | validation: 0.06148770454380795]
	TIME [epoch: 6.28 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048796955288672096		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.048796955288672096 | validation: 0.060144003244344685]
	TIME [epoch: 6.28 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0479300210469149		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.0479300210469149 | validation: 0.05542140835044116]
	TIME [epoch: 6.31 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0492684004255041		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.0492684004255041 | validation: 0.06188874434613243]
	TIME [epoch: 6.31 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052319296450290376		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.052319296450290376 | validation: 0.0573573789661036]
	TIME [epoch: 6.29 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048482157724435725		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.048482157724435725 | validation: 0.05428419488965095]
	TIME [epoch: 6.32 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049073087540687274		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.049073087540687274 | validation: 0.06318073727329564]
	TIME [epoch: 6.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04973030177262944		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.04973030177262944 | validation: 0.05877094277172088]
	TIME [epoch: 6.29 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047156971145876914		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.047156971145876914 | validation: 0.056745705467381426]
	TIME [epoch: 6.29 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05003356527076855		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.05003356527076855 | validation: 0.05952929847074417]
	TIME [epoch: 6.28 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05014422516184962		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.05014422516184962 | validation: 0.05943794488623471]
	TIME [epoch: 6.31 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04827408599516579		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.04827408599516579 | validation: 0.05954055049974664]
	TIME [epoch: 6.31 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04891890014405183		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.04891890014405183 | validation: 0.05593962889830846]
	TIME [epoch: 6.31 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04664090643501321		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.04664090643501321 | validation: 0.05613801659786303]
	TIME [epoch: 6.31 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04813381424284681		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.04813381424284681 | validation: 0.05518734137633979]
	TIME [epoch: 6.29 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049287233061914446		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.049287233061914446 | validation: 0.05876819161725928]
	TIME [epoch: 6.31 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04963940015934877		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.04963940015934877 | validation: 0.05914851593214823]
	TIME [epoch: 6.29 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04845431604036812		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.04845431604036812 | validation: 0.05968618795995315]
	TIME [epoch: 6.29 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05250008149880409		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.05250008149880409 | validation: 0.05729871303335576]
	TIME [epoch: 6.32 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04940051707657574		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.04940051707657574 | validation: 0.058458336684433634]
	TIME [epoch: 6.29 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049030618797102535		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.049030618797102535 | validation: 0.06190298748274506]
	TIME [epoch: 6.31 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0473569297665857		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.0473569297665857 | validation: 0.05840440955417479]
	TIME [epoch: 6.31 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04933237919730192		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.04933237919730192 | validation: 0.05755304595803772]
	TIME [epoch: 6.28 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046458501178839345		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.046458501178839345 | validation: 0.05954870061373049]
	TIME [epoch: 6.29 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04895339742349896		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.04895339742349896 | validation: 0.056386591158260994]
	TIME [epoch: 6.32 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04902159825689159		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.04902159825689159 | validation: 0.05490072874407126]
	TIME [epoch: 6.32 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0499884171819311		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.0499884171819311 | validation: 0.056613139169969566]
	TIME [epoch: 6.31 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046921997931606896		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.046921997931606896 | validation: 0.05872424087112354]
	TIME [epoch: 6.28 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047109365849964904		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.047109365849964904 | validation: 0.05619902415059672]
	TIME [epoch: 6.29 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04803321369375625		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.04803321369375625 | validation: 0.05676749781345456]
	TIME [epoch: 6.31 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04661388798752526		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.04661388798752526 | validation: 0.057858733501748486]
	TIME [epoch: 6.31 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04936078738503009		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.04936078738503009 | validation: 0.05573623922161837]
	TIME [epoch: 6.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04900937261713534		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.04900937261713534 | validation: 0.05752937786649653]
	TIME [epoch: 6.31 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04601565588147005		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.04601565588147005 | validation: 0.06055562720420196]
	TIME [epoch: 6.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04935835896019117		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.04935835896019117 | validation: 0.054936244275272154]
	TIME [epoch: 6.28 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049886956428578204		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.049886956428578204 | validation: 0.05923992500821054]
	TIME [epoch: 6.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046955281361899415		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.046955281361899415 | validation: 0.05561705632845191]
	TIME [epoch: 6.32 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05011849189269473		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.05011849189269473 | validation: 0.055646633145830564]
	TIME [epoch: 6.31 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051414998553028314		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.051414998553028314 | validation: 0.05790626328324731]
	TIME [epoch: 6.32 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04872567313635022		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.04872567313635022 | validation: 0.05596680532476139]
	TIME [epoch: 6.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05013453653427379		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.05013453653427379 | validation: 0.061370636252477184]
	TIME [epoch: 6.32 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04931889231649293		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.04931889231649293 | validation: 0.05477429508101187]
	TIME [epoch: 6.27 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047001513703165354		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.047001513703165354 | validation: 0.05445911512001511]
	TIME [epoch: 6.28 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047258548238861636		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.047258548238861636 | validation: 0.058313516317713115]
	TIME [epoch: 6.33 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04691209616509866		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.04691209616509866 | validation: 0.05638977402945661]
	TIME [epoch: 6.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04787557918226645		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.04787557918226645 | validation: 0.05342782571333874]
	TIME [epoch: 6.31 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04913752181787555		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.04913752181787555 | validation: 0.05683585911685845]
	TIME [epoch: 6.28 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0492971508708481		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.0492971508708481 | validation: 0.05964417179654409]
	TIME [epoch: 6.29 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047623399785358664		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.047623399785358664 | validation: 0.05725986770868702]
	TIME [epoch: 6.32 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04874854841618088		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.04874854841618088 | validation: 0.060347074297353843]
	TIME [epoch: 6.31 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04768434951645558		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.04768434951645558 | validation: 0.06223250289845422]
	TIME [epoch: 6.32 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04962432151123551		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.04962432151123551 | validation: 0.056509321453714526]
	TIME [epoch: 6.32 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04719062018530953		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.04719062018530953 | validation: 0.0567806948107627]
	TIME [epoch: 6.29 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047253073513437965		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.047253073513437965 | validation: 0.0547706228386986]
	TIME [epoch: 6.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04866370148965214		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.04866370148965214 | validation: 0.05393765161050158]
	TIME [epoch: 6.32 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04602354401167131		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.04602354401167131 | validation: 0.0571808544286607]
	TIME [epoch: 6.32 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04725558735265531		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.04725558735265531 | validation: 0.05582914853203983]
	TIME [epoch: 6.32 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04794278591988224		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.04794278591988224 | validation: 0.05681723646357726]
	TIME [epoch: 6.32 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04760163985473528		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.04760163985473528 | validation: 0.0571119407275452]
	TIME [epoch: 6.32 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04833357975415169		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.04833357975415169 | validation: 0.055397623621513674]
	TIME [epoch: 6.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04775178297464733		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.04775178297464733 | validation: 0.0578737607107864]
	TIME [epoch: 6.31 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04826174201103879		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.04826174201103879 | validation: 0.057919321206581026]
	TIME [epoch: 6.32 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04846508861616736		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.04846508861616736 | validation: 0.05659869154300379]
	TIME [epoch: 6.35 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0475685318590652		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.0475685318590652 | validation: 0.05382123507737522]
	TIME [epoch: 6.33 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04984067293648302		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.04984067293648302 | validation: 0.057681199203821976]
	TIME [epoch: 6.28 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04758431599980395		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.04758431599980395 | validation: 0.05554004319123575]
	TIME [epoch: 6.29 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04795780787824898		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.04795780787824898 | validation: 0.05689264125534706]
	TIME [epoch: 6.29 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047229014922183774		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.047229014922183774 | validation: 0.05710213964993576]
	TIME [epoch: 6.28 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047329974859538576		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.047329974859538576 | validation: 0.05978547222717021]
	TIME [epoch: 6.31 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04910794598696089		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.04910794598696089 | validation: 0.05566625513730399]
	TIME [epoch: 6.33 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04819608194381985		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.04819608194381985 | validation: 0.055508248105321345]
	TIME [epoch: 6.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049976683766251945		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.049976683766251945 | validation: 0.059434991456123215]
	TIME [epoch: 6.27 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04731301372536857		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.04731301372536857 | validation: 0.0596604251742337]
	TIME [epoch: 6.32 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04668989496619198		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.04668989496619198 | validation: 0.05547544533420104]
	TIME [epoch: 6.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04846772674113484		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.04846772674113484 | validation: 0.05624450810116989]
	TIME [epoch: 6.28 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05047460064575121		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.05047460064575121 | validation: 0.05497682200826448]
	TIME [epoch: 6.31 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04854255855540254		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.04854255855540254 | validation: 0.057723202906333884]
	TIME [epoch: 6.31 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0482393601239334		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.0482393601239334 | validation: 0.05736221879855308]
	TIME [epoch: 6.32 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04893034164793961		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.04893034164793961 | validation: 0.056050486600785376]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_193424/states/model_phi1_2a_v_mmd1_935.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4467.211 seconds.
