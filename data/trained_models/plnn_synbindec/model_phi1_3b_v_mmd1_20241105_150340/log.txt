Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2764597885

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.698791969642915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.698791969642915 | validation: 4.638673160592467]
	TIME [epoch: 249 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.22301550625233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.22301550625233 | validation: 3.589916087032784]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.267849425685851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.267849425685851 | validation: 3.689743935056044]
	TIME [epoch: 1.38 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.876789350045112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.876789350045112 | validation: 3.2195054142275645]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.14990321939585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14990321939585 | validation: 3.373815907472237]
	TIME [epoch: 1.38 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.166533817973891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.166533817973891 | validation: 3.2656398334615084]
	TIME [epoch: 1.38 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.064522718650543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.064522718650543 | validation: 2.99716291387284]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.994889590051589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.994889590051589 | validation: 2.930292649948333]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.957872848385834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.957872848385834 | validation: 2.9677145867828125]
	TIME [epoch: 1.38 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.907535223808701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.907535223808701 | validation: 2.911228249523282]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.86062254428198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.86062254428198 | validation: 2.7921536517752252]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.814725043181806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.814725043181806 | validation: 2.767795986465973]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7643009430974868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7643009430974868 | validation: 2.6948196500431907]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7242338355405717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7242338355405717 | validation: 2.662069257772432]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6857070737274458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6857070737274458 | validation: 2.6741951790705887]
	TIME [epoch: 1.38 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7061296189876276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7061296189876276 | validation: 2.9893204341736803]
	TIME [epoch: 1.38 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.897757823090512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.897757823090512 | validation: 2.6696269899548817]
	TIME [epoch: 1.38 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.700977633167186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.700977633167186 | validation: 2.6687593337633646]
	TIME [epoch: 1.38 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.650278058341241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.650278058341241 | validation: 2.69435241328885]
	TIME [epoch: 1.38 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.761662110600611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.761662110600611 | validation: 2.7489290535963424]
	TIME [epoch: 1.38 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.681638015867776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.681638015867776 | validation: 2.5282648787194013]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.566243902956176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.566243902956176 | validation: 2.4740258306577463]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.544494842266552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.544494842266552 | validation: 2.5625726495104306]
	TIME [epoch: 1.37 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5409318796124065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5409318796124065 | validation: 2.441846972870625]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.494748088609449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.494748088609449 | validation: 2.4306595132801925]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.463778244228833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.463778244228833 | validation: 2.437497561579507]
	TIME [epoch: 1.38 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.45511867504188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.45511867504188 | validation: 2.3959578187151296]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4411013357030455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4411013357030455 | validation: 2.422133320514087]
	TIME [epoch: 1.38 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.43187342512539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.43187342512539 | validation: 2.384191338773694]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4271651764884417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4271651764884417 | validation: 2.4454611748009665]
	TIME [epoch: 1.38 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.437613805403145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.437613805403145 | validation: 2.3895026531653634]
	TIME [epoch: 1.38 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4218830548558627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4218830548558627 | validation: 2.4134761038424566]
	TIME [epoch: 1.38 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3994406784081654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3994406784081654 | validation: 2.340986736411396]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3623534165248046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3623534165248046 | validation: 2.3291688580920775]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.339341270127269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.339341270127269 | validation: 2.3156328984542323]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.321554550389467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.321554550389467 | validation: 2.3020621686718665]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.306146374396958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.306146374396958 | validation: 2.2969459919257615]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.291625417157975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.291625417157975 | validation: 2.283726012153592]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.279616729392559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.279616729392559 | validation: 2.263862863353969]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.261531506959877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.261531506959877 | validation: 2.265165526155365]
	TIME [epoch: 1.39 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.243968801983385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.243968801983385 | validation: 2.26431288174715]
	TIME [epoch: 1.4 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.221894077672884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.221894077672884 | validation: 2.2585742034417886]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1834826462977164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1834826462977164 | validation: 2.257004777442939]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1021283532218966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1021283532218966 | validation: 2.2028079141093166]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.984156058906475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.984156058906475 | validation: 2.114059355550534]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.831660894588386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.831660894588386 | validation: 1.9310835620182425]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6179365492012563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6179365492012563 | validation: 1.7637616764210584]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3610690123680222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3610690123680222 | validation: 1.8119384460302976]
	TIME [epoch: 1.39 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.20342846118027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.20342846118027 | validation: 1.3326344048527359]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6657664634938556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6657664634938556 | validation: 1.5579481002369253]
	TIME [epoch: 1.38 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6814258597237968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6814258597237968 | validation: 4.280805636887537]
	TIME [epoch: 1.38 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4578085210418807		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.4578085210418807 | validation: 3.356637320309907]
	TIME [epoch: 1.38 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.056750216407462		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.056750216407462 | validation: 2.4034215749597543]
	TIME [epoch: 1.38 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.391376241709167		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.391376241709167 | validation: 1.5967851158216861]
	TIME [epoch: 1.38 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7397979922524485		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.7397979922524485 | validation: 1.0971635757524532]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2923628546633918		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.2923628546633918 | validation: 0.99420554835309]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207049644696528		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.207049644696528 | validation: 0.984644242884325]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1878286701507836		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.1878286701507836 | validation: 0.9641216451490966]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0937956991437323		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.0937956991437323 | validation: 0.9760596847160894]
	TIME [epoch: 1.38 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096028275374727		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.096028275374727 | validation: 0.8565749602988373]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9848249327788904		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.9848249327788904 | validation: 0.7852199347465193]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89300885737503		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.89300885737503 | validation: 0.7913162353685036]
	TIME [epoch: 1.38 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013520646358886		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9013520646358886 | validation: 0.7668816238003132]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766673799535888		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.8766673799535888 | validation: 0.7749296611330748]
	TIME [epoch: 1.38 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420586679841522		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.8420586679841522 | validation: 0.7354791362631492]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.825723703424963		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.825723703424963 | validation: 0.7248853359889584]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127992829331823		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.8127992829331823 | validation: 0.7169543014360481]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800920274306892		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.800920274306892 | validation: 0.7574332953061106]
	TIME [epoch: 1.38 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955925193177505		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7955925193177505 | validation: 0.7366494676327583]
	TIME [epoch: 1.38 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479391878589894		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8479391878589894 | validation: 1.324686500439055]
	TIME [epoch: 1.38 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2074453838118695		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.2074453838118695 | validation: 0.9331785049664442]
	TIME [epoch: 1.38 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160647033232285		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.1160647033232285 | validation: 0.7725426914191417]
	TIME [epoch: 1.38 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364089850197743		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.8364089850197743 | validation: 0.8739663230656389]
	TIME [epoch: 1.38 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86592115862011		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.86592115862011 | validation: 0.705175627844466]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820723567501986		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.7820723567501986 | validation: 0.7166363968722221]
	TIME [epoch: 1.38 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721422129258438		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.7721422129258438 | validation: 0.7927124918863623]
	TIME [epoch: 1.38 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868104349617937		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.7868104349617937 | validation: 0.7013780315622393]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585085150445849		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.7585085150445849 | validation: 0.709154977811492]
	TIME [epoch: 1.37 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75180846690527		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.75180846690527 | validation: 0.7374736112474672]
	TIME [epoch: 1.37 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513593430863751		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7513593430863751 | validation: 0.7189076935402681]
	TIME [epoch: 1.38 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454229920070785		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.7454229920070785 | validation: 0.7066243007680174]
	TIME [epoch: 1.37 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7462304249598802		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7462304249598802 | validation: 0.7038517609842853]
	TIME [epoch: 1.37 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400707696140032		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.7400707696140032 | validation: 0.766535962148345]
	TIME [epoch: 1.37 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517322784999964		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.7517322784999964 | validation: 0.7325635874538567]
	TIME [epoch: 1.37 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764041379937419		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7764041379937419 | validation: 1.0435320909863834]
	TIME [epoch: 1.37 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033675337304056		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.9033675337304056 | validation: 0.8861638743633858]
	TIME [epoch: 1.37 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0413953131241138		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.0413953131241138 | validation: 0.8201088977045958]
	TIME [epoch: 1.37 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742830847100924		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7742830847100924 | validation: 0.7678112711385188]
	TIME [epoch: 1.37 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606220591288688		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.7606220591288688 | validation: 0.7572903577114161]
	TIME [epoch: 1.38 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949581266830384		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7949581266830384 | validation: 0.7848672311402672]
	TIME [epoch: 1.38 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474552800919193		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7474552800919193 | validation: 0.7228776601980095]
	TIME [epoch: 1.38 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343702543766701		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.7343702543766701 | validation: 0.7387477134336606]
	TIME [epoch: 1.38 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327261249145635		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7327261249145635 | validation: 0.7347424198033162]
	TIME [epoch: 1.38 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256261187040801		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.7256261187040801 | validation: 0.7303911602721782]
	TIME [epoch: 1.38 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278399919499314		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7278399919499314 | validation: 0.7420038399951376]
	TIME [epoch: 1.38 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221136942237317		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.7221136942237317 | validation: 0.7561593680162759]
	TIME [epoch: 1.38 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784375795955751		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.784375795955751 | validation: 0.964751555264755]
	TIME [epoch: 1.38 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8255432730855756		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8255432730855756 | validation: 0.8312719620510945]
	TIME [epoch: 1.38 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8965381080390441		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8965381080390441 | validation: 0.8378121392802369]
	TIME [epoch: 1.38 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658149308800714		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.7658149308800714 | validation: 0.7284042935110492]
	TIME [epoch: 1.38 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165496041232033		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7165496041232033 | validation: 0.7213071433930831]
	TIME [epoch: 1.38 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178666184792459		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7178666184792459 | validation: 0.7298278195645062]
	TIME [epoch: 1.38 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207926807165518		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7207926807165518 | validation: 0.7292619304932378]
	TIME [epoch: 1.37 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102158552571731		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7102158552571731 | validation: 0.7333880717143567]
	TIME [epoch: 1.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105945492126293		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7105945492126293 | validation: 0.7259636065231699]
	TIME [epoch: 1.37 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704001063934715		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.704001063934715 | validation: 0.7366387962435819]
	TIME [epoch: 1.37 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299909003364283		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7299909003364283 | validation: 1.0465372702813311]
	TIME [epoch: 1.37 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564867942979736		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8564867942979736 | validation: 0.9674332956865546]
	TIME [epoch: 1.37 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0648896914044292		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.0648896914044292 | validation: 0.8096608912881715]
	TIME [epoch: 1.37 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437554015649502		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.7437554015649502 | validation: 0.866164262386064]
	TIME [epoch: 1.37 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792142418529917		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.792142418529917 | validation: 0.7995750698805131]
	TIME [epoch: 1.37 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030476236575276		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8030476236575276 | validation: 0.7567247948485577]
	TIME [epoch: 1.38 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7141557279192962		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7141557279192962 | validation: 0.8127410569845367]
	TIME [epoch: 1.38 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7482655347020563		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7482655347020563 | validation: 0.7220719362187882]
	TIME [epoch: 1.37 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040907062386917		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7040907062386917 | validation: 0.7412225703974541]
	TIME [epoch: 1.37 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7030755272105268		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7030755272105268 | validation: 0.7209410637124306]
	TIME [epoch: 1.37 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050226920356761		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7050226920356761 | validation: 0.7408372856392842]
	TIME [epoch: 1.37 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974782696947222		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.6974782696947222 | validation: 0.7159879665827509]
	TIME [epoch: 1.38 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886497889781815		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.6886497889781815 | validation: 0.7254824585328673]
	TIME [epoch: 1.38 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68621664530808		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.68621664530808 | validation: 0.7294844438403763]
	TIME [epoch: 1.37 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777255169473192		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.6777255169473192 | validation: 0.7166298616871121]
	TIME [epoch: 1.37 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709290971993154		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.709290971993154 | validation: 1.0615443894647691]
	TIME [epoch: 1.38 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648397411596207		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8648397411596207 | validation: 0.8770533389367867]
	TIME [epoch: 1.37 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037209720352226		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9037209720352226 | validation: 0.7702810262949961]
	TIME [epoch: 1.37 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969029100139684		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.6969029100139684 | validation: 0.7516372430001301]
	TIME [epoch: 1.38 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909571037120245		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.6909571037120245 | validation: 0.7246864766417069]
	TIME [epoch: 1.37 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181032123522024		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7181032123522024 | validation: 0.7347713299826832]
	TIME [epoch: 1.38 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779730971779556		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6779730971779556 | validation: 0.6866633572951975]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65843659872021		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.65843659872021 | validation: 0.6737697980313224]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531289946801708		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6531289946801708 | validation: 0.7232029013091247]
	TIME [epoch: 1.38 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679314922900708		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.679314922900708 | validation: 0.707379056619927]
	TIME [epoch: 1.38 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501629965265866		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7501629965265866 | validation: 0.849665066553789]
	TIME [epoch: 1.38 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664213440682133		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7664213440682133 | validation: 0.6694279578247517]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869125521971432		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6869125521971432 | validation: 0.7487141781371783]
	TIME [epoch: 1.38 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834256049314147		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6834256049314147 | validation: 0.6885591132948689]
	TIME [epoch: 1.38 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063539907115911		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7063539907115911 | validation: 0.7047195288833902]
	TIME [epoch: 1.38 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332580583921702		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6332580583921702 | validation: 0.6272444735954963]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489567011130132		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.6489567011130132 | validation: 0.7496102879557692]
	TIME [epoch: 1.38 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564323639434911		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.6564323639434911 | validation: 0.6391210140591288]
	TIME [epoch: 1.38 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737756886759396		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6737756886759396 | validation: 0.7426114222121062]
	TIME [epoch: 1.37 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533773838817121		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6533773838817121 | validation: 0.6489180282168036]
	TIME [epoch: 1.37 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893166239465092		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6893166239465092 | validation: 0.66807678048646]
	TIME [epoch: 1.37 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6085011753457411		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6085011753457411 | validation: 0.5758999499317456]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570186938892049		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.570186938892049 | validation: 0.5973629967171606]
	TIME [epoch: 1.38 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562682394898725		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.5562682394898725 | validation: 0.5592575966733276]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5539553300668161		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.5539553300668161 | validation: 0.599995635776775]
	TIME [epoch: 1.38 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719764335805566		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.5719764335805566 | validation: 0.5574058137801784]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6124039817844408		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6124039817844408 | validation: 0.7040200651850032]
	TIME [epoch: 1.38 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065013351716445		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6065013351716445 | validation: 0.6779230158377496]
	TIME [epoch: 1.38 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689495766450849		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6689495766450849 | validation: 0.6429285931577557]
	TIME [epoch: 1.38 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5653529036469933		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.5653529036469933 | validation: 0.5209009014424096]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5134720196983044		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.5134720196983044 | validation: 0.5229245135468544]
	TIME [epoch: 1.38 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49382604897511856		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.49382604897511856 | validation: 0.5225678600329032]
	TIME [epoch: 1.38 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49330216342005623		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.49330216342005623 | validation: 0.5055546170461453]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48847809306750756		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.48847809306750756 | validation: 0.4865044643763608]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47961937013753286		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.47961937013753286 | validation: 0.5108182729415823]
	TIME [epoch: 1.38 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46901602624326755		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.46901602624326755 | validation: 0.4773602319606452]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4800791244595851		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.4800791244595851 | validation: 0.8795397976201405]
	TIME [epoch: 1.38 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761262729716683		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6761262729716683 | validation: 0.9131175589767534]
	TIME [epoch: 1.38 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420793030401941		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7420793030401941 | validation: 0.4696254611810009]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185934431408532		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.5185934431408532 | validation: 0.6616024551517585]
	TIME [epoch: 1.38 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612578828934786		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5612578828934786 | validation: 0.4636667339619269]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4650023898611037		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.4650023898611037 | validation: 0.4263468341201299]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4661352201327466		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.4661352201327466 | validation: 0.45045642935683605]
	TIME [epoch: 1.38 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4413211233714982		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.4413211233714982 | validation: 0.43848913972897724]
	TIME [epoch: 1.38 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42681763379723203		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.42681763379723203 | validation: 0.40808096862963295]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4243924412843213		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.4243924412843213 | validation: 0.4242081254012675]
	TIME [epoch: 1.38 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41765118706935894		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.41765118706935894 | validation: 0.40417136446830604]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40443503497129146		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.40443503497129146 | validation: 0.4053350348536876]
	TIME [epoch: 1.38 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4005907638333743		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.4005907638333743 | validation: 0.4095993235173335]
	TIME [epoch: 1.38 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4020740959326405		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.4020740959326405 | validation: 0.44268611338027397]
	TIME [epoch: 1.38 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4081050490358826		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.4081050490358826 | validation: 0.44645317753128283]
	TIME [epoch: 1.38 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.432204712701667		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.432204712701667 | validation: 0.5653399507317849]
	TIME [epoch: 1.38 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47198341622249784		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.47198341622249784 | validation: 0.4661850764416527]
	TIME [epoch: 1.38 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4621233733546666		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.4621233733546666 | validation: 0.3664228706896904]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3685053858048147		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.3685053858048147 | validation: 0.3771716447916662]
	TIME [epoch: 1.38 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36242521405935946		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.36242521405935946 | validation: 0.337036135500255]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3781585757916706		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.3781585757916706 | validation: 0.38169671358890045]
	TIME [epoch: 1.38 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3533270014414765		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.3533270014414765 | validation: 0.33674533059970124]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3557102328203281		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.3557102328203281 | validation: 0.3587839534118927]
	TIME [epoch: 1.38 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3506606832725885		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.3506606832725885 | validation: 0.3373740919568131]
	TIME [epoch: 1.38 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33473817964921326		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.33473817964921326 | validation: 0.38210875276945183]
	TIME [epoch: 1.37 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3414829632826988		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.3414829632826988 | validation: 0.3504241512973179]
	TIME [epoch: 1.38 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530554497755836		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.3530554497755836 | validation: 0.5770725140239255]
	TIME [epoch: 1.37 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4381372099146273		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.4381372099146273 | validation: 0.623654512066527]
	TIME [epoch: 1.37 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342674370431437		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5342674370431437 | validation: 0.32426779451024834]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33194163181306363		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.33194163181306363 | validation: 0.4056013915191845]
	TIME [epoch: 1.38 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3685309710337075		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.3685309710337075 | validation: 0.32894638548921357]
	TIME [epoch: 1.38 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.387208005253939		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.387208005253939 | validation: 0.3131138797516255]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162048957398125		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.3162048957398125 | validation: 0.35386169382505583]
	TIME [epoch: 1.38 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31870591623290745		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.31870591623290745 | validation: 0.30662319447557923]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.309229932638323		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.309229932638323 | validation: 0.3219213579145735]
	TIME [epoch: 1.38 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29423254044458225		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.29423254044458225 | validation: 0.2996137699663614]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28598609247036505		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.28598609247036505 | validation: 0.2913818474468221]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27835287100817885		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.27835287100817885 | validation: 0.299834527209754]
	TIME [epoch: 1.38 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27139917669559604		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.27139917669559604 | validation: 0.2767797373268982]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751877317861104		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.2751877317861104 | validation: 0.34109576519501794]
	TIME [epoch: 1.38 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31680410090156946		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.31680410090156946 | validation: 0.33004507135707467]
	TIME [epoch: 1.38 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3744134496766697		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.3744134496766697 | validation: 0.345401765758745]
	TIME [epoch: 1.38 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3189512379700297		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.3189512379700297 | validation: 0.710300413303063]
	TIME [epoch: 1.38 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4596049553405453		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.4596049553405453 | validation: 0.5010728943069989]
	TIME [epoch: 267 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289550773943038		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.4289550773943038 | validation: 0.2729642631063522]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077419112094065		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.3077419112094065 | validation: 0.365287882296373]
	TIME [epoch: 2.74 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34616181611458013		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.34616181611458013 | validation: 0.26712951390199724]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25016939539044036		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.25016939539044036 | validation: 0.28364725062284857]
	TIME [epoch: 2.74 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2523285172889111		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.2523285172889111 | validation: 0.3013617091810524]
	TIME [epoch: 2.74 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24259088801612538		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.24259088801612538 | validation: 0.2802669794357677]
	TIME [epoch: 2.74 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23235092621626421		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.23235092621626421 | validation: 0.2792420347815121]
	TIME [epoch: 2.74 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23595649864889828		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.23595649864889828 | validation: 0.2600018301760167]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24085724654844118		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.24085724654844118 | validation: 0.31698485316666103]
	TIME [epoch: 2.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28082225947302314		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.28082225947302314 | validation: 0.2750021922249453]
	TIME [epoch: 2.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26398297823058736		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.26398297823058736 | validation: 0.30415187895424833]
	TIME [epoch: 2.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743869708720649		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.2743869708720649 | validation: 0.2667807400357506]
	TIME [epoch: 2.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24537635943705532		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.24537635943705532 | validation: 0.3285560786115822]
	TIME [epoch: 2.74 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047052229956716		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.3047052229956716 | validation: 0.2884539223000557]
	TIME [epoch: 2.74 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25858668839408017		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.25858668839408017 | validation: 0.337342417173078]
	TIME [epoch: 2.74 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2570784144895956		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.2570784144895956 | validation: 0.301758198039747]
	TIME [epoch: 2.74 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26322841233534866		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.26322841233534866 | validation: 0.47162694112529463]
	TIME [epoch: 2.74 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195306838062357		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.3195306838062357 | validation: 0.38219485491749955]
	TIME [epoch: 2.74 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3333507375652031		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.3333507375652031 | validation: 0.2650813031212522]
	TIME [epoch: 2.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22920715372701883		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.22920715372701883 | validation: 0.3288919775037482]
	TIME [epoch: 2.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30273775428566824		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.30273775428566824 | validation: 0.2630132370930485]
	TIME [epoch: 2.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24167919390747444		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.24167919390747444 | validation: 0.2978215390255504]
	TIME [epoch: 2.74 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23580126695958237		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.23580126695958237 | validation: 0.3359370164796439]
	TIME [epoch: 2.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26256944965669016		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.26256944965669016 | validation: 0.2685437999628634]
	TIME [epoch: 2.74 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21045237761755972		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.21045237761755972 | validation: 0.25007077188678506]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20117654820921463		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.20117654820921463 | validation: 0.2806057228121773]
	TIME [epoch: 2.74 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404204883634816		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.2404204883634816 | validation: 0.2707641353459987]
	TIME [epoch: 2.74 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24423657110860825		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.24423657110860825 | validation: 0.2681974346261394]
	TIME [epoch: 2.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2029627976213708		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.2029627976213708 | validation: 0.25444680480317483]
	TIME [epoch: 2.74 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20325302186340907		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.20325302186340907 | validation: 0.262992900542866]
	TIME [epoch: 2.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19981386318895855		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.19981386318895855 | validation: 0.3032255900466827]
	TIME [epoch: 2.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2261420272505753		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.2261420272505753 | validation: 0.29942606191842785]
	TIME [epoch: 2.74 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2582363445187052		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.2582363445187052 | validation: 0.3260068479907823]
	TIME [epoch: 2.74 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26355693314295414		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.26355693314295414 | validation: 0.25859848447097733]
	TIME [epoch: 2.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20423791188826812		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.20423791188826812 | validation: 0.26128491304005136]
	TIME [epoch: 2.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20154150718199615		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.20154150718199615 | validation: 0.2629704769653528]
	TIME [epoch: 2.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1872268115562222		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.1872268115562222 | validation: 0.24893387974314468]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1823401264280611		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.1823401264280611 | validation: 0.26717739001440094]
	TIME [epoch: 2.74 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2169581292333245		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.2169581292333245 | validation: 0.33365421726300837]
	TIME [epoch: 2.74 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3011026026930945		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.3011026026930945 | validation: 0.277804025780663]
	TIME [epoch: 2.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22924586556853455		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.22924586556853455 | validation: 0.2737762328038706]
	TIME [epoch: 2.74 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20791169466468076		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.20791169466468076 | validation: 0.2617053095535751]
	TIME [epoch: 2.74 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18579652367925953		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.18579652367925953 | validation: 0.2746842439860506]
	TIME [epoch: 2.74 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22119678703455492		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.22119678703455492 | validation: 0.25346322324168197]
	TIME [epoch: 2.86 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18005068190894727		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.18005068190894727 | validation: 0.28951993479818555]
	TIME [epoch: 2.74 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2498392164751931		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.2498392164751931 | validation: 0.2994854649767588]
	TIME [epoch: 2.74 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639887173771931		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.2639887173771931 | validation: 0.33815316403228607]
	TIME [epoch: 2.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508995374241363		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.2508995374241363 | validation: 0.27904221586653727]
	TIME [epoch: 2.74 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19720218792316138		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.19720218792316138 | validation: 0.2437432269362333]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18190722233419146		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.18190722233419146 | validation: 0.2823877919179421]
	TIME [epoch: 2.75 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17506749018287684		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.17506749018287684 | validation: 0.24815618578899762]
	TIME [epoch: 2.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17695428111019937		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.17695428111019937 | validation: 0.26214633698765805]
	TIME [epoch: 2.74 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22550259741707307		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.22550259741707307 | validation: 0.5671601623452869]
	TIME [epoch: 2.74 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3813832314637348		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3813832314637348 | validation: 0.2285577777035806]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16763047024894775		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.16763047024894775 | validation: 0.2890959438360219]
	TIME [epoch: 2.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22680178284718341		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.22680178284718341 | validation: 0.5342630765682047]
	TIME [epoch: 2.74 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3772905269357186		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.3772905269357186 | validation: 0.36756245560089]
	TIME [epoch: 2.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27635365279594093		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.27635365279594093 | validation: 0.2762171891393579]
	TIME [epoch: 2.74 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21969983094812776		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.21969983094812776 | validation: 0.27090726017097205]
	TIME [epoch: 2.74 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21816513169846846		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.21816513169846846 | validation: 0.24107212338612866]
	TIME [epoch: 2.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16737378050522644		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.16737378050522644 | validation: 0.23444838384843966]
	TIME [epoch: 2.74 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17192968933145558		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.17192968933145558 | validation: 0.25946257265375844]
	TIME [epoch: 2.74 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22449992713797354		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.22449992713797354 | validation: 0.23475944462799153]
	TIME [epoch: 2.74 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21435180195662176		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.21435180195662176 | validation: 0.24380303093906308]
	TIME [epoch: 2.74 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16923013463567896		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.16923013463567896 | validation: 0.22916395604430467]
	TIME [epoch: 2.73 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16156818427097525		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.16156818427097525 | validation: 0.22035378658869367]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1532777989053345		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.1532777989053345 | validation: 0.231839266909601]
	TIME [epoch: 2.74 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14796221845264168		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.14796221845264168 | validation: 0.22585752028011274]
	TIME [epoch: 2.74 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14951014607381408		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.14951014607381408 | validation: 0.2992160966781676]
	TIME [epoch: 2.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19763590246904503		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.19763590246904503 | validation: 0.23911560039901444]
	TIME [epoch: 2.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17671786819742089		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.17671786819742089 | validation: 0.23173376904312729]
	TIME [epoch: 2.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17082855767921892		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.17082855767921892 | validation: 0.7094954171518008]
	TIME [epoch: 2.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4752505498038486		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.4752505498038486 | validation: 0.23521116682629295]
	TIME [epoch: 2.74 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727081216466069		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.1727081216466069 | validation: 0.3045496628934868]
	TIME [epoch: 2.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2676367468703191		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.2676367468703191 | validation: 0.24413206318708802]
	TIME [epoch: 2.74 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20450098229714825		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.20450098229714825 | validation: 0.2318547858249306]
	TIME [epoch: 2.74 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15898070788168847		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.15898070788168847 | validation: 0.2198309687918803]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590895873392869		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.1590895873392869 | validation: 0.2182123280729695]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1438746107854163		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.1438746107854163 | validation: 0.22131262500394566]
	TIME [epoch: 2.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14694222100316012		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.14694222100316012 | validation: 0.20655677467940411]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14149162527019502		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.14149162527019502 | validation: 0.20847182820079016]
	TIME [epoch: 2.74 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1383583310797141		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.1383583310797141 | validation: 0.22654445929753964]
	TIME [epoch: 2.74 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18128228630015375		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.18128228630015375 | validation: 0.28473534799798855]
	TIME [epoch: 2.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2579725551126325		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.2579725551126325 | validation: 0.2262248254694615]
	TIME [epoch: 2.73 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18247669691908092		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.18247669691908092 | validation: 0.22660243641350913]
	TIME [epoch: 2.74 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16774990729705294		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.16774990729705294 | validation: 0.217051939482381]
	TIME [epoch: 2.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14807073478708196		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.14807073478708196 | validation: 0.2010300677079384]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329018168009574		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.1329018168009574 | validation: 0.20448830773295867]
	TIME [epoch: 2.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978685159891482		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.12978685159891482 | validation: 0.22271411146901166]
	TIME [epoch: 2.74 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16263586554426027		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.16263586554426027 | validation: 0.2933622755977299]
	TIME [epoch: 2.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2092732586698324		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.2092732586698324 | validation: 0.22151268871924215]
	TIME [epoch: 2.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1723213652675262		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.1723213652675262 | validation: 0.2751906481363941]
	TIME [epoch: 2.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2101492883886846		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.2101492883886846 | validation: 0.22854189264621236]
	TIME [epoch: 2.73 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20188086702212352		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.20188086702212352 | validation: 0.2147215026570778]
	TIME [epoch: 2.74 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15910071822805555		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.15910071822805555 | validation: 0.2256395510613224]
	TIME [epoch: 2.73 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1592679631258448		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.1592679631258448 | validation: 0.2312634114282089]
	TIME [epoch: 2.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23261656988401969		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.23261656988401969 | validation: 0.23153075923019595]
	TIME [epoch: 2.74 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15135120860100476		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.15135120860100476 | validation: 0.2232698316530761]
	TIME [epoch: 2.74 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576285115025105		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.1576285115025105 | validation: 0.20976544834794073]
	TIME [epoch: 2.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14180131104566268		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.14180131104566268 | validation: 0.24809526374714852]
	TIME [epoch: 2.74 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1616150282927195		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.1616150282927195 | validation: 0.23477160664538996]
	TIME [epoch: 2.73 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17682498259750762		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.17682498259750762 | validation: 0.20783373926305632]
	TIME [epoch: 2.73 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13000485004249368		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.13000485004249368 | validation: 0.20657154442166245]
	TIME [epoch: 2.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16620511061382223		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.16620511061382223 | validation: 0.24073575930180288]
	TIME [epoch: 2.73 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2021061473750703		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.2021061473750703 | validation: 0.21999349246523697]
	TIME [epoch: 2.73 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418479831522826		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.1418479831522826 | validation: 0.2098457540445281]
	TIME [epoch: 2.73 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13429240370251247		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.13429240370251247 | validation: 0.2306555037904881]
	TIME [epoch: 2.73 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16799428414097692		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.16799428414097692 | validation: 0.27069614412587617]
	TIME [epoch: 2.73 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17420434733703297		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.17420434733703297 | validation: 0.22012104128049997]
	TIME [epoch: 2.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17030390160151299		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.17030390160151299 | validation: 0.20595736160909814]
	TIME [epoch: 2.73 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14394387136123127		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.14394387136123127 | validation: 0.19908402467580477]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12468451949808962		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.12468451949808962 | validation: 0.2038418025646192]
	TIME [epoch: 2.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13217168402391127		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.13217168402391127 | validation: 0.36121907129517966]
	TIME [epoch: 2.73 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42019309040762537		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.42019309040762537 | validation: 0.3025526492150042]
	TIME [epoch: 2.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34298292577974315		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.34298292577974315 | validation: 0.30914622114933255]
	TIME [epoch: 2.84 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20468065866727764		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.20468065866727764 | validation: 0.27301488527762147]
	TIME [epoch: 2.73 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19942101227658043		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.19942101227658043 | validation: 0.2491347981887321]
	TIME [epoch: 2.73 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21271634555053578		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.21271634555053578 | validation: 0.2996411443713618]
	TIME [epoch: 2.72 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1947662698437525		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.1947662698437525 | validation: 0.2558665812322161]
	TIME [epoch: 2.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18130728803294804		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.18130728803294804 | validation: 0.23784177095740136]
	TIME [epoch: 2.73 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1581445607620854		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.1581445607620854 | validation: 0.22880246428028192]
	TIME [epoch: 2.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15110038408509524		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.15110038408509524 | validation: 0.21939879386436176]
	TIME [epoch: 2.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14979834374911813		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.14979834374911813 | validation: 0.22733494640853247]
	TIME [epoch: 2.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364478492035068		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.1364478492035068 | validation: 0.21260435897195704]
	TIME [epoch: 2.72 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1257726180915855		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.1257726180915855 | validation: 0.7301165140745949]
	TIME [epoch: 2.73 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46100219572788453		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.46100219572788453 | validation: 0.26575835341385257]
	TIME [epoch: 2.73 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18141917648905184		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.18141917648905184 | validation: 0.29172282986580794]
	TIME [epoch: 2.73 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665480050967028		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.2665480050967028 | validation: 0.22342241452623474]
	TIME [epoch: 2.73 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15052401314167052		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.15052401314167052 | validation: 0.21265688772176825]
	TIME [epoch: 2.73 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16248290179358266		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.16248290179358266 | validation: 0.20002460265423683]
	TIME [epoch: 2.73 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13253800710322763		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.13253800710322763 | validation: 0.20386762853259552]
	TIME [epoch: 2.73 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12600342445331117		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.12600342445331117 | validation: 0.20337898539529295]
	TIME [epoch: 2.72 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258720831957062		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.1258720831957062 | validation: 0.19241658945885046]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11590207640969247		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.11590207640969247 | validation: 0.19800694744553837]
	TIME [epoch: 2.74 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754455969286444		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.11754455969286444 | validation: 0.19682672306906096]
	TIME [epoch: 2.74 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100517497799523		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.12100517497799523 | validation: 0.20419147722335299]
	TIME [epoch: 2.74 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11569616104148152		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.11569616104148152 | validation: 0.19678655487687766]
	TIME [epoch: 2.73 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11543641982768482		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.11543641982768482 | validation: 0.2020330148260936]
	TIME [epoch: 2.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11865660780002654		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.11865660780002654 | validation: 0.20638321500714665]
	TIME [epoch: 2.73 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11788462539248638		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.11788462539248638 | validation: 0.21211121102624883]
	TIME [epoch: 2.73 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13346659273243408		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.13346659273243408 | validation: 0.2099339828938719]
	TIME [epoch: 2.74 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563221645076307		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.13563221645076307 | validation: 0.23098703332543935]
	TIME [epoch: 2.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1693324471721067		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.1693324471721067 | validation: 0.1992858907723057]
	TIME [epoch: 2.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1069428684313072		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.1069428684313072 | validation: 0.20171007604683022]
	TIME [epoch: 2.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11563877764099127		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.11563877764099127 | validation: 0.20361765048900413]
	TIME [epoch: 2.74 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1155227908847915		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.1155227908847915 | validation: 0.2611599219958382]
	TIME [epoch: 2.74 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1662341800855995		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.1662341800855995 | validation: 0.3189377256909673]
	TIME [epoch: 2.74 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35125041760538495		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.35125041760538495 | validation: 0.20869239251044475]
	TIME [epoch: 2.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268361660445643		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.1268361660445643 | validation: 0.2270209116870716]
	TIME [epoch: 2.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16886684350228506		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.16886684350228506 | validation: 0.19150258583549348]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11985101737085475		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.11985101737085475 | validation: 0.2029765063091249]
	TIME [epoch: 2.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307972623482327		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.1307972623482327 | validation: 0.19313359446660894]
	TIME [epoch: 2.74 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310775703571821		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.1310775703571821 | validation: 0.18459714563517138]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11371041648642177		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.11371041648642177 | validation: 0.22891555956931298]
	TIME [epoch: 2.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22441281286226733		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.22441281286226733 | validation: 0.19624384789641136]
	TIME [epoch: 2.74 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463032141234127		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.11463032141234127 | validation: 0.20044952269862476]
	TIME [epoch: 2.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11526676019033598		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.11526676019033598 | validation: 0.45948951007617644]
	TIME [epoch: 2.74 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436247819935494		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6436247819935494 | validation: 0.39842159872739347]
	TIME [epoch: 2.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270695471477299		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5270695471477299 | validation: 0.4157043493010966]
	TIME [epoch: 2.73 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295580180227158		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5295580180227158 | validation: 0.2242287329154587]
	TIME [epoch: 2.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22959211106157948		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.22959211106157948 | validation: 0.22613124597586787]
	TIME [epoch: 2.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15890725585653317		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.15890725585653317 | validation: 0.9059340974003774]
	TIME [epoch: 2.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411364329883766		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6411364329883766 | validation: 0.8069587683264831]
	TIME [epoch: 2.74 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5762721864387157		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5762721864387157 | validation: 0.24445242257019884]
	TIME [epoch: 2.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17037524630971157		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.17037524630971157 | validation: 0.24820348981334217]
	TIME [epoch: 2.73 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25721659526013096		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.25721659526013096 | validation: 0.23636495253853235]
	TIME [epoch: 2.73 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19661364783064314		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.19661364783064314 | validation: 0.2523785530751762]
	TIME [epoch: 2.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1964145448331231		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.1964145448331231 | validation: 0.22842654485195282]
	TIME [epoch: 2.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19232677401696383		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.19232677401696383 | validation: 0.21831695915951826]
	TIME [epoch: 2.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18431731422426448		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.18431731422426448 | validation: 0.2088021408770554]
	TIME [epoch: 2.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16008157340153445		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.16008157340153445 | validation: 0.20501773832901593]
	TIME [epoch: 2.74 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14958059232469822		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.14958059232469822 | validation: 0.19634873281456883]
	TIME [epoch: 2.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292764412119098		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.1292764412119098 | validation: 0.19409018635921715]
	TIME [epoch: 2.73 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12656388940910232		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.12656388940910232 | validation: 0.1847113635963659]
	TIME [epoch: 2.73 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12216460578890068		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.12216460578890068 | validation: 0.19607734079417197]
	TIME [epoch: 2.74 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12337279205530802		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.12337279205530802 | validation: 0.19083003571352158]
	TIME [epoch: 2.73 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12875689488971243		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.12875689488971243 | validation: 0.18350335120949401]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11777447673155206		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.11777447673155206 | validation: 0.186510682642917]
	TIME [epoch: 2.73 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217631423759962		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.1217631423759962 | validation: 0.1921005400012887]
	TIME [epoch: 2.73 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11509014464802078		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.11509014464802078 | validation: 0.20094293669491217]
	TIME [epoch: 2.73 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327498353610004		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.11327498353610004 | validation: 0.2023523539057778]
	TIME [epoch: 2.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264439635074074		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.11264439635074074 | validation: 0.18502321929356047]
	TIME [epoch: 2.73 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10916649542804599		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.10916649542804599 | validation: 0.19404645859286376]
	TIME [epoch: 2.73 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11094376908410393		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.11094376908410393 | validation: 0.19380232150263083]
	TIME [epoch: 2.73 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11094165119538824		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.11094165119538824 | validation: 0.19087181769449366]
	TIME [epoch: 2.73 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10761542885844534		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.10761542885844534 | validation: 0.177349162764506]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10716827277733713		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.10716827277733713 | validation: 0.19248063310968622]
	TIME [epoch: 2.74 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10750301697783368		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.10750301697783368 | validation: 0.19128324825052317]
	TIME [epoch: 2.74 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222978979132032		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.10222978979132032 | validation: 0.1948248113564441]
	TIME [epoch: 2.74 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10978937851093705		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.10978937851093705 | validation: 0.20119983816934184]
	TIME [epoch: 2.74 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10939549961060188		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.10939549961060188 | validation: 0.17896307884271026]
	TIME [epoch: 2.74 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10349043279413014		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.10349043279413014 | validation: 0.18406533296327296]
	TIME [epoch: 2.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020858132865826		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.1020858132865826 | validation: 0.2059676260008356]
	TIME [epoch: 2.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13405929750711648		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.13405929750711648 | validation: 0.20425902665214887]
	TIME [epoch: 2.73 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12375713696099762		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.12375713696099762 | validation: 0.17710498146560225]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10285944004977783		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.10285944004977783 | validation: 0.17694150367873074]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968158595230918		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.09968158595230918 | validation: 0.1787894068955772]
	TIME [epoch: 2.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11191372105373809		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.11191372105373809 | validation: 0.1995966172880683]
	TIME [epoch: 2.74 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11717786889045421		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.11717786889045421 | validation: 0.2011179107456368]
	TIME [epoch: 2.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12574513647321778		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.12574513647321778 | validation: 0.18157208790995247]
	TIME [epoch: 2.74 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1091814272370954		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.1091814272370954 | validation: 0.2049188546464131]
	TIME [epoch: 2.74 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13421736535382575		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.13421736535382575 | validation: 0.6840629514173191]
	TIME [epoch: 2.73 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5853781847291026		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5853781847291026 | validation: 0.6935310841280453]
	TIME [epoch: 2.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5340939131593624		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5340939131593624 | validation: 0.37771341115420853]
	TIME [epoch: 2.74 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26046777303394225		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.26046777303394225 | validation: 0.22972013961036347]
	TIME [epoch: 2.74 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21882057134152832		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.21882057134152832 | validation: 0.19439407463268366]
	TIME [epoch: 2.74 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18107932413567518		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.18107932413567518 | validation: 0.1912646237436946]
	TIME [epoch: 2.74 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15210163017852035		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.15210163017852035 | validation: 0.20188750523087284]
	TIME [epoch: 2.74 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14069729238195033		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.14069729238195033 | validation: 0.20832032896652938]
	TIME [epoch: 2.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14285046528230133		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.14285046528230133 | validation: 0.17167565106476879]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327723017502852		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.12327723017502852 | validation: 0.1583705832987807]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11803999288174385		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.11803999288174385 | validation: 0.17621597230124744]
	TIME [epoch: 2.74 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11273925653823376		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.11273925653823376 | validation: 0.17522499734150326]
	TIME [epoch: 2.74 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824128154336299		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.10824128154336299 | validation: 0.1590591164801421]
	TIME [epoch: 2.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09928061740828574		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.09928061740828574 | validation: 0.18171082468786903]
	TIME [epoch: 2.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086735252067414		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1086735252067414 | validation: 0.18785472620218877]
	TIME [epoch: 2.73 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220825050201545		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.12220825050201545 | validation: 0.19665025315443663]
	TIME [epoch: 2.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14067778172872805		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.14067778172872805 | validation: 0.20615288068605322]
	TIME [epoch: 2.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356150547841311		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.1356150547841311 | validation: 0.17466606307539512]
	TIME [epoch: 2.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788519316637775		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.09788519316637775 | validation: 0.17515160807789976]
	TIME [epoch: 2.74 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405171024215597		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.10405171024215597 | validation: 0.253722634702979]
	TIME [epoch: 2.74 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17258577406554054		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.17258577406554054 | validation: 0.17967846940463264]
	TIME [epoch: 2.74 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11498147299459131		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.11498147299459131 | validation: 0.17259192271856913]
	TIME [epoch: 2.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09749615173328004		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.09749615173328004 | validation: 0.20025858422090048]
	TIME [epoch: 2.74 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11582916692210019		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.11582916692210019 | validation: 0.1764277763876192]
	TIME [epoch: 2.74 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365558492506036		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.10365558492506036 | validation: 0.1971485279954937]
	TIME [epoch: 2.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13354010362109184		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.13354010362109184 | validation: 0.2037231298725768]
	TIME [epoch: 2.74 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1108140450967629		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1108140450967629 | validation: 0.16563239217294365]
	TIME [epoch: 2.74 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932224240849756		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.0932224240849756 | validation: 0.16910538247898377]
	TIME [epoch: 2.74 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08918058567287565		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.08918058567287565 | validation: 0.1940815918038199]
	TIME [epoch: 2.73 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11529495684828517		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.11529495684828517 | validation: 0.17992924604518146]
	TIME [epoch: 2.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1337118929080963		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.1337118929080963 | validation: 0.2067677940210627]
	TIME [epoch: 2.74 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334195329052007		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.14334195329052007 | validation: 0.1873319084342193]
	TIME [epoch: 2.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11620961596986625		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.11620961596986625 | validation: 0.17053437276876293]
	TIME [epoch: 2.73 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830102250540407		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.09830102250540407 | validation: 0.17915481853096135]
	TIME [epoch: 2.73 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11006943512212701		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.11006943512212701 | validation: 0.17299791267379386]
	TIME [epoch: 2.74 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871233367536467		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.11871233367536467 | validation: 0.18493007779641724]
	TIME [epoch: 2.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09931881968116921		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.09931881968116921 | validation: 0.19367307553453195]
	TIME [epoch: 2.74 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304942951738056		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1304942951738056 | validation: 0.1618194998999758]
	TIME [epoch: 2.74 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08992514062026995		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.08992514062026995 | validation: 0.16544475717867554]
	TIME [epoch: 2.74 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901338576439669		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.10901338576439669 | validation: 0.19621665672571287]
	TIME [epoch: 2.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12781691630134454		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.12781691630134454 | validation: 0.1814881048782738]
	TIME [epoch: 2.74 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955503736546484		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.0955503736546484 | validation: 0.16498151822144133]
	TIME [epoch: 2.74 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618591160410032		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.09618591160410032 | validation: 0.1747030944321177]
	TIME [epoch: 2.74 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09695129518856518		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.09695129518856518 | validation: 0.16792285565761791]
	TIME [epoch: 2.74 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369662270408786		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.08369662270408786 | validation: 0.1505635438700242]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813547689613381		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.08813547689613381 | validation: 0.17396779364669698]
	TIME [epoch: 2.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743381471806489		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.09743381471806489 | validation: 0.1639115390692384]
	TIME [epoch: 2.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054271562464152		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.08054271562464152 | validation: 0.16416113431771473]
	TIME [epoch: 2.74 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868994857725376		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.0868994857725376 | validation: 0.16629067973835854]
	TIME [epoch: 2.74 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878925808462886		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.0878925808462886 | validation: 0.19957969950304733]
	TIME [epoch: 2.74 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22315622932241727		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.22315622932241727 | validation: 0.17360415212838712]
	TIME [epoch: 2.74 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996117351509635		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.09996117351509635 | validation: 0.21363232999343654]
	TIME [epoch: 2.74 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14740716225339198		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.14740716225339198 | validation: 0.16092126165278955]
	TIME [epoch: 2.74 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367953782664683		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.08367953782664683 | validation: 0.15699069915722044]
	TIME [epoch: 2.73 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11050405045007977		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.11050405045007977 | validation: 0.19096514717964064]
	TIME [epoch: 2.74 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13276189669344896		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.13276189669344896 | validation: 0.16656848882084932]
	TIME [epoch: 2.74 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09301394233898247		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.09301394233898247 | validation: 0.1607545365473274]
	TIME [epoch: 2.74 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10046944347084236		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.10046944347084236 | validation: 0.17921318720370938]
	TIME [epoch: 2.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12210990218850859		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.12210990218850859 | validation: 0.17785295876211404]
	TIME [epoch: 2.74 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245338610314376		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.11245338610314376 | validation: 0.15909077804283403]
	TIME [epoch: 2.74 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432228443518165		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.08432228443518165 | validation: 0.15533878079407226]
	TIME [epoch: 2.73 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083898826195333		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.083898826195333 | validation: 0.15738327023905674]
	TIME [epoch: 2.73 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271010551743732		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.08271010551743732 | validation: 0.15663404117450408]
	TIME [epoch: 2.74 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08196759157587075		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.08196759157587075 | validation: 0.156313012450808]
	TIME [epoch: 2.74 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795431172715621		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.0795431172715621 | validation: 0.18113198403926087]
	TIME [epoch: 2.74 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746983974309517		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.10746983974309517 | validation: 0.22192728985070273]
	TIME [epoch: 2.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15230323687997022		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.15230323687997022 | validation: 0.17627805044622297]
	TIME [epoch: 2.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800931510146846		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1800931510146846 | validation: 0.17259907831964383]
	TIME [epoch: 2.73 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11013787974120132		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.11013787974120132 | validation: 0.24055206927173353]
	TIME [epoch: 2.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15649078050408124		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.15649078050408124 | validation: 0.15091332608666277]
	TIME [epoch: 2.74 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477566518684451		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08477566518684451 | validation: 0.2094060234986598]
	TIME [epoch: 2.74 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15171073792309345		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.15171073792309345 | validation: 0.15867467165307078]
	TIME [epoch: 2.73 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668434649687981		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.08668434649687981 | validation: 0.16008379885979693]
	TIME [epoch: 2.74 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304707522166162		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.10304707522166162 | validation: 0.15763277880939947]
	TIME [epoch: 2.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08119183271835927		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.08119183271835927 | validation: 0.1500756578383659]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08173982865626536		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.08173982865626536 | validation: 0.15719253894892737]
	TIME [epoch: 2.73 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430525415084539		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.07430525415084539 | validation: 0.16168457131998065]
	TIME [epoch: 2.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381421087532247		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.07381421087532247 | validation: 0.16833648691334865]
	TIME [epoch: 2.74 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081330239859035		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.10081330239859035 | validation: 0.1656712542044399]
	TIME [epoch: 2.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.089681927869036		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.089681927869036 | validation: 0.16505840241640093]
	TIME [epoch: 2.73 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08819394684692361		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.08819394684692361 | validation: 0.1545039443537296]
	TIME [epoch: 2.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361912067386863		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.08361912067386863 | validation: 0.15341499381988094]
	TIME [epoch: 2.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632092227367733		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.07632092227367733 | validation: 0.14774379734909301]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07330139986246616		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.07330139986246616 | validation: 0.1483383653006139]
	TIME [epoch: 2.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07281437458935687		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.07281437458935687 | validation: 0.15720872706582886]
	TIME [epoch: 2.73 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07301897296065209		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.07301897296065209 | validation: 0.15025082368406448]
	TIME [epoch: 2.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755118247037937		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.08755118247037937 | validation: 0.17657861002914094]
	TIME [epoch: 2.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568392702466003		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.10568392702466003 | validation: 0.1537789492416693]
	TIME [epoch: 2.73 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08399095191429659		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.08399095191429659 | validation: 0.15825618046382256]
	TIME [epoch: 2.74 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12630642938562173		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.12630642938562173 | validation: 0.16466312138834965]
	TIME [epoch: 2.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912571076729785		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.08912571076729785 | validation: 0.1675546877964994]
	TIME [epoch: 2.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365945598728403		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.08365945598728403 | validation: 0.16053253225971595]
	TIME [epoch: 2.72 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091436265077495		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.09091436265077495 | validation: 0.16846770394091606]
	TIME [epoch: 2.73 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0922093130946304		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.0922093130946304 | validation: 0.1658815196114562]
	TIME [epoch: 2.72 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773026058341033		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.09773026058341033 | validation: 0.1539749495796056]
	TIME [epoch: 2.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196162078747402		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.07196162078747402 | validation: 0.14024171718096987]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.072826464215295		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.072826464215295 | validation: 0.3633322829906228]
	TIME [epoch: 2.73 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516917539796889		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2516917539796889 | validation: 0.15766355852015346]
	TIME [epoch: 2.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08006743021330326		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.08006743021330326 | validation: 0.16383663492267087]
	TIME [epoch: 267 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09827948754977545		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.09827948754977545 | validation: 0.1570801181309682]
	TIME [epoch: 5.86 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07605829977273978		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.07605829977273978 | validation: 0.14692105120212479]
	TIME [epoch: 5.86 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760670358394074		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.07760670358394074 | validation: 0.15122492633867576]
	TIME [epoch: 5.85 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821964526912421		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.07821964526912421 | validation: 0.15968479244394232]
	TIME [epoch: 5.86 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0972490383815563		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.0972490383815563 | validation: 0.17424358872457293]
	TIME [epoch: 5.85 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116777809202638		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.11116777809202638 | validation: 0.15466088188757365]
	TIME [epoch: 5.86 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08551832846161694		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.08551832846161694 | validation: 0.14935605791999781]
	TIME [epoch: 5.85 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07778280178037873		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.07778280178037873 | validation: 0.1552360748654781]
	TIME [epoch: 5.89 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08015970092486445		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.08015970092486445 | validation: 0.2171380038038899]
	TIME [epoch: 5.85 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251431706924012		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.251431706924012 | validation: 0.1984917776286037]
	TIME [epoch: 5.86 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23263723816668297		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.23263723816668297 | validation: 0.15758534694697196]
	TIME [epoch: 5.85 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0984016185196554		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.0984016185196554 | validation: 0.19079769497352891]
	TIME [epoch: 5.85 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10856339282364438		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.10856339282364438 | validation: 0.18246859970150897]
	TIME [epoch: 5.85 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09455353982450941		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.09455353982450941 | validation: 0.16511190078253055]
	TIME [epoch: 5.85 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08195083338838091		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.08195083338838091 | validation: 0.15943767288807653]
	TIME [epoch: 5.85 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787566969559319		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.0787566969559319 | validation: 0.16151015069138364]
	TIME [epoch: 5.85 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07975987324497656		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.07975987324497656 | validation: 0.1559181421989146]
	TIME [epoch: 5.85 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002751471586673		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.08002751471586673 | validation: 0.15224034906871814]
	TIME [epoch: 5.85 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823046861784265		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.07823046861784265 | validation: 0.15931814806384248]
	TIME [epoch: 5.85 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647423006554928		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.07647423006554928 | validation: 0.1461460410236554]
	TIME [epoch: 5.85 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311776196632626		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07311776196632626 | validation: 0.1632963794098278]
	TIME [epoch: 5.86 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545908968395076		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.07545908968395076 | validation: 0.14587789022148132]
	TIME [epoch: 5.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003393927957312		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.07003393927957312 | validation: 0.14927097406241668]
	TIME [epoch: 5.85 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086905671983766		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.07086905671983766 | validation: 0.13135382555470257]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07130227038352642		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.07130227038352642 | validation: 0.17143901730495573]
	TIME [epoch: 5.85 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225541534168937		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.11225541534168937 | validation: 0.1543644722587194]
	TIME [epoch: 5.85 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522484107715853		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.07522484107715853 | validation: 0.1435986522419135]
	TIME [epoch: 5.86 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054957406483282		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.07054957406483282 | validation: 0.13698295858518048]
	TIME [epoch: 5.86 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674548404649251		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.0674548404649251 | validation: 0.17484808781080177]
	TIME [epoch: 5.85 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18912415563220705		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.18912415563220705 | validation: 0.13443099845307557]
	TIME [epoch: 5.85 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07683335595096619		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.07683335595096619 | validation: 0.14154649386632806]
	TIME [epoch: 5.85 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09025688959436025		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.09025688959436025 | validation: 0.15656751514644524]
	TIME [epoch: 5.86 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08109832713137674		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.08109832713137674 | validation: 0.1486515938893986]
	TIME [epoch: 5.85 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15164706174635617		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.15164706174635617 | validation: 0.1541776545337232]
	TIME [epoch: 5.85 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10088995186642688		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.10088995186642688 | validation: 0.1615778500317504]
	TIME [epoch: 5.86 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08214632335911583		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.08214632335911583 | validation: 0.14447092598371866]
	TIME [epoch: 5.85 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932387598500776		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.07932387598500776 | validation: 0.1367764615941982]
	TIME [epoch: 5.86 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07277105011573452		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.07277105011573452 | validation: 0.14517920520681243]
	TIME [epoch: 5.85 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06943184112161488		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.06943184112161488 | validation: 0.13655834823728216]
	TIME [epoch: 5.86 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034228571662646		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.07034228571662646 | validation: 0.13243956200160936]
	TIME [epoch: 5.86 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06974284320912762		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.06974284320912762 | validation: 0.15092440912428887]
	TIME [epoch: 5.86 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863669852664823		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.07863669852664823 | validation: 0.1452291813481182]
	TIME [epoch: 5.86 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07672337781842066		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.07672337781842066 | validation: 0.16208536670331564]
	TIME [epoch: 5.86 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893515016714861		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.0893515016714861 | validation: 0.1359643547473317]
	TIME [epoch: 5.85 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06952331345085071		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.06952331345085071 | validation: 0.13243265879939092]
	TIME [epoch: 5.85 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054733004616759		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.07054733004616759 | validation: 0.14784692460192106]
	TIME [epoch: 5.85 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351594338072182		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.08351594338072182 | validation: 0.13610841462560558]
	TIME [epoch: 5.85 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215606962789455		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.07215606962789455 | validation: 0.1418883088457973]
	TIME [epoch: 5.85 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308962381516715		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.07308962381516715 | validation: 0.12866426918808724]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670671169506674		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.0670671169506674 | validation: 0.13633826335482777]
	TIME [epoch: 5.85 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06922286015975355		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.06922286015975355 | validation: 0.16824990201843515]
	TIME [epoch: 5.85 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18286165184987696		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.18286165184987696 | validation: 0.1531495647370397]
	TIME [epoch: 5.85 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640961702247875		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.08640961702247875 | validation: 0.16655908035903677]
	TIME [epoch: 5.85 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09914299928268182		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.09914299928268182 | validation: 0.15622620557015196]
	TIME [epoch: 5.85 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08410610360215583		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.08410610360215583 | validation: 0.13369387748843484]
	TIME [epoch: 5.85 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003821693570116		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.07003821693570116 | validation: 0.13442007971297074]
	TIME [epoch: 5.85 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649118836376623		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.0649118836376623 | validation: 0.4800590804440013]
	TIME [epoch: 5.85 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34450474021039684		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.34450474021039684 | validation: 0.36148442985842383]
	TIME [epoch: 5.85 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24543421171531382		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.24543421171531382 | validation: 0.1381515071938686]
	TIME [epoch: 5.86 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08703746862199992		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.08703746862199992 | validation: 0.1523508978948921]
	TIME [epoch: 5.86 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09518227417934112		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.09518227417934112 | validation: 0.1577316438862343]
	TIME [epoch: 5.86 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08102028549360114		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.08102028549360114 | validation: 0.1307019560846495]
	TIME [epoch: 5.86 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07526496799943377		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.07526496799943377 | validation: 0.13158365867523955]
	TIME [epoch: 5.85 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07040977266677129		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.07040977266677129 | validation: 0.1281823445963679]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07094037903597938		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.07094037903597938 | validation: 0.1245569759576263]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553094854688646		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.06553094854688646 | validation: 0.1298420816612187]
	TIME [epoch: 5.87 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06687422329717939		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.06687422329717939 | validation: 0.1295699530523471]
	TIME [epoch: 5.87 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641628512519272		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.0641628512519272 | validation: 0.13034142436808313]
	TIME [epoch: 5.88 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396610024253492		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.06396610024253492 | validation: 0.12371974893828458]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06204212124934642		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.06204212124934642 | validation: 0.13311617526987535]
	TIME [epoch: 5.88 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008682337509194		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07008682337509194 | validation: 0.132699155716125]
	TIME [epoch: 5.87 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06644947250376522		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.06644947250376522 | validation: 0.1257164887132669]
	TIME [epoch: 5.88 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061777409487924706		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.061777409487924706 | validation: 0.13908374521169786]
	TIME [epoch: 5.88 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259898865274593		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.06259898865274593 | validation: 0.13094120505434811]
	TIME [epoch: 5.88 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0640229828920696		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.0640229828920696 | validation: 0.134478745263049]
	TIME [epoch: 5.87 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498689239733228		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.06498689239733228 | validation: 0.13552694724207034]
	TIME [epoch: 5.88 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07621951574957513		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.07621951574957513 | validation: 0.1658631844910728]
	TIME [epoch: 5.87 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321303564885121		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.09321303564885121 | validation: 0.13628190244472238]
	TIME [epoch: 5.88 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272286807031023		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.06272286807031023 | validation: 0.12798668051559392]
	TIME [epoch: 5.87 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017986744195564		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.08017986744195564 | validation: 0.14999523578354848]
	TIME [epoch: 5.88 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883330894500504		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07883330894500504 | validation: 0.12794376258805154]
	TIME [epoch: 5.87 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062139635634488884		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.062139635634488884 | validation: 0.1169098339310514]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125592648480447		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.07125592648480447 | validation: 0.14515639554895873]
	TIME [epoch: 5.88 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07137308064003502		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.07137308064003502 | validation: 0.14058175960495709]
	TIME [epoch: 5.88 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292840227390588		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.06292840227390588 | validation: 0.12665150020839247]
	TIME [epoch: 5.88 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413238316245522		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.06413238316245522 | validation: 0.1262250834401833]
	TIME [epoch: 5.88 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059296504297924546		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.059296504297924546 | validation: 0.1309847700032537]
	TIME [epoch: 5.88 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05868588979625955		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.05868588979625955 | validation: 0.1333505907895466]
	TIME [epoch: 5.89 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504228906061137		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.05504228906061137 | validation: 0.14586389291295915]
	TIME [epoch: 5.89 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795012802647902		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.0795012802647902 | validation: 0.14446962422116452]
	TIME [epoch: 5.89 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06440649320321024		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.06440649320321024 | validation: 0.13875214770469055]
	TIME [epoch: 5.89 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06830470217932867		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.06830470217932867 | validation: 0.12799598377782004]
	TIME [epoch: 5.89 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10052971760945957		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.10052971760945957 | validation: 0.1340813494590608]
	TIME [epoch: 5.88 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06716127654068098		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.06716127654068098 | validation: 0.14411398224214864]
	TIME [epoch: 5.87 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264805346770025		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.06264805346770025 | validation: 0.12912590697114426]
	TIME [epoch: 5.88 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06349838502364107		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.06349838502364107 | validation: 0.151341647146265]
	TIME [epoch: 5.87 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710850870550427		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.08710850870550427 | validation: 0.13311134180260434]
	TIME [epoch: 5.88 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06627966623637259		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.06627966623637259 | validation: 0.11555202468824022]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903281491642583		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.05903281491642583 | validation: 0.13349431469220865]
	TIME [epoch: 5.88 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05896891839104042		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.05896891839104042 | validation: 0.1249695346217981]
	TIME [epoch: 5.88 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06326070478925103		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.06326070478925103 | validation: 0.15494473423763006]
	TIME [epoch: 5.87 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0814791993451721		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.0814791993451721 | validation: 0.11743114798193957]
	TIME [epoch: 5.88 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06211662421050069		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.06211662421050069 | validation: 0.13005176227860415]
	TIME [epoch: 5.87 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059672453946459446		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.059672453946459446 | validation: 0.1248592256433752]
	TIME [epoch: 5.87 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058335312627559976		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.058335312627559976 | validation: 0.12619039564802056]
	TIME [epoch: 5.87 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061836808840052396		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.061836808840052396 | validation: 0.14378787493596837]
	TIME [epoch: 5.87 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07203826623331462		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.07203826623331462 | validation: 0.12046157021729242]
	TIME [epoch: 5.88 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310052898763635		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.06310052898763635 | validation: 0.1394525657671087]
	TIME [epoch: 5.88 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954309270106152		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.07954309270106152 | validation: 0.11897417507446113]
	TIME [epoch: 5.89 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06706052338609625		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.06706052338609625 | validation: 0.13043194280419587]
	TIME [epoch: 5.88 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044515339672172		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.06044515339672172 | validation: 0.11866989713093368]
	TIME [epoch: 5.89 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573169530418772		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.05573169530418772 | validation: 0.11903944390576765]
	TIME [epoch: 5.88 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059256580616491784		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.059256580616491784 | validation: 0.1386254314013142]
	TIME [epoch: 5.88 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168179206127683		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.07168179206127683 | validation: 0.11436157082398912]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07881081889934728		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.07881081889934728 | validation: 0.1384658062530073]
	TIME [epoch: 5.88 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05947074837654421		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.05947074837654421 | validation: 0.12991895169787168]
	TIME [epoch: 5.87 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061644909444576845		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.061644909444576845 | validation: 0.12689584265223144]
	TIME [epoch: 5.87 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060788021056359984		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.060788021056359984 | validation: 0.1251677132486539]
	TIME [epoch: 5.87 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636418664390159		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.0636418664390159 | validation: 0.12348741019890151]
	TIME [epoch: 5.87 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05670238863091303		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.05670238863091303 | validation: 0.1186401802880147]
	TIME [epoch: 5.87 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057281283029647335		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.057281283029647335 | validation: 0.13195401770335066]
	TIME [epoch: 5.87 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628010029206668		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.06628010029206668 | validation: 0.1118870616681718]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061116953518836094		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.061116953518836094 | validation: 0.15632535493304744]
	TIME [epoch: 5.88 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09687741315344325		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.09687741315344325 | validation: 0.12474926573389332]
	TIME [epoch: 5.87 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200968341072379		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.05200968341072379 | validation: 0.10574091185107311]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668125156892034		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.06668125156892034 | validation: 0.1411267389988173]
	TIME [epoch: 5.88 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226497019284507		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.07226497019284507 | validation: 0.11585583871181443]
	TIME [epoch: 5.87 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060376916540166574		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.060376916540166574 | validation: 0.11476010786392249]
	TIME [epoch: 5.88 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05385056535271276		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.05385056535271276 | validation: 0.12900280096588124]
	TIME [epoch: 5.88 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076909531205916		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.06076909531205916 | validation: 0.11455171840815011]
	TIME [epoch: 5.88 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05399597164477515		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.05399597164477515 | validation: 0.11731077810234587]
	TIME [epoch: 5.87 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05191076737769205		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.05191076737769205 | validation: 0.1106344304246993]
	TIME [epoch: 5.87 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404091405361949		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.05404091405361949 | validation: 0.11189210276369398]
	TIME [epoch: 5.87 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05444099251127885		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.05444099251127885 | validation: 0.10703325755877709]
	TIME [epoch: 5.87 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030744395089186		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.05030744395089186 | validation: 0.12338056886979028]
	TIME [epoch: 5.87 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05454382547672305		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.05454382547672305 | validation: 0.11968143097644797]
	TIME [epoch: 5.87 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454238481085552		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06454238481085552 | validation: 0.14694709141946247]
	TIME [epoch: 5.88 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835939214741314		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.07835939214741314 | validation: 0.12244076495520098]
	TIME [epoch: 5.88 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435030025124146		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06435030025124146 | validation: 0.11595415758005517]
	TIME [epoch: 5.88 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05673112516321423		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.05673112516321423 | validation: 0.11796880532563943]
	TIME [epoch: 5.87 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843699801868809		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06843699801868809 | validation: 0.12075370211284793]
	TIME [epoch: 5.88 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059297586307269515		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.059297586307269515 | validation: 0.11023206899667355]
	TIME [epoch: 5.87 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513525372261913		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.05513525372261913 | validation: 0.10938494663355035]
	TIME [epoch: 5.88 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051335935880546045		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.051335935880546045 | validation: 0.10354889787808133]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05140947345912095		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.05140947345912095 | validation: 0.10941264927495838]
	TIME [epoch: 5.86 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532715702312262		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.0532715702312262 | validation: 0.11621172293946552]
	TIME [epoch: 5.86 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371795943911273		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.05371795943911273 | validation: 0.1321898125136791]
	TIME [epoch: 5.86 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06714750882438185		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.06714750882438185 | validation: 0.10805633351421773]
	TIME [epoch: 5.86 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860013002843607		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.05860013002843607 | validation: 0.13429035727341568]
	TIME [epoch: 5.85 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591318504778703		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.0591318504778703 | validation: 0.11840114183986206]
	TIME [epoch: 5.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793840953910632		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.05793840953910632 | validation: 0.12608533478271414]
	TIME [epoch: 5.85 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674674128527488		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.0674674128527488 | validation: 0.10683817038444188]
	TIME [epoch: 5.85 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251528715410791		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.05251528715410791 | validation: 0.1157613838663436]
	TIME [epoch: 5.87 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077953983648865		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.05077953983648865 | validation: 0.10887918837271374]
	TIME [epoch: 5.87 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051340925950260595		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.051340925950260595 | validation: 0.1047359047303774]
	TIME [epoch: 5.87 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05805487787959035		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.05805487787959035 | validation: 0.1304435667682645]
	TIME [epoch: 5.87 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258425397183393		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07258425397183393 | validation: 0.10760473692130375]
	TIME [epoch: 5.88 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0540165953729174		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.0540165953729174 | validation: 0.12878170273478884]
	TIME [epoch: 5.88 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07257271980314359		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.07257271980314359 | validation: 0.10865769839655677]
	TIME [epoch: 5.89 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053299319682186666		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.053299319682186666 | validation: 0.1982988850182541]
	TIME [epoch: 5.88 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3377064733999639		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.3377064733999639 | validation: 0.1702071385706976]
	TIME [epoch: 5.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713631215435692		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.2713631215435692 | validation: 0.12055038564968984]
	TIME [epoch: 5.89 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08872771037012064		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.08872771037012064 | validation: 0.123906078023398]
	TIME [epoch: 5.88 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538120230070845		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.07538120230070845 | validation: 0.11505153549823398]
	TIME [epoch: 5.88 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07230456953158505		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07230456953158505 | validation: 0.1020687360210826]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05328089453909945		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.05328089453909945 | validation: 0.11814326018664603]
	TIME [epoch: 5.86 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336546162875067		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.06336546162875067 | validation: 0.11548349519400039]
	TIME [epoch: 5.85 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05796925337660256		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.05796925337660256 | validation: 0.10593439479092043]
	TIME [epoch: 5.85 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077470086840703		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.05077470086840703 | validation: 0.10596386596634588]
	TIME [epoch: 5.86 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049204047802649774		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.049204047802649774 | validation: 0.10428841944591415]
	TIME [epoch: 5.85 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051313634289699914		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.051313634289699914 | validation: 0.10323556002801726]
	TIME [epoch: 5.88 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519644165226375		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.0519644165226375 | validation: 0.1082150627311192]
	TIME [epoch: 5.88 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200984848961353		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.05200984848961353 | validation: 0.11080490518797895]
	TIME [epoch: 5.88 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053011379824136916		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.053011379824136916 | validation: 0.10956412960102127]
	TIME [epoch: 5.88 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05156031504181659		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05156031504181659 | validation: 0.10873312220136029]
	TIME [epoch: 5.88 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04815708004134494		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.04815708004134494 | validation: 0.10714017344134044]
	TIME [epoch: 5.88 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348675602795204		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.10348675602795204 | validation: 0.13736493212337977]
	TIME [epoch: 5.88 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777406027173969		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.07777406027173969 | validation: 0.12100119149022333]
	TIME [epoch: 5.88 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567604143411079		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0567604143411079 | validation: 0.09997834095448499]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05296372708920316		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.05296372708920316 | validation: 0.11528530088260541]
	TIME [epoch: 5.86 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05538560349769904		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.05538560349769904 | validation: 0.1089604057561549]
	TIME [epoch: 5.85 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048503113271415085		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.048503113271415085 | validation: 0.09851111111441761]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047393307775691035		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.047393307775691035 | validation: 0.10715445287015096]
	TIME [epoch: 5.87 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049256137733773804		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.049256137733773804 | validation: 0.11108743151910755]
	TIME [epoch: 5.86 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061921389966051746		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.061921389966051746 | validation: 0.10522892710828509]
	TIME [epoch: 5.86 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04839199141132322		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.04839199141132322 | validation: 0.11128137860951173]
	TIME [epoch: 5.87 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05104514804515131		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.05104514804515131 | validation: 0.09830057463151652]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04845252459086272		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.04845252459086272 | validation: 0.10884054972513063]
	TIME [epoch: 5.86 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783980712474202		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.04783980712474202 | validation: 0.09878030585799619]
	TIME [epoch: 5.87 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04687032140630652		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.04687032140630652 | validation: 0.10674152148837242]
	TIME [epoch: 5.86 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05477417078686509		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.05477417078686509 | validation: 0.10534914235629418]
	TIME [epoch: 5.87 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698776973288604		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.07698776973288604 | validation: 0.12258507455132238]
	TIME [epoch: 5.86 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052378423265991264		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.052378423265991264 | validation: 0.10114039018550836]
	TIME [epoch: 5.86 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05097604131376594		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.05097604131376594 | validation: 0.09247270075622532]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05002556925492635		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.05002556925492635 | validation: 0.11043083981901912]
	TIME [epoch: 5.87 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209006182215305		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.05209006182215305 | validation: 0.09600532828407131]
	TIME [epoch: 5.87 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055101538743181085		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.055101538743181085 | validation: 0.10929478744998788]
	TIME [epoch: 5.87 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048369287283619944		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.048369287283619944 | validation: 0.094049461312791]
	TIME [epoch: 5.87 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475340283737081		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.0475340283737081 | validation: 0.09979657610666481]
	TIME [epoch: 5.87 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657012981746468		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.04657012981746468 | validation: 0.10686943126214626]
	TIME [epoch: 5.88 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077777880121108		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.05077777880121108 | validation: 0.08822972434681292]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05536194061797896		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.05536194061797896 | validation: 0.11873776756919839]
	TIME [epoch: 5.88 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06533488971796596		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06533488971796596 | validation: 0.09898168854124255]
	TIME [epoch: 5.89 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04677639824761406		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.04677639824761406 | validation: 0.10003559400893476]
	TIME [epoch: 5.88 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04790196899996719		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.04790196899996719 | validation: 0.11058683835132364]
	TIME [epoch: 5.88 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04402320712860961		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.04402320712860961 | validation: 0.1005380577819865]
	TIME [epoch: 5.88 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631555723121415		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.04631555723121415 | validation: 0.11384142524684973]
	TIME [epoch: 5.88 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04478981775112808		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.04478981775112808 | validation: 0.09417129950054161]
	TIME [epoch: 5.88 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835612797935009		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.04835612797935009 | validation: 0.11441143499088735]
	TIME [epoch: 5.88 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717473389325988		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.04717473389325988 | validation: 0.096945016149664]
	TIME [epoch: 5.87 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311469193251625		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05311469193251625 | validation: 0.11039345770644277]
	TIME [epoch: 5.87 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05448321131815554		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.05448321131815554 | validation: 0.10512133818575059]
	TIME [epoch: 5.87 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047251037310302946		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.047251037310302946 | validation: 0.09135365550348075]
	TIME [epoch: 5.88 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058592474588388996		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.058592474588388996 | validation: 0.13070708901892772]
	TIME [epoch: 5.87 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988975044852232		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06988975044852232 | validation: 0.11766427359822917]
	TIME [epoch: 5.88 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05344755459441654		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05344755459441654 | validation: 0.09707249698275094]
	TIME [epoch: 5.87 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06745105892057762		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.06745105892057762 | validation: 0.11545548303072198]
	TIME [epoch: 5.88 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05742335255891992		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.05742335255891992 | validation: 0.11142609469775705]
	TIME [epoch: 5.87 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532164128032504		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.04532164128032504 | validation: 0.09071128787323596]
	TIME [epoch: 5.87 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045321503042797924		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.045321503042797924 | validation: 0.1075369346218048]
	TIME [epoch: 5.87 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047563411818456275		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.047563411818456275 | validation: 0.10191880396459328]
	TIME [epoch: 5.87 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04332227765194323		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.04332227765194323 | validation: 0.10377614289893916]
	TIME [epoch: 5.87 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042103882956064445		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.042103882956064445 | validation: 0.10185193491469864]
	TIME [epoch: 5.87 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04512460213676391		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.04512460213676391 | validation: 0.09076637378640315]
	TIME [epoch: 5.87 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04408034810163024		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.04408034810163024 | validation: 0.09778540863883933]
	TIME [epoch: 5.88 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04403636433609498		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.04403636433609498 | validation: 0.19410410496124977]
	TIME [epoch: 5.87 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681763553162281		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.2681763553162281 | validation: 0.16334229158309413]
	TIME [epoch: 5.87 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2171889922028465		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.2171889922028465 | validation: 0.11604937452765043]
	TIME [epoch: 5.87 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096690709815295		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08096690709815295 | validation: 0.08618023961151258]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926497773944276		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.05926497773944276 | validation: 0.09465679429549098]
	TIME [epoch: 5.87 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0510801718237437		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.0510801718237437 | validation: 0.09952346854377424]
	TIME [epoch: 5.87 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051013381851206775		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.051013381851206775 | validation: 0.09735897594125069]
	TIME [epoch: 5.88 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657746424692902		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.04657746424692902 | validation: 0.09406438349968171]
	TIME [epoch: 5.88 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04847865317236732		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.04847865317236732 | validation: 0.09681172943682272]
	TIME [epoch: 5.88 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04531752588047677		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.04531752588047677 | validation: 0.08842969405752121]
	TIME [epoch: 5.95 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04511064188514038		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.04511064188514038 | validation: 0.09072918495153613]
	TIME [epoch: 5.88 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045135723235675995		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.045135723235675995 | validation: 0.09590496469113413]
	TIME [epoch: 5.87 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541342074780306		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.04541342074780306 | validation: 0.1013520796404031]
	TIME [epoch: 5.88 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04361076101088394		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.04361076101088394 | validation: 0.08706643602025921]
	TIME [epoch: 5.87 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04422204032045336		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.04422204032045336 | validation: 0.08701986926678396]
	TIME [epoch: 5.88 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619431456008613		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.04619431456008613 | validation: 0.0996781551621359]
	TIME [epoch: 5.88 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044743578651537275		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.044743578651537275 | validation: 0.09294244141814507]
	TIME [epoch: 5.88 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043083652683973045		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.043083652683973045 | validation: 0.09039626608165735]
	TIME [epoch: 5.88 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559838960238402		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.04559838960238402 | validation: 0.08445717472676406]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049137820964079086		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.049137820964079086 | validation: 0.07892196887760858]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050643658394854574		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.050643658394854574 | validation: 0.11209604099068256]
	TIME [epoch: 5.89 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05407041091771337		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05407041091771337 | validation: 0.08918930974112634]
	TIME [epoch: 5.88 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545786291555284		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.04545786291555284 | validation: 0.07882224280007205]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592007570036469		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.04592007570036469 | validation: 0.10197579237839222]
	TIME [epoch: 5.87 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018788107849248		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05018788107849248 | validation: 0.07805208620667817]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04446770299223119		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.04446770299223119 | validation: 0.07516342019280782]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043150915156337656		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.043150915156337656 | validation: 0.10366473902954075]
	TIME [epoch: 5.88 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282178253068835		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05282178253068835 | validation: 0.08509848840901363]
	TIME [epoch: 5.88 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461780686616713		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.04461780686616713 | validation: 0.09005117488391684]
	TIME [epoch: 5.87 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042089220328594704		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.042089220328594704 | validation: 0.09310158363093195]
	TIME [epoch: 5.88 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05214359594062563		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.05214359594062563 | validation: 0.09017749379795549]
	TIME [epoch: 5.88 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046012565410978416		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.046012565410978416 | validation: 0.0915661416868049]
	TIME [epoch: 5.87 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617038259032191		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.04617038259032191 | validation: 0.10130758240566468]
	TIME [epoch: 5.88 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483672927293642		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0483672927293642 | validation: 0.1167352306440437]
	TIME [epoch: 5.87 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14618502496719704		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.14618502496719704 | validation: 0.1002926334021017]
	TIME [epoch: 5.88 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11730525409025766		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.11730525409025766 | validation: 0.08955564825016464]
	TIME [epoch: 5.88 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052358026806939906		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.052358026806939906 | validation: 0.09011616525821647]
	TIME [epoch: 5.87 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04550639417925426		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.04550639417925426 | validation: 0.08784969254319683]
	TIME [epoch: 5.88 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206835001450745		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05206835001450745 | validation: 0.08686461700893128]
	TIME [epoch: 5.88 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04483884228461982		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.04483884228461982 | validation: 0.08536225488722969]
	TIME [epoch: 5.88 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04182191613048415		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.04182191613048415 | validation: 0.08725292981099761]
	TIME [epoch: 5.88 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043815981602745566		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.043815981602745566 | validation: 0.09155427504586372]
	TIME [epoch: 5.88 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043783346249075365		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.043783346249075365 | validation: 0.08188768733311574]
	TIME [epoch: 5.88 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301611197494063		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.04301611197494063 | validation: 0.08836503710573629]
	TIME [epoch: 5.88 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041979492250783984		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.041979492250783984 | validation: 0.08683785080760754]
	TIME [epoch: 5.88 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04122501936529718		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04122501936529718 | validation: 0.09124483770024287]
	TIME [epoch: 5.88 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041067054076095004		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.041067054076095004 | validation: 0.08317837855890328]
	TIME [epoch: 5.87 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04219049954807333		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.04219049954807333 | validation: 0.09961410368761181]
	TIME [epoch: 5.88 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662869149350712		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.04662869149350712 | validation: 0.07120211841085354]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589505038588305		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.04589505038588305 | validation: 0.09746783500137862]
	TIME [epoch: 5.87 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046950280898612064		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.046950280898612064 | validation: 0.08531045631646902]
	TIME [epoch: 5.88 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163948940275957		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04163948940275957 | validation: 0.06993579154092514]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05132612480185219		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.05132612480185219 | validation: 0.09751629749564583]
	TIME [epoch: 5.88 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05103785693716934		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.05103785693716934 | validation: 0.08992445640157962]
	TIME [epoch: 5.87 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146609786740069		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.04146609786740069 | validation: 0.08897754089999055]
	TIME [epoch: 5.88 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129598022601856		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.04129598022601856 | validation: 0.08714857244176427]
	TIME [epoch: 5.87 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04067816324492267		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.04067816324492267 | validation: 0.09270274803593428]
	TIME [epoch: 5.88 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040474954991425775		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.040474954991425775 | validation: 0.0884191757404945]
	TIME [epoch: 5.87 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041356340554342334		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.041356340554342334 | validation: 0.08224588422825918]
	TIME [epoch: 5.88 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042208743497638194		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.042208743497638194 | validation: 0.08505090240125057]
	TIME [epoch: 5.87 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038973073182731494		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.038973073182731494 | validation: 0.08701112399867804]
	TIME [epoch: 5.88 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044720602142331325		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.044720602142331325 | validation: 0.09498065831133434]
	TIME [epoch: 5.88 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493374128783864		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.0493374128783864 | validation: 0.0859625496703122]
	TIME [epoch: 5.87 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043808168284950176		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.043808168284950176 | validation: 0.08728084523989282]
	TIME [epoch: 5.87 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319756660793109		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.05319756660793109 | validation: 0.0977446277472635]
	TIME [epoch: 5.87 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04867475025848984		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.04867475025848984 | validation: 0.0871641310736364]
	TIME [epoch: 5.88 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509799928166158		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.04509799928166158 | validation: 0.08489496609349274]
	TIME [epoch: 5.88 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301463232128395		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.04301463232128395 | validation: 0.09558929441903714]
	TIME [epoch: 5.88 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04555219764192822		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04555219764192822 | validation: 0.08103176264998987]
	TIME [epoch: 5.88 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372520423755043		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.04372520423755043 | validation: 0.09802008490793512]
	TIME [epoch: 5.88 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905108400014463		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.04905108400014463 | validation: 0.08743592223202339]
	TIME [epoch: 5.87 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038741200709029394		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.038741200709029394 | validation: 0.07722579371826069]
	TIME [epoch: 5.87 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188642833087584		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.04188642833087584 | validation: 0.0943516030001918]
	TIME [epoch: 5.88 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04216212908359726		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04216212908359726 | validation: 0.0881956216663739]
	TIME [epoch: 5.87 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037771388056161724		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.037771388056161724 | validation: 0.08122865245002397]
	TIME [epoch: 5.87 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905782196262946		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.03905782196262946 | validation: 0.09802571060802306]
	TIME [epoch: 5.87 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046550519111659076		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.046550519111659076 | validation: 0.09421950478751318]
	TIME [epoch: 5.88 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042711213775402		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.04042711213775402 | validation: 0.08053960267197126]
	TIME [epoch: 5.88 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980790793894147		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.03980790793894147 | validation: 0.08218145225961108]
	TIME [epoch: 5.88 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054101392174779		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.04054101392174779 | validation: 0.07598464464706547]
	TIME [epoch: 5.88 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037798799581231765		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.037798799581231765 | validation: 0.08346421593045888]
	TIME [epoch: 5.88 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865588585882114		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.03865588585882114 | validation: 0.08464330728644918]
	TIME [epoch: 5.88 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035480498105969295		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.035480498105969295 | validation: 0.07749383701506429]
	TIME [epoch: 5.88 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037227854014743704		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.037227854014743704 | validation: 0.08801310781318818]
	TIME [epoch: 5.87 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886761547828878		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.03886761547828878 | validation: 0.08370825976830379]
	TIME [epoch: 5.87 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03990200641613692		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.03990200641613692 | validation: 0.07764984267935914]
	TIME [epoch: 5.87 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042229918287061546		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.042229918287061546 | validation: 0.10255915858818737]
	TIME [epoch: 5.87 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741226576695003		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.09741226576695003 | validation: 0.10196881953426402]
	TIME [epoch: 5.88 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09950480608321105		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.09950480608321105 | validation: 0.08724559163606876]
	TIME [epoch: 5.88 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045801022972626504		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.045801022972626504 | validation: 0.08364144842396108]
	TIME [epoch: 5.88 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604261997115858		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.04604261997115858 | validation: 0.07793394205029941]
	TIME [epoch: 5.87 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04811145496852965		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.04811145496852965 | validation: 0.07800362957518288]
	TIME [epoch: 5.88 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04250634059846261		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.04250634059846261 | validation: 0.09457871722705463]
	TIME [epoch: 5.88 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04669335163068202		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.04669335163068202 | validation: 0.08443166119582461]
	TIME [epoch: 5.88 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768184254448995		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.04768184254448995 | validation: 0.08852887251888628]
	TIME [epoch: 5.88 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829084296187415		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.03829084296187415 | validation: 0.08911957152909211]
	TIME [epoch: 5.87 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040644242359993824		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.040644242359993824 | validation: 0.07700733105368761]
	TIME [epoch: 5.87 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038449675535678865		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.038449675535678865 | validation: 0.08476132377531748]
	TIME [epoch: 5.88 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03926170902271896		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.03926170902271896 | validation: 0.07740734285747802]
	TIME [epoch: 5.88 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040344717583975856		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.040344717583975856 | validation: 0.07097012486743086]
	TIME [epoch: 5.88 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038248314876240565		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.038248314876240565 | validation: 0.09172439280595364]
	TIME [epoch: 5.89 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042675918242414124		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.042675918242414124 | validation: 0.07436890130322113]
	TIME [epoch: 5.88 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03819235948162525		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.03819235948162525 | validation: 0.08000677469404688]
	TIME [epoch: 5.87 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700644708453419		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.03700644708453419 | validation: 0.07270981316616365]
	TIME [epoch: 5.87 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036792075255217445		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.036792075255217445 | validation: 0.07413510351507883]
	TIME [epoch: 5.88 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036352896390351816		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.036352896390351816 | validation: 0.07890358263579869]
	TIME [epoch: 5.88 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035209952257727654		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.035209952257727654 | validation: 0.07603829508128357]
	TIME [epoch: 5.88 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820092925381653		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.03820092925381653 | validation: 0.0865768186460808]
	TIME [epoch: 5.87 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584466505706914		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.03584466505706914 | validation: 0.08250434174681064]
	TIME [epoch: 5.87 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046399352695972985		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.046399352695972985 | validation: 0.08327360443768105]
	TIME [epoch: 5.87 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110771790250313		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04110771790250313 | validation: 0.08064844939641878]
	TIME [epoch: 5.87 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874189999763981		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.03874189999763981 | validation: 0.0710857928878822]
	TIME [epoch: 5.87 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881795048537142		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.03881795048537142 | validation: 0.09426566776359432]
	TIME [epoch: 5.87 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056362281785786		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.056362281785786 | validation: 0.07751857460448719]
	TIME [epoch: 5.88 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671801944909906		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.03671801944909906 | validation: 0.07740655529038522]
	TIME [epoch: 5.88 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581974194279527		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.03581974194279527 | validation: 0.08830628210490503]
	TIME [epoch: 5.88 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037286179258308835		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.037286179258308835 | validation: 0.08007682430833035]
	TIME [epoch: 5.88 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039425958448752046		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.039425958448752046 | validation: 0.08356930085777482]
	TIME [epoch: 5.88 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039192980404381864		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.039192980404381864 | validation: 0.08356841907723017]
	TIME [epoch: 5.87 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03889991620020144		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.03889991620020144 | validation: 0.07783038907883479]
	TIME [epoch: 5.87 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038486762496094534		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.038486762496094534 | validation: 0.08942068317462996]
	TIME [epoch: 5.87 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04274162603642117		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.04274162603642117 | validation: 0.09222225032101705]
	TIME [epoch: 5.87 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036978755415158056		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.036978755415158056 | validation: 0.06966437398304422]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179341549377027		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.04179341549377027 | validation: 0.08282211501838523]
	TIME [epoch: 5.88 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03878332421575851		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.03878332421575851 | validation: 0.08513152869211917]
	TIME [epoch: 5.88 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04500279879148782		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04500279879148782 | validation: 0.07413977088580331]
	TIME [epoch: 5.88 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03930759688909176		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.03930759688909176 | validation: 0.07949781844086912]
	TIME [epoch: 5.89 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967922622380561		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03967922622380561 | validation: 0.06945838561846032]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03786683802406735		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.03786683802406735 | validation: 0.07343233144805611]
	TIME [epoch: 5.86 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549263823172633		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.03549263823172633 | validation: 0.08188034546271472]
	TIME [epoch: 5.86 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722583716385351		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.03722583716385351 | validation: 0.0660413900335238]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03845356786468636		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03845356786468636 | validation: 0.07975235237874415]
	TIME [epoch: 5.88 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03706349633427485		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.03706349633427485 | validation: 0.0660316770073799]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048116771120993156		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.048116771120993156 | validation: 0.08798185592471541]
	TIME [epoch: 5.88 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042685016308454914		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.042685016308454914 | validation: 0.08166844301514348]
	TIME [epoch: 5.88 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036263706128909654		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.036263706128909654 | validation: 0.07985831636253071]
	TIME [epoch: 5.88 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03686197694933414		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.03686197694933414 | validation: 0.07586575927565134]
	TIME [epoch: 5.87 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038076242077684834		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.038076242077684834 | validation: 0.07959971887646848]
	TIME [epoch: 5.88 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717793735460579		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.03717793735460579 | validation: 0.06896593958299777]
	TIME [epoch: 5.87 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036248623386648936		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.036248623386648936 | validation: 0.07889500058232]
	TIME [epoch: 5.88 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037570981945976384		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.037570981945976384 | validation: 0.06939977062134565]
	TIME [epoch: 5.88 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419971274381763		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.0419971274381763 | validation: 0.06830583359882648]
	TIME [epoch: 5.88 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03810855233391365		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.03810855233391365 | validation: 0.07743165370869687]
	TIME [epoch: 5.88 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041480407489364726		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.041480407489364726 | validation: 0.0735636162207374]
	TIME [epoch: 5.88 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735062172987833		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.03735062172987833 | validation: 0.07485189132074041]
	TIME [epoch: 5.88 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384861933343922		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.05384861933343922 | validation: 0.09218801016700257]
	TIME [epoch: 5.88 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04506947832870548		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04506947832870548 | validation: 0.07471478617517191]
	TIME [epoch: 5.87 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038242333755707285		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.038242333755707285 | validation: 0.07994057932031085]
	TIME [epoch: 5.87 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0343873265931422		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.0343873265931422 | validation: 0.0716663919059643]
	TIME [epoch: 5.88 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383431261977342		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03383431261977342 | validation: 0.06900222526758783]
	TIME [epoch: 5.88 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479131868962599		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.03479131868962599 | validation: 0.06971469709257995]
	TIME [epoch: 5.88 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485630822858875		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.03485630822858875 | validation: 0.06898465936078356]
	TIME [epoch: 5.88 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03332882756508772		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03332882756508772 | validation: 0.08515871702043216]
	TIME [epoch: 5.87 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03585592282908749		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03585592282908749 | validation: 0.06806020188497298]
	TIME [epoch: 5.87 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364152661804447		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.03364152661804447 | validation: 0.07730322501309378]
	TIME [epoch: 5.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03800130379693852		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03800130379693852 | validation: 0.06569906795045871]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037383732451612375		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.037383732451612375 | validation: 0.07324304528432794]
	TIME [epoch: 5.85 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037480233160911425		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.037480233160911425 | validation: 0.07299250306176117]
	TIME [epoch: 5.85 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438323226310681		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03438323226310681 | validation: 0.07203157931102588]
	TIME [epoch: 5.85 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033470196836748435		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.033470196836748435 | validation: 0.0662263917080239]
	TIME [epoch: 5.85 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882472856516849		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.03882472856516849 | validation: 0.054741059343521775]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473156847523047		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03473156847523047 | validation: 0.09750361880029465]
	TIME [epoch: 5.85 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04633540034375539		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.04633540034375539 | validation: 0.08417418792254516]
	TIME [epoch: 5.85 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037889584562418456		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.037889584562418456 | validation: 0.0653590003016322]
	TIME [epoch: 5.85 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386223483433735		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.0386223483433735 | validation: 0.0655809434796486]
	TIME [epoch: 5.85 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03316502072430639		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03316502072430639 | validation: 0.07566741807110748]
	TIME [epoch: 5.84 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036681196594464886		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.036681196594464886 | validation: 0.0761910250485383]
	TIME [epoch: 5.85 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03398670884481073		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03398670884481073 | validation: 0.07526909575836754]
	TIME [epoch: 5.86 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0371460196274246		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.0371460196274246 | validation: 0.1004361942915854]
	TIME [epoch: 5.85 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434108245314611		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.06434108245314611 | validation: 0.0675851944861394]
	TIME [epoch: 5.86 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04045742633324335		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.04045742633324335 | validation: 0.06612301329944674]
	TIME [epoch: 5.86 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035096358717484046		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.035096358717484046 | validation: 0.06375327717863093]
	TIME [epoch: 5.86 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03421089968295637		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.03421089968295637 | validation: 0.07126457174701514]
	TIME [epoch: 5.86 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473052291556501		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03473052291556501 | validation: 0.06869877724493749]
	TIME [epoch: 5.85 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334876957396293		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.03334876957396293 | validation: 0.06341622936797901]
	TIME [epoch: 5.88 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033001021115853416		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.033001021115853416 | validation: 0.07148115564941984]
	TIME [epoch: 5.88 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03455078125216654		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.03455078125216654 | validation: 0.06818915246942896]
	TIME [epoch: 5.88 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03274961306902024		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.03274961306902024 | validation: 0.08187082457426777]
	TIME [epoch: 5.88 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037530575839091716		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.037530575839091716 | validation: 0.06391271358809392]
	TIME [epoch: 5.88 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03211076853127084		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.03211076853127084 | validation: 0.06243999985765913]
	TIME [epoch: 5.88 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031903579439637335		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.031903579439637335 | validation: 0.08966490988334745]
	TIME [epoch: 5.88 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039102736278087226		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.039102736278087226 | validation: 0.06736632811804849]
	TIME [epoch: 5.84 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462944274711654		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03462944274711654 | validation: 0.07167711855625938]
	TIME [epoch: 5.84 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034409475853570966		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.034409475853570966 | validation: 0.058932868653145565]
	TIME [epoch: 5.84 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035014672162085196		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.035014672162085196 | validation: 0.07602248658973348]
	TIME [epoch: 5.85 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03845658595977266		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03845658595977266 | validation: 0.07183772721538625]
	TIME [epoch: 5.87 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414809881429686		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03414809881429686 | validation: 0.05860116527008141]
	TIME [epoch: 5.89 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03276079806996997		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03276079806996997 | validation: 0.06843709946526619]
	TIME [epoch: 5.87 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032778167068661254		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.032778167068661254 | validation: 0.05998562897914134]
	TIME [epoch: 5.88 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441730208958331		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03441730208958331 | validation: 0.06605208678694467]
	TIME [epoch: 5.87 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032485113627959326		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.032485113627959326 | validation: 0.07804593973399643]
	TIME [epoch: 5.88 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032242405134802164		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.032242405134802164 | validation: 0.06728681905158139]
	TIME [epoch: 5.87 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03313794928279811		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.03313794928279811 | validation: 0.07271400152868716]
	TIME [epoch: 5.88 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034080439666923985		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.034080439666923985 | validation: 0.07511200221334947]
	TIME [epoch: 5.89 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0348917138586207		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.0348917138586207 | validation: 0.0771169529830214]
	TIME [epoch: 5.88 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778939289776594		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03778939289776594 | validation: 0.059619754590572574]
	TIME [epoch: 5.88 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03373744928213572		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03373744928213572 | validation: 0.0721552534646974]
	TIME [epoch: 5.88 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033006715156790364		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.033006715156790364 | validation: 0.057358101785915064]
	TIME [epoch: 5.88 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029726772573364625		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.029726772573364625 | validation: 0.05966903751376767]
	TIME [epoch: 5.87 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462544242448812		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03462544242448812 | validation: 0.06268299209305676]
	TIME [epoch: 5.87 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035609928107584565		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.035609928107584565 | validation: 0.06313276268030743]
	TIME [epoch: 5.87 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0337870988942106		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0337870988942106 | validation: 0.06507506979214023]
	TIME [epoch: 5.87 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03136192695701624		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.03136192695701624 | validation: 0.07052284110945249]
	TIME [epoch: 5.87 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03388390385190378		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.03388390385190378 | validation: 0.05668782875821135]
	TIME [epoch: 5.85 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256521951320503		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.03256521951320503 | validation: 0.07192151560533769]
	TIME [epoch: 5.84 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032580145963417496		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.032580145963417496 | validation: 0.06193058780591013]
	TIME [epoch: 5.84 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03388007683125391		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.03388007683125391 | validation: 0.06716591202232489]
	TIME [epoch: 5.84 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031818029297110695		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.031818029297110695 | validation: 0.06943426719317197]
	TIME [epoch: 5.84 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03453916788763566		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03453916788763566 | validation: 0.05845670970325553]
	TIME [epoch: 5.85 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030068127905313304		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.030068127905313304 | validation: 0.06074903465930205]
	TIME [epoch: 5.85 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03121165623169375		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03121165623169375 | validation: 0.060676552795390894]
	TIME [epoch: 5.85 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032722402882792975		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.032722402882792975 | validation: 0.05963185271072069]
	TIME [epoch: 5.88 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242433919340743		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.03242433919340743 | validation: 0.06494502024293533]
	TIME [epoch: 5.87 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033506474755562246		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.033506474755562246 | validation: 0.06532463481152086]
	TIME [epoch: 5.87 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031063210264963816		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.031063210264963816 | validation: 0.04947204336461541]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034395587078358644		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.034395587078358644 | validation: 0.07031382481156022]
	TIME [epoch: 5.87 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736987694804366		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03736987694804366 | validation: 0.054636084334327185]
	TIME [epoch: 5.87 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02971571415527721		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.02971571415527721 | validation: 0.07082061845107367]
	TIME [epoch: 5.88 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031037640962795023		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.031037640962795023 | validation: 0.0572135082644341]
	TIME [epoch: 5.87 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03091025553499899		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.03091025553499899 | validation: 0.0514186629138363]
	TIME [epoch: 5.87 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03229603998075481		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.03229603998075481 | validation: 0.06252459085791118]
	TIME [epoch: 5.88 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034122816431888005		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.034122816431888005 | validation: 0.06296894847559519]
	TIME [epoch: 5.88 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03266918486588138		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03266918486588138 | validation: 0.05649841033226421]
	TIME [epoch: 5.87 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030463833664790742		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.030463833664790742 | validation: 0.059550333838597806]
	TIME [epoch: 5.87 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03138010336402629		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.03138010336402629 | validation: 0.06361400252346054]
	TIME [epoch: 5.87 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030586970263255648		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.030586970263255648 | validation: 0.05835850082190524]
	TIME [epoch: 5.87 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03182364987646394		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.03182364987646394 | validation: 0.06335268865561287]
	TIME [epoch: 5.87 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03428851069228863		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03428851069228863 | validation: 0.06439474065762076]
	TIME [epoch: 5.87 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029983155277428262		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.029983155277428262 | validation: 0.0654215207588378]
	TIME [epoch: 5.87 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03243381740467029		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03243381740467029 | validation: 0.055685061638327794]
	TIME [epoch: 5.87 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03088288488690684		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.03088288488690684 | validation: 0.07352125214457327]
	TIME [epoch: 5.88 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034583478657297814		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.034583478657297814 | validation: 0.052911081264678195]
	TIME [epoch: 5.87 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033659568567098466		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.033659568567098466 | validation: 0.06995215074531583]
	TIME [epoch: 5.87 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0336366601444007		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.0336366601444007 | validation: 0.06875437816776626]
	TIME [epoch: 5.87 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038443025050860925		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.038443025050860925 | validation: 0.058514927518834296]
	TIME [epoch: 5.87 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02989431201179257		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.02989431201179257 | validation: 0.07227056911488731]
	TIME [epoch: 5.87 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307116760488871		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.03307116760488871 | validation: 0.06750491889672125]
	TIME [epoch: 5.88 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204686012674549		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03204686012674549 | validation: 0.05817499877465995]
	TIME [epoch: 5.87 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033998796991358646		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.033998796991358646 | validation: 0.06830755305895263]
	TIME [epoch: 5.88 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363907151948443		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.03363907151948443 | validation: 0.06705999653657718]
	TIME [epoch: 5.88 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032282719805723274		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.032282719805723274 | validation: 0.06556265634219072]
	TIME [epoch: 5.87 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029294058265385014		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.029294058265385014 | validation: 0.06779602010157569]
	TIME [epoch: 5.87 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03027121316320638		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.03027121316320638 | validation: 0.0454720147276692]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030582711695916625		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.030582711695916625 | validation: 0.05963431497999558]
	TIME [epoch: 5.88 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02849718666680859		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.02849718666680859 | validation: 0.05936594758733519]
	TIME [epoch: 5.89 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029112836224342287		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.029112836224342287 | validation: 0.05681094731616243]
	TIME [epoch: 5.88 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02932881429123936		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.02932881429123936 | validation: 0.05035007262534911]
	TIME [epoch: 5.88 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02961460844261169		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.02961460844261169 | validation: 0.05930055010897282]
	TIME [epoch: 5.89 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031539306491737765		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.031539306491737765 | validation: 0.0662318236710992]
	TIME [epoch: 5.89 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030914229730481278		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.030914229730481278 | validation: 0.05820550675420544]
	TIME [epoch: 5.88 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031023755092681793		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.031023755092681793 | validation: 0.05837725956809947]
	TIME [epoch: 5.88 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030816115434647327		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.030816115434647327 | validation: 0.07632119726773377]
	TIME [epoch: 5.87 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03472885918098622		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.03472885918098622 | validation: 0.05730433858338277]
	TIME [epoch: 5.88 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02810098932670968		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.02810098932670968 | validation: 0.05068283004491088]
	TIME [epoch: 5.87 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02872582794212275		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.02872582794212275 | validation: 0.06468498213891763]
	TIME [epoch: 5.88 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031180974431659153		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.031180974431659153 | validation: 0.05554829170960676]
	TIME [epoch: 5.87 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026846084153813624		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.026846084153813624 | validation: 0.05287564581786104]
	TIME [epoch: 5.88 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027678026179888904		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.027678026179888904 | validation: 0.054355691242032295]
	TIME [epoch: 5.88 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028719152332203703		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.028719152332203703 | validation: 0.0680950319133373]
	TIME [epoch: 5.88 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027261873122237885		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.027261873122237885 | validation: 0.061179605819503075]
	TIME [epoch: 5.88 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031149616851383018		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.031149616851383018 | validation: 0.05785888192985886]
	TIME [epoch: 5.88 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030962911904540244		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.030962911904540244 | validation: 0.05325165615393314]
	TIME [epoch: 5.88 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028094420202059395		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.028094420202059395 | validation: 0.05808356300749989]
	TIME [epoch: 5.87 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030952466420513126		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.030952466420513126 | validation: 0.06315525069480589]
	TIME [epoch: 5.88 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030110208856252794		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.030110208856252794 | validation: 0.05673501795374447]
	TIME [epoch: 5.88 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030252813146197242		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.030252813146197242 | validation: 0.051516807047784066]
	TIME [epoch: 5.88 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030087189655714885		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.030087189655714885 | validation: 0.060035632446461075]
	TIME [epoch: 5.89 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028054620172349942		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.028054620172349942 | validation: 0.05881567306681254]
	TIME [epoch: 5.89 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03060075384206971		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.03060075384206971 | validation: 0.052578808593685404]
	TIME [epoch: 5.89 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032397352816693925		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.032397352816693925 | validation: 0.05709347898196981]
	TIME [epoch: 5.88 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03079765501851621		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.03079765501851621 | validation: 0.06478835387567783]
	TIME [epoch: 5.87 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029240364400464118		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.029240364400464118 | validation: 0.049649681383770454]
	TIME [epoch: 5.87 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030045592822817355		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.030045592822817355 | validation: 0.06116291094605693]
	TIME [epoch: 5.87 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028588188564102276		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.028588188564102276 | validation: 0.054107216351942114]
	TIME [epoch: 5.87 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028383957843478597		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.028383957843478597 | validation: 0.05085723054591547]
	TIME [epoch: 280 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029024769911043614		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.029024769911043614 | validation: 0.0507945320595234]
	TIME [epoch: 12.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028289019120629456		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.028289019120629456 | validation: 0.06770541178689758]
	TIME [epoch: 12.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046483356711018775		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.046483356711018775 | validation: 0.06748534942888655]
	TIME [epoch: 12.5 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538725411784188		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.03538725411784188 | validation: 0.05735900602048886]
	TIME [epoch: 12.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026420755297208567		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.026420755297208567 | validation: 0.06486260133049777]
	TIME [epoch: 12.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02984355109896804		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.02984355109896804 | validation: 0.05035176233729133]
	TIME [epoch: 12.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02568601065055315		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.02568601065055315 | validation: 0.05581865386609978]
	TIME [epoch: 12.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0282898261271137		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.0282898261271137 | validation: 0.05628118063993]
	TIME [epoch: 12.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030908390483249847		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.030908390483249847 | validation: 0.059092222117948956]
	TIME [epoch: 12.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0290133181595707		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0290133181595707 | validation: 0.0649669374843011]
	TIME [epoch: 12.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133096810594767		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03133096810594767 | validation: 0.0491294761239248]
	TIME [epoch: 12.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030380290325289584		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.030380290325289584 | validation: 0.05155587106137918]
	TIME [epoch: 12.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03124591897164127		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03124591897164127 | validation: 0.05173818272141856]
	TIME [epoch: 12.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027819286218033284		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.027819286218033284 | validation: 0.05806843385775014]
	TIME [epoch: 12.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029303470270313714		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.029303470270313714 | validation: 0.05488435242744273]
	TIME [epoch: 12.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02815969497221666		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.02815969497221666 | validation: 0.04708796256673529]
	TIME [epoch: 12.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029373782423681806		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.029373782423681806 | validation: 0.05103611037092716]
	TIME [epoch: 12.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030636677926647263		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.030636677926647263 | validation: 0.05309219150645365]
	TIME [epoch: 12.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03032303696827749		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03032303696827749 | validation: 0.055831590542349076]
	TIME [epoch: 12.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028130183044732866		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.028130183044732866 | validation: 0.0590917617955212]
	TIME [epoch: 12.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029632303400927897		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.029632303400927897 | validation: 0.0637527398350805]
	TIME [epoch: 12.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027768063726298326		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.027768063726298326 | validation: 0.04281248260820708]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1023.pth
	Model improved!!!
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027719089906864013		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.027719089906864013 | validation: 0.061125649323359316]
	TIME [epoch: 12.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029202808146057916		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.029202808146057916 | validation: 0.052312608511791536]
	TIME [epoch: 12.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02846566371109444		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.02846566371109444 | validation: 0.05726687522772936]
	TIME [epoch: 12.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02737495604376941		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.02737495604376941 | validation: 0.05267709028705824]
	TIME [epoch: 12.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748691492531423		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.02748691492531423 | validation: 0.047063774119613204]
	TIME [epoch: 12.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025957969475265744		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.025957969475265744 | validation: 0.061271597775096644]
	TIME [epoch: 12.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02776814176194397		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.02776814176194397 | validation: 0.055027897998836506]
	TIME [epoch: 12.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025752711324327486		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.025752711324327486 | validation: 0.05042228470827035]
	TIME [epoch: 12.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028992013144172643		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.028992013144172643 | validation: 0.05025933130467908]
	TIME [epoch: 12.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02556491132214333		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.02556491132214333 | validation: 0.05191798976664418]
	TIME [epoch: 12.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02789571634184135		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.02789571634184135 | validation: 0.054960914583892954]
	TIME [epoch: 12.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029695259741840785		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.029695259741840785 | validation: 0.0482974554268904]
	TIME [epoch: 12.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028344960812596343		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.028344960812596343 | validation: 0.05100482553139571]
	TIME [epoch: 12.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02738263770888298		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.02738263770888298 | validation: 0.05296236603259648]
	TIME [epoch: 12.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030895739819011274		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.030895739819011274 | validation: 0.05155088785286686]
	TIME [epoch: 12.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027565755982586784		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.027565755982586784 | validation: 0.05861825766282813]
	TIME [epoch: 12.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028231881941040794		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.028231881941040794 | validation: 0.05359671208929248]
	TIME [epoch: 12.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028981936048411688		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.028981936048411688 | validation: 0.0582680458264494]
	TIME [epoch: 12.5 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028656920998218788		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.028656920998218788 | validation: 0.04671794635534391]
	TIME [epoch: 12.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029731103151133788		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.029731103151133788 | validation: 0.05731385350799097]
	TIME [epoch: 12.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029874024728476282		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.029874024728476282 | validation: 0.041873877430290946]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1044.pth
	Model improved!!!
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027021013718235523		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.027021013718235523 | validation: 0.0575511598811943]
	TIME [epoch: 12.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028503239229490846		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.028503239229490846 | validation: 0.05342129227701664]
	TIME [epoch: 12.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025051998016000523		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.025051998016000523 | validation: 0.06006332695369753]
	TIME [epoch: 12.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026119188925216887		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.026119188925216887 | validation: 0.04991397943782194]
	TIME [epoch: 12.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027702614551676507		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.027702614551676507 | validation: 0.055375708635915614]
	TIME [epoch: 12.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02601164228693347		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.02601164228693347 | validation: 0.06406354315281396]
	TIME [epoch: 12.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031410478960001216		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.031410478960001216 | validation: 0.04858006943484553]
	TIME [epoch: 12.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02585543373415324		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.02585543373415324 | validation: 0.053951961038013446]
	TIME [epoch: 12.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029980550702597496		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.029980550702597496 | validation: 0.05371335879454478]
	TIME [epoch: 12.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026871320484013226		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.026871320484013226 | validation: 0.06172386475634668]
	TIME [epoch: 12.5 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029642199837983822		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.029642199837983822 | validation: 0.045176365481086604]
	TIME [epoch: 12.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027691133837189123		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.027691133837189123 | validation: 0.05962879141949954]
	TIME [epoch: 12.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02767353308101247		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.02767353308101247 | validation: 0.0506464437488616]
	TIME [epoch: 12.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02676527112873214		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.02676527112873214 | validation: 0.04530332548240325]
	TIME [epoch: 12.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025402734150895084		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.025402734150895084 | validation: 0.06343343006812899]
	TIME [epoch: 12.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885649939188327		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.02885649939188327 | validation: 0.05254001567678712]
	TIME [epoch: 12.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028233724641027993		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.028233724641027993 | validation: 0.05745538366035494]
	TIME [epoch: 12.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02449795715693069		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.02449795715693069 | validation: 0.05193764943479698]
	TIME [epoch: 12.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027684414332139577		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.027684414332139577 | validation: 0.049158907272533484]
	TIME [epoch: 12.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025586439986573443		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.025586439986573443 | validation: 0.06207344947278614]
	TIME [epoch: 12.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028571084914427308		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.028571084914427308 | validation: 0.046503380344600026]
	TIME [epoch: 12.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028275294696404798		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.028275294696404798 | validation: 0.04926134671150624]
	TIME [epoch: 12.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025938569057249318		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.025938569057249318 | validation: 0.055236497236052375]
	TIME [epoch: 12.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02782614082700081		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.02782614082700081 | validation: 0.054725216685105785]
	TIME [epoch: 12.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026264301566323135		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.026264301566323135 | validation: 0.05820250176076091]
	TIME [epoch: 12.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750113017626024		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.02750113017626024 | validation: 0.060103577587172774]
	TIME [epoch: 12.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03050079899556236		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.03050079899556236 | validation: 0.03831900843374872]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1071.pth
	Model improved!!!
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029954358301368177		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.029954358301368177 | validation: 0.048578316287579616]
	TIME [epoch: 12.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02549495702112664		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.02549495702112664 | validation: 0.05577378140421551]
	TIME [epoch: 12.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026094640872960255		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.026094640872960255 | validation: 0.041778959374980246]
	TIME [epoch: 12.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023598472903892578		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.023598472903892578 | validation: 0.05172998792750986]
	TIME [epoch: 12.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024876931959808314		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.024876931959808314 | validation: 0.05017926032739095]
	TIME [epoch: 12.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02771797184355573		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.02771797184355573 | validation: 0.049304173458761424]
	TIME [epoch: 12.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025455457667768105		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.025455457667768105 | validation: 0.048107631217357893]
	TIME [epoch: 12.5 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029530146346400308		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.029530146346400308 | validation: 0.047550519175434396]
	TIME [epoch: 12.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026085851331490044		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.026085851331490044 | validation: 0.04330969337868167]
	TIME [epoch: 12.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026262092179461828		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.026262092179461828 | validation: 0.046149099315932876]
	TIME [epoch: 12.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02424584629445268		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02424584629445268 | validation: 0.046982043602048135]
	TIME [epoch: 12.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023130222561522952		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.023130222561522952 | validation: 0.04555939257079124]
	TIME [epoch: 12.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024718906469442064		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.024718906469442064 | validation: 0.04597773515235514]
	TIME [epoch: 12.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024746419830952555		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.024746419830952555 | validation: 0.0522228731523386]
	TIME [epoch: 12.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026370988867778337		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.026370988867778337 | validation: 0.03905024392596005]
	TIME [epoch: 12.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023714028684083958		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.023714028684083958 | validation: 0.050662740967896004]
	TIME [epoch: 12.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026429340179856		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.026429340179856 | validation: 0.05099389331475597]
	TIME [epoch: 12.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02578777133543946		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.02578777133543946 | validation: 0.05047001004675031]
	TIME [epoch: 12.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025962051054718032		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.025962051054718032 | validation: 0.05202039076538927]
	TIME [epoch: 12.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027179783115650006		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.027179783115650006 | validation: 0.06002197133404498]
	TIME [epoch: 12.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04000167621748717		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.04000167621748717 | validation: 0.056306304603214286]
	TIME [epoch: 12.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030759860257814103		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.030759860257814103 | validation: 0.05428202334327277]
	TIME [epoch: 12.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025588096544586252		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.025588096544586252 | validation: 0.057321630750189125]
	TIME [epoch: 12.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023537279761457378		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.023537279761457378 | validation: 0.036832709238512944]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1095.pth
	Model improved!!!
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02740720949465599		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02740720949465599 | validation: 0.043643562026470364]
	TIME [epoch: 12.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02549232081308537		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.02549232081308537 | validation: 0.048532605771600915]
	TIME [epoch: 12.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024841621754572874		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.024841621754572874 | validation: 0.04188728299538574]
	TIME [epoch: 12.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02287941909279166		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.02287941909279166 | validation: 0.04294971064154668]
	TIME [epoch: 12.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02609484298953284		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02609484298953284 | validation: 0.04796491777048887]
	TIME [epoch: 12.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028669691163251864		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.028669691163251864 | validation: 0.04586433502956791]
	TIME [epoch: 12.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02397779397329564		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.02397779397329564 | validation: 0.052521139053821414]
	TIME [epoch: 12.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023559511537293243		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.023559511537293243 | validation: 0.047461258203113005]
	TIME [epoch: 12.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023420228245504687		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.023420228245504687 | validation: 0.05094889499222338]
	TIME [epoch: 12.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025499865147040703		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.025499865147040703 | validation: 0.037520206500472886]
	TIME [epoch: 12.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027445307593689558		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.027445307593689558 | validation: 0.05462183464071573]
	TIME [epoch: 12.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026289874453430492		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.026289874453430492 | validation: 0.05221892565523483]
	TIME [epoch: 12.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027929859839626363		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.027929859839626363 | validation: 0.046101697038595985]
	TIME [epoch: 12.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027114576012628292		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.027114576012628292 | validation: 0.0539672653089967]
	TIME [epoch: 12.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02542275571042803		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.02542275571042803 | validation: 0.054964438637100224]
	TIME [epoch: 12.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025991890218219193		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.025991890218219193 | validation: 0.0523405422679151]
	TIME [epoch: 12.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0247013099421839		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.0247013099421839 | validation: 0.057040707715511244]
	TIME [epoch: 12.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02358397258493044		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.02358397258493044 | validation: 0.0486831044338526]
	TIME [epoch: 12.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026475093905822422		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.026475093905822422 | validation: 0.044893419942405755]
	TIME [epoch: 12.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025186386650770557		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.025186386650770557 | validation: 0.05435384532227264]
	TIME [epoch: 12.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02455066695297244		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.02455066695297244 | validation: 0.0423395093117675]
	TIME [epoch: 12.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02525992767063845		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.02525992767063845 | validation: 0.04703708237897421]
	TIME [epoch: 12.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02365072275111958		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.02365072275111958 | validation: 0.056420366473337286]
	TIME [epoch: 12.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02252131401786579		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02252131401786579 | validation: 0.04240847729987213]
	TIME [epoch: 12.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02516037196496798		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.02516037196496798 | validation: 0.04206166987416909]
	TIME [epoch: 12.5 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023824819598253305		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.023824819598253305 | validation: 0.05132260948545338]
	TIME [epoch: 12.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025736483182909015		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.025736483182909015 | validation: 0.04823685176721774]
	TIME [epoch: 12.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025200074414722096		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.025200074414722096 | validation: 0.04660712922742382]
	TIME [epoch: 12.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023920813885803575		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.023920813885803575 | validation: 0.0465513732443574]
	TIME [epoch: 12.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02398781953798479		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.02398781953798479 | validation: 0.04113081423147774]
	TIME [epoch: 12.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02516226704503766		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.02516226704503766 | validation: 0.05623429540432269]
	TIME [epoch: 12.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02443105175091425		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.02443105175091425 | validation: 0.051319335637395304]
	TIME [epoch: 12.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025908197213505117		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.025908197213505117 | validation: 0.04410964404399912]
	TIME [epoch: 12.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02604674657963317		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.02604674657963317 | validation: 0.039993642427613024]
	TIME [epoch: 12.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024384880731372793		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.024384880731372793 | validation: 0.04560563219559678]
	TIME [epoch: 12.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02406069445942184		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02406069445942184 | validation: 0.04504982382464493]
	TIME [epoch: 12.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024106822791669674		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.024106822791669674 | validation: 0.058239251426252185]
	TIME [epoch: 12.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026832954889836193		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.026832954889836193 | validation: 0.045009203721321424]
	TIME [epoch: 12.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021871529698864405		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.021871529698864405 | validation: 0.04280297565821987]
	TIME [epoch: 12.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021777558976509452		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.021777558976509452 | validation: 0.037758815138049254]
	TIME [epoch: 12.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025017185613090704		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.025017185613090704 | validation: 0.04876955185338727]
	TIME [epoch: 12.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02421300631868138		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.02421300631868138 | validation: 0.04114276183518463]
	TIME [epoch: 12.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02391997879827514		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.02391997879827514 | validation: 0.052804393574172895]
	TIME [epoch: 12.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022153459927562427		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.022153459927562427 | validation: 0.050621622159253234]
	TIME [epoch: 12.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023749209773621476		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.023749209773621476 | validation: 0.050244972520150366]
	TIME [epoch: 12.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023944810983568078		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.023944810983568078 | validation: 0.05680790371175373]
	TIME [epoch: 12.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023515573996736556		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.023515573996736556 | validation: 0.048174376959481305]
	TIME [epoch: 12.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223536669681461		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.0223536669681461 | validation: 0.051412710735387335]
	TIME [epoch: 12.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021511185739078956		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.021511185739078956 | validation: 0.058810372729418005]
	TIME [epoch: 12.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024479599014142865		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.024479599014142865 | validation: 0.04622571245159471]
	TIME [epoch: 12.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023597296147317107		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.023597296147317107 | validation: 0.046393318007662124]
	TIME [epoch: 12.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02469791883820286		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.02469791883820286 | validation: 0.048048762232798393]
	TIME [epoch: 12.5 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02503739754454822		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02503739754454822 | validation: 0.04486251282451899]
	TIME [epoch: 12.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02284632852855731		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02284632852855731 | validation: 0.04770464881474934]
	TIME [epoch: 12.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023071942140942155		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.023071942140942155 | validation: 0.05282786708829795]
	TIME [epoch: 12.5 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02459836149094679		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.02459836149094679 | validation: 0.051012891235397186]
	TIME [epoch: 12.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024595506800656555		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.024595506800656555 | validation: 0.052980495757685736]
	TIME [epoch: 12.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026320594063133314		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.026320594063133314 | validation: 0.05134921745329041]
	TIME [epoch: 12.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024782607199802523		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.024782607199802523 | validation: 0.04423198499940759]
	TIME [epoch: 12.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024657612217058488		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.024657612217058488 | validation: 0.05539258533804077]
	TIME [epoch: 12.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028083639089200946		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.028083639089200946 | validation: 0.06069558006734863]
	TIME [epoch: 12.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025224584063353987		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.025224584063353987 | validation: 0.039497531263416756]
	TIME [epoch: 12.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023845953534876427		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.023845953534876427 | validation: 0.04944054267603612]
	TIME [epoch: 12.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022245647676934207		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.022245647676934207 | validation: 0.058407214647192764]
	TIME [epoch: 12.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024251055376529267		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.024251055376529267 | validation: 0.04751316224642066]
	TIME [epoch: 12.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022314369852224375		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.022314369852224375 | validation: 0.04518276347233148]
	TIME [epoch: 12.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023998686616466893		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.023998686616466893 | validation: 0.049528051012546086]
	TIME [epoch: 12.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021182760919019286		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.021182760919019286 | validation: 0.06010052045798826]
	TIME [epoch: 12.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025070825084764853		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.025070825084764853 | validation: 0.0472510823341507]
	TIME [epoch: 12.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021554489671425103		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.021554489671425103 | validation: 0.051120714462436934]
	TIME [epoch: 12.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539764740371613		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.02539764740371613 | validation: 0.045671812553539]
	TIME [epoch: 12.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02289418334912831		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.02289418334912831 | validation: 0.046032527001757244]
	TIME [epoch: 12.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025890695567926815		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.025890695567926815 | validation: 0.051928647781928496]
	TIME [epoch: 12.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027178031422097165		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.027178031422097165 | validation: 0.04671669095576308]
	TIME [epoch: 12.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02212170861190226		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02212170861190226 | validation: 0.04541689798482266]
	TIME [epoch: 12.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02401209518281558		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.02401209518281558 | validation: 0.05752319519227083]
	TIME [epoch: 12.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022334638503191426		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.022334638503191426 | validation: 0.04319229906492579]
	TIME [epoch: 12.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023705722310063248		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.023705722310063248 | validation: 0.04469759864110634]
	TIME [epoch: 12.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021896522996319615		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.021896522996319615 | validation: 0.04601990852835519]
	TIME [epoch: 12.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021798858457130522		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.021798858457130522 | validation: 0.0492346824717505]
	TIME [epoch: 12.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02295562757883981		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02295562757883981 | validation: 0.03940404626452774]
	TIME [epoch: 12.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02230107499525345		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.02230107499525345 | validation: 0.0617480000345786]
	TIME [epoch: 12.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024882189082100444		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.024882189082100444 | validation: 0.04926015462495508]
	TIME [epoch: 12.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02380514724365767		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.02380514724365767 | validation: 0.055051404178948854]
	TIME [epoch: 12.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028393509371908418		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.028393509371908418 | validation: 0.04416486995170732]
	TIME [epoch: 12.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026058736292445307		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.026058736292445307 | validation: 0.04372217779203054]
	TIME [epoch: 12.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023084850327277345		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.023084850327277345 | validation: 0.03427375476453344]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022505652724387985		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.022505652724387985 | validation: 0.047413848383496576]
	TIME [epoch: 12.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02360187941591864		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02360187941591864 | validation: 0.050274621004328814]
	TIME [epoch: 12.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026427204873472228		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.026427204873472228 | validation: 0.037072208857701884]
	TIME [epoch: 12.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02525590429107476		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.02525590429107476 | validation: 0.04247039046427409]
	TIME [epoch: 12.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024426277573060748		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.024426277573060748 | validation: 0.048752063272255244]
	TIME [epoch: 12.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215980631481838		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02215980631481838 | validation: 0.05200199051552811]
	TIME [epoch: 12.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022916828118668322		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.022916828118668322 | validation: 0.034354891728696046]
	TIME [epoch: 12.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020802349553082418		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.020802349553082418 | validation: 0.039036292567735636]
	TIME [epoch: 12.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023255694931988247		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.023255694931988247 | validation: 0.04372142150362174]
	TIME [epoch: 12.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020363812249462054		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.020363812249462054 | validation: 0.03687402330582782]
	TIME [epoch: 12.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02350511602961369		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.02350511602961369 | validation: 0.05307422065131212]
	TIME [epoch: 12.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024487234909084377		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.024487234909084377 | validation: 0.0445198473893027]
	TIME [epoch: 12.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024123851486610715		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.024123851486610715 | validation: 0.0425574224760063]
	TIME [epoch: 12.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023742672346115015		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.023742672346115015 | validation: 0.044472352519567916]
	TIME [epoch: 12.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021765446367697693		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.021765446367697693 | validation: 0.04362242280030265]
	TIME [epoch: 12.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023195910170692495		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.023195910170692495 | validation: 0.04780581680042989]
	TIME [epoch: 12.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021468930841052444		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.021468930841052444 | validation: 0.044930900085219584]
	TIME [epoch: 12.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02075371754149659		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.02075371754149659 | validation: 0.04529056927666334]
	TIME [epoch: 12.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021952808527710657		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.021952808527710657 | validation: 0.04711175913939833]
	TIME [epoch: 12.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02362581868442158		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02362581868442158 | validation: 0.049441590529310825]
	TIME [epoch: 12.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024126545337196284		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.024126545337196284 | validation: 0.05716026252256835]
	TIME [epoch: 12.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026337983187501263		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.026337983187501263 | validation: 0.03552976138383275]
	TIME [epoch: 12.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020764877099280828		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.020764877099280828 | validation: 0.042484709122647485]
	TIME [epoch: 12.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021620503547709283		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.021620503547709283 | validation: 0.04719357521085624]
	TIME [epoch: 12.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02137343866443668		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.02137343866443668 | validation: 0.040658479879927704]
	TIME [epoch: 12.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022750131628303385		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.022750131628303385 | validation: 0.044183038112666544]
	TIME [epoch: 12.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02335861010606084		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02335861010606084 | validation: 0.041537956065253435]
	TIME [epoch: 12.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021992404591930238		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.021992404591930238 | validation: 0.04626416112067196]
	TIME [epoch: 12.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02253073599417944		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.02253073599417944 | validation: 0.05067102165057276]
	TIME [epoch: 12.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022848634075675844		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.022848634075675844 | validation: 0.04477005425307262]
	TIME [epoch: 12.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021285218924419064		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.021285218924419064 | validation: 0.05318573578028072]
	TIME [epoch: 12.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019876282016543536		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.019876282016543536 | validation: 0.039225561153936266]
	TIME [epoch: 12.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021685204247947513		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.021685204247947513 | validation: 0.0423976474072291]
	TIME [epoch: 12.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023125501975296645		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.023125501975296645 | validation: 0.0459291185151005]
	TIME [epoch: 12.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023102295964584383		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.023102295964584383 | validation: 0.05580137397633705]
	TIME [epoch: 12.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029107856254848335		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.029107856254848335 | validation: 0.045782337051855373]
	TIME [epoch: 12.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022478550891897858		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.022478550891897858 | validation: 0.03569353281040342]
	TIME [epoch: 12.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02214284390759969		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.02214284390759969 | validation: 0.04331314476293801]
	TIME [epoch: 12.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023271461630345873		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.023271461630345873 | validation: 0.042628892695896085]
	TIME [epoch: 12.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02582306312195752		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.02582306312195752 | validation: 0.049989465278763204]
	TIME [epoch: 12.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023556444716050697		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.023556444716050697 | validation: 0.04177188622828331]
	TIME [epoch: 12.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02431241159568014		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.02431241159568014 | validation: 0.04888751420578903]
	TIME [epoch: 12.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023794895056883984		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.023794895056883984 | validation: 0.041084993359242455]
	TIME [epoch: 12.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022282626029119314		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.022282626029119314 | validation: 0.0469921556332987]
	TIME [epoch: 12.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020392456495988878		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.020392456495988878 | validation: 0.04413146814388217]
	TIME [epoch: 12.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021715750338598558		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.021715750338598558 | validation: 0.03756423928394018]
	TIME [epoch: 12.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021702275175388076		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.021702275175388076 | validation: 0.040888074604358574]
	TIME [epoch: 12.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02372817612319994		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02372817612319994 | validation: 0.05598528882693499]
	TIME [epoch: 12.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025428286550680503		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.025428286550680503 | validation: 0.045180659709977436]
	TIME [epoch: 12.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025128786377925864		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.025128786377925864 | validation: 0.03605108070771578]
	TIME [epoch: 12.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024253123946481856		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.024253123946481856 | validation: 0.044798268168371624]
	TIME [epoch: 12.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02096229987544626		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.02096229987544626 | validation: 0.04347352419293472]
	TIME [epoch: 12.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021482267623173443		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.021482267623173443 | validation: 0.05355710110319391]
	TIME [epoch: 12.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02364423749981054		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02364423749981054 | validation: 0.04645651052694402]
	TIME [epoch: 12.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023169643188397777		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.023169643188397777 | validation: 0.04121095058471258]
	TIME [epoch: 12.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023214996047493046		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.023214996047493046 | validation: 0.04100229535938445]
	TIME [epoch: 12.5 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02280186027917137		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.02280186027917137 | validation: 0.05498328411901631]
	TIME [epoch: 12.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02169373693422796		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.02169373693422796 | validation: 0.035556068673764596]
	TIME [epoch: 12.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021005853353983		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.021005853353983 | validation: 0.055473305752423166]
	TIME [epoch: 12.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026788124395168485		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.026788124395168485 | validation: 0.043824431444943426]
	TIME [epoch: 12.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023607890887737747		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.023607890887737747 | validation: 0.04044909765667625]
	TIME [epoch: 12.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022013387671864556		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.022013387671864556 | validation: 0.03534038223428946]
	TIME [epoch: 12.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348400640237854		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02348400640237854 | validation: 0.03756967648285701]
	TIME [epoch: 12.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023400775818590503		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.023400775818590503 | validation: 0.0435154028492186]
	TIME [epoch: 12.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020644812685083353		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.020644812685083353 | validation: 0.04634884452542433]
	TIME [epoch: 12.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02187118570634577		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.02187118570634577 | validation: 0.0483618007487366]
	TIME [epoch: 12.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022041344391905637		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.022041344391905637 | validation: 0.051329851506904604]
	TIME [epoch: 12.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0230353755991038		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.0230353755991038 | validation: 0.04819723847207822]
	TIME [epoch: 12.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024237328334963476		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.024237328334963476 | validation: 0.04905364111061442]
	TIME [epoch: 12.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022279736212284372		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.022279736212284372 | validation: 0.0415336708077158]
	TIME [epoch: 12.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021964043632184006		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.021964043632184006 | validation: 0.04944604003225367]
	TIME [epoch: 12.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022758245894355333		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.022758245894355333 | validation: 0.04352471563262653]
	TIME [epoch: 12.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023562103804794492		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.023562103804794492 | validation: 0.03665255947182109]
	TIME [epoch: 12.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0195308254563626		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.0195308254563626 | validation: 0.04479399380624274]
	TIME [epoch: 12.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021605799581688054		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.021605799581688054 | validation: 0.05096601951290051]
	TIME [epoch: 12.5 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023559857350699875		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.023559857350699875 | validation: 0.03402338719788311]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1258.pth
	Model improved!!!
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022846423182690725		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.022846423182690725 | validation: 0.041932095283028215]
	TIME [epoch: 12.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021978905245087953		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.021978905245087953 | validation: 0.03951885011394438]
	TIME [epoch: 12.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020973192325495344		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.020973192325495344 | validation: 0.03946723176542965]
	TIME [epoch: 12.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022213622322371764		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.022213622322371764 | validation: 0.04336033703221657]
	TIME [epoch: 12.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024908287979311706		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.024908287979311706 | validation: 0.04588266177163141]
	TIME [epoch: 12.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026878701262469894		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.026878701262469894 | validation: 0.04694775835025364]
	TIME [epoch: 12.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03213576433352647		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.03213576433352647 | validation: 0.048133771945595795]
	TIME [epoch: 12.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030572683439152704		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.030572683439152704 | validation: 0.04965115872970745]
	TIME [epoch: 12.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024839701046160734		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.024839701046160734 | validation: 0.05112145558776479]
	TIME [epoch: 12.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024482570036333754		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.024482570036333754 | validation: 0.042055852220576645]
	TIME [epoch: 12.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022874641948249286		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.022874641948249286 | validation: 0.04283977368791933]
	TIME [epoch: 12.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023207635484768122		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.023207635484768122 | validation: 0.04002937378270212]
	TIME [epoch: 12.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023697641109848455		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.023697641109848455 | validation: 0.040750530050691895]
	TIME [epoch: 12.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019463815498949537		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.019463815498949537 | validation: 0.04022779911650971]
	TIME [epoch: 12.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02051073401474221		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.02051073401474221 | validation: 0.039520032003659694]
	TIME [epoch: 12.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02056194367899618		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.02056194367899618 | validation: 0.0390744889206381]
	TIME [epoch: 12.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019622684676487284		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.019622684676487284 | validation: 0.03842780554161888]
	TIME [epoch: 12.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021659339114284925		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.021659339114284925 | validation: 0.03407890083909284]
	TIME [epoch: 12.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021470364599205764		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.021470364599205764 | validation: 0.0341287765037158]
	TIME [epoch: 12.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02061150172622223		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.02061150172622223 | validation: 0.03883378843387732]
	TIME [epoch: 12.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02326831797108632		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.02326831797108632 | validation: 0.03407499004048183]
	TIME [epoch: 12.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02189972595701818		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02189972595701818 | validation: 0.04743068081758106]
	TIME [epoch: 12.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020369107988081413		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.020369107988081413 | validation: 0.03787929539726396]
	TIME [epoch: 12.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02156678848931064		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.02156678848931064 | validation: 0.046363361937274944]
	TIME [epoch: 12.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023181932928257556		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.023181932928257556 | validation: 0.036160721068852146]
	TIME [epoch: 12.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02188167003810743		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.02188167003810743 | validation: 0.04730512558452737]
	TIME [epoch: 12.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023068899884367392		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.023068899884367392 | validation: 0.044158513215972583]
	TIME [epoch: 12.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021475160985677408		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.021475160985677408 | validation: 0.03187269260557555]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1286.pth
	Model improved!!!
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020913791514483276		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.020913791514483276 | validation: 0.04965860444798273]
	TIME [epoch: 12.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024040272989750592		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.024040272989750592 | validation: 0.034435062349406645]
	TIME [epoch: 12.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02257485451734698		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02257485451734698 | validation: 0.0414261851526056]
	TIME [epoch: 12.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02042781076339132		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.02042781076339132 | validation: 0.03931162108403507]
	TIME [epoch: 12.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206269782153108		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.0206269782153108 | validation: 0.04339423028326339]
	TIME [epoch: 12.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023140424679738238		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.023140424679738238 | validation: 0.053678066124671575]
	TIME [epoch: 12.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020508491557021013		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.020508491557021013 | validation: 0.034907656730286674]
	TIME [epoch: 12.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020414600472235204		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.020414600472235204 | validation: 0.03456847019295382]
	TIME [epoch: 12.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029036657091198258		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.029036657091198258 | validation: 0.03062475928242563]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1295.pth
	Model improved!!!
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025950200246284055		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.025950200246284055 | validation: 0.0390062427001584]
	TIME [epoch: 12.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02112331944817331		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.02112331944817331 | validation: 0.035470837531364055]
	TIME [epoch: 12.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021228398775846268		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.021228398775846268 | validation: 0.034114386893396374]
	TIME [epoch: 12.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0222432669433742		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.0222432669433742 | validation: 0.03749759795459381]
	TIME [epoch: 12.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021074604034299345		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.021074604034299345 | validation: 0.03196632237994159]
	TIME [epoch: 12.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01998545436635465		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.01998545436635465 | validation: 0.042070938199094514]
	TIME [epoch: 12.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020655820069810274		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.020655820069810274 | validation: 0.03482708572667995]
	TIME [epoch: 12.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02096797427666467		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.02096797427666467 | validation: 0.03526216120139624]
	TIME [epoch: 12.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021623137822199743		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.021623137822199743 | validation: 0.03457627530544083]
	TIME [epoch: 12.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019478178449597785		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.019478178449597785 | validation: 0.03833519101821747]
	TIME [epoch: 12.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02099031687311268		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.02099031687311268 | validation: 0.03964191218876099]
	TIME [epoch: 12.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01991700232784877		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.01991700232784877 | validation: 0.03900292293944904]
	TIME [epoch: 12.5 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018567025984420213		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.018567025984420213 | validation: 0.04367994554100704]
	TIME [epoch: 12.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019541158937402547		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.019541158937402547 | validation: 0.04123693756998653]
	TIME [epoch: 12.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020287299568467834		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.020287299568467834 | validation: 0.038862089281409155]
	TIME [epoch: 12.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02105504876456358		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.02105504876456358 | validation: 0.036688835743130456]
	TIME [epoch: 12.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019510973264708346		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.019510973264708346 | validation: 0.039202396122098984]
	TIME [epoch: 12.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02107809828312755		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02107809828312755 | validation: 0.05243090601312144]
	TIME [epoch: 12.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019120060580940363		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.019120060580940363 | validation: 0.030211611428896556]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1314.pth
	Model improved!!!
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019491242939513556		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.019491242939513556 | validation: 0.03489639505407451]
	TIME [epoch: 12.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022412243568890853		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.022412243568890853 | validation: 0.043390328346052456]
	TIME [epoch: 12.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019795565172857858		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.019795565172857858 | validation: 0.03420805502607698]
	TIME [epoch: 12.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018768236279348628		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.018768236279348628 | validation: 0.04070428414716568]
	TIME [epoch: 12.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02018278336864003		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.02018278336864003 | validation: 0.040415643735129014]
	TIME [epoch: 12.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018113002399029462		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.018113002399029462 | validation: 0.04272893076186936]
	TIME [epoch: 12.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01758794647307451		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.01758794647307451 | validation: 0.044544767564288906]
	TIME [epoch: 12.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020005523954117168		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.020005523954117168 | validation: 0.04282711646486538]
	TIME [epoch: 12.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020427469632700417		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.020427469632700417 | validation: 0.036530381130157365]
	TIME [epoch: 12.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018638832705076793		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.018638832705076793 | validation: 0.04408991659578507]
	TIME [epoch: 12.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020893040940195763		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.020893040940195763 | validation: 0.043596771109940105]
	TIME [epoch: 12.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02026262962258218		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.02026262962258218 | validation: 0.046191791129597086]
	TIME [epoch: 12.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021355483122993874		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.021355483122993874 | validation: 0.04242858586143722]
	TIME [epoch: 12.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019507359516726993		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.019507359516726993 | validation: 0.035292661142683104]
	TIME [epoch: 12.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020424101477816786		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.020424101477816786 | validation: 0.050385870247825884]
	TIME [epoch: 12.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021168301918072834		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.021168301918072834 | validation: 0.043634011131729836]
	TIME [epoch: 12.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020316349951705816		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.020316349951705816 | validation: 0.0465167019145195]
	TIME [epoch: 12.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021966613467620518		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.021966613467620518 | validation: 0.03179678688841649]
	TIME [epoch: 12.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02110362331069359		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.02110362331069359 | validation: 0.037839204450102006]
	TIME [epoch: 12.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02068108624264078		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02068108624264078 | validation: 0.04569632840853169]
	TIME [epoch: 12.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020586541153406576		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.020586541153406576 | validation: 0.046750033311567164]
	TIME [epoch: 12.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171143103715047		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.02171143103715047 | validation: 0.032089084080768236]
	TIME [epoch: 12.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020773918665948913		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.020773918665948913 | validation: 0.0484229602615227]
	TIME [epoch: 12.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0207096187781785		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.0207096187781785 | validation: 0.03609644403872388]
	TIME [epoch: 12.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020749984277267614		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.020749984277267614 | validation: 0.0421546684543033]
	TIME [epoch: 12.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023678258886385545		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.023678258886385545 | validation: 0.0474622095849444]
	TIME [epoch: 12.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020538683066855058		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.020538683066855058 | validation: 0.042157869045366225]
	TIME [epoch: 12.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01931381785847763		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.01931381785847763 | validation: 0.03149342053125217]
	TIME [epoch: 12.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020239542608111272		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.020239542608111272 | validation: 0.030839616974128983]
	TIME [epoch: 12.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02082343032640252		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.02082343032640252 | validation: 0.035259976012489314]
	TIME [epoch: 12.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020981609062935433		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.020981609062935433 | validation: 0.042511546030741545]
	TIME [epoch: 12.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02098552566526053		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.02098552566526053 | validation: 0.04138780814965753]
	TIME [epoch: 12.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019953321861382205		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.019953321861382205 | validation: 0.044719831330366624]
	TIME [epoch: 12.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02081283539011471		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02081283539011471 | validation: 0.045621615825935685]
	TIME [epoch: 12.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021122900454456774		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.021122900454456774 | validation: 0.037208176987675715]
	TIME [epoch: 12.5 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967324576604246		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.01967324576604246 | validation: 0.041817786714172016]
	TIME [epoch: 12.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02030729003315833		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.02030729003315833 | validation: 0.037895313717113556]
	TIME [epoch: 12.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021309780280884035		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.021309780280884035 | validation: 0.03805473968913373]
	TIME [epoch: 12.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018486882517094832		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.018486882517094832 | validation: 0.03183074440308297]
	TIME [epoch: 12.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019495091031301027		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.019495091031301027 | validation: 0.03457483861701257]
	TIME [epoch: 12.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02099362081855819		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.02099362081855819 | validation: 0.04552002162837426]
	TIME [epoch: 12.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021970509764973487		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.021970509764973487 | validation: 0.04592956259292415]
	TIME [epoch: 12.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022242669625262463		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.022242669625262463 | validation: 0.04016400904628029]
	TIME [epoch: 12.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021759380936009248		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.021759380936009248 | validation: 0.03906232252095259]
	TIME [epoch: 12.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002352356912147		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.02002352356912147 | validation: 0.0360938516339002]
	TIME [epoch: 12.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020004057930419355		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.020004057930419355 | validation: 0.03897270144607508]
	TIME [epoch: 12.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018627036545797372		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.018627036545797372 | validation: 0.04590765157799444]
	TIME [epoch: 12.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021274252897595963		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.021274252897595963 | validation: 0.04140608327535385]
	TIME [epoch: 12.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021309854919428557		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.021309854919428557 | validation: 0.04038586295173022]
	TIME [epoch: 12.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020987951942039694		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.020987951942039694 | validation: 0.03618178575270863]
	TIME [epoch: 12.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01993644498019942		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.01993644498019942 | validation: 0.04256290711610188]
	TIME [epoch: 12.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020157467419446858		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.020157467419446858 | validation: 0.03963242649040835]
	TIME [epoch: 12.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021481430025883368		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.021481430025883368 | validation: 0.04258927848847016]
	TIME [epoch: 12.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02096923473557007		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.02096923473557007 | validation: 0.04354652587630638]
	TIME [epoch: 12.5 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021184547803362916		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.021184547803362916 | validation: 0.041856544964165104]
	TIME [epoch: 12.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02138484070698612		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.02138484070698612 | validation: 0.046450437966456905]
	TIME [epoch: 12.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020341485660183227		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.020341485660183227 | validation: 0.04113748435427516]
	TIME [epoch: 12.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019485878790801953		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.019485878790801953 | validation: 0.03524397504055287]
	TIME [epoch: 12.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019474620985043108		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.019474620985043108 | validation: 0.03748262095605081]
	TIME [epoch: 12.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019610891513012964		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.019610891513012964 | validation: 0.035006490142623435]
	TIME [epoch: 12.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02084990598568583		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.02084990598568583 | validation: 0.04532147499291244]
	TIME [epoch: 12.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020709346048703177		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.020709346048703177 | validation: 0.038933801458177125]
	TIME [epoch: 12.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019450560804600742		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.019450560804600742 | validation: 0.04299056205534807]
	TIME [epoch: 12.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019607987678434805		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.019607987678434805 | validation: 0.038479665524143516]
	TIME [epoch: 12.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020582876783822757		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.020582876783822757 | validation: 0.050171532022684956]
	TIME [epoch: 12.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020337746690169647		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.020337746690169647 | validation: 0.03681823691213178]
	TIME [epoch: 12.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021021685271144928		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.021021685271144928 | validation: 0.03623220521071846]
	TIME [epoch: 12.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021788237477913832		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.021788237477913832 | validation: 0.033540355800152515]
	TIME [epoch: 12.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021299015998570665		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.021299015998570665 | validation: 0.03636631546162929]
	TIME [epoch: 12.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022069336773511514		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.022069336773511514 | validation: 0.044680224671000426]
	TIME [epoch: 12.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020548970507632987		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.020548970507632987 | validation: 0.0431397455146912]
	TIME [epoch: 12.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02354381576910609		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.02354381576910609 | validation: 0.034235961384200785]
	TIME [epoch: 12.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020946715433821258		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.020946715433821258 | validation: 0.04214432941766913]
	TIME [epoch: 12.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02185112311937121		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.02185112311937121 | validation: 0.04202938988821251]
	TIME [epoch: 12.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020681632916686173		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.020681632916686173 | validation: 0.04729256655488637]
	TIME [epoch: 12.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018413319343619134		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.018413319343619134 | validation: 0.0408613631877654]
	TIME [epoch: 12.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0205322869548372		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.0205322869548372 | validation: 0.03562769400866169]
	TIME [epoch: 12.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019737615921914472		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.019737615921914472 | validation: 0.038391905932749604]
	TIME [epoch: 12.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019076062631337477		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.019076062631337477 | validation: 0.03683127822500324]
	TIME [epoch: 12.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018446371952676593		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.018446371952676593 | validation: 0.033670259302413984]
	TIME [epoch: 12.5 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018417121495826687		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.018417121495826687 | validation: 0.03895354777809468]
	TIME [epoch: 12.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019715867839196807		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.019715867839196807 | validation: 0.04940602839801775]
	TIME [epoch: 12.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019282724884098705		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.019282724884098705 | validation: 0.0335499084989228]
	TIME [epoch: 12.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02089567174020699		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.02089567174020699 | validation: 0.04159231819932766]
	TIME [epoch: 12.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019255103306051957		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.019255103306051957 | validation: 0.02942155196789349]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1399.pth
	Model improved!!!
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020782990364681002		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.020782990364681002 | validation: 0.03699309066861844]
	TIME [epoch: 12.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018796624991985658		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.018796624991985658 | validation: 0.043247672118464954]
	TIME [epoch: 12.4 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019496406833606983		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.019496406833606983 | validation: 0.03783158746402328]
	TIME [epoch: 12.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018838692337964893		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.018838692337964893 | validation: 0.03415917749938357]
	TIME [epoch: 12.5 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023551183237271848		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.023551183237271848 | validation: 0.03933856459317542]
	TIME [epoch: 12.5 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019926480158166116		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.019926480158166116 | validation: 0.031518845365334384]
	TIME [epoch: 12.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019201150433546376		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.019201150433546376 | validation: 0.034490659902960984]
	TIME [epoch: 12.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020644057335307532		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.020644057335307532 | validation: 0.04038886165958742]
	TIME [epoch: 12.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020508792244209175		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.020508792244209175 | validation: 0.0338245655061527]
	TIME [epoch: 12.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02059224306196397		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.02059224306196397 | validation: 0.032188204346999864]
	TIME [epoch: 12.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018897688589331302		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.018897688589331302 | validation: 0.03863710974422822]
	TIME [epoch: 12.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020270762477249314		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.020270762477249314 | validation: 0.038168795027128936]
	TIME [epoch: 12.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021052204149930025		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.021052204149930025 | validation: 0.0363011285636925]
	TIME [epoch: 12.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022459278507985708		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.022459278507985708 | validation: 0.04417791129252892]
	TIME [epoch: 12.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023960688169529235		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.023960688169529235 | validation: 0.04026106377430249]
	TIME [epoch: 12.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020197803649889746		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.020197803649889746 | validation: 0.04120930386674009]
	TIME [epoch: 12.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023053125352852748		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.023053125352852748 | validation: 0.04102416463635372]
	TIME [epoch: 12.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018753247792157975		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.018753247792157975 | validation: 0.04502210390186158]
	TIME [epoch: 12.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01985615331404031		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.01985615331404031 | validation: 0.03906829375009044]
	TIME [epoch: 12.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021943170960852562		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.021943170960852562 | validation: 0.04504120507209012]
	TIME [epoch: 12.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021636863029296762		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.021636863029296762 | validation: 0.04179138178175495]
	TIME [epoch: 12.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01981121839355412		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.01981121839355412 | validation: 0.034460629283165116]
	TIME [epoch: 12.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019661192975620852		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.019661192975620852 | validation: 0.04231475170938526]
	TIME [epoch: 12.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01983768452367916		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.01983768452367916 | validation: 0.03966173267425113]
	TIME [epoch: 12.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020780128877560068		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.020780128877560068 | validation: 0.030189394295028718]
	TIME [epoch: 12.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019886491122194358		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.019886491122194358 | validation: 0.03840223318783997]
	TIME [epoch: 12.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020302159837843767		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.020302159837843767 | validation: 0.04471541524152781]
	TIME [epoch: 12.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999928556841304		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.01999928556841304 | validation: 0.03546777618940557]
	TIME [epoch: 12.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018336357231949672		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.018336357231949672 | validation: 0.04443119958983717]
	TIME [epoch: 12.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019611387970751574		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.019611387970751574 | validation: 0.0380356816706213]
	TIME [epoch: 12.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020271839328557303		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.020271839328557303 | validation: 0.038982100694798044]
	TIME [epoch: 12.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01946298336622744		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.01946298336622744 | validation: 0.040251129490427966]
	TIME [epoch: 12.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019284167333968177		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.019284167333968177 | validation: 0.03784351321822338]
	TIME [epoch: 12.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021328544238450924		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.021328544238450924 | validation: 0.03687415438077838]
	TIME [epoch: 12.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021072443527388244		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.021072443527388244 | validation: 0.04610556513588043]
	TIME [epoch: 12.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018087277631154548		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.018087277631154548 | validation: 0.03907944512179537]
	TIME [epoch: 12.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020152879261342002		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.020152879261342002 | validation: 0.03892348058915889]
	TIME [epoch: 12.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01735516644957228		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.01735516644957228 | validation: 0.028506939587008708]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1437.pth
	Model improved!!!
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020974678197230395		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.020974678197230395 | validation: 0.028968568103083847]
	TIME [epoch: 12.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018857838214704846		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.018857838214704846 | validation: 0.03274794125894507]
	TIME [epoch: 12.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019429403318070713		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.019429403318070713 | validation: 0.041799721227442445]
	TIME [epoch: 12.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020970961935959487		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.020970961935959487 | validation: 0.03387617474472269]
	TIME [epoch: 12.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020077735874041627		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.020077735874041627 | validation: 0.03590356720219482]
	TIME [epoch: 12.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018017952979890475		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.018017952979890475 | validation: 0.03865977093659978]
	TIME [epoch: 12.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020057325480008634		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.020057325480008634 | validation: 0.046348596484984676]
	TIME [epoch: 12.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01879101147283203		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.01879101147283203 | validation: 0.04403994969145369]
	TIME [epoch: 12.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01924733383721526		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.01924733383721526 | validation: 0.045074264251382806]
	TIME [epoch: 12.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01998254857182423		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.01998254857182423 | validation: 0.03408920210283458]
	TIME [epoch: 12.5 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019750746015644347		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.019750746015644347 | validation: 0.04112450390302959]
	TIME [epoch: 12.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02078618247068154		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.02078618247068154 | validation: 0.030421363886559128]
	TIME [epoch: 12.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021536566543840934		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.021536566543840934 | validation: 0.03888086026853166]
	TIME [epoch: 12.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020615441043806016		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.020615441043806016 | validation: 0.030809229329042842]
	TIME [epoch: 12.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021699764665928392		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.021699764665928392 | validation: 0.03591384028125596]
	TIME [epoch: 12.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020267282119992566		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.020267282119992566 | validation: 0.03451069894868636]
	TIME [epoch: 12.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954442467864667		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.01954442467864667 | validation: 0.03496914712449961]
	TIME [epoch: 12.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01704582973852033		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.01704582973852033 | validation: 0.03769862396183482]
	TIME [epoch: 12.4 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020528256366015133		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.020528256366015133 | validation: 0.04564686086404346]
	TIME [epoch: 12.4 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01812483419672803		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.01812483419672803 | validation: 0.0337904750108608]
	TIME [epoch: 12.4 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02202976209678096		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.02202976209678096 | validation: 0.033434919599092484]
	TIME [epoch: 12.4 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01704864872889922		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.01704864872889922 | validation: 0.028805753617146237]
	TIME [epoch: 12.4 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02152980542248182		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.02152980542248182 | validation: 0.03555882170407581]
	TIME [epoch: 12.4 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018525162299186947		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.018525162299186947 | validation: 0.03458524926184103]
	TIME [epoch: 12.4 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017478066250081387		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.017478066250081387 | validation: 0.041642730098178385]
	TIME [epoch: 12.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020553471543353994		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.020553471543353994 | validation: 0.034565743895479416]
	TIME [epoch: 12.5 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019099415392305745		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.019099415392305745 | validation: 0.03582759253092497]
	TIME [epoch: 12.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02079451821735757		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.02079451821735757 | validation: 0.04050029604975941]
	TIME [epoch: 12.5 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018060279442761017		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.018060279442761017 | validation: 0.03484819109229161]
	TIME [epoch: 12.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02045179610223673		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.02045179610223673 | validation: 0.031577447222559334]
	TIME [epoch: 12.5 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020728749093212057		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.020728749093212057 | validation: 0.029612827440224088]
	TIME [epoch: 12.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019758910407938708		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.019758910407938708 | validation: 0.04143025351767491]
	TIME [epoch: 12.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01939329788149806		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.01939329788149806 | validation: 0.031410133311390774]
	TIME [epoch: 12.5 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019791037256181233		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.019791037256181233 | validation: 0.05098806553385732]
	TIME [epoch: 12.5 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018958103830014103		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.018958103830014103 | validation: 0.04262849486870768]
	TIME [epoch: 12.4 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02057097762535734		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.02057097762535734 | validation: 0.041253936790874184]
	TIME [epoch: 12.4 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961803002365755		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.01961803002365755 | validation: 0.034807005232647674]
	TIME [epoch: 12.4 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02061202939930589		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.02061202939930589 | validation: 0.04129623796590536]
	TIME [epoch: 12.4 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018081350326568407		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.018081350326568407 | validation: 0.03649781757468249]
	TIME [epoch: 12.4 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017662996529187344		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.017662996529187344 | validation: 0.03990182431845804]
	TIME [epoch: 12.5 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020000277498615385		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.020000277498615385 | validation: 0.03130117571456783]
	TIME [epoch: 12.5 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020307075414715447		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.020307075414715447 | validation: 0.03447991649808663]
	TIME [epoch: 12.5 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019629670478564436		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.019629670478564436 | validation: 0.034057830494326216]
	TIME [epoch: 12.5 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020146678290983572		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.020146678290983572 | validation: 0.03476373491454239]
	TIME [epoch: 12.4 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02076177274542399		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.02076177274542399 | validation: 0.03986261077093051]
	TIME [epoch: 12.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02078770032602615		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.02078770032602615 | validation: 0.04160249426068289]
	TIME [epoch: 12.4 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019287964562827366		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.019287964562827366 | validation: 0.04213840148703584]
	TIME [epoch: 12.4 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021264938628483904		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.021264938628483904 | validation: 0.03770520404917164]
	TIME [epoch: 12.4 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020821224505458806		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.020821224505458806 | validation: 0.04263000586285947]
	TIME [epoch: 12.4 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0187428075615849		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.0187428075615849 | validation: 0.030403669727264285]
	TIME [epoch: 12.4 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018491252659059005		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.018491252659059005 | validation: 0.03688123717058274]
	TIME [epoch: 12.4 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016995669898297698		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.016995669898297698 | validation: 0.03370821731074158]
	TIME [epoch: 12.4 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021398517450511773		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.021398517450511773 | validation: 0.029536650288791155]
	TIME [epoch: 12.4 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019738181545223495		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.019738181545223495 | validation: 0.03429858435341322]
	TIME [epoch: 12.4 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02190126014085731		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.02190126014085731 | validation: 0.04270888227517623]
	TIME [epoch: 12.4 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018540536775474143		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.018540536775474143 | validation: 0.036568670916328795]
	TIME [epoch: 12.4 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019303413162328684		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.019303413162328684 | validation: 0.03503898697702108]
	TIME [epoch: 12.4 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018203240541161816		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.018203240541161816 | validation: 0.03931543725308541]
	TIME [epoch: 12.4 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019091129327836485		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.019091129327836485 | validation: 0.03237698853656739]
	TIME [epoch: 12.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020294702144136405		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.020294702144136405 | validation: 0.03209709724290214]
	TIME [epoch: 12.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020444313028328682		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.020444313028328682 | validation: 0.038321966671455265]
	TIME [epoch: 12.4 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020226723735143635		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.020226723735143635 | validation: 0.04050199905804777]
	TIME [epoch: 12.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020165453007589616		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.020165453007589616 | validation: 0.04332860677002072]
	TIME [epoch: 12.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019558068322853405		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.019558068322853405 | validation: 0.040243386399172534]
	TIME [epoch: 12.5 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01996924209889289		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.01996924209889289 | validation: 0.04388457197417104]
	TIME [epoch: 12.5 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01979250159484031		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.01979250159484031 | validation: 0.034480675563365405]
	TIME [epoch: 12.5 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021627059263044902		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.021627059263044902 | validation: 0.03562965052342492]
	TIME [epoch: 12.5 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0205637323269269		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.0205637323269269 | validation: 0.0348521175561662]
	TIME [epoch: 12.5 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019142115763439806		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.019142115763439806 | validation: 0.0512604492667628]
	TIME [epoch: 12.5 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01781244883372416		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.01781244883372416 | validation: 0.04281371954070519]
	TIME [epoch: 12.5 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018361805866274197		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.018361805866274197 | validation: 0.038318179198696595]
	TIME [epoch: 12.5 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017091266821264762		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.017091266821264762 | validation: 0.04056652942704826]
	TIME [epoch: 12.5 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017361209193347445		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.017361209193347445 | validation: 0.04240008720348071]
	TIME [epoch: 12.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019603255528234213		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.019603255528234213 | validation: 0.03619260745482188]
	TIME [epoch: 12.5 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019305916353221046		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.019305916353221046 | validation: 0.034218866927495206]
	TIME [epoch: 12.5 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01983499425594921		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.01983499425594921 | validation: 0.03645931623933705]
	TIME [epoch: 12.5 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019019669878904213		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.019019669878904213 | validation: 0.0345564392792485]
	TIME [epoch: 12.5 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020047220608159458		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.020047220608159458 | validation: 0.03358267857389763]
	TIME [epoch: 12.5 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019357015315824157		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.019357015315824157 | validation: 0.04416821304390947]
	TIME [epoch: 12.5 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017160940244320994		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.017160940244320994 | validation: 0.03416204421138661]
	TIME [epoch: 12.5 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01788304259810921		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.01788304259810921 | validation: 0.042734123429686605]
	TIME [epoch: 12.5 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020207933834716273		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.020207933834716273 | validation: 0.046994876466311876]
	TIME [epoch: 12.5 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019099673256725104		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.019099673256725104 | validation: 0.03932962969107006]
	TIME [epoch: 12.5 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01824120440258241		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.01824120440258241 | validation: 0.03636677220615841]
	TIME [epoch: 12.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019620272459561595		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.019620272459561595 | validation: 0.04140608523454803]
	TIME [epoch: 12.5 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02059369539413253		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.02059369539413253 | validation: 0.03919949668483185]
	TIME [epoch: 12.5 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02023329370451714		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.02023329370451714 | validation: 0.03414159336623557]
	TIME [epoch: 12.5 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019739997339863306		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.019739997339863306 | validation: 0.028096776506414657]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1525.pth
	Model improved!!!
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02111080396459682		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.02111080396459682 | validation: 0.03530867367377421]
	TIME [epoch: 12.4 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018455643585182625		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.018455643585182625 | validation: 0.03698030696492068]
	TIME [epoch: 12.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019900931933352926		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.019900931933352926 | validation: 0.03626409238796476]
	TIME [epoch: 12.4 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02147905576258707		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.02147905576258707 | validation: 0.03967436478065672]
	TIME [epoch: 12.4 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020191663750965115		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.020191663750965115 | validation: 0.0439915263107091]
	TIME [epoch: 12.4 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01982212478759602		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.01982212478759602 | validation: 0.04337907470944701]
	TIME [epoch: 12.4 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01972836949467819		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.01972836949467819 | validation: 0.04098597405129084]
	TIME [epoch: 12.4 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897819560693821		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.01897819560693821 | validation: 0.03705603320291241]
	TIME [epoch: 12.4 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018346316073611896		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.018346316073611896 | validation: 0.03826398869608836]
	TIME [epoch: 12.4 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017169680842485355		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.017169680842485355 | validation: 0.026453496224178488]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1535.pth
	Model improved!!!
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01878704989909247		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.01878704989909247 | validation: 0.03261267829795154]
	TIME [epoch: 12.5 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017529412111282793		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.017529412111282793 | validation: 0.033235943183554384]
	TIME [epoch: 12.5 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017554903710905453		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.017554903710905453 | validation: 0.040478298753249854]
	TIME [epoch: 12.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01874046885982021		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.01874046885982021 | validation: 0.03974964240911319]
	TIME [epoch: 12.5 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020305116364017453		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.020305116364017453 | validation: 0.030628314154471672]
	TIME [epoch: 12.5 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017088016302658153		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.017088016302658153 | validation: 0.03882092840021137]
	TIME [epoch: 12.5 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020139543746451584		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.020139543746451584 | validation: 0.03636934917229145]
	TIME [epoch: 12.5 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01801928311933146		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.01801928311933146 | validation: 0.03934705029474438]
	TIME [epoch: 12.5 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01647168954437184		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.01647168954437184 | validation: 0.036143279472026855]
	TIME [epoch: 12.5 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017732167551706302		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.017732167551706302 | validation: 0.03186887888455425]
	TIME [epoch: 12.5 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02088322290851349		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.02088322290851349 | validation: 0.031632897396127894]
	TIME [epoch: 12.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020037706678507527		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.020037706678507527 | validation: 0.03841667818319416]
	TIME [epoch: 12.5 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017476957939719803		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.017476957939719803 | validation: 0.03576268147081807]
	TIME [epoch: 12.5 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019105318875627452		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.019105318875627452 | validation: 0.03963740811343916]
	TIME [epoch: 12.5 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020016129110433708		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.020016129110433708 | validation: 0.03258697530291389]
	TIME [epoch: 12.5 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018445436155711214		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.018445436155711214 | validation: 0.03537658910285307]
	TIME [epoch: 12.5 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01745272338569099		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.01745272338569099 | validation: 0.03317529881231245]
	TIME [epoch: 12.5 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017168257711015104		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.017168257711015104 | validation: 0.03991644711668877]
	TIME [epoch: 12.5 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019279131136490362		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.019279131136490362 | validation: 0.040377490039432064]
	TIME [epoch: 12.5 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017181763656000843		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.017181763656000843 | validation: 0.04037709019865942]
	TIME [epoch: 12.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018742963854789966		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.018742963854789966 | validation: 0.040606637613739685]
	TIME [epoch: 12.5 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019372059593336183		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.019372059593336183 | validation: 0.03355892110310108]
	TIME [epoch: 12.5 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01802781900220722		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.01802781900220722 | validation: 0.030692525750544555]
	TIME [epoch: 12.5 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876749646209873		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.01876749646209873 | validation: 0.038776207665438794]
	TIME [epoch: 12.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019219825459273		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.019219825459273 | validation: 0.03350597384597969]
	TIME [epoch: 12.5 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019293721569486492		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.019293721569486492 | validation: 0.038850862756649784]
	TIME [epoch: 12.5 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01909249156965367		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.01909249156965367 | validation: 0.04256531629643012]
	TIME [epoch: 12.5 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841250206805461		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.01841250206805461 | validation: 0.038840158390961346]
	TIME [epoch: 12.5 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019874762059139472		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.019874762059139472 | validation: 0.037926891526960715]
	TIME [epoch: 12.5 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019318577317088883		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.019318577317088883 | validation: 0.039907856843409285]
	TIME [epoch: 12.5 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018535137206536483		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.018535137206536483 | validation: 0.030372742191352733]
	TIME [epoch: 12.5 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01740889101065638		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.01740889101065638 | validation: 0.042118962141645094]
	TIME [epoch: 12.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01839932882899287		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.01839932882899287 | validation: 0.038729105565876935]
	TIME [epoch: 12.5 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016608057647725487		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.016608057647725487 | validation: 0.026861144972276763]
	TIME [epoch: 12.5 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016709694376456443		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.016709694376456443 | validation: 0.03011439815130819]
	TIME [epoch: 12.5 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017790724776411266		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.017790724776411266 | validation: 0.041354014713489234]
	TIME [epoch: 12.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01712919518246219		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.01712919518246219 | validation: 0.03166260195122169]
	TIME [epoch: 12.5 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017119250131270664		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.017119250131270664 | validation: 0.031961281913914445]
	TIME [epoch: 12.5 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018604071009910694		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.018604071009910694 | validation: 0.029919186891727557]
	TIME [epoch: 12.5 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019591276170868307		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.019591276170868307 | validation: 0.03240772648645237]
	TIME [epoch: 12.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019313964460255018		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.019313964460255018 | validation: 0.04116412470013151]
	TIME [epoch: 12.5 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016670267700833828		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.016670267700833828 | validation: 0.02808353075076583]
	TIME [epoch: 12.5 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017711760776152944		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.017711760776152944 | validation: 0.03221288744219506]
	TIME [epoch: 12.5 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01905403748244921		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.01905403748244921 | validation: 0.04323401742491262]
	TIME [epoch: 12.5 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018123248432264264		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.018123248432264264 | validation: 0.03859321682581948]
	TIME [epoch: 12.5 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020496211805614586		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.020496211805614586 | validation: 0.02476533729656845]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1581.pth
	Model improved!!!
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01682930753565761		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.01682930753565761 | validation: 0.028602655758892582]
	TIME [epoch: 12.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019176768403202753		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.019176768403202753 | validation: 0.03627490955979597]
	TIME [epoch: 12.4 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897152283271247		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.01897152283271247 | validation: 0.0374171259336187]
	TIME [epoch: 12.4 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018791989161086448		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.018791989161086448 | validation: 0.04016163879988088]
	TIME [epoch: 12.4 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01800588583360969		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.01800588583360969 | validation: 0.04484369372153904]
	TIME [epoch: 12.4 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018462093062869292		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.018462093062869292 | validation: 0.04052373308361436]
	TIME [epoch: 12.4 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01769360793809613		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.01769360793809613 | validation: 0.043087983792192024]
	TIME [epoch: 12.4 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021838067954634655		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.021838067954634655 | validation: 0.030290856864580853]
	TIME [epoch: 12.4 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018725987045713516		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.018725987045713516 | validation: 0.04178386458864001]
	TIME [epoch: 12.4 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01931175503763512		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.01931175503763512 | validation: 0.03922201575753468]
	TIME [epoch: 12.4 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017725608592987933		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.017725608592987933 | validation: 0.03325093638303596]
	TIME [epoch: 12.4 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01941247851495576		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.01941247851495576 | validation: 0.02741256097675836]
	TIME [epoch: 12.4 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017784815057940453		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.017784815057940453 | validation: 0.03692932884935093]
	TIME [epoch: 12.4 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01814067148605959		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.01814067148605959 | validation: 0.03074448942869038]
	TIME [epoch: 12.4 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020026280954060276		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.020026280954060276 | validation: 0.030727825375918772]
	TIME [epoch: 12.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017673179232463067		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.017673179232463067 | validation: 0.04231114328801597]
	TIME [epoch: 12.4 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019638583688606107		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.019638583688606107 | validation: 0.040510266175100486]
	TIME [epoch: 12.4 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018534728792516792		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.018534728792516792 | validation: 0.04328655208675262]
	TIME [epoch: 12.4 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02033998559519374		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.02033998559519374 | validation: 0.03075520492945788]
	TIME [epoch: 12.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01868713409665807		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.01868713409665807 | validation: 0.0284302722744336]
	TIME [epoch: 12.5 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01883997018163352		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.01883997018163352 | validation: 0.04151238985620598]
	TIME [epoch: 12.5 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019964793790475843		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.019964793790475843 | validation: 0.04343010463270467]
	TIME [epoch: 12.5 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016211393817509183		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.016211393817509183 | validation: 0.02935909443146305]
	TIME [epoch: 12.5 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018905880336649278		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.018905880336649278 | validation: 0.03669151924006967]
	TIME [epoch: 12.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01730819099219768		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.01730819099219768 | validation: 0.03502564856496345]
	TIME [epoch: 12.5 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019478072295626547		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.019478072295626547 | validation: 0.036981198282628545]
	TIME [epoch: 12.5 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017683646734855698		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.017683646734855698 | validation: 0.0348347873884872]
	TIME [epoch: 12.5 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132009082332831		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.02132009082332831 | validation: 0.03440507064212713]
	TIME [epoch: 12.5 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01968534343157464		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.01968534343157464 | validation: 0.03207316552559256]
	TIME [epoch: 12.5 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019217505346076327		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.019217505346076327 | validation: 0.0394081597870482]
	TIME [epoch: 12.5 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02082168224184036		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.02082168224184036 | validation: 0.030953759530856842]
	TIME [epoch: 12.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017268534328621677		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.017268534328621677 | validation: 0.03493829169078271]
	TIME [epoch: 12.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019925954518919244		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.019925954518919244 | validation: 0.03390301309181681]
	TIME [epoch: 12.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019853497519138084		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.019853497519138084 | validation: 0.029792224214138907]
	TIME [epoch: 12.5 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01771512578731618		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.01771512578731618 | validation: 0.035289474505809236]
	TIME [epoch: 12.5 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018869362569092198		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.018869362569092198 | validation: 0.03690200553622054]
	TIME [epoch: 12.5 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018008720472478347		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.018008720472478347 | validation: 0.03114689297276149]
	TIME [epoch: 12.5 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0187915677423363		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.0187915677423363 | validation: 0.034162269979779745]
	TIME [epoch: 12.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017859872759938746		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.017859872759938746 | validation: 0.04162492787412421]
	TIME [epoch: 12.5 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018470400520972145		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.018470400520972145 | validation: 0.0374476883268249]
	TIME [epoch: 12.5 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017612935216802513		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.017612935216802513 | validation: 0.028648782636290726]
	TIME [epoch: 12.5 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017542179621639696		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.017542179621639696 | validation: 0.032293540016034954]
	TIME [epoch: 12.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020583942812106154		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.020583942812106154 | validation: 0.04792439315443928]
	TIME [epoch: 12.5 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019723726147292266		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.019723726147292266 | validation: 0.03257008713943826]
	TIME [epoch: 12.5 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016887928567993215		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.016887928567993215 | validation: 0.03642095089566202]
	TIME [epoch: 12.5 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018813100095402532		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.018813100095402532 | validation: 0.03406331100894818]
	TIME [epoch: 12.5 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01691745806154074		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.01691745806154074 | validation: 0.03468083243947142]
	TIME [epoch: 12.5 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01641560863223021		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.01641560863223021 | validation: 0.04008572507238836]
	TIME [epoch: 12.5 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020316890789979624		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.020316890789979624 | validation: 0.031477763780471825]
	TIME [epoch: 12.5 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017154665830266126		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.017154665830266126 | validation: 0.03369032783008145]
	TIME [epoch: 12.5 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018439845016718123		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.018439845016718123 | validation: 0.03664386084385956]
	TIME [epoch: 12.5 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020867205888209464		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.020867205888209464 | validation: 0.04373500479787218]
	TIME [epoch: 12.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01956076116242203		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.01956076116242203 | validation: 0.034140915617367096]
	TIME [epoch: 12.5 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018590944614083428		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.018590944614083428 | validation: 0.035328142774514106]
	TIME [epoch: 12.5 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017628679147407206		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.017628679147407206 | validation: 0.039103757379625695]
	TIME [epoch: 12.5 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019217429616470723		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.019217429616470723 | validation: 0.030273977118088824]
	TIME [epoch: 12.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017790152992641077		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.017790152992641077 | validation: 0.03105380492442781]
	TIME [epoch: 12.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017687690376452642		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.017687690376452642 | validation: 0.04123406852096482]
	TIME [epoch: 12.5 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017438006941003958		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.017438006941003958 | validation: 0.028336040984266342]
	TIME [epoch: 12.5 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01846446666359446		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.01846446666359446 | validation: 0.03416095663687261]
	TIME [epoch: 12.5 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020035132021195223		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.020035132021195223 | validation: 0.0357096805006396]
	TIME [epoch: 12.5 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01837442194714537		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.01837442194714537 | validation: 0.0404359929020173]
	TIME [epoch: 12.5 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017772136297100578		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.017772136297100578 | validation: 0.03861905777695841]
	TIME [epoch: 12.5 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020011351023126206		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.020011351023126206 | validation: 0.03256938382625548]
	TIME [epoch: 12.5 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016573423539086165		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.016573423539086165 | validation: 0.04175387402206785]
	TIME [epoch: 12.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01792155242399482		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.01792155242399482 | validation: 0.040247532808641656]
	TIME [epoch: 12.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01894187662703393		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.01894187662703393 | validation: 0.028901935408070536]
	TIME [epoch: 12.5 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019617550622503036		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.019617550622503036 | validation: 0.029089095917479102]
	TIME [epoch: 12.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01811582704296215		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.01811582704296215 | validation: 0.028707818549997058]
	TIME [epoch: 12.5 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0197119995761575		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.0197119995761575 | validation: 0.03213055854082352]
	TIME [epoch: 12.5 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01701271495770496		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.01701271495770496 | validation: 0.030838690097044874]
	TIME [epoch: 12.5 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016112875373476276		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.016112875373476276 | validation: 0.029406559400417545]
	TIME [epoch: 12.5 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01702697090008839		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.01702697090008839 | validation: 0.03995429472637427]
	TIME [epoch: 12.5 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016416301381420476		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.016416301381420476 | validation: 0.037756054847741416]
	TIME [epoch: 12.5 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01622496627527403		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.01622496627527403 | validation: 0.03321854465959032]
	TIME [epoch: 12.5 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016851524545611		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.016851524545611 | validation: 0.03568303738476175]
	TIME [epoch: 12.5 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01647502479748888		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.01647502479748888 | validation: 0.025293504299195226]
	TIME [epoch: 12.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01689262325072824		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.01689262325072824 | validation: 0.03131737918033824]
	TIME [epoch: 12.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015949960575018202		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.015949960575018202 | validation: 0.02998082398592088]
	TIME [epoch: 12.5 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018589688590098446		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.018589688590098446 | validation: 0.03753952039402493]
	TIME [epoch: 12.5 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019473014040813617		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.019473014040813617 | validation: 0.035495640390032925]
	TIME [epoch: 12.5 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017857912162890552		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.017857912162890552 | validation: 0.03621350477459965]
	TIME [epoch: 12.4 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01697499773749749		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.01697499773749749 | validation: 0.037196315741174546]
	TIME [epoch: 12.5 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019667291183777184		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.019667291183777184 | validation: 0.0321251857934175]
	TIME [epoch: 12.5 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01873037135306693		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.01873037135306693 | validation: 0.035274391222965155]
	TIME [epoch: 12.5 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018681671052837946		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.018681671052837946 | validation: 0.036717817629516494]
	TIME [epoch: 12.5 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0203877520682279		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.0203877520682279 | validation: 0.03787878175274954]
	TIME [epoch: 12.5 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019485822040958944		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.019485822040958944 | validation: 0.029637065415603903]
	TIME [epoch: 12.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01935356240585866		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.01935356240585866 | validation: 0.0381422306744021]
	TIME [epoch: 12.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018440389745749717		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.018440389745749717 | validation: 0.03137182752890618]
	TIME [epoch: 12.5 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017456552421610438		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.017456552421610438 | validation: 0.038711601925281516]
	TIME [epoch: 12.5 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015318637189610134		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.015318637189610134 | validation: 0.03222780320281262]
	TIME [epoch: 12.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018618198962623414		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.018618198962623414 | validation: 0.034977994357760334]
	TIME [epoch: 12.5 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017556007415457344		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.017556007415457344 | validation: 0.034086504887255974]
	TIME [epoch: 12.5 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018713052375508323		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.018713052375508323 | validation: 0.031023752452800948]
	TIME [epoch: 12.4 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016274223848229342		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.016274223848229342 | validation: 0.03442293158900222]
	TIME [epoch: 12.4 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016125632614663418		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.016125632614663418 | validation: 0.04070935000703113]
	TIME [epoch: 12.4 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019107549503805325		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.019107549503805325 | validation: 0.04740240191295676]
	TIME [epoch: 12.4 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019353429440342473		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.019353429440342473 | validation: 0.04305065028571295]
	TIME [epoch: 12.4 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017738886171016918		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.017738886171016918 | validation: 0.04050566445099198]
	TIME [epoch: 12.4 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017766577250027712		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.017766577250027712 | validation: 0.03554254039236713]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20241105_150340/states/model_phi1_3b_v_mmd1_1682.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 13723.197 seconds.
