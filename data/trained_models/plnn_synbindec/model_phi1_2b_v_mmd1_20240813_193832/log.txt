Args:
Namespace(name='model_phi1_2b_v_mmd1', outdir='out/model_training/model_phi1_2b_v_mmd1', training_data='data/training_data/basic/data_phi1_2b/training', validation_data='data/training_data/basic/data_phi1_2b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2555266984

Training model...

Saving initial model state to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 6.033871207509641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.033871207509641 | validation: 5.191100347697335]
	TIME [epoch: 105 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.893570105387479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.893570105387479 | validation: 5.2923871781129295]
	TIME [epoch: 3.35 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.6591141558754305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6591141558754305 | validation: 4.907497310033318]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.316953029030814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.316953029030814 | validation: 4.75959249884255]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.1752939054308715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1752939054308715 | validation: 4.55345392070558]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.055226579378482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.055226579378482 | validation: 4.4484394594856145]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.927108879472637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.927108879472637 | validation: 4.363576696731545]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.776913492991137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.776913492991137 | validation: 4.2701796416008495]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.757195914710307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.757195914710307 | validation: 4.516658653111328]
	TIME [epoch: 3.25 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.964353474767886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.964353474767886 | validation: 4.237216424213151]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.739568224901166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.739568224901166 | validation: 4.067327028668157]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.558929946292597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.558929946292597 | validation: 4.069565111905456]
	TIME [epoch: 3.27 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.630837280624128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.630837280624128 | validation: 3.9354150824364753]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.375742995381273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.375742995381273 | validation: 4.090949493564006]
	TIME [epoch: 3.26 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.439800556955127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.439800556955127 | validation: 3.8681325479648283]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.327234921893808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.327234921893808 | validation: 3.7617149382509227]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.311577378236381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.311577378236381 | validation: 3.6326910529892134]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.982030875719674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.982030875719674 | validation: 4.892654701880606]
	TIME [epoch: 3.26 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.435780582012108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435780582012108 | validation: 4.126927426334112]
	TIME [epoch: 3.26 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.276573781463906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.276573781463906 | validation: 3.451603375806389]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.920377939556796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.920377939556796 | validation: 3.3326049761900087]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7929290612643163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7929290612643163 | validation: 3.2745480122555097]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5064928864645246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5064928864645246 | validation: 2.836056647708268]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.425550050620882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.425550050620882 | validation: 2.6300819978253935]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.1306933067964327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1306933067964327 | validation: 2.8445835331040867]
	TIME [epoch: 3.26 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.2413910006228215		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.2413910006228215 | validation: 2.551664106884007]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.976152682865294		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.976152682865294 | validation: 2.6114210894402166]
	TIME [epoch: 3.26 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.09655999258427		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.09655999258427 | validation: 2.455414942386577]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8695131713816187		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.8695131713816187 | validation: 2.365760099583106]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9256207666313045		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.9256207666313045 | validation: 2.683967833380792]
	TIME [epoch: 3.25 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.078252174273824		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.078252174273824 | validation: 2.595525404940164]
	TIME [epoch: 3.25 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8273135432980263		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8273135432980263 | validation: 2.2612451175974115]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.718473572624143		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.718473572624143 | validation: 2.2353360734961076]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6144760090355406		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.6144760090355406 | validation: 2.2460284756273823]
	TIME [epoch: 3.26 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.656162062966719		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.656162062966719 | validation: 2.1472901625818657]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5651020950296557		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.5651020950296557 | validation: 2.1976435063779]
	TIME [epoch: 3.26 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6126508905439554		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.6126508905439554 | validation: 2.213435103413605]
	TIME [epoch: 3.25 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0599765167769384		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.0599765167769384 | validation: 3.3166453106535303]
	TIME [epoch: 3.26 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.440858943564021		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.440858943564021 | validation: 1.9808223083695489]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.233489115887276		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.233489115887276 | validation: 1.7858247925019335]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7275837276337704		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7275837276337704 | validation: 1.7623761191724903]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.925328105833513		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.925328105833513 | validation: 1.6324287278372296]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7146336145310497		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.7146336145310497 | validation: 1.5056323928459268]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4937703087776637		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.4937703087776637 | validation: 1.8305089391853528]
	TIME [epoch: 3.26 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.864092527441385		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.864092527441385 | validation: 1.5810147692569465]
	TIME [epoch: 3.26 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5889632767354418		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.5889632767354418 | validation: 1.561585611818914]
	TIME [epoch: 3.26 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4459441285109134		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4459441285109134 | validation: 1.5283854145933227]
	TIME [epoch: 3.27 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.454066689056271		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.454066689056271 | validation: 1.70409756146829]
	TIME [epoch: 3.26 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7049807186873918		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.7049807186873918 | validation: 1.8971265962647212]
	TIME [epoch: 3.26 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6192027252432557		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6192027252432557 | validation: 1.5818258523857762]
	TIME [epoch: 3.25 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.533020686812459		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.533020686812459 | validation: 1.5859370696585444]
	TIME [epoch: 3.26 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.528043558275662		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.528043558275662 | validation: 1.4633585036867736]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3954215424364136		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.3954215424364136 | validation: 1.6279209332942857]
	TIME [epoch: 3.29 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5029696827060302		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5029696827060302 | validation: 1.6685631669559307]
	TIME [epoch: 3.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5175236372622707		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.5175236372622707 | validation: 1.6562192644467666]
	TIME [epoch: 3.27 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.465343995069916		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.465343995069916 | validation: 1.628020802717581]
	TIME [epoch: 3.27 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4625330915654313		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.4625330915654313 | validation: 1.6686297138706678]
	TIME [epoch: 3.27 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4803225062320273		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4803225062320273 | validation: 1.485447332938346]
	TIME [epoch: 3.26 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3435186285808067		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3435186285808067 | validation: 1.471890415136622]
	TIME [epoch: 3.25 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4472460468343624		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.4472460468343624 | validation: 2.4711442532584034]
	TIME [epoch: 3.26 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7968742991697757		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.7968742991697757 | validation: 1.6146428149224783]
	TIME [epoch: 3.26 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5948440362832623		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.5948440362832623 | validation: 1.444202684416216]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3647213622192607		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3647213622192607 | validation: 1.523686766835095]
	TIME [epoch: 3.24 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3412467881820407		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.3412467881820407 | validation: 1.474760414855809]
	TIME [epoch: 3.27 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3135657993365542		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.3135657993365542 | validation: 1.4442238829700305]
	TIME [epoch: 3.28 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2956719781444908		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.2956719781444908 | validation: 1.4657486032413762]
	TIME [epoch: 3.25 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3187336836846044		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.3187336836846044 | validation: 1.5231511268702431]
	TIME [epoch: 3.25 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6049937674767105		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.6049937674767105 | validation: 1.369944291001678]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.572782110483459		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.572782110483459 | validation: 2.4587344238762894]
	TIME [epoch: 3.26 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7397523284007321		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.7397523284007321 | validation: 1.4709893746712865]
	TIME [epoch: 3.25 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3954085498015882		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.3954085498015882 | validation: 1.3921001389308132]
	TIME [epoch: 3.26 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2719688855659588		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.2719688855659588 | validation: 1.5114996336268887]
	TIME [epoch: 3.25 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3029300623432678		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3029300623432678 | validation: 1.4232256868598838]
	TIME [epoch: 3.24 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.228229948915165		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.228229948915165 | validation: 1.3558736448180753]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2586084976147007		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.2586084976147007 | validation: 1.4615555533973383]
	TIME [epoch: 3.28 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6442192951674013		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.6442192951674013 | validation: 1.5023314641311836]
	TIME [epoch: 3.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4149002131673463		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.4149002131673463 | validation: 1.745030471971333]
	TIME [epoch: 3.26 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.307314925256732		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.307314925256732 | validation: 1.3040927642908593]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2342817499854486		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2342817499854486 | validation: 1.3253811696171023]
	TIME [epoch: 3.25 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1575545352348737		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1575545352348737 | validation: 1.3756400287013357]
	TIME [epoch: 3.25 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.221727221967008		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.221727221967008 | validation: 1.4156587807369931]
	TIME [epoch: 3.25 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4081447428187936		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.4081447428187936 | validation: 1.2270386531920074]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3066706855241743		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.3066706855241743 | validation: 1.9617823132625138]
	TIME [epoch: 3.26 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4162438094367218		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.4162438094367218 | validation: 1.40427698742479]
	TIME [epoch: 3.26 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2446110441480425		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2446110441480425 | validation: 1.2795378128698127]
	TIME [epoch: 3.26 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.118614147578948		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.118614147578948 | validation: 1.3876353479517145]
	TIME [epoch: 3.28 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1025076576184554		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1025076576184554 | validation: 1.2513701036911091]
	TIME [epoch: 3.29 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0767642701928848		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0767642701928848 | validation: 1.2094385649781445]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0620877817251007		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0620877817251007 | validation: 1.304644083499319]
	TIME [epoch: 3.27 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2054578207576043		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.2054578207576043 | validation: 1.9121830970987521]
	TIME [epoch: 3.27 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3951735648150931		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3951735648150931 | validation: 1.4312484493859294]
	TIME [epoch: 3.27 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2209753965156285		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2209753965156285 | validation: 1.130767668122023]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0467872998883847		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0467872998883847 | validation: 1.436525087850929]
	TIME [epoch: 3.27 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0850893375138302		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0850893375138302 | validation: 1.217198550956672]
	TIME [epoch: 3.27 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9865907110118306		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9865907110118306 | validation: 1.2583207754769015]
	TIME [epoch: 3.27 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9805531097766502		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9805531097766502 | validation: 1.120446730093171]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9522865246083807		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9522865246083807 | validation: 1.419769603302948]
	TIME [epoch: 3.28 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2227306890496306		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2227306890496306 | validation: 1.1265036709817273]
	TIME [epoch: 3.29 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1663264326251537		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.1663264326251537 | validation: 1.5088003086295056]
	TIME [epoch: 3.26 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1008193254931748		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1008193254931748 | validation: 1.2235316740249027]
	TIME [epoch: 3.26 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8823750947697844		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.8823750947697844 | validation: 1.0833309170516443]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8589903841438706		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8589903841438706 | validation: 1.2702316099876072]
	TIME [epoch: 3.25 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9900972607171578		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9900972607171578 | validation: 1.6671328128707472]
	TIME [epoch: 3.25 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1003598440539495		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1003598440539495 | validation: 1.0390547717472043]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8787842578304177		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8787842578304177 | validation: 0.9819242138370655]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8217403943321152		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.8217403943321152 | validation: 1.1261239472630649]
	TIME [epoch: 3.26 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9800675248900756		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9800675248900756 | validation: 0.9848700778840099]
	TIME [epoch: 3.26 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9192289694298155		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9192289694298155 | validation: 1.496749132917884]
	TIME [epoch: 3.29 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0010754640355821		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0010754640355821 | validation: 1.042448493175933]
	TIME [epoch: 3.28 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8485965177720974		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8485965177720974 | validation: 0.941824314580118]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8318933860564282		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.8318933860564282 | validation: 1.1371655427494556]
	TIME [epoch: 3.26 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7605523522935274		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7605523522935274 | validation: 0.865012480358711]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7079351847674429		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7079351847674429 | validation: 0.8403454238530387]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7595562766225168		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7595562766225168 | validation: 1.8194528925941733]
	TIME [epoch: 3.26 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1485307563809983		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1485307563809983 | validation: 1.458329439549045]
	TIME [epoch: 3.26 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2141048211805603		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.2141048211805603 | validation: 1.002394169553171]
	TIME [epoch: 3.26 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.93670692634951		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.93670692634951 | validation: 0.8815462463213255]
	TIME [epoch: 3.26 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7167566979225342		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7167566979225342 | validation: 0.9387277798256206]
	TIME [epoch: 3.27 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7965495448571149		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7965495448571149 | validation: 0.8356338557403862]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6477658658146777		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6477658658146777 | validation: 0.8328848255451832]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7283176356201502		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.7283176356201502 | validation: 0.7643329410668412]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5987422413091174		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5987422413091174 | validation: 0.79571675063275]
	TIME [epoch: 3.25 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6734206175811652		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6734206175811652 | validation: 1.3492464859889355]
	TIME [epoch: 3.25 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8993883203266473		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8993883203266473 | validation: 1.2986007110337743]
	TIME [epoch: 3.25 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9383510814017799		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9383510814017799 | validation: 0.7152595019250407]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6578940049629162		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6578940049629162 | validation: 0.7005978171279055]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5789978672937128		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5789978672937128 | validation: 1.0496942820698163]
	TIME [epoch: 3.26 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9480525780202588		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9480525780202588 | validation: 0.9999242194147773]
	TIME [epoch: 3.26 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8301492942810106		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.8301492942810106 | validation: 0.8955807916783011]
	TIME [epoch: 3.29 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5529863716422283		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5529863716422283 | validation: 0.6865980752929484]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5475866154667617		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5475866154667617 | validation: 0.6209798614922001]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5142191676345933		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5142191676345933 | validation: 0.6386217169672439]
	TIME [epoch: 3.26 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4835204909163423		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4835204909163423 | validation: 0.6833126476094498]
	TIME [epoch: 3.26 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48216134155315293		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.48216134155315293 | validation: 0.8219308726671054]
	TIME [epoch: 3.26 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7516854858280013		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7516854858280013 | validation: 0.5805822451515467]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4495697116146682		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.4495697116146682 | validation: 0.6285363952374183]
	TIME [epoch: 3.26 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5605868771799484		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5605868771799484 | validation: 0.9346395431372003]
	TIME [epoch: 3.26 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5823169924713347		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5823169924713347 | validation: 0.7084339387234819]
	TIME [epoch: 3.26 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5415916686927942		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5415916686927942 | validation: 0.6354778951125586]
	TIME [epoch: 3.27 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5695642288594804		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5695642288594804 | validation: 0.5346577050207912]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7536659177472618		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7536659177472618 | validation: 1.2091297090766642]
	TIME [epoch: 3.28 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8777719576938876		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8777719576938876 | validation: 1.1957238136159647]
	TIME [epoch: 3.27 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.659004924960801		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.659004924960801 | validation: 0.5730525654525976]
	TIME [epoch: 3.26 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5922669068518436		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5922669068518436 | validation: 0.903665508246786]
	TIME [epoch: 3.26 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5044837337475253		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5044837337475253 | validation: 0.4569025705596557]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39582055281595696		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.39582055281595696 | validation: 0.6136865743037015]
	TIME [epoch: 3.26 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4474044861331238		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.4474044861331238 | validation: 0.45982439638463624]
	TIME [epoch: 3.27 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38280964264570005		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.38280964264570005 | validation: 0.6267608540534223]
	TIME [epoch: 3.26 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4665992820433244		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.4665992820433244 | validation: 0.5639649201787146]
	TIME [epoch: 3.26 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36549451018272117		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.36549451018272117 | validation: 1.0100996132395614]
	TIME [epoch: 3.27 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7184029686464404		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7184029686464404 | validation: 0.47425629690982407]
	TIME [epoch: 3.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5765484865665222		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5765484865665222 | validation: 0.7757768430221512]
	TIME [epoch: 3.28 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5198111549336621		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5198111549336621 | validation: 0.6202563376252823]
	TIME [epoch: 3.27 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40462275183072116		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.40462275183072116 | validation: 0.39000628455624864]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3225634853892232		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3225634853892232 | validation: 0.6394963430505923]
	TIME [epoch: 3.27 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35833742669172064		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.35833742669172064 | validation: 0.5213298155160185]
	TIME [epoch: 3.26 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5460840442570714		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5460840442570714 | validation: 0.41530655716582426]
	TIME [epoch: 3.26 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3966316947962		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.3966316947962 | validation: 0.4354227876265888]
	TIME [epoch: 3.26 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3014987866794715		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3014987866794715 | validation: 0.5511108882511356]
	TIME [epoch: 3.26 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3741540359436136		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.3741540359436136 | validation: 0.5188017646923037]
	TIME [epoch: 3.26 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40339973768977777		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.40339973768977777 | validation: 0.48519522736074444]
	TIME [epoch: 3.27 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44432696660131665		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.44432696660131665 | validation: 0.5241494925253994]
	TIME [epoch: 3.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33966686673954816		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.33966686673954816 | validation: 0.4319382589615094]
	TIME [epoch: 3.28 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31493049608085355		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.31493049608085355 | validation: 0.3976954865764638]
	TIME [epoch: 3.26 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3061481401904972		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.3061481401904972 | validation: 0.3903139500362541]
	TIME [epoch: 3.26 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27190260316222614		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.27190260316222614 | validation: 0.6651363397172869]
	TIME [epoch: 3.27 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4467867611475222		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4467867611475222 | validation: 0.3520681311368843]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3916352863077025		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.3916352863077025 | validation: 0.6741512140799554]
	TIME [epoch: 3.26 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35093675742721564		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.35093675742721564 | validation: 0.4289180989220495]
	TIME [epoch: 3.26 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3139406995228793		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3139406995228793 | validation: 0.34815076078747564]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24890636344468098		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.24890636344468098 | validation: 0.28720192032033026]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2295810983617385		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.2295810983617385 | validation: 0.27031982994971404]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2486135601605733		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.2486135601605733 | validation: 0.4943863349037922]
	TIME [epoch: 3.31 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2771812934018142		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.2771812934018142 | validation: 0.45022118330601296]
	TIME [epoch: 3.27 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39134945558707657		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.39134945558707657 | validation: 0.31009073052099884]
	TIME [epoch: 3.26 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38629740183329464		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.38629740183329464 | validation: 0.35137551700527436]
	TIME [epoch: 3.26 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31402374069695127		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.31402374069695127 | validation: 0.42876241016810646]
	TIME [epoch: 3.26 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2995287818642355		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.2995287818642355 | validation: 0.8843469284814636]
	TIME [epoch: 3.26 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.735255216975645		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.735255216975645 | validation: 0.2995888355115454]
	TIME [epoch: 3.26 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27862884153085277		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.27862884153085277 | validation: 0.5162170081958877]
	TIME [epoch: 3.26 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27906040667401427		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.27906040667401427 | validation: 0.23948542022476485]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23146318149653797		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.23146318149653797 | validation: 0.25999475430673685]
	TIME [epoch: 3.26 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2321966578944793		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2321966578944793 | validation: 1.3936156493698464]
	TIME [epoch: 3.27 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1971193767654356		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.1971193767654356 | validation: 0.6785717459231209]
	TIME [epoch: 3.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45656322470345556		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.45656322470345556 | validation: 0.4275891419512771]
	TIME [epoch: 3.28 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2820035641955604		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.2820035641955604 | validation: 0.4834074048581508]
	TIME [epoch: 3.26 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2738627957220072		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.2738627957220072 | validation: 0.25567075647491994]
	TIME [epoch: 3.26 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2815718319021304		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2815718319021304 | validation: 0.2740831489434052]
	TIME [epoch: 3.26 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24846261713231527		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.24846261713231527 | validation: 0.28140888413240966]
	TIME [epoch: 3.26 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2197399498006039		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.2197399498006039 | validation: 0.3282492277108724]
	TIME [epoch: 3.26 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24901032647184868		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.24901032647184868 | validation: 0.27889603769345545]
	TIME [epoch: 3.27 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2217157246357164		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.2217157246357164 | validation: 0.2094258928843921]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1856162694570706		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1856162694570706 | validation: 0.268478537024921]
	TIME [epoch: 3.26 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19099666541212207		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.19099666541212207 | validation: 0.24362026547791757]
	TIME [epoch: 3.26 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18655996287498455		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.18655996287498455 | validation: 0.32618819992891573]
	TIME [epoch: 3.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2749520244025775		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2749520244025775 | validation: 0.4188090360151668]
	TIME [epoch: 3.28 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41328747916152164		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.41328747916152164 | validation: 0.7721846620660483]
	TIME [epoch: 3.27 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46046203010794395		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.46046203010794395 | validation: 0.3801962205238038]
	TIME [epoch: 3.26 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2557575314676311		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2557575314676311 | validation: 0.23448707248394066]
	TIME [epoch: 3.26 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20957313874091094		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.20957313874091094 | validation: 0.20682491090819255]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18912785593470066		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.18912785593470066 | validation: 0.23496198722284026]
	TIME [epoch: 107 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25487331833187965		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.25487331833187965 | validation: 0.18851013965488328]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1744010050538357		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.1744010050538357 | validation: 0.3502671053819663]
	TIME [epoch: 6.46 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.246563317119972		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.246563317119972 | validation: 0.7815828343576601]
	TIME [epoch: 6.45 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5687501689961616		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5687501689961616 | validation: 0.41336275318523896]
	TIME [epoch: 6.44 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30293278967835313		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.30293278967835313 | validation: 0.4407804309348886]
	TIME [epoch: 6.44 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3074185172115624		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3074185172115624 | validation: 0.24379226307753465]
	TIME [epoch: 6.45 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17876590988104607		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.17876590988104607 | validation: 0.17841756613235443]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17138589557899986		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.17138589557899986 | validation: 0.20619507197243953]
	TIME [epoch: 6.44 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17367454576206764		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.17367454576206764 | validation: 0.1860370053472976]
	TIME [epoch: 6.47 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15285333377401578		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.15285333377401578 | validation: 0.3603976876426842]
	TIME [epoch: 6.47 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3023978739629113		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3023978739629113 | validation: 0.177636310262864]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16769068811661447		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.16769068811661447 | validation: 0.29602324851479533]
	TIME [epoch: 6.45 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18408609146518418		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.18408609146518418 | validation: 0.3696267802766174]
	TIME [epoch: 6.41 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21448207018181698		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.21448207018181698 | validation: 0.21539835070054147]
	TIME [epoch: 6.43 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18555085124594067		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.18555085124594067 | validation: 0.29829303522431055]
	TIME [epoch: 6.47 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18159035510959542		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.18159035510959542 | validation: 0.18150896707730244]
	TIME [epoch: 6.43 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15626621658806333		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.15626621658806333 | validation: 0.24870994498621327]
	TIME [epoch: 6.42 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23274549278637174		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.23274549278637174 | validation: 0.5349637729513125]
	TIME [epoch: 6.42 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3308788907069534		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.3308788907069534 | validation: 0.537763772720435]
	TIME [epoch: 6.44 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2130702275125938		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2130702275125938 | validation: 0.16565670999426402]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18542492835545213		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.18542492835545213 | validation: 0.15692497995838195]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16712391598532		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.16712391598532 | validation: 0.2939219189345424]
	TIME [epoch: 6.45 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23983203649394846		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.23983203649394846 | validation: 0.2937545904566758]
	TIME [epoch: 6.41 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21949786313830527		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.21949786313830527 | validation: 0.13944340403470534]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13757692103523478		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.13757692103523478 | validation: 0.6204399509627505]
	TIME [epoch: 6.43 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24360160550190518		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.24360160550190518 | validation: 0.1490258034023843]
	TIME [epoch: 6.43 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1298031440037838		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.1298031440037838 | validation: 0.15959357354820047]
	TIME [epoch: 6.47 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1258529324951712		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.1258529324951712 | validation: 0.13394089418071395]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15085561459909336		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.15085561459909336 | validation: 0.33618565580542553]
	TIME [epoch: 6.46 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20098882043483685		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.20098882043483685 | validation: 0.29182529848988964]
	TIME [epoch: 6.45 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16664399117788675		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.16664399117788675 | validation: 0.28878414555398574]
	TIME [epoch: 6.43 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2807202152815177		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2807202152815177 | validation: 0.5824923361214259]
	TIME [epoch: 6.45 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3034945122673154		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.3034945122673154 | validation: 0.43090097188655624]
	TIME [epoch: 6.48 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27145964295421776		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.27145964295421776 | validation: 0.20039420746891531]
	TIME [epoch: 6.43 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1551594059261804		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.1551594059261804 | validation: 0.16531782055689326]
	TIME [epoch: 6.46 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12124866595559607		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.12124866595559607 | validation: 0.13879935959302506]
	TIME [epoch: 6.45 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15724527096952487		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.15724527096952487 | validation: 0.1775443597746083]
	TIME [epoch: 6.46 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17057512463166327		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.17057512463166327 | validation: 0.16358857905420984]
	TIME [epoch: 6.44 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15584726739669963		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.15584726739669963 | validation: 0.17075422053395284]
	TIME [epoch: 6.49 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15546093203637162		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.15546093203637162 | validation: 0.3881775697166308]
	TIME [epoch: 6.44 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22151405883352784		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.22151405883352784 | validation: 0.18726955867324008]
	TIME [epoch: 6.44 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14911916012525814		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.14911916012525814 | validation: 0.15157732430436738]
	TIME [epoch: 6.44 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13066867507413632		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.13066867507413632 | validation: 0.1168515828839504]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12143547591139595		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.12143547591139595 | validation: 0.2696923610455244]
	TIME [epoch: 6.46 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17002968954502845		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.17002968954502845 | validation: 0.37932277235546946]
	TIME [epoch: 6.47 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1989744164108006		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1989744164108006 | validation: 0.40071232552869485]
	TIME [epoch: 6.44 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23430651043230516		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.23430651043230516 | validation: 0.1917361378744808]
	TIME [epoch: 6.45 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11843468246346255		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.11843468246346255 | validation: 0.11067052468808031]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1176454422505814		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1176454422505814 | validation: 0.18131301293258956]
	TIME [epoch: 6.44 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11784904762701898		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.11784904762701898 | validation: 0.1065474124830138]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1149924232776615		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1149924232776615 | validation: 0.12255671070053409]
	TIME [epoch: 6.46 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11162707989145067		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.11162707989145067 | validation: 0.1307123690157103]
	TIME [epoch: 6.46 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10699000051577748		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.10699000051577748 | validation: 0.14371514632278345]
	TIME [epoch: 6.44 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13744779169508664		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.13744779169508664 | validation: 0.1492290898582157]
	TIME [epoch: 6.45 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10191872527749372		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.10191872527749372 | validation: 0.10047985471572396]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10200872140276872		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.10200872140276872 | validation: 0.1021623954231711]
	TIME [epoch: 6.46 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10881845924715458		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.10881845924715458 | validation: 0.4054558672898842]
	TIME [epoch: 6.43 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24347622251822734		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.24347622251822734 | validation: 0.3417364040987666]
	TIME [epoch: 6.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15914517502463166		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.15914517502463166 | validation: 0.10209670605786228]
	TIME [epoch: 6.41 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1033860400905595		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1033860400905595 | validation: 0.09317136131310072]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09197003623131741		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.09197003623131741 | validation: 0.09734994184433057]
	TIME [epoch: 6.42 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1049568147107724		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.1049568147107724 | validation: 0.15756482322184995]
	TIME [epoch: 6.47 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12150550334152921		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.12150550334152921 | validation: 0.18204945414637114]
	TIME [epoch: 6.43 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09456958428249537		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.09456958428249537 | validation: 0.13163916887162003]
	TIME [epoch: 6.44 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12715049109468668		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.12715049109468668 | validation: 0.4943086240147799]
	TIME [epoch: 6.42 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2944993846806137		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.2944993846806137 | validation: 0.12693871474315968]
	TIME [epoch: 6.41 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12005146711950844		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.12005146711950844 | validation: 0.21518189890307182]
	TIME [epoch: 6.42 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11194485196447253		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.11194485196447253 | validation: 0.1418915596920842]
	TIME [epoch: 6.44 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10344501001662332		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.10344501001662332 | validation: 0.12530440196504108]
	TIME [epoch: 6.42 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09354233697102361		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.09354233697102361 | validation: 0.10166194518641254]
	TIME [epoch: 6.43 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08095848980049672		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.08095848980049672 | validation: 0.11120261539334463]
	TIME [epoch: 6.41 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09296149571835616		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09296149571835616 | validation: 0.7356445439602518]
	TIME [epoch: 6.41 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33397245607618875		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.33397245607618875 | validation: 0.3936093113385816]
	TIME [epoch: 6.41 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2792910880530531		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.2792910880530531 | validation: 0.1981337861870169]
	TIME [epoch: 6.46 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2506513524392693		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.2506513524392693 | validation: 0.34713349711399516]
	TIME [epoch: 6.42 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3611257623091572		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.3611257623091572 | validation: 0.12470891911135396]
	TIME [epoch: 6.41 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11359029251013861		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11359029251013861 | validation: 0.10295107284775847]
	TIME [epoch: 6.42 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0897170732873973		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.0897170732873973 | validation: 0.08359007493694555]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07649070342393552		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.07649070342393552 | validation: 0.10398175000677363]
	TIME [epoch: 6.43 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08913413198747588		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.08913413198747588 | validation: 0.143307254617229]
	TIME [epoch: 6.45 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10687276814454215		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.10687276814454215 | validation: 0.3319989699120039]
	TIME [epoch: 6.45 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1552593822770416		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.1552593822770416 | validation: 0.12321841291595986]
	TIME [epoch: 6.43 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09016807628848333		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.09016807628848333 | validation: 0.08564423568287349]
	TIME [epoch: 6.42 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08043136282281085		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.08043136282281085 | validation: 0.08352516384988548]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07616736132837981		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07616736132837981 | validation: 0.076805778951092]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07625479599251961		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.07625479599251961 | validation: 0.09152882259945705]
	TIME [epoch: 6.44 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07365823991115772		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.07365823991115772 | validation: 0.0987851953458079]
	TIME [epoch: 6.43 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07759812096555585		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.07759812096555585 | validation: 0.11171108244318673]
	TIME [epoch: 6.41 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22842631070517203		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.22842631070517203 | validation: 0.08548383523008535]
	TIME [epoch: 6.41 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1422749854482187		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1422749854482187 | validation: 0.10883397542217384]
	TIME [epoch: 6.43 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1040248515636695		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1040248515636695 | validation: 0.08184271877406456]
	TIME [epoch: 6.43 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07575798864338908		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.07575798864338908 | validation: 0.07466660079569412]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07059849094126358		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.07059849094126358 | validation: 0.10275168767777097]
	TIME [epoch: 6.42 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07777051901544971		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.07777051901544971 | validation: 0.09277297875809766]
	TIME [epoch: 6.42 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.087289407072556		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.087289407072556 | validation: 0.09690347141094463]
	TIME [epoch: 6.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07118635287449382		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.07118635287449382 | validation: 0.09776003270809902]
	TIME [epoch: 6.43 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08382159758051919		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.08382159758051919 | validation: 0.09257507573563675]
	TIME [epoch: 6.44 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07621426479555744		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.07621426479555744 | validation: 0.0808537229311704]
	TIME [epoch: 6.44 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09751891957393821		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.09751891957393821 | validation: 0.13499217582473103]
	TIME [epoch: 6.44 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11148874654756068		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.11148874654756068 | validation: 0.0758489116665475]
	TIME [epoch: 6.45 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06779734461526822		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.06779734461526822 | validation: 0.08209878907383839]
	TIME [epoch: 6.42 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07246057461362318		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07246057461362318 | validation: 0.1059443997766514]
	TIME [epoch: 6.44 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0796297453893435		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.0796297453893435 | validation: 0.10647638762546995]
	TIME [epoch: 6.47 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09243831756407947		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.09243831756407947 | validation: 0.09827681160666046]
	TIME [epoch: 6.44 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09116823625913581		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.09116823625913581 | validation: 0.10503827124386798]
	TIME [epoch: 6.45 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07566261900151929		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07566261900151929 | validation: 0.0725315726396823]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06705736332713158		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.06705736332713158 | validation: 0.08794930542871687]
	TIME [epoch: 6.44 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09533863767152474		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.09533863767152474 | validation: 0.3972790249792981]
	TIME [epoch: 6.44 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18301619884428036		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.18301619884428036 | validation: 0.16139643839008624]
	TIME [epoch: 6.48 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09848948657304477		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.09848948657304477 | validation: 0.07246928392347145]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06967409936746179		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.06967409936746179 | validation: 0.07423917082589204]
	TIME [epoch: 6.44 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06417170557437309		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.06417170557437309 | validation: 0.07267538970684244]
	TIME [epoch: 6.43 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060627773783560365		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.060627773783560365 | validation: 0.08249987272337839]
	TIME [epoch: 6.44 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06400567304740135		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.06400567304740135 | validation: 0.08169538954407785]
	TIME [epoch: 6.45 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06843832803978764		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.06843832803978764 | validation: 0.06618565797714564]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06649260245817236		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.06649260245817236 | validation: 0.08357117570754996]
	TIME [epoch: 6.45 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.071750141266544		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.071750141266544 | validation: 0.073740496596935]
	TIME [epoch: 6.44 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06782283459712021		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06782283459712021 | validation: 0.0767510697831329]
	TIME [epoch: 6.45 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06601496621307519		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06601496621307519 | validation: 0.5927975676952342]
	TIME [epoch: 6.44 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23211160078121		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.23211160078121 | validation: 0.16393165799659215]
	TIME [epoch: 6.44 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11293290594765765		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.11293290594765765 | validation: 0.20207672551923506]
	TIME [epoch: 6.47 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10996708663718444		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.10996708663718444 | validation: 0.14744093787012888]
	TIME [epoch: 6.45 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1346151160714018		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.1346151160714018 | validation: 0.10443557415156862]
	TIME [epoch: 6.42 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08238651257288054		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.08238651257288054 | validation: 0.0620987260484328]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0581489356970219		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.0581489356970219 | validation: 0.07544395805116111]
	TIME [epoch: 6.43 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059742557710723385		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.059742557710723385 | validation: 0.07229812240726512]
	TIME [epoch: 6.44 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06834872664085978		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06834872664085978 | validation: 0.07964115902614205]
	TIME [epoch: 6.47 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06129713514362689		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.06129713514362689 | validation: 0.06316926442164648]
	TIME [epoch: 6.43 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05474787947998416		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.05474787947998416 | validation: 0.07420622325807769]
	TIME [epoch: 6.43 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060677453849934185		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.060677453849934185 | validation: 0.0791269397881565]
	TIME [epoch: 6.43 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13251759704642857		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.13251759704642857 | validation: 0.14553519518673147]
	TIME [epoch: 6.45 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12045431239551233		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.12045431239551233 | validation: 0.09910561932967546]
	TIME [epoch: 6.45 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08514779881319709		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08514779881319709 | validation: 0.07141937594024621]
	TIME [epoch: 6.46 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057181500594520734		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.057181500594520734 | validation: 0.09161069648896866]
	TIME [epoch: 6.42 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06826465252297668		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06826465252297668 | validation: 0.07669005848877647]
	TIME [epoch: 6.45 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06144080417453366		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.06144080417453366 | validation: 0.06693486685287622]
	TIME [epoch: 6.44 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057145880175831484		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.057145880175831484 | validation: 0.1962491874588061]
	TIME [epoch: 6.42 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12374696407109473		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.12374696407109473 | validation: 0.20115217594898938]
	TIME [epoch: 6.45 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10005169118208775		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.10005169118208775 | validation: 0.0603512426049438]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059685345601447226		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.059685345601447226 | validation: 0.0641955732333785]
	TIME [epoch: 6.46 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053372498451206035		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.053372498451206035 | validation: 0.053013476409357066]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054747931790593826		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.054747931790593826 | validation: 0.06493562745246871]
	TIME [epoch: 6.45 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054738646743416004		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.054738646743416004 | validation: 0.06553371765238478]
	TIME [epoch: 6.44 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05301346163733702		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.05301346163733702 | validation: 0.059629971824643226]
	TIME [epoch: 6.44 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05368605559261856		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.05368605559261856 | validation: 0.06379746839258234]
	TIME [epoch: 6.46 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0535854126225854		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0535854126225854 | validation: 0.06928435991239036]
	TIME [epoch: 6.45 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06206176962431739		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06206176962431739 | validation: 0.06873682871640333]
	TIME [epoch: 6.43 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05900525865258939		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.05900525865258939 | validation: 0.16371958202835943]
	TIME [epoch: 6.44 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08154083465967915		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.08154083465967915 | validation: 0.06115121067997095]
	TIME [epoch: 6.42 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05461519570948867		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.05461519570948867 | validation: 0.0552009816943196]
	TIME [epoch: 6.45 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05846423988135657		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.05846423988135657 | validation: 0.07559718403759433]
	TIME [epoch: 6.46 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05834447264563896		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05834447264563896 | validation: 0.16883259101403214]
	TIME [epoch: 6.43 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11817116140711488		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.11817116140711488 | validation: 0.0855996675056821]
	TIME [epoch: 6.45 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06644663279607257		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06644663279607257 | validation: 0.06944055387426604]
	TIME [epoch: 6.45 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05800083220054299		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05800083220054299 | validation: 0.065359402667452]
	TIME [epoch: 6.45 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05393373519444445		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.05393373519444445 | validation: 0.05768367435444997]
	TIME [epoch: 6.45 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049774563814299136		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.049774563814299136 | validation: 0.05759226110867939]
	TIME [epoch: 6.47 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04980229755171668		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.04980229755171668 | validation: 0.07177197227253929]
	TIME [epoch: 6.46 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05488195308352519		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05488195308352519 | validation: 0.057896635726583615]
	TIME [epoch: 6.45 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05191964359456423		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.05191964359456423 | validation: 0.059702360292714765]
	TIME [epoch: 6.47 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04749729750314144		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.04749729750314144 | validation: 0.05791221528057864]
	TIME [epoch: 6.45 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0501829371934636		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.0501829371934636 | validation: 0.07256390800947066]
	TIME [epoch: 6.48 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06875857381014518		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.06875857381014518 | validation: 0.06240621905226974]
	TIME [epoch: 6.46 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05193607663674002		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.05193607663674002 | validation: 0.05520854743656001]
	TIME [epoch: 6.45 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05292324952984141		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05292324952984141 | validation: 0.05318839836873666]
	TIME [epoch: 6.45 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05509721710017225		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05509721710017225 | validation: 0.2615688773774407]
	TIME [epoch: 6.44 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11443636140790459		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.11443636140790459 | validation: 0.06865293878086068]
	TIME [epoch: 6.46 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05983535939544425		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.05983535939544425 | validation: 0.05380627860639964]
	TIME [epoch: 6.49 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050261667285758115		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.050261667285758115 | validation: 0.05294447158544638]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048965926294060144		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.048965926294060144 | validation: 0.061477427130774955]
	TIME [epoch: 6.42 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05575122607141318		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.05575122607141318 | validation: 0.07075497436416832]
	TIME [epoch: 6.43 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051955908730871525		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.051955908730871525 | validation: 0.048569463108567856]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046308756889103		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.046308756889103 | validation: 0.06648998691217732]
	TIME [epoch: 6.42 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04580107423120459		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.04580107423120459 | validation: 0.07111799319508715]
	TIME [epoch: 6.44 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06689077745381687		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06689077745381687 | validation: 0.07332179308522564]
	TIME [epoch: 6.43 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05771948800321951		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05771948800321951 | validation: 0.07186785581631672]
	TIME [epoch: 6.43 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052379918163012976		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.052379918163012976 | validation: 0.0491623224586279]
	TIME [epoch: 6.43 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04688844027365709		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.04688844027365709 | validation: 0.08436883902211667]
	TIME [epoch: 6.43 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06916134680807291		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.06916134680807291 | validation: 0.07494887932916112]
	TIME [epoch: 6.42 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051902294657600004		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.051902294657600004 | validation: 0.04824784302371193]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04671418051963278		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.04671418051963278 | validation: 0.05123865465378491]
	TIME [epoch: 6.43 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04558802559355887		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.04558802559355887 | validation: 0.050302606901098446]
	TIME [epoch: 6.42 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051106028975927476		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.051106028975927476 | validation: 0.062415400635621544]
	TIME [epoch: 6.41 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0487309766484845		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0487309766484845 | validation: 0.04681108512383161]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04353087726752312		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04353087726752312 | validation: 0.056113972723313005]
	TIME [epoch: 6.43 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04462860018905453		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.04462860018905453 | validation: 0.06181812988544003]
	TIME [epoch: 6.45 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055389753384608595		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.055389753384608595 | validation: 0.06486432305458939]
	TIME [epoch: 6.43 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04915398634018309		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04915398634018309 | validation: 0.06685436007804045]
	TIME [epoch: 6.41 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05117904590735465		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.05117904590735465 | validation: 0.05064419950576469]
	TIME [epoch: 6.42 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04349152305877424		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.04349152305877424 | validation: 0.06391435478700017]
	TIME [epoch: 6.42 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05266157835380178		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.05266157835380178 | validation: 0.08893379409355526]
	TIME [epoch: 6.43 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05092449065078088		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.05092449065078088 | validation: 0.07076999528567333]
	TIME [epoch: 6.43 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11184884805284737		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.11184884805284737 | validation: 0.15157272717475084]
	TIME [epoch: 6.42 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12011379745309082		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.12011379745309082 | validation: 0.8440925231292061]
	TIME [epoch: 6.42 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5530244166232106		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.5530244166232106 | validation: 0.7384035095661449]
	TIME [epoch: 6.43 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3011441414922734		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.3011441414922734 | validation: 0.069512330359764]
	TIME [epoch: 6.43 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06682849274251017		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.06682849274251017 | validation: 0.05267655453289399]
	TIME [epoch: 6.43 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046409511157569146		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.046409511157569146 | validation: 0.045945546061446935]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04344388026872126		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04344388026872126 | validation: 0.04430495662518347]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039270662351926514		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.039270662351926514 | validation: 0.04909926592392044]
	TIME [epoch: 6.42 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0513388079347296		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.0513388079347296 | validation: 0.09184756961949747]
	TIME [epoch: 6.41 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05432508124857494		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.05432508124857494 | validation: 0.046868713446471776]
	TIME [epoch: 6.42 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041706938818816305		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.041706938818816305 | validation: 0.04638413293763984]
	TIME [epoch: 6.46 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03862599433659328		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.03862599433659328 | validation: 0.04174188356283762]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04064698503910817		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.04064698503910817 | validation: 0.04584416409804775]
	TIME [epoch: 6.41 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04087688236832564		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.04087688236832564 | validation: 0.046503977479306215]
	TIME [epoch: 6.41 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040946691457891246		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.040946691457891246 | validation: 0.045250834995645806]
	TIME [epoch: 6.41 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04016196338519934		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.04016196338519934 | validation: 0.05339173191310767]
	TIME [epoch: 6.42 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04340517280766848		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.04340517280766848 | validation: 0.05754789954922127]
	TIME [epoch: 6.47 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06024259940957501		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.06024259940957501 | validation: 0.07719748937187754]
	TIME [epoch: 6.44 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04955223127769781		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.04955223127769781 | validation: 0.050201814957721025]
	TIME [epoch: 6.41 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040819018406733604		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.040819018406733604 | validation: 0.045134751186747396]
	TIME [epoch: 6.42 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040932083246288636		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.040932083246288636 | validation: 0.0477956859358656]
	TIME [epoch: 6.41 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043545221041082105		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.043545221041082105 | validation: 0.0474188900203449]
	TIME [epoch: 6.42 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04006167555069808		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.04006167555069808 | validation: 0.04514392883726074]
	TIME [epoch: 6.46 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039887630566626746		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.039887630566626746 | validation: 0.04507738097396188]
	TIME [epoch: 6.44 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04592593230886628		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.04592593230886628 | validation: 0.044104544643884636]
	TIME [epoch: 6.43 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04014638393358332		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.04014638393358332 | validation: 0.053451105480410745]
	TIME [epoch: 6.42 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04189294960230684		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04189294960230684 | validation: 0.0495732007663768]
	TIME [epoch: 6.41 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15827026811976097		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.15827026811976097 | validation: 0.4171345510570194]
	TIME [epoch: 6.44 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22235504918543086		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.22235504918543086 | validation: 0.060531418101457714]
	TIME [epoch: 6.47 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06091172916354344		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.06091172916354344 | validation: 0.0767066754356765]
	TIME [epoch: 6.45 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05838403359628318		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.05838403359628318 | validation: 0.0655794150145581]
	TIME [epoch: 6.42 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04728874069512529		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.04728874069512529 | validation: 0.042009202405398974]
	TIME [epoch: 6.41 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040113899019059346		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.040113899019059346 | validation: 0.04441256605875296]
	TIME [epoch: 6.42 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03947867023338524		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03947867023338524 | validation: 0.04250703548710246]
	TIME [epoch: 6.42 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03812403040219136		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.03812403040219136 | validation: 0.03941082983342733]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03868151960231496		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03868151960231496 | validation: 0.04408970170661271]
	TIME [epoch: 6.44 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043549619258341554		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.043549619258341554 | validation: 0.05503173823544382]
	TIME [epoch: 6.43 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045556661503275336		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.045556661503275336 | validation: 0.04763511300081403]
	TIME [epoch: 6.43 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04148027449893983		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.04148027449893983 | validation: 0.04341426712422805]
	TIME [epoch: 6.44 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03874119695327631		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.03874119695327631 | validation: 0.03888680218763277]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047700099097594664		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.047700099097594664 | validation: 0.040522920493939185]
	TIME [epoch: 6.47 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03796199050002494		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.03796199050002494 | validation: 0.04238759990941759]
	TIME [epoch: 6.44 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037773894489965334		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.037773894489965334 | validation: 0.0423682492443444]
	TIME [epoch: 6.41 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03863916501210002		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.03863916501210002 | validation: 0.05715629339443591]
	TIME [epoch: 6.42 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042222387324022566		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.042222387324022566 | validation: 0.04529210253560001]
	TIME [epoch: 6.43 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03585676331000143		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.03585676331000143 | validation: 0.03945725528888061]
	TIME [epoch: 6.43 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037027220772533315		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.037027220772533315 | validation: 0.04122267160940038]
	TIME [epoch: 6.46 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03899956272867042		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03899956272867042 | validation: 0.06921008613269566]
	TIME [epoch: 6.43 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04733509624202316		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04733509624202316 | validation: 0.0425777107788084]
	TIME [epoch: 6.44 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1190308839926593		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.1190308839926593 | validation: 0.27032397335978925]
	TIME [epoch: 6.44 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11653198996671843		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.11653198996671843 | validation: 0.05098602747642163]
	TIME [epoch: 6.45 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04450302605703924		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04450302605703924 | validation: 0.16764891124109962]
	TIME [epoch: 6.46 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1030658709965541		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.1030658709965541 | validation: 0.09265361728676345]
	TIME [epoch: 6.48 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05239399950833842		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.05239399950833842 | validation: 0.04264890634921731]
	TIME [epoch: 6.43 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04019642550918291		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.04019642550918291 | validation: 0.044529495397392614]
	TIME [epoch: 6.44 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03850555554364164		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.03850555554364164 | validation: 0.04265917325543109]
	TIME [epoch: 6.43 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036571888927613164		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.036571888927613164 | validation: 0.040456817228732976]
	TIME [epoch: 6.44 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03774747748464771		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03774747748464771 | validation: 0.038067948820454986]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03581414160822147		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.03581414160822147 | validation: 0.04279777944213112]
	TIME [epoch: 6.48 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036916692634198944		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.036916692634198944 | validation: 0.04153840855359305]
	TIME [epoch: 6.45 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03553893590154339		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03553893590154339 | validation: 0.04179127378238225]
	TIME [epoch: 6.41 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03690456402829465		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.03690456402829465 | validation: 0.04096352157581996]
	TIME [epoch: 6.46 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03685097864987965		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03685097864987965 | validation: 0.06630534691717392]
	TIME [epoch: 6.45 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052013078902212344		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.052013078902212344 | validation: 0.04154898916813837]
	TIME [epoch: 6.47 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03719429612615373		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03719429612615373 | validation: 0.04246264242213247]
	TIME [epoch: 6.48 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039129066181820576		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.039129066181820576 | validation: 0.04010734095378744]
	TIME [epoch: 6.44 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036800534233025664		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.036800534233025664 | validation: 0.04037132854862329]
	TIME [epoch: 6.45 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036473973886906846		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.036473973886906846 | validation: 0.06254965778876358]
	TIME [epoch: 6.42 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04841719768624182		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.04841719768624182 | validation: 0.04118273860693619]
	TIME [epoch: 6.44 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037613826295041064		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.037613826295041064 | validation: 0.03998675201565464]
	TIME [epoch: 6.47 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03844717593717725		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.03844717593717725 | validation: 0.04179118692288205]
	TIME [epoch: 6.49 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040286839357953845		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.040286839357953845 | validation: 0.04270117028868094]
	TIME [epoch: 6.44 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03528420963572573		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03528420963572573 | validation: 0.04119104978541016]
	TIME [epoch: 6.42 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040678171219842296		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.040678171219842296 | validation: 0.04695948563468872]
	TIME [epoch: 6.44 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03528339112937565		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.03528339112937565 | validation: 0.04351643240739339]
	TIME [epoch: 6.42 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03612219449455796		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.03612219449455796 | validation: 0.04208073574431662]
	TIME [epoch: 6.45 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03446794847654899		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.03446794847654899 | validation: 0.040240796774718694]
	TIME [epoch: 6.44 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03904781441448396		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03904781441448396 | validation: 0.04446310684436694]
	TIME [epoch: 6.42 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035555491389681934		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.035555491389681934 | validation: 0.045607036807993155]
	TIME [epoch: 6.44 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034020605153432054		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.034020605153432054 | validation: 0.054075130362219406]
	TIME [epoch: 6.42 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047053523861781535		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.047053523861781535 | validation: 0.04501346128648065]
	TIME [epoch: 6.43 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03773448670839103		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.03773448670839103 | validation: 0.038819443255168284]
	TIME [epoch: 6.46 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03632475225438997		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.03632475225438997 | validation: 0.0481705630416409]
	TIME [epoch: 6.45 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038431099282058095		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.038431099282058095 | validation: 0.04348341623262296]
	TIME [epoch: 6.44 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04203692888977599		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.04203692888977599 | validation: 0.051357175222425694]
	TIME [epoch: 6.44 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03792171835955627		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03792171835955627 | validation: 0.03849772260189851]
	TIME [epoch: 6.44 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03624593263020259		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.03624593263020259 | validation: 0.03930799617919398]
	TIME [epoch: 6.46 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035054237443628025		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.035054237443628025 | validation: 0.04468375971648331]
	TIME [epoch: 6.46 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03820401786831849		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03820401786831849 | validation: 0.05520302569626622]
	TIME [epoch: 6.49 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043248830534239155		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.043248830534239155 | validation: 0.043589953397494496]
	TIME [epoch: 6.41 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03785072420563223		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03785072420563223 | validation: 0.03678541842573517]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04759011629521723		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.04759011629521723 | validation: 0.035843163100386294]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03497908591941374		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.03497908591941374 | validation: 0.039665133007545636]
	TIME [epoch: 6.45 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03394461444983335		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03394461444983335 | validation: 0.03691308534083475]
	TIME [epoch: 6.48 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035294811278839947		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.035294811278839947 | validation: 0.036125026739474764]
	TIME [epoch: 6.46 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03623314952699998		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03623314952699998 | validation: 0.042181495820387466]
	TIME [epoch: 6.45 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033777961280716154		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.033777961280716154 | validation: 0.04790388304496591]
	TIME [epoch: 6.46 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03710310794401026		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03710310794401026 | validation: 0.03811395202447873]
	TIME [epoch: 6.43 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03340693878084118		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03340693878084118 | validation: 0.043700147162095616]
	TIME [epoch: 6.46 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034988026977017084		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.034988026977017084 | validation: 0.0412627329930399]
	TIME [epoch: 6.51 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03507018675061775		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03507018675061775 | validation: 0.04286953869180866]
	TIME [epoch: 6.47 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0407081815916439		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.0407081815916439 | validation: 0.04165688098303255]
	TIME [epoch: 6.44 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03650285487614757		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03650285487614757 | validation: 0.06644119593183194]
	TIME [epoch: 6.44 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040752917911235004		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.040752917911235004 | validation: 0.045243242625635706]
	TIME [epoch: 6.44 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03881089777532529		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.03881089777532529 | validation: 0.035540115290707545]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03293791354765448		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.03293791354765448 | validation: 0.03794031384972413]
	TIME [epoch: 6.45 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03311101364181585		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.03311101364181585 | validation: 0.03996347257384502]
	TIME [epoch: 6.43 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054502125413701086		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.054502125413701086 | validation: 0.04674315994152747]
	TIME [epoch: 115 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039499280629727726		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.039499280629727726 | validation: 0.04384477099008678]
	TIME [epoch: 14 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0352065319869751		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.0352065319869751 | validation: 0.04549163234480098]
	TIME [epoch: 14 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03522757090060318		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.03522757090060318 | validation: 0.03817187787858008]
	TIME [epoch: 14 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03176585604503455		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.03176585604503455 | validation: 0.038213789324671024]
	TIME [epoch: 14 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03562867181634757		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03562867181634757 | validation: 0.03924782201305551]
	TIME [epoch: 14.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03159519728855691		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03159519728855691 | validation: 0.03936724005517518]
	TIME [epoch: 14 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031316720532655905		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.031316720532655905 | validation: 0.03641944070463953]
	TIME [epoch: 14 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030163059546135143		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.030163059546135143 | validation: 0.04224342489930573]
	TIME [epoch: 14.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030865594894992243		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.030865594894992243 | validation: 0.03717877197495254]
	TIME [epoch: 14 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02988250348794941		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.02988250348794941 | validation: 0.035337955274847586]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032093991328329075		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.032093991328329075 | validation: 0.04045241170417372]
	TIME [epoch: 14.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03335401843234762		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.03335401843234762 | validation: 0.03808469230233005]
	TIME [epoch: 14 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03170157877260513		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.03170157877260513 | validation: 0.03952470818795486]
	TIME [epoch: 14 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03376889782601487		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03376889782601487 | validation: 0.05688197725443198]
	TIME [epoch: 14 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036759244377707896		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.036759244377707896 | validation: 0.045737969848733205]
	TIME [epoch: 14.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03519071871883407		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03519071871883407 | validation: 0.04163366355957364]
	TIME [epoch: 14 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031525632955181085		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.031525632955181085 | validation: 0.04302535505480077]
	TIME [epoch: 14.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03439003717854005		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03439003717854005 | validation: 0.03742636625838529]
	TIME [epoch: 14 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03348256365052667		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03348256365052667 | validation: 0.04432268952967536]
	TIME [epoch: 14 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037302933848456996		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.037302933848456996 | validation: 0.0372098372373157]
	TIME [epoch: 14.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0348219708410924		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.0348219708410924 | validation: 0.03865975925285858]
	TIME [epoch: 14 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03502595941744357		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03502595941744357 | validation: 0.043181449223890854]
	TIME [epoch: 14 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035861772845638504		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.035861772845638504 | validation: 0.0411188127238999]
	TIME [epoch: 14 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032809477397955075		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.032809477397955075 | validation: 0.03763568173250601]
	TIME [epoch: 14 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032202872142879965		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.032202872142879965 | validation: 0.03886714414317077]
	TIME [epoch: 14 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03225691183436635		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.03225691183436635 | validation: 0.038922646346671703]
	TIME [epoch: 14 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03038148380905649		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.03038148380905649 | validation: 0.042247101696664534]
	TIME [epoch: 14 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033720908226209476		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.033720908226209476 | validation: 0.0411308371313484]
	TIME [epoch: 14 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03229314798821755		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.03229314798821755 | validation: 0.0376231136897278]
	TIME [epoch: 14 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03539579316009243		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.03539579316009243 | validation: 0.04139466004130637]
	TIME [epoch: 14 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031611888715131806		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.031611888715131806 | validation: 0.09323058098848258]
	TIME [epoch: 14.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047419970405272266		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.047419970405272266 | validation: 0.03643709257671906]
	TIME [epoch: 13.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031658645135832185		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.031658645135832185 | validation: 0.03923148497769954]
	TIME [epoch: 14 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03546992431080181		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.03546992431080181 | validation: 0.04248126141777045]
	TIME [epoch: 14 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0370076549064675		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.0370076549064675 | validation: 0.041419438818593564]
	TIME [epoch: 14 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03194463788615925		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.03194463788615925 | validation: 0.03868701643427237]
	TIME [epoch: 14 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03222503341117706		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.03222503341117706 | validation: 0.03438300886546701]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030551326440543446		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.030551326440543446 | validation: 0.03975989123874033]
	TIME [epoch: 14 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04440334947809979		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.04440334947809979 | validation: 0.08734682204569531]
	TIME [epoch: 14 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042594284542532626		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.042594284542532626 | validation: 0.03587367679553622]
	TIME [epoch: 14 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03405692987867167		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.03405692987867167 | validation: 0.040309643713620974]
	TIME [epoch: 14 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0339983256410311		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.0339983256410311 | validation: 0.03932734563593416]
	TIME [epoch: 14 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029038064320812535		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.029038064320812535 | validation: 0.05800907297703729]
	TIME [epoch: 14 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04060477174219082		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04060477174219082 | validation: 0.034400786426961594]
	TIME [epoch: 14 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03134704956563575		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.03134704956563575 | validation: 0.03689184503506409]
	TIME [epoch: 14 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032457177554633974		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.032457177554633974 | validation: 0.03513882359789429]
	TIME [epoch: 14 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02900708982721379		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02900708982721379 | validation: 0.034436398141360014]
	TIME [epoch: 13.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03001247297029129		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.03001247297029129 | validation: 0.03637439746611887]
	TIME [epoch: 14 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030043256182401598		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.030043256182401598 | validation: 0.03444487112379824]
	TIME [epoch: 14 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03027428527310112		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.03027428527310112 | validation: 0.03762438146033501]
	TIME [epoch: 14 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03179062613541522		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.03179062613541522 | validation: 0.03643534822823482]
	TIME [epoch: 14.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030774093646251315		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.030774093646251315 | validation: 0.03457141762698886]
	TIME [epoch: 14 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033524284228993784		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.033524284228993784 | validation: 0.03926265343308238]
	TIME [epoch: 14 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037647253654329284		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.037647253654329284 | validation: 0.03428559722097112]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03291068556913771		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03291068556913771 | validation: 0.03833665005264597]
	TIME [epoch: 14 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029325140111703375		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.029325140111703375 | validation: 0.14231380506929667]
	TIME [epoch: 14 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10467626571639099		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.10467626571639099 | validation: 0.1463749375421108]
	TIME [epoch: 14.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0768585086963462		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.0768585086963462 | validation: 0.049424200120989296]
	TIME [epoch: 14.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03557544212449154		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.03557544212449154 | validation: 0.035914128507291414]
	TIME [epoch: 14 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030251087198004158		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.030251087198004158 | validation: 0.03530570798432997]
	TIME [epoch: 14.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030121887369395042		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.030121887369395042 | validation: 0.0353423329939145]
	TIME [epoch: 14 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02904116430034012		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.02904116430034012 | validation: 0.030968610752040215]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029374204383551338		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.029374204383551338 | validation: 0.03545472387989382]
	TIME [epoch: 14.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03050770441071418		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.03050770441071418 | validation: 0.03612622505245976]
	TIME [epoch: 14 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028474790833300942		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.028474790833300942 | validation: 0.06903527702508337]
	TIME [epoch: 14.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04315055409400852		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.04315055409400852 | validation: 0.06846611038326289]
	TIME [epoch: 14 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04409357885431785		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.04409357885431785 | validation: 0.03476337898955689]
	TIME [epoch: 14 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02853020251750893		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.02853020251750893 | validation: 0.036274382053196684]
	TIME [epoch: 14.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03273087363872272		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.03273087363872272 | validation: 0.0603241425831069]
	TIME [epoch: 14.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04164101417354092		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.04164101417354092 | validation: 0.041889419734140794]
	TIME [epoch: 14 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03103188427844399		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.03103188427844399 | validation: 0.03274794624376892]
	TIME [epoch: 14.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02896223195667911		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.02896223195667911 | validation: 0.03326351066537211]
	TIME [epoch: 14 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029656616656166195		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.029656616656166195 | validation: 0.035014687162932136]
	TIME [epoch: 14 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029173964118206858		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.029173964118206858 | validation: 0.03317599869821136]
	TIME [epoch: 14.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02931101287077459		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.02931101287077459 | validation: 0.032447067431070335]
	TIME [epoch: 14 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030551795229207368		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.030551795229207368 | validation: 0.031344942342564845]
	TIME [epoch: 14 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02963894391989965		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.02963894391989965 | validation: 0.033340328206339816]
	TIME [epoch: 14 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03037984531709946		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.03037984531709946 | validation: 0.05483405762099543]
	TIME [epoch: 14 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042648207549307954		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.042648207549307954 | validation: 0.04685945781724884]
	TIME [epoch: 14 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03270333016949886		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.03270333016949886 | validation: 0.03372589571656073]
	TIME [epoch: 14.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029736778660390713		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.029736778660390713 | validation: 0.033088048219462166]
	TIME [epoch: 14.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02889718301384358		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.02889718301384358 | validation: 0.033870687524767816]
	TIME [epoch: 14 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029389086812106918		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.029389086812106918 | validation: 0.03323139687330578]
	TIME [epoch: 14 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02924681934778115		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02924681934778115 | validation: 0.03405225865804633]
	TIME [epoch: 14 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02845167380814427		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02845167380814427 | validation: 0.0348154315253426]
	TIME [epoch: 14.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028457589096308734		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.028457589096308734 | validation: 0.03456662793231539]
	TIME [epoch: 14 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02968554560367269		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02968554560367269 | validation: 0.0339920766753578]
	TIME [epoch: 14 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02888271673762571		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.02888271673762571 | validation: 0.0351331817736388]
	TIME [epoch: 14.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028465321733295964		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.028465321733295964 | validation: 0.03427841292320198]
	TIME [epoch: 14 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029183060543692064		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.029183060543692064 | validation: 0.03856122958281275]
	TIME [epoch: 14 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029024661029328585		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.029024661029328585 | validation: 0.03193740060169078]
	TIME [epoch: 14.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028504392561928292		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.028504392561928292 | validation: 0.03396833246727015]
	TIME [epoch: 14 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02756655438637423		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02756655438637423 | validation: 0.03906114499466168]
	TIME [epoch: 14.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029186101647200605		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.029186101647200605 | validation: 0.031865064254373104]
	TIME [epoch: 14.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02898049750989587		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.02898049750989587 | validation: 0.03423348503327342]
	TIME [epoch: 14 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028693877307365993		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.028693877307365993 | validation: 0.03670949118539172]
	TIME [epoch: 14 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029041053449689144		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.029041053449689144 | validation: 0.03445816992906543]
	TIME [epoch: 14.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028463286592861584		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.028463286592861584 | validation: 0.0347130868133583]
	TIME [epoch: 14.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029903793540061		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.029903793540061 | validation: 0.03962458127543412]
	TIME [epoch: 14 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029201668421879712		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.029201668421879712 | validation: 0.037420362771026734]
	TIME [epoch: 14.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02866508561684731		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.02866508561684731 | validation: 0.038350834412769604]
	TIME [epoch: 14 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03311624134675026		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.03311624134675026 | validation: 0.03641187844645191]
	TIME [epoch: 14.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02931144481246684		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.02931144481246684 | validation: 0.03573962848621457]
	TIME [epoch: 14.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042183406301080785		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.042183406301080785 | validation: 0.09471319286644986]
	TIME [epoch: 14 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053260374108038466		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.053260374108038466 | validation: 0.05482974924936188]
	TIME [epoch: 14 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033754700151604526		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.033754700151604526 | validation: 0.03572022184878297]
	TIME [epoch: 14.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029956606229725107		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.029956606229725107 | validation: 0.03423153215611196]
	TIME [epoch: 14 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029290468997707313		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.029290468997707313 | validation: 0.03681522161196769]
	TIME [epoch: 14.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028315577373844547		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.028315577373844547 | validation: 0.0337648896749664]
	TIME [epoch: 14.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02818605997873553		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.02818605997873553 | validation: 0.034226427831642335]
	TIME [epoch: 14 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029164418212156547		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.029164418212156547 | validation: 0.03142186185490894]
	TIME [epoch: 14.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027962367800431237		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.027962367800431237 | validation: 0.034765751046552225]
	TIME [epoch: 14.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028208400268353834		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.028208400268353834 | validation: 0.03179944455386091]
	TIME [epoch: 14 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027773450649573414		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.027773450649573414 | validation: 0.034132543285324085]
	TIME [epoch: 14.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027355251984983328		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.027355251984983328 | validation: 0.03251592076482098]
	TIME [epoch: 14.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027531006536024277		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.027531006536024277 | validation: 0.03393375640335541]
	TIME [epoch: 14.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029909082433142126		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.029909082433142126 | validation: 0.03382275967256126]
	TIME [epoch: 14.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026756245993840903		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.026756245993840903 | validation: 0.03488781574088464]
	TIME [epoch: 14 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029519576102148344		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.029519576102148344 | validation: 0.05321338785384975]
	TIME [epoch: 14 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03708949900563105		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.03708949900563105 | validation: 0.04830469580617257]
	TIME [epoch: 14.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03134447307319889		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.03134447307319889 | validation: 0.03159718990434624]
	TIME [epoch: 14.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028153479835347234		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.028153479835347234 | validation: 0.03567223628445614]
	TIME [epoch: 14 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027441377747733926		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.027441377747733926 | validation: 0.036561737467141794]
	TIME [epoch: 14.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030191436203197277		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.030191436203197277 | validation: 0.040244163335797]
	TIME [epoch: 14 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029445440584174597		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.029445440584174597 | validation: 0.03278294074271774]
	TIME [epoch: 14 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03164040175183458		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.03164040175183458 | validation: 0.058445627725225525]
	TIME [epoch: 14.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03494008422821793		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.03494008422821793 | validation: 0.034237214931276856]
	TIME [epoch: 14 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028981185363581484		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.028981185363581484 | validation: 0.032904574250809636]
	TIME [epoch: 14.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02768285224123256		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02768285224123256 | validation: 0.032623758925587186]
	TIME [epoch: 14.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02684238426311605		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02684238426311605 | validation: 0.03192201003506342]
	TIME [epoch: 14.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02831988755035073		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.02831988755035073 | validation: 0.033321563429499906]
	TIME [epoch: 14.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026553674768365412		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.026553674768365412 | validation: 0.031219533289328832]
	TIME [epoch: 14 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028661218804671026		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.028661218804671026 | validation: 0.03228084240980209]
	TIME [epoch: 14 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028439839097668236		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.028439839097668236 | validation: 0.07160442262698889]
	TIME [epoch: 14.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053066317357873996		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.053066317357873996 | validation: 0.0862771662957802]
	TIME [epoch: 14 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0530598298296827		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.0530598298296827 | validation: 0.05238402599363506]
	TIME [epoch: 14 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035399440166071786		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.035399440166071786 | validation: 0.032262144782550455]
	TIME [epoch: 14.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027422278473168166		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.027422278473168166 | validation: 0.030261344497551218]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027801142556185855		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.027801142556185855 | validation: 0.030100888187563374]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02761176286305542		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.02761176286305542 | validation: 0.03377512590560111]
	TIME [epoch: 14.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027214069925080006		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.027214069925080006 | validation: 0.0420002511864259]
	TIME [epoch: 14 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039023299397850666		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.039023299397850666 | validation: 0.059414565021215594]
	TIME [epoch: 14 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0396606946849862		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.0396606946849862 | validation: 0.042022985085887844]
	TIME [epoch: 14.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03114645832774926		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.03114645832774926 | validation: 0.032711600592185454]
	TIME [epoch: 14.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027794127990666356		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.027794127990666356 | validation: 0.03220707659222721]
	TIME [epoch: 14.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026217425262377442		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.026217425262377442 | validation: 0.032623597873442364]
	TIME [epoch: 14.1 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027193427206871294		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.027193427206871294 | validation: 0.03243021417839991]
	TIME [epoch: 14.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02825147340217328		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.02825147340217328 | validation: 0.03463843574478127]
	TIME [epoch: 14.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027617860406546055		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.027617860406546055 | validation: 0.03224329979125213]
	TIME [epoch: 14.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02682091857332297		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.02682091857332297 | validation: 0.033986598868119014]
	TIME [epoch: 14.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027526626578802955		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.027526626578802955 | validation: 0.031606267608466715]
	TIME [epoch: 14.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02736982901490711		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.02736982901490711 | validation: 0.030648591130994874]
	TIME [epoch: 14.1 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02669169139304916		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02669169139304916 | validation: 0.030443816977769822]
	TIME [epoch: 14.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02793640322739903		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02793640322739903 | validation: 0.03182394327564574]
	TIME [epoch: 14.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027463499595047344		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.027463499595047344 | validation: 0.031861218764711235]
	TIME [epoch: 14.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027392745078480267		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.027392745078480267 | validation: 0.03141361541258188]
	TIME [epoch: 14.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028117294495063422		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.028117294495063422 | validation: 0.032457874412870784]
	TIME [epoch: 14.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02644268441335035		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.02644268441335035 | validation: 0.03406938822522978]
	TIME [epoch: 14.1 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026994132511547662		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.026994132511547662 | validation: 0.032776449062420394]
	TIME [epoch: 14.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02626357467342313		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.02626357467342313 | validation: 0.03252150313073155]
	TIME [epoch: 14.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026448625451664882		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.026448625451664882 | validation: 0.036600688436150684]
	TIME [epoch: 14.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027851633340786913		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.027851633340786913 | validation: 0.0307491857130337]
	TIME [epoch: 14.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026442102588562416		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.026442102588562416 | validation: 0.034869054345668435]
	TIME [epoch: 14.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027079721555657722		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.027079721555657722 | validation: 0.034881550732713366]
	TIME [epoch: 14.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026582937974174464		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.026582937974174464 | validation: 0.03355890722009121]
	TIME [epoch: 14.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028193060409943238		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.028193060409943238 | validation: 0.03162431977087197]
	TIME [epoch: 14.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027194422620626428		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.027194422620626428 | validation: 0.03409942588382421]
	TIME [epoch: 14.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02627073481830241		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.02627073481830241 | validation: 0.0338038093369064]
	TIME [epoch: 14.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026457697748414923		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.026457697748414923 | validation: 0.031337188580540486]
	TIME [epoch: 14.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028136195244782412		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.028136195244782412 | validation: 0.03477057484175654]
	TIME [epoch: 14.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027980215692491796		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.027980215692491796 | validation: 0.03409195092274394]
	TIME [epoch: 14.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028364020759785037		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.028364020759785037 | validation: 0.03421500061808259]
	TIME [epoch: 14.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02963502446599479		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02963502446599479 | validation: 0.03443842099582305]
	TIME [epoch: 14.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029169057277699223		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.029169057277699223 | validation: 0.03089955234333333]
	TIME [epoch: 14.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027072044176069003		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.027072044176069003 | validation: 0.03489700731715716]
	TIME [epoch: 14.1 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026408432743722877		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.026408432743722877 | validation: 0.03335397736796476]
	TIME [epoch: 14.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02820357424517847		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.02820357424517847 | validation: 0.03245359130481291]
	TIME [epoch: 14.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026148088209631333		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.026148088209631333 | validation: 0.03288441790179277]
	TIME [epoch: 14.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02718604825442962		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.02718604825442962 | validation: 0.03340602588589534]
	TIME [epoch: 14.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0270642464995675		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.0270642464995675 | validation: 0.0324346995480968]
	TIME [epoch: 14.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027978372736088226		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.027978372736088226 | validation: 0.03293377823638955]
	TIME [epoch: 14.1 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02625111755595646		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02625111755595646 | validation: 0.03502565733486867]
	TIME [epoch: 14.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02648569928564443		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.02648569928564443 | validation: 0.0344740714756361]
	TIME [epoch: 14.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027011841235324872		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.027011841235324872 | validation: 0.03324633933887918]
	TIME [epoch: 14.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027217090608307855		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.027217090608307855 | validation: 0.034354806688451874]
	TIME [epoch: 14.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026186450739511587		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.026186450739511587 | validation: 0.032357623709792664]
	TIME [epoch: 14.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02886957484603945		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.02886957484603945 | validation: 0.031217646470976326]
	TIME [epoch: 14.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02770687779340105		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.02770687779340105 | validation: 0.03270994256617519]
	TIME [epoch: 14.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026548916414623064		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.026548916414623064 | validation: 0.033248759019692646]
	TIME [epoch: 14.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028317421488744998		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.028317421488744998 | validation: 0.0314445245123005]
	TIME [epoch: 14.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02525073381929082		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.02525073381929082 | validation: 0.03229231334781529]
	TIME [epoch: 14.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025980940052754205		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.025980940052754205 | validation: 0.03455236357170271]
	TIME [epoch: 14.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028653853449895662		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.028653853449895662 | validation: 0.031029175598493348]
	TIME [epoch: 14.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0259482309866504		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.0259482309866504 | validation: 0.031862677075392906]
	TIME [epoch: 14.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026836277332968783		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.026836277332968783 | validation: 0.03475311011774252]
	TIME [epoch: 14.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026481325479406195		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.026481325479406195 | validation: 0.033571141339013066]
	TIME [epoch: 14.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027904065709402744		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.027904065709402744 | validation: 0.04899117799138597]
	TIME [epoch: 14.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03423971111647918		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.03423971111647918 | validation: 0.03893609255064489]
	TIME [epoch: 14.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027598397483681952		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.027598397483681952 | validation: 0.03120223678977968]
	TIME [epoch: 14.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02548172498062147		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02548172498062147 | validation: 0.032333867823284235]
	TIME [epoch: 14.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025082767267543424		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.025082767267543424 | validation: 0.03437431697313095]
	TIME [epoch: 14 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02547556782969572		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.02547556782969572 | validation: 0.03402235914116055]
	TIME [epoch: 14 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024528073149916473		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.024528073149916473 | validation: 0.03271006565459686]
	TIME [epoch: 14.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026688115155437765		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.026688115155437765 | validation: 0.031766868700685236]
	TIME [epoch: 14 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026320556399820408		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.026320556399820408 | validation: 0.03452303722800802]
	TIME [epoch: 14.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024676142805963183		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.024676142805963183 | validation: 0.031852592767145]
	TIME [epoch: 14.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02708420955188945		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.02708420955188945 | validation: 0.03306606073637456]
	TIME [epoch: 14 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028330885954232932		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.028330885954232932 | validation: 0.02983339821095113]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025944698772487885		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.025944698772487885 | validation: 0.03226509239591212]
	TIME [epoch: 14.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026875820810922352		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.026875820810922352 | validation: 0.03369312233400068]
	TIME [epoch: 14.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025536522530971613		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.025536522530971613 | validation: 0.03199237330796094]
	TIME [epoch: 14.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02718087487053522		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.02718087487053522 | validation: 0.03366577848863387]
	TIME [epoch: 14.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0257809072426712		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.0257809072426712 | validation: 0.035358144595280624]
	TIME [epoch: 14.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02734842022108859		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.02734842022108859 | validation: 0.031990156070285894]
	TIME [epoch: 14.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027306874488048968		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.027306874488048968 | validation: 0.03170660203906537]
	TIME [epoch: 14.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02656934230415943		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.02656934230415943 | validation: 0.03098287146297715]
	TIME [epoch: 14.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025520732710765506		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.025520732710765506 | validation: 0.03237462280250628]
	TIME [epoch: 14.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026795717816030847		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.026795717816030847 | validation: 0.03105397492366826]
	TIME [epoch: 14 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025955024606013073		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.025955024606013073 | validation: 0.0312693673339431]
	TIME [epoch: 14.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02950881510918757		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.02950881510918757 | validation: 0.033543864803223536]
	TIME [epoch: 14.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02759666044587731		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.02759666044587731 | validation: 0.033085013578800754]
	TIME [epoch: 14.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02669796311374687		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.02669796311374687 | validation: 0.031808230611509246]
	TIME [epoch: 14.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02678793026517342		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.02678793026517342 | validation: 0.032826433310017736]
	TIME [epoch: 14.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02734212217609938		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.02734212217609938 | validation: 0.030973244048369528]
	TIME [epoch: 14 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02508019206650556		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.02508019206650556 | validation: 0.03271411525605359]
	TIME [epoch: 14.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0276267307255132		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.0276267307255132 | validation: 0.029284683792817214]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02544946340407175		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.02544946340407175 | validation: 0.03305931012265632]
	TIME [epoch: 14.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02631376986343812		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.02631376986343812 | validation: 0.03285485474704481]
	TIME [epoch: 14.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02757513682266516		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.02757513682266516 | validation: 0.034581666074476484]
	TIME [epoch: 14 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026613682188743062		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.026613682188743062 | validation: 0.0327538732135097]
	TIME [epoch: 14.1 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025475884947296933		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.025475884947296933 | validation: 0.028908961484018447]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027423921023827097		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.027423921023827097 | validation: 0.0363856437407542]
	TIME [epoch: 14 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026338148012032908		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.026338148012032908 | validation: 0.03025295225765804]
	TIME [epoch: 14 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026516009534086483		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.026516009534086483 | validation: 0.030717986703580825]
	TIME [epoch: 14.1 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024835418551664608		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.024835418551664608 | validation: 0.03053127710494313]
	TIME [epoch: 14.1 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02543236640135456		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.02543236640135456 | validation: 0.03120823305491743]
	TIME [epoch: 14 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025562955894151883		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.025562955894151883 | validation: 0.03188725589125559]
	TIME [epoch: 14.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02567273521737334		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.02567273521737334 | validation: 0.030962885958599556]
	TIME [epoch: 14 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026855004825904354		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.026855004825904354 | validation: 0.032334405908570965]
	TIME [epoch: 14.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027479535788951754		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.027479535788951754 | validation: 0.036763385027558694]
	TIME [epoch: 14 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026944576318475293		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.026944576318475293 | validation: 0.02903871223298778]
	TIME [epoch: 14.1 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026667731502207415		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.026667731502207415 | validation: 0.031444469480934614]
	TIME [epoch: 14.1 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024433700674745998		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.024433700674745998 | validation: 0.031163032737899388]
	TIME [epoch: 14.1 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026933920502012827		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.026933920502012827 | validation: 0.03023191086737383]
	TIME [epoch: 14.1 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025879523974343014		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.025879523974343014 | validation: 0.03290476766012979]
	TIME [epoch: 14.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02503599588463185		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.02503599588463185 | validation: 0.03588526558736713]
	TIME [epoch: 14.1 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024876791562016116		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.024876791562016116 | validation: 0.035554777462737705]
	TIME [epoch: 14 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02516056629599598		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.02516056629599598 | validation: 0.028835615732607796]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025974493183332555		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.025974493183332555 | validation: 0.03213081340868872]
	TIME [epoch: 14.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02718307486992848		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.02718307486992848 | validation: 0.031447662574913986]
	TIME [epoch: 14.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025538344384080713		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.025538344384080713 | validation: 0.0370351347400235]
	TIME [epoch: 14.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029231606933879672		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.029231606933879672 | validation: 0.03675193782404492]
	TIME [epoch: 14 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025129015683295314		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.025129015683295314 | validation: 0.029321786874526808]
	TIME [epoch: 14.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02590840334909031		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.02590840334909031 | validation: 0.030155362748118055]
	TIME [epoch: 14.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025772913942895864		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.025772913942895864 | validation: 0.03201962142922503]
	TIME [epoch: 14.1 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025985093406195214		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.025985093406195214 | validation: 0.032010615419065615]
	TIME [epoch: 14 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024748952311446925		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.024748952311446925 | validation: 0.02945288980891303]
	TIME [epoch: 14.1 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024301607512363797		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.024301607512363797 | validation: 0.032212422166708214]
	TIME [epoch: 14.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024933785694581836		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.024933785694581836 | validation: 0.03414947724699221]
	TIME [epoch: 14.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023463982288876133		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.023463982288876133 | validation: 0.029148007445819586]
	TIME [epoch: 14.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025038150124324556		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.025038150124324556 | validation: 0.02895451378168598]
	TIME [epoch: 14 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026158452344684013		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.026158452344684013 | validation: 0.038984863804385594]
	TIME [epoch: 14.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029679401511230347		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.029679401511230347 | validation: 0.034951163846478256]
	TIME [epoch: 14.1 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024673431650588627		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.024673431650588627 | validation: 0.031427757121776166]
	TIME [epoch: 14.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02622394786845636		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.02622394786845636 | validation: 0.02970285868750837]
	TIME [epoch: 14.1 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026391167602626595		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.026391167602626595 | validation: 0.03173628952284568]
	TIME [epoch: 14.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02566996495126812		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.02566996495126812 | validation: 0.031025734406390476]
	TIME [epoch: 14.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02513702656924891		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.02513702656924891 | validation: 0.03034577880755031]
	TIME [epoch: 14.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02718950572735103		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.02718950572735103 | validation: 0.03177171968584992]
	TIME [epoch: 14.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026172791264492366		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.026172791264492366 | validation: 0.029977091586753497]
	TIME [epoch: 14 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024851792816197094		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.024851792816197094 | validation: 0.030021918020213197]
	TIME [epoch: 14.1 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026486171781176363		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.026486171781176363 | validation: 0.03172721349849248]
	TIME [epoch: 14.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025237043494303647		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.025237043494303647 | validation: 0.028209798717145924]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025551888339205884		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.025551888339205884 | validation: 0.03064123771521344]
	TIME [epoch: 14.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024619988582903103		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.024619988582903103 | validation: 0.028850862053433474]
	TIME [epoch: 14.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024005235714898508		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.024005235714898508 | validation: 0.03202670150454414]
	TIME [epoch: 14.1 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025889502105865977		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.025889502105865977 | validation: 0.03377903679740334]
	TIME [epoch: 14.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026293415514928444		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.026293415514928444 | validation: 0.029151540837935554]
	TIME [epoch: 14.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025979643553511816		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.025979643553511816 | validation: 0.03007789175545449]
	TIME [epoch: 14.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025158101572552362		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.025158101572552362 | validation: 0.03016009897255797]
	TIME [epoch: 14.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025025245400825808		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.025025245400825808 | validation: 0.029226793195066443]
	TIME [epoch: 14.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025278901983913964		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.025278901983913964 | validation: 0.03152850266661104]
	TIME [epoch: 14.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025621625319803235		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.025621625319803235 | validation: 0.03208585774606448]
	TIME [epoch: 14.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025507573171710343		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.025507573171710343 | validation: 0.030081062135970463]
	TIME [epoch: 14.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025036192067028194		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.025036192067028194 | validation: 0.03646788662220401]
	TIME [epoch: 14.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02819407561487313		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.02819407561487313 | validation: 0.03569704242217318]
	TIME [epoch: 14.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02571991877701993		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.02571991877701993 | validation: 0.032570814353232266]
	TIME [epoch: 14.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02478511816642401		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.02478511816642401 | validation: 0.031063953243506078]
	TIME [epoch: 14.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02530100501261766		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.02530100501261766 | validation: 0.033533623553724586]
	TIME [epoch: 14.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024473290068694746		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.024473290068694746 | validation: 0.031718294745862734]
	TIME [epoch: 14.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02515592782451407		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.02515592782451407 | validation: 0.02955576115537578]
	TIME [epoch: 14.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023921524223821135		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.023921524223821135 | validation: 0.0307989566123595]
	TIME [epoch: 14 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02513600865038482		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.02513600865038482 | validation: 0.0353181343051283]
	TIME [epoch: 14.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029075030990903236		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.029075030990903236 | validation: 0.03561879712747732]
	TIME [epoch: 14.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0292770338788858		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.0292770338788858 | validation: 0.0327549072766056]
	TIME [epoch: 14.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025541481393594127		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.025541481393594127 | validation: 0.03475782306540894]
	TIME [epoch: 14.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02661718170004372		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.02661718170004372 | validation: 0.03102254940830715]
	TIME [epoch: 14.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02439786497081902		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.02439786497081902 | validation: 0.03079923805795385]
	TIME [epoch: 14.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024503014785692227		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.024503014785692227 | validation: 0.029365245835865045]
	TIME [epoch: 14.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024890027421283108		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.024890027421283108 | validation: 0.028700973269057385]
	TIME [epoch: 14.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02488548388261849		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.02488548388261849 | validation: 0.03078993741124706]
	TIME [epoch: 14.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02593399890279728		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.02593399890279728 | validation: 0.030970993646545653]
	TIME [epoch: 14.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025106478739841946		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.025106478739841946 | validation: 0.02907004893458494]
	TIME [epoch: 14.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023872380663262728		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.023872380663262728 | validation: 0.03325276153681989]
	TIME [epoch: 14.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02489289703568441		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.02489289703568441 | validation: 0.034447815550759195]
	TIME [epoch: 14.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02519920221170601		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.02519920221170601 | validation: 0.030867709185396087]
	TIME [epoch: 14.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024083823142418283		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.024083823142418283 | validation: 0.03351234321405142]
	TIME [epoch: 14.1 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02326994311680311		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.02326994311680311 | validation: 0.031159159664999436]
	TIME [epoch: 14.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024522775627007692		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.024522775627007692 | validation: 0.031245198312437324]
	TIME [epoch: 14.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027073465648781984		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.027073465648781984 | validation: 0.028924228557224343]
	TIME [epoch: 14.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02514373118794462		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.02514373118794462 | validation: 0.034461990399021133]
	TIME [epoch: 14.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02426662292984648		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.02426662292984648 | validation: 0.030845962777266367]
	TIME [epoch: 14.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027037045405338068		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.027037045405338068 | validation: 0.033345130767302604]
	TIME [epoch: 14.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025941282515936366		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.025941282515936366 | validation: 0.03039360968996077]
	TIME [epoch: 14.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025132430307744792		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.025132430307744792 | validation: 0.03101989437462766]
	TIME [epoch: 14.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02655992884202134		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.02655992884202134 | validation: 0.031078344938270493]
	TIME [epoch: 14.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023359370036519753		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.023359370036519753 | validation: 0.029218803540989102]
	TIME [epoch: 14.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024838015969038653		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.024838015969038653 | validation: 0.029217876529635606]
	TIME [epoch: 14.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025720087564810118		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.025720087564810118 | validation: 0.027616509249164856]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024773346189986726		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.024773346189986726 | validation: 0.028867560652976965]
	TIME [epoch: 14.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025818060750850864		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.025818060750850864 | validation: 0.029894006730144324]
	TIME [epoch: 14.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025804012167016797		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.025804012167016797 | validation: 0.029239248194680996]
	TIME [epoch: 14.1 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025757107294538284		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.025757107294538284 | validation: 0.029442982838968124]
	TIME [epoch: 14.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025028715018149333		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.025028715018149333 | validation: 0.031406638066810676]
	TIME [epoch: 14.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02424546570464514		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.02424546570464514 | validation: 0.02723134869864845]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02562626095097463		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.02562626095097463 | validation: 0.03207383365177671]
	TIME [epoch: 14.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024938739297094226		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.024938739297094226 | validation: 0.030288436409234665]
	TIME [epoch: 14.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024193639374061084		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.024193639374061084 | validation: 0.032685375589068574]
	TIME [epoch: 14.1 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02545816994416089		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.02545816994416089 | validation: 0.03480657730502617]
	TIME [epoch: 14.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024889827769666147		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.024889827769666147 | validation: 0.03001171710653458]
	TIME [epoch: 14.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0254739348425606		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.0254739348425606 | validation: 0.03199083482408104]
	TIME [epoch: 14.1 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02505826599020821		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.02505826599020821 | validation: 0.031563420123806835]
	TIME [epoch: 14.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025609134423288247		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.025609134423288247 | validation: 0.032330833730124006]
	TIME [epoch: 14.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023893459519303877		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.023893459519303877 | validation: 0.03469902246493434]
	TIME [epoch: 14.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024542544930973593		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.024542544930973593 | validation: 0.03007125491480417]
	TIME [epoch: 14.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024116878595166674		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.024116878595166674 | validation: 0.031052529344252712]
	TIME [epoch: 14.1 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02508268401716444		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.02508268401716444 | validation: 0.030612072712205105]
	TIME [epoch: 14.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025496924257031747		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.025496924257031747 | validation: 0.028990943500263586]
	TIME [epoch: 14.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024848978173625025		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.024848978173625025 | validation: 0.03054649749898305]
	TIME [epoch: 14.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02341364943910211		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.02341364943910211 | validation: 0.02940768624786702]
	TIME [epoch: 14.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026215684148306308		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.026215684148306308 | validation: 0.029944131745971672]
	TIME [epoch: 14.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02478680348341219		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.02478680348341219 | validation: 0.030660953034554544]
	TIME [epoch: 14.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024199915959073406		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.024199915959073406 | validation: 0.030236564146372793]
	TIME [epoch: 14.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025228525710906564		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.025228525710906564 | validation: 0.030316564324302214]
	TIME [epoch: 14.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024893089462465486		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.024893089462465486 | validation: 0.03251531594717016]
	TIME [epoch: 14.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024274516161397898		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.024274516161397898 | validation: 0.03367226113854576]
	TIME [epoch: 14.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024567741171898107		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.024567741171898107 | validation: 0.032876407972521594]
	TIME [epoch: 14.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02608125968107395		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.02608125968107395 | validation: 0.03427997227198919]
	TIME [epoch: 14.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026126129530358023		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.026126129530358023 | validation: 0.03189268757263817]
	TIME [epoch: 14.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024964730995243038		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.024964730995243038 | validation: 0.02976517873346973]
	TIME [epoch: 14.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02546209255403667		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.02546209255403667 | validation: 0.02938938192075996]
	TIME [epoch: 14.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025095081208680703		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.025095081208680703 | validation: 0.0322435158076137]
	TIME [epoch: 14.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025390745857131664		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.025390745857131664 | validation: 0.02968095456697405]
	TIME [epoch: 14.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025148134491137522		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.025148134491137522 | validation: 0.031471702738780104]
	TIME [epoch: 14.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026633796763772123		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.026633796763772123 | validation: 0.031248141184663597]
	TIME [epoch: 14.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024581606322363097		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.024581606322363097 | validation: 0.030529146446521712]
	TIME [epoch: 14.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024160676623652358		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.024160676623652358 | validation: 0.02895893756592643]
	TIME [epoch: 14.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023836635332467986		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.023836635332467986 | validation: 0.030125356955395834]
	TIME [epoch: 14.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024354093654064927		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.024354093654064927 | validation: 0.03149814514920362]
	TIME [epoch: 14.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024697758300775856		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.024697758300775856 | validation: 0.0331816718774381]
	TIME [epoch: 14.1 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023759978164323646		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.023759978164323646 | validation: 0.029267726858474025]
	TIME [epoch: 14.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022628662898827877		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.022628662898827877 | validation: 0.030352753395830413]
	TIME [epoch: 14.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02361961221697768		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.02361961221697768 | validation: 0.028861433497308603]
	TIME [epoch: 14.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024987730454522537		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.024987730454522537 | validation: 0.029251328467593037]
	TIME [epoch: 14.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024268584056303406		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.024268584056303406 | validation: 0.03340064414476631]
	TIME [epoch: 14.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024836830202836852		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.024836830202836852 | validation: 0.03581636074386292]
	TIME [epoch: 14.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024378576359492265		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.024378576359492265 | validation: 0.030485687698214487]
	TIME [epoch: 14 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024559897296631952		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.024559897296631952 | validation: 0.031257112581620186]
	TIME [epoch: 14.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024786213837828613		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.024786213837828613 | validation: 0.03206223963079365]
	TIME [epoch: 14.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02440254846484015		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.02440254846484015 | validation: 0.026766430281855505]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024459061242066437		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.024459061242066437 | validation: 0.02983311945143879]
	TIME [epoch: 14.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024131227378996173		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.024131227378996173 | validation: 0.029514460345227644]
	TIME [epoch: 14.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025118562263343885		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.025118562263343885 | validation: 0.031067295866696182]
	TIME [epoch: 14.1 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02633767240870769		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.02633767240870769 | validation: 0.02669210850173877]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_875.pth
	Model improved!!!
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023994737505880583		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.023994737505880583 | validation: 0.0333153636692922]
	TIME [epoch: 14.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025028555751841652		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.025028555751841652 | validation: 0.030988343001709825]
	TIME [epoch: 14.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024863281243589903		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.024863281243589903 | validation: 0.03248775658730402]
	TIME [epoch: 14.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02411839679315739		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.02411839679315739 | validation: 0.030070282235258303]
	TIME [epoch: 14.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025304415879424762		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.025304415879424762 | validation: 0.029535075333673173]
	TIME [epoch: 14.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025207624353293302		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.025207624353293302 | validation: 0.032499401668250515]
	TIME [epoch: 14.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026538641314387948		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.026538641314387948 | validation: 0.03123938230436085]
	TIME [epoch: 14.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023831783199999006		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.023831783199999006 | validation: 0.03472629488804531]
	TIME [epoch: 14.1 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024991756521088766		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.024991756521088766 | validation: 0.030509046110943645]
	TIME [epoch: 14.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025312042961126102		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.025312042961126102 | validation: 0.030152402068587755]
	TIME [epoch: 14.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025395628573685478		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.025395628573685478 | validation: 0.030095915948302618]
	TIME [epoch: 14.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02499329523780887		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.02499329523780887 | validation: 0.03137086131919208]
	TIME [epoch: 14.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023230066516373077		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.023230066516373077 | validation: 0.03080453889467686]
	TIME [epoch: 14.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025500827257901336		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.025500827257901336 | validation: 0.03017092604844713]
	TIME [epoch: 14.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024814920332919106		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.024814920332919106 | validation: 0.03078428244642503]
	TIME [epoch: 14.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026315370445193623		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.026315370445193623 | validation: 0.03063795719453339]
	TIME [epoch: 14.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02556205979912333		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.02556205979912333 | validation: 0.028760508879593652]
	TIME [epoch: 14.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02567768415616631		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.02567768415616631 | validation: 0.030501317022115983]
	TIME [epoch: 14.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024339394239819914		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.024339394239819914 | validation: 0.029186021133833652]
	TIME [epoch: 14.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024145155615341952		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.024145155615341952 | validation: 0.02971507870096072]
	TIME [epoch: 14.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024359004201139567		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.024359004201139567 | validation: 0.031471003033039305]
	TIME [epoch: 14.1 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025175777021947315		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.025175777021947315 | validation: 0.03003293146062294]
	TIME [epoch: 14.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02388999292733106		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.02388999292733106 | validation: 0.028967849968807847]
	TIME [epoch: 14.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02434350020582452		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.02434350020582452 | validation: 0.029232616850243517]
	TIME [epoch: 14.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024819629737101973		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.024819629737101973 | validation: 0.031119210033260528]
	TIME [epoch: 14.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026646597596466492		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.026646597596466492 | validation: 0.029567544781047336]
	TIME [epoch: 14.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02464563990889837		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.02464563990889837 | validation: 0.02877233871511274]
	TIME [epoch: 14.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025475695378308207		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.025475695378308207 | validation: 0.02906230590133015]
	TIME [epoch: 14.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024455481234397683		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.024455481234397683 | validation: 0.033168463447426404]
	TIME [epoch: 14.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024325265909623577		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.024325265909623577 | validation: 0.03282234432370827]
	TIME [epoch: 14 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024235543078367776		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.024235543078367776 | validation: 0.03053458120083354]
	TIME [epoch: 14.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024393939091063434		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.024393939091063434 | validation: 0.02661641854959905]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025379774899222517		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.025379774899222517 | validation: 0.028586601276201107]
	TIME [epoch: 14.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02261244915432497		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.02261244915432497 | validation: 0.03181183440553317]
	TIME [epoch: 14.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02355694058758283		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.02355694058758283 | validation: 0.029973141941626848]
	TIME [epoch: 14.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02526226315166756		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.02526226315166756 | validation: 0.029634075386964012]
	TIME [epoch: 14.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02528412595725689		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.02528412595725689 | validation: 0.03169677268843772]
	TIME [epoch: 14.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02480231092512317		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.02480231092512317 | validation: 0.03151266100008997]
	TIME [epoch: 14.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02365235310845739		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.02365235310845739 | validation: 0.029503992780361615]
	TIME [epoch: 14.1 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02353468536075235		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.02353468536075235 | validation: 0.029748429872644755]
	TIME [epoch: 14.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02395570512568779		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.02395570512568779 | validation: 0.03133104437434749]
	TIME [epoch: 14.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0249813832572627		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.0249813832572627 | validation: 0.0317990910692705]
	TIME [epoch: 14.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025173807960218047		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.025173807960218047 | validation: 0.031051076288079765]
	TIME [epoch: 14.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024105149607734636		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.024105149607734636 | validation: 0.03200696953210488]
	TIME [epoch: 14.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025851782816683558		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.025851782816683558 | validation: 0.03006511727464849]
	TIME [epoch: 14.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025063822382012213		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.025063822382012213 | validation: 0.030935980096850793]
	TIME [epoch: 14.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024163637837547505		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.024163637837547505 | validation: 0.03262434114259736]
	TIME [epoch: 14.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023698265646621707		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.023698265646621707 | validation: 0.0331787325425143]
	TIME [epoch: 14.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024003439142800675		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.024003439142800675 | validation: 0.028717566281294078]
	TIME [epoch: 14.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02414981074900653		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.02414981074900653 | validation: 0.03277377246866929]
	TIME [epoch: 14.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0251737237252547		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.0251737237252547 | validation: 0.032950564631041436]
	TIME [epoch: 14.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024762157008476865		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.024762157008476865 | validation: 0.029724343587565667]
	TIME [epoch: 14.1 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02398545582450928		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.02398545582450928 | validation: 0.030627934388581385]
	TIME [epoch: 14.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02290185652534521		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.02290185652534521 | validation: 0.031221258326489087]
	TIME [epoch: 14.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024321042858858985		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.024321042858858985 | validation: 0.03004898911920652]
	TIME [epoch: 14.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024408006591416745		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.024408006591416745 | validation: 0.030875411120681123]
	TIME [epoch: 14.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023214396866599596		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.023214396866599596 | validation: 0.029005309533816737]
	TIME [epoch: 14.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024104896051743714		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.024104896051743714 | validation: 0.031460677636407294]
	TIME [epoch: 14.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0245819694243211		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.0245819694243211 | validation: 0.029152981592726508]
	TIME [epoch: 14.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02434959251078535		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.02434959251078535 | validation: 0.0301434156707747]
	TIME [epoch: 14.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024330900969533865		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.024330900969533865 | validation: 0.03040352981549345]
	TIME [epoch: 14.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026130033970757163		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.026130033970757163 | validation: 0.029233578114659816]
	TIME [epoch: 14.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02421281142939966		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.02421281142939966 | validation: 0.028236563571996755]
	TIME [epoch: 14.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024970812986366962		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.024970812986366962 | validation: 0.031012309621175977]
	TIME [epoch: 14.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025062527382324094		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.025062527382324094 | validation: 0.029452239199564818]
	TIME [epoch: 14.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025283846356562167		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.025283846356562167 | validation: 0.02823719266118681]
	TIME [epoch: 14.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02401017128012465		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.02401017128012465 | validation: 0.02946832172888368]
	TIME [epoch: 14.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024083765102570764		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.024083765102570764 | validation: 0.028761560413389994]
	TIME [epoch: 14.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024608024709243437		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.024608024709243437 | validation: 0.030284295175839044]
	TIME [epoch: 14.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023743022169625264		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.023743022169625264 | validation: 0.03028610442725051]
	TIME [epoch: 14.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023336462201468282		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.023336462201468282 | validation: 0.030597486391430402]
	TIME [epoch: 14.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023999522112069044		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.023999522112069044 | validation: 0.032193494349368655]
	TIME [epoch: 14.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02444987974192162		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.02444987974192162 | validation: 0.030080881712816962]
	TIME [epoch: 14.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025628353749983276		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.025628353749983276 | validation: 0.03056162464995893]
	TIME [epoch: 14.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02432469911795258		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.02432469911795258 | validation: 0.031211146759441716]
	TIME [epoch: 14.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024812682353827296		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.024812682353827296 | validation: 0.028452924351624515]
	TIME [epoch: 14.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02526049488624842		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.02526049488624842 | validation: 0.0283621880568476]
	TIME [epoch: 14.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02480230140874986		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.02480230140874986 | validation: 0.028710164739461946]
	TIME [epoch: 14.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023495071169068547		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.023495071169068547 | validation: 0.029864408592842513]
	TIME [epoch: 14.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02359337013815131		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.02359337013815131 | validation: 0.029965073666344634]
	TIME [epoch: 14.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026029365403212476		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.026029365403212476 | validation: 0.030414252632406963]
	TIME [epoch: 14.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024222624003484856		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.024222624003484856 | validation: 0.029129081963268022]
	TIME [epoch: 14.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025945814534697104		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.025945814534697104 | validation: 0.029292323879633583]
	TIME [epoch: 14.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024046095612637604		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.024046095612637604 | validation: 0.0269651457883426]
	TIME [epoch: 14.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024588258234093177		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.024588258234093177 | validation: 0.028990002419096036]
	TIME [epoch: 14.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023795451381018334		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.023795451381018334 | validation: 0.03086022865451843]
	TIME [epoch: 14.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023857846252265603		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.023857846252265603 | validation: 0.02836716593355338]
	TIME [epoch: 14.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026532686321731336		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.026532686321731336 | validation: 0.030059844358965717]
	TIME [epoch: 14.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024490048777444574		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.024490048777444574 | validation: 0.030163560772640152]
	TIME [epoch: 14.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024029281476239395		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.024029281476239395 | validation: 0.03122506076084875]
	TIME [epoch: 14.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02381895444895462		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.02381895444895462 | validation: 0.03094515373078747]
	TIME [epoch: 14.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025063068562510386		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.025063068562510386 | validation: 0.030110764790328988]
	TIME [epoch: 14.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024007137456973127		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.024007137456973127 | validation: 0.031677750446650076]
	TIME [epoch: 14.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02423116577309673		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.02423116577309673 | validation: 0.031071170200576266]
	TIME [epoch: 14.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025964314760970605		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.025964314760970605 | validation: 0.030594476372195725]
	TIME [epoch: 14.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023764794170478193		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.023764794170478193 | validation: 0.029108937415406173]
	TIME [epoch: 14.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02366068409666731		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.02366068409666731 | validation: 0.029153216658802147]
	TIME [epoch: 14.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023784786664752266		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.023784786664752266 | validation: 0.03306313532463143]
	TIME [epoch: 14.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02335244725139103		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.02335244725139103 | validation: 0.029434874150952358]
	TIME [epoch: 14.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023914427442127902		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.023914427442127902 | validation: 0.03162262829337192]
	TIME [epoch: 14.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02325705698001594		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.02325705698001594 | validation: 0.02912651943513174]
	TIME [epoch: 14.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024440023686470434		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.024440023686470434 | validation: 0.029617594932154115]
	TIME [epoch: 14.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024471861388574722		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.024471861388574722 | validation: 0.028628446826710254]
	TIME [epoch: 14.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02338024084963904		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.02338024084963904 | validation: 0.03320637389620728]
	TIME [epoch: 14.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023245065621438854		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.023245065621438854 | validation: 0.027571899846365124]
	TIME [epoch: 14.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02421470133565916		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.02421470133565916 | validation: 0.02879609883266851]
	TIME [epoch: 14.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024868163612144122		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.024868163612144122 | validation: 0.02836659919634667]
	TIME [epoch: 14.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023667378689807526		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.023667378689807526 | validation: 0.029382772349634646]
	TIME [epoch: 14.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02379264534418523		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.02379264534418523 | validation: 0.028967797763649362]
	TIME [epoch: 14.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024004447437890077		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.024004447437890077 | validation: 0.029363178558964694]
	TIME [epoch: 14.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02421724599729743		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.02421724599729743 | validation: 0.029068296711784326]
	TIME [epoch: 14.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024771236106446444		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.024771236106446444 | validation: 0.030284468264582566]
	TIME [epoch: 14.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023152424994832395		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.023152424994832395 | validation: 0.029902950259796942]
	TIME [epoch: 14.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024937508786797624		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.024937508786797624 | validation: 0.028471623867514347]
	TIME [epoch: 14.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026381500620590394		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.026381500620590394 | validation: 0.02924207749457878]
	TIME [epoch: 14.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023393315838889057		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.023393315838889057 | validation: 0.029228124832904507]
	TIME [epoch: 14.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024229339603605616		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.024229339603605616 | validation: 0.028858224524095533]
	TIME [epoch: 14.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024043683580816697		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.024043683580816697 | validation: 0.030599201789339947]
	TIME [epoch: 14.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024475837819148562		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.024475837819148562 | validation: 0.028365964249354576]
	TIME [epoch: 14.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024106940075210997		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.024106940075210997 | validation: 0.029618803435527585]
	TIME [epoch: 14.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023897293833012152		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.023897293833012152 | validation: 0.02971786720778484]
	TIME [epoch: 14.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024481400291159656		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.024481400291159656 | validation: 0.03208719440385917]
	TIME [epoch: 14.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02229931787829883		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.02229931787829883 | validation: 0.026800888309547508]
	TIME [epoch: 14.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023869403636403233		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.023869403636403233 | validation: 0.03277900184606063]
	TIME [epoch: 14.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0243761950141316		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.0243761950141316 | validation: 0.02785031316009175]
	TIME [epoch: 14.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023438459294699544		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.023438459294699544 | validation: 0.029187632661603086]
	TIME [epoch: 131 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022565263997898516		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.022565263997898516 | validation: 0.030351201622711894]
	TIME [epoch: 30.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023933192839576456		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.023933192839576456 | validation: 0.030775158185741842]
	TIME [epoch: 30.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02472791863297228		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.02472791863297228 | validation: 0.028466725099006954]
	TIME [epoch: 30.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024431221532769373		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.024431221532769373 | validation: 0.03262693792016322]
	TIME [epoch: 30.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02400909746485729		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.02400909746485729 | validation: 0.02930565595934906]
	TIME [epoch: 30.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02337128086601293		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.02337128086601293 | validation: 0.027811647763739164]
	TIME [epoch: 30.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023616055479348266		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.023616055479348266 | validation: 0.028948455270506598]
	TIME [epoch: 30.4 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193832/states/model_phi1_2b_v_mmd1_1008.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10385.584 seconds.
