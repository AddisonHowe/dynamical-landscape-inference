Args:
Namespace(name='model_phi1_2c_v_mmd1', outdir='out/model_training/model_phi1_2c_v_mmd1', training_data='data/training_data/basic/data_phi1_2c/training', validation_data='data/training_data/basic/data_phi1_2c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1675714999

Training model...

Saving initial model state to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.921299568677915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.921299568677915 | validation: 4.637896700546683]
	TIME [epoch: 109 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.502639305657172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.502639305657172 | validation: 3.853062850746978]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.743666017416216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.743666017416216 | validation: 3.96854719993282]
	TIME [epoch: 6.57 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.514483341916356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.514483341916356 | validation: 3.59653182812193]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.452017574514067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.452017574514067 | validation: 3.239645533140891]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0657453723569272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0657453723569272 | validation: 2.738820214819797]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9059853353577116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9059853353577116 | validation: 2.8248520277237392]
	TIME [epoch: 6.6 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.736215651623032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.736215651623032 | validation: 2.551549754135113]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4616404456083725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4616404456083725 | validation: 2.351593698871831]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4573674673471806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4573674673471806 | validation: 2.421884629483654]
	TIME [epoch: 6.59 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4387929673867195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4387929673867195 | validation: 2.1840880326402803]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.240214418336363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.240214418336363 | validation: 2.155357331688381]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.379944461449616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.379944461449616 | validation: 2.1529417506954296]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.236776373558561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.236776373558561 | validation: 2.0478686739419545]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1964687607815327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1964687607815327 | validation: 2.093321247645337]
	TIME [epoch: 6.56 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.244300378574954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.244300378574954 | validation: 1.9977721073797785]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.022308780321798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.022308780321798 | validation: 1.9318786381441555]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0655235906326785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0655235906326785 | validation: 1.925996508741391]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.047134589654664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047134589654664 | validation: 1.878287358588285]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9003066324654467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9003066324654467 | validation: 1.7888199401496288]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8332079437819633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8332079437819633 | validation: 1.749064186422566]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9031794458198505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9031794458198505 | validation: 1.976361936438638]
	TIME [epoch: 6.61 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9494289482791234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9494289482791234 | validation: 1.740105896822476]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8135272411861312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8135272411861312 | validation: 1.6663333732458996]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6282475648243295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6282475648243295 | validation: 1.6083367047871235]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5926746666636606		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.5926746666636606 | validation: 1.6328991267616346]
	TIME [epoch: 6.57 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.645895536894727		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.645895536894727 | validation: 1.5153163892811519]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5967522094132032		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.5967522094132032 | validation: 1.453454984148345]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5759907068517167		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5759907068517167 | validation: 1.6743761577266043]
	TIME [epoch: 6.58 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5922066902729395		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.5922066902729395 | validation: 1.4106607253008765]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4368417660467814		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.4368417660467814 | validation: 1.4152566677823952]
	TIME [epoch: 6.58 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4229922115838742		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4229922115838742 | validation: 1.3695322668424696]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4550148608100821		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.4550148608100821 | validation: 1.4572013458296826]
	TIME [epoch: 6.62 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5113629529144275		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.5113629529144275 | validation: 1.563246004093424]
	TIME [epoch: 6.6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5351831504111635		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.5351831504111635 | validation: 1.4438057063617862]
	TIME [epoch: 6.59 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4202370238123114		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.4202370238123114 | validation: 1.3272204523260624]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3872092351045664		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.3872092351045664 | validation: 1.5202421256454324]
	TIME [epoch: 6.58 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.501346400566331		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.501346400566331 | validation: 1.3213492882294027]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.330713562292833		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.330713562292833 | validation: 1.414814287110312]
	TIME [epoch: 6.6 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.38179965915096		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.38179965915096 | validation: 1.3219114470887732]
	TIME [epoch: 6.59 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3498297507440782		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3498297507440782 | validation: 1.3321440004870573]
	TIME [epoch: 6.59 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3896425338817582		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.3896425338817582 | validation: 1.3025341486922297]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.357609969648871		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.357609969648871 | validation: 1.3184307125529544]
	TIME [epoch: 6.59 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2971307245485817		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2971307245485817 | validation: 1.4236974241958285]
	TIME [epoch: 6.62 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3067642494234775		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3067642494234775 | validation: 1.3430510042638801]
	TIME [epoch: 6.59 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.540879083462539		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.540879083462539 | validation: 1.497983402029929]
	TIME [epoch: 6.59 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4223101531268312		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4223101531268312 | validation: 1.3131753550036676]
	TIME [epoch: 6.58 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2934983800307245		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2934983800307245 | validation: 1.3276851531574083]
	TIME [epoch: 6.58 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3103822582435043		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.3103822582435043 | validation: 1.391812048809582]
	TIME [epoch: 6.59 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3278721216052518		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.3278721216052518 | validation: 1.3226150649992645]
	TIME [epoch: 6.62 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2873248656465845		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2873248656465845 | validation: 1.2547285903487186]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.286601739788218		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.286601739788218 | validation: 1.3895003673234887]
	TIME [epoch: 6.59 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2878359160369015		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2878359160369015 | validation: 1.27148281474844]
	TIME [epoch: 6.59 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5011004659839853		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5011004659839853 | validation: 1.5122582109210276]
	TIME [epoch: 6.58 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3790718325639322		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.3790718325639322 | validation: 1.5367680224563487]
	TIME [epoch: 6.6 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4020266971310482		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4020266971310482 | validation: 1.4336983106773806]
	TIME [epoch: 6.61 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3517941968172298		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3517941968172298 | validation: 1.3525368541639644]
	TIME [epoch: 6.59 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3226957243722193		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.3226957243722193 | validation: 1.2639851816723287]
	TIME [epoch: 6.58 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2484719174410737		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2484719174410737 | validation: 1.2727450870755126]
	TIME [epoch: 6.58 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.280370255055957		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.280370255055957 | validation: 1.289653130352517]
	TIME [epoch: 6.58 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2214617461178356		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2214617461178356 | validation: 1.2534180644651778]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6315423178943511		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.6315423178943511 | validation: 1.6361701937061914]
	TIME [epoch: 6.6 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4966321283260997		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4966321283260997 | validation: 1.4012928598019738]
	TIME [epoch: 6.59 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3329414791218048		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.3329414791218048 | validation: 1.267405357248772]
	TIME [epoch: 6.58 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2486581593239243		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2486581593239243 | validation: 1.2779371612414192]
	TIME [epoch: 6.58 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2642745315159787		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.2642745315159787 | validation: 1.270639656354424]
	TIME [epoch: 6.6 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2467967681497711		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2467967681497711 | validation: 1.250435547034143]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2653073314746863		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2653073314746863 | validation: 1.3257686760975915]
	TIME [epoch: 6.59 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.29641534386732		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.29641534386732 | validation: 1.2933585718619391]
	TIME [epoch: 6.6 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3531394247361226		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3531394247361226 | validation: 1.288318908270513]
	TIME [epoch: 6.59 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2445644528524535		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2445644528524535 | validation: 1.4408656031503833]
	TIME [epoch: 6.58 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3476851429249233		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.3476851429249233 | validation: 1.2766677282535277]
	TIME [epoch: 6.63 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2338637029459978		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2338637029459978 | validation: 1.4469582671859529]
	TIME [epoch: 6.6 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.379839578639038		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.379839578639038 | validation: 1.2788936129623942]
	TIME [epoch: 6.59 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4904179185800117		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.4904179185800117 | validation: 1.496413034944339]
	TIME [epoch: 6.58 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.389234547210028		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.389234547210028 | validation: 1.2511217947445752]
	TIME [epoch: 6.6 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2438321397700607		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2438321397700607 | validation: 1.295496895263967]
	TIME [epoch: 6.58 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.273350542699819		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.273350542699819 | validation: 1.3939498068782308]
	TIME [epoch: 6.62 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.39647560335996		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.39647560335996 | validation: 1.3266080188839273]
	TIME [epoch: 6.59 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2403344901855227		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2403344901855227 | validation: 1.2556544909732616]
	TIME [epoch: 6.59 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2206346114138853		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.2206346114138853 | validation: 1.2287536994757464]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2082578574697218		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.2082578574697218 | validation: 1.2451935340153943]
	TIME [epoch: 6.57 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2085191490427145		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2085191490427145 | validation: 1.2844482835625333]
	TIME [epoch: 6.56 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2222277370382264		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.2222277370382264 | validation: 1.1990363194054365]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2173676237781703		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2173676237781703 | validation: 1.8524216864328267]
	TIME [epoch: 6.58 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.749875985507405		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.749875985507405 | validation: 1.5931804906942082]
	TIME [epoch: 6.57 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5438402678812877		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.5438402678812877 | validation: 1.3058980799300115]
	TIME [epoch: 6.58 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2465113741395242		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.2465113741395242 | validation: 1.3067423219364516]
	TIME [epoch: 6.58 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6996926733865965		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.6996926733865965 | validation: 2.2646381253463357]
	TIME [epoch: 6.6 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8868110405806848		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.8868110405806848 | validation: 1.3277061616254775]
	TIME [epoch: 6.57 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3101844439484882		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3101844439484882 | validation: 1.3083476249004684]
	TIME [epoch: 6.58 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2652954332726232		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2652954332726232 | validation: 1.2490919488913128]
	TIME [epoch: 6.58 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2276363456366512		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2276363456366512 | validation: 1.2494009674066773]
	TIME [epoch: 6.58 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.228036774344703		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.228036774344703 | validation: 1.3287415276815986]
	TIME [epoch: 6.58 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3074983879406719		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3074983879406719 | validation: 1.316105909765037]
	TIME [epoch: 6.61 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2564453241601952		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2564453241601952 | validation: 1.233261227840766]
	TIME [epoch: 6.57 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1969170761489947		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1969170761489947 | validation: 1.2457345768235801]
	TIME [epoch: 6.58 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.178237060835417		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.178237060835417 | validation: 1.2466065776723494]
	TIME [epoch: 6.58 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.220767304383098		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.220767304383098 | validation: 1.2619125791099095]
	TIME [epoch: 6.58 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2371798488315273		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2371798488315273 | validation: 1.3140214957873815]
	TIME [epoch: 6.58 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.283797406110502		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.283797406110502 | validation: 1.2816894647616512]
	TIME [epoch: 6.61 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.228668867115863		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.228668867115863 | validation: 1.2502673777594149]
	TIME [epoch: 6.58 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1773937634313092		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.1773937634313092 | validation: 1.2789392514000686]
	TIME [epoch: 6.59 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2253708738831182		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.2253708738831182 | validation: 1.234357831499477]
	TIME [epoch: 6.59 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1568784211833667		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1568784211833667 | validation: 1.2334308050535885]
	TIME [epoch: 6.59 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1333134884187683		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.1333134884187683 | validation: 1.2551919534864973]
	TIME [epoch: 6.62 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3169597592361877		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.3169597592361877 | validation: 1.2973925170672729]
	TIME [epoch: 6.6 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.198140324030759		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.198140324030759 | validation: 1.2417307799924318]
	TIME [epoch: 6.59 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1931872537473058		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1931872537473058 | validation: 2.5000894408663936]
	TIME [epoch: 6.58 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.505351244264981		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 2.505351244264981 | validation: 2.2409418270144887]
	TIME [epoch: 6.58 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8043918629105118		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.8043918629105118 | validation: 1.3539908895638895]
	TIME [epoch: 6.6 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.288939994302717		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.288939994302717 | validation: 1.2771072021436047]
	TIME [epoch: 6.62 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.229919454034881		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.229919454034881 | validation: 1.2568001178938328]
	TIME [epoch: 6.59 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1595995739013698		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1595995739013698 | validation: 1.41935779312034]
	TIME [epoch: 6.59 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3480249157627056		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.3480249157627056 | validation: 1.5256314028536773]
	TIME [epoch: 6.58 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4304928577314011		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.4304928577314011 | validation: 1.3903079658969528]
	TIME [epoch: 6.59 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2959767621007674		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.2959767621007674 | validation: 1.2765125811151838]
	TIME [epoch: 6.6 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2382547708486307		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2382547708486307 | validation: 1.2539279457664199]
	TIME [epoch: 6.61 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3862074127678214		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.3862074127678214 | validation: 1.7473664930948858]
	TIME [epoch: 6.58 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4968249572405292		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.4968249572405292 | validation: 1.3025514724894105]
	TIME [epoch: 6.59 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3261175666126697		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 1.3261175666126697 | validation: 1.438108474726824]
	TIME [epoch: 6.59 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.378187437740248		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.378187437740248 | validation: 1.3556313162483973]
	TIME [epoch: 6.59 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2724693736888693		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.2724693736888693 | validation: 1.2820474177980343]
	TIME [epoch: 6.62 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2319020260397269		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.2319020260397269 | validation: 1.2669693343083186]
	TIME [epoch: 6.59 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1887836385023491		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1887836385023491 | validation: 1.2493590673879575]
	TIME [epoch: 6.59 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1937205008334009		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1937205008334009 | validation: 1.358175390715899]
	TIME [epoch: 6.58 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3178595764847627		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.3178595764847627 | validation: 1.346406129097305]
	TIME [epoch: 6.59 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2319585321008515		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.2319585321008515 | validation: 1.320113607925337]
	TIME [epoch: 6.6 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.286475573416367		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.286475573416367 | validation: 1.3257880907542547]
	TIME [epoch: 6.62 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.234467506256252		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.234467506256252 | validation: 1.244840065828595]
	TIME [epoch: 6.59 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1820634414619604		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.1820634414619604 | validation: 1.2344589627632003]
	TIME [epoch: 6.59 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1677820075224175		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1677820075224175 | validation: 1.2490582523058513]
	TIME [epoch: 6.59 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1832023901229238		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.1832023901229238 | validation: 1.2392564645378858]
	TIME [epoch: 6.59 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1863236056601973		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.1863236056601973 | validation: 1.2499589916567375]
	TIME [epoch: 6.6 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.18318579930965		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.18318579930965 | validation: 1.2219292529016865]
	TIME [epoch: 6.62 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.180032265144161		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.180032265144161 | validation: 1.2498027601458883]
	TIME [epoch: 6.59 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2291041137540368		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.2291041137540368 | validation: 1.2424732100575056]
	TIME [epoch: 6.58 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1927034639931187		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1927034639931187 | validation: 1.2550467021916676]
	TIME [epoch: 6.59 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2003077576773769		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.2003077576773769 | validation: 1.2199779362551872]
	TIME [epoch: 6.6 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1922321456202796		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.1922321456202796 | validation: 1.224439803530379]
	TIME [epoch: 6.62 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1883250667131329		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1883250667131329 | validation: 1.2287815562348516]
	TIME [epoch: 6.6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.237335065118014		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.237335065118014 | validation: 1.2396331677370807]
	TIME [epoch: 6.59 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2159362538329417		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.2159362538329417 | validation: 1.268466837688757]
	TIME [epoch: 6.59 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.19960682006418		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.19960682006418 | validation: 1.2158654428944793]
	TIME [epoch: 6.59 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2207837929574956		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.2207837929574956 | validation: 1.2656397153907504]
	TIME [epoch: 6.6 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1852814730107863		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.1852814730107863 | validation: 1.2181304544905125]
	TIME [epoch: 6.63 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.157828570728316		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.157828570728316 | validation: 1.2373082930060146]
	TIME [epoch: 6.6 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1690424671290982		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.1690424671290982 | validation: 1.3237679540367324]
	TIME [epoch: 6.59 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2378177897427864		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.2378177897427864 | validation: 1.2106313379075977]
	TIME [epoch: 6.59 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1567181566864915		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.1567181566864915 | validation: 1.3056191942641533]
	TIME [epoch: 6.59 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2930682830701632		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.2930682830701632 | validation: 1.2657141929328404]
	TIME [epoch: 6.61 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1911309785500923		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1911309785500923 | validation: 1.210680550788519]
	TIME [epoch: 6.61 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1516907748445258		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1516907748445258 | validation: 1.2141700904882204]
	TIME [epoch: 6.59 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1342155132604572		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.1342155132604572 | validation: 1.2086219455377183]
	TIME [epoch: 6.59 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1512642085038869		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.1512642085038869 | validation: 1.3469376584578132]
	TIME [epoch: 6.59 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2502314346909467		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.2502314346909467 | validation: 1.1985249669986062]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1332056184242107		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.1332056184242107 | validation: 1.199000194135977]
	TIME [epoch: 6.63 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1377725917955441		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1377725917955441 | validation: 1.177688251169303]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.113263569903146		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.113263569903146 | validation: 1.1907189900477007]
	TIME [epoch: 6.58 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2013316269610335		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.2013316269610335 | validation: 1.2757637540790614]
	TIME [epoch: 6.57 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.18958731674237		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.18958731674237 | validation: 1.2059993949337156]
	TIME [epoch: 6.57 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.190423047945631		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.190423047945631 | validation: 1.188527161451174]
	TIME [epoch: 6.58 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.148172504476893		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.148172504476893 | validation: 1.1854672048956776]
	TIME [epoch: 6.59 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1080433276849022		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.1080433276849022 | validation: 1.3877780418975911]
	TIME [epoch: 6.57 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3004212741061432		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.3004212741061432 | validation: 1.1874980422024242]
	TIME [epoch: 6.57 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1118023869006115		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1118023869006115 | validation: 1.175418098221634]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1086773830669223		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.1086773830669223 | validation: 1.17780072041942]
	TIME [epoch: 6.59 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1338182388499567		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.1338182388499567 | validation: 1.1705649103809657]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.084789354619314		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.084789354619314 | validation: 1.176548137087276]
	TIME [epoch: 6.59 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1114996273300677		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.1114996273300677 | validation: 1.169435211678718]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0954722969063893		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0954722969063893 | validation: 1.223355977745361]
	TIME [epoch: 6.6 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1202927192213137		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.1202927192213137 | validation: 1.1469812225579386]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0414150338105432		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0414150338105432 | validation: 1.1723679592012635]
	TIME [epoch: 6.59 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.091214184953364		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.091214184953364 | validation: 1.1444238324447562]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0816550731317973		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.0816550731317973 | validation: 1.1453337752921904]
	TIME [epoch: 6.58 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0489824983197054		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.0489824983197054 | validation: 1.1557761791442136]
	TIME [epoch: 6.56 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.409180590428971		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.409180590428971 | validation: 1.653295411158241]
	TIME [epoch: 6.58 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.578516540384087		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.578516540384087 | validation: 1.1917069167141667]
	TIME [epoch: 6.57 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.112321018364855		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.112321018364855 | validation: 1.1824005983478136]
	TIME [epoch: 6.61 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.082102358221146		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.082102358221146 | validation: 1.157070967085239]
	TIME [epoch: 6.58 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0484894995784975		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.0484894995784975 | validation: 1.1996310165577424]
	TIME [epoch: 6.56 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0742677266829916		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0742677266829916 | validation: 1.1792074879087917]
	TIME [epoch: 6.57 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.100981980082618		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.100981980082618 | validation: 1.259890302604097]
	TIME [epoch: 6.56 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1186361362560127		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.1186361362560127 | validation: 1.1991297607148803]
	TIME [epoch: 6.58 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1060043933873884		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.1060043933873884 | validation: 1.176918153643953]
	TIME [epoch: 6.6 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1127932604560429		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.1127932604560429 | validation: 1.1442181360378008]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0447138225989614		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.0447138225989614 | validation: 1.11589808048939]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0528276383035446		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.0528276383035446 | validation: 1.1413156104108475]
	TIME [epoch: 6.59 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0385593503657011		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0385593503657011 | validation: 1.1161010753055773]
	TIME [epoch: 6.59 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0654027916391247		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0654027916391247 | validation: 1.1161890395102343]
	TIME [epoch: 6.61 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0434545576853647		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.0434545576853647 | validation: 1.1407401920545641]
	TIME [epoch: 6.58 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0534118745409247		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0534118745409247 | validation: 1.1226825349612084]
	TIME [epoch: 6.59 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0697349272998262		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.0697349272998262 | validation: 1.1086356630341505]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.02236908927911		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.02236908927911 | validation: 1.1053367928117364]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0580121475280437		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.0580121475280437 | validation: 1.2116433462284357]
	TIME [epoch: 6.61 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.142692621977897		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.142692621977897 | validation: 1.134993743858183]
	TIME [epoch: 6.64 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0476210562154555		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.0476210562154555 | validation: 1.1316302124441557]
	TIME [epoch: 6.59 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1019399952546325		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.1019399952546325 | validation: 1.1378902835118907]
	TIME [epoch: 6.58 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0635321406208464		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0635321406208464 | validation: 1.1947207677668268]
	TIME [epoch: 6.57 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0978918792044283		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.0978918792044283 | validation: 1.1177491874230565]
	TIME [epoch: 6.57 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0113511385818625		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.0113511385818625 | validation: 1.1475238493444722]
	TIME [epoch: 115 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0608685394117487		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.0608685394117487 | validation: 1.1279021043135962]
	TIME [epoch: 14.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.013975674764109		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.013975674764109 | validation: 1.0985907748573756]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0094494201339583		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.0094494201339583 | validation: 1.0895668765641906]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9898651713486276		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.9898651713486276 | validation: 1.1934601357421377]
	TIME [epoch: 14.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1231950490749656		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 1.1231950490749656 | validation: 1.1654731879869915]
	TIME [epoch: 14.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0793459557127045		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.0793459557127045 | validation: 1.1147311869131749]
	TIME [epoch: 14.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9943223017752225		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.9943223017752225 | validation: 1.0985684907798035]
	TIME [epoch: 14.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9954171708108008		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9954171708108008 | validation: 1.0976951996295572]
	TIME [epoch: 14.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0094031965403252		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.0094031965403252 | validation: 1.1147306633168368]
	TIME [epoch: 14.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9741501019321914		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9741501019321914 | validation: 1.105929606974853]
	TIME [epoch: 14.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9579012966669174		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9579012966669174 | validation: 1.0808504620589912]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.981877871989105		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.981877871989105 | validation: 1.0833701949544616]
	TIME [epoch: 14.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9577702356281714		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9577702356281714 | validation: 1.0953511983468387]
	TIME [epoch: 14.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9780842229201455		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9780842229201455 | validation: 1.0953289031215736]
	TIME [epoch: 14.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0009442204423162		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.0009442204423162 | validation: 1.0670801593154966]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9735690627949528		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9735690627949528 | validation: 1.1058349313259452]
	TIME [epoch: 14.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9797158833831319		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9797158833831319 | validation: 1.0848138888345393]
	TIME [epoch: 14.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9509277197143235		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9509277197143235 | validation: 1.0586398797016157]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8958354196508351		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8958354196508351 | validation: 1.0336788437598063]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.892422743043231		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.892422743043231 | validation: 1.020942012655367]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8602753421282461		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.8602753421282461 | validation: 0.9709364900913346]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8238926895118515		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.8238926895118515 | validation: 0.8727237004900765]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7895606066515582		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.7895606066515582 | validation: 0.7945063668966127]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7665045106232387		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.7665045106232387 | validation: 0.8111965568322216]
	TIME [epoch: 14.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7118370173513211		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.7118370173513211 | validation: 0.73688478523174]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7673958875243445		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7673958875243445 | validation: 0.7558014848469883]
	TIME [epoch: 14.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6584793891345948		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.6584793891345948 | validation: 0.7216288617951543]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6333799957875061		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.6333799957875061 | validation: 0.692558331925159]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6002047672810302		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.6002047672810302 | validation: 0.6672284229144426]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.65150954165194		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.65150954165194 | validation: 0.8491917305745649]
	TIME [epoch: 14.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6823726464257128		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.6823726464257128 | validation: 0.7095319501074793]
	TIME [epoch: 14.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6303422650310782		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.6303422650310782 | validation: 0.6830725117591243]
	TIME [epoch: 14.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5704631737388353		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5704631737388353 | validation: 0.6338517469904879]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5809350351671787		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.5809350351671787 | validation: 0.6200982628288925]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5797120770539793		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.5797120770539793 | validation: 0.6275785720841719]
	TIME [epoch: 14.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5237548057074113		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5237548057074113 | validation: 0.6709766061434177]
	TIME [epoch: 14.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6356408143629468		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.6356408143629468 | validation: 0.6784653640200887]
	TIME [epoch: 14.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5946440010911564		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5946440010911564 | validation: 0.6637828622154942]
	TIME [epoch: 14.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5557126397763799		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.5557126397763799 | validation: 0.6000235924327146]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5092132253851975		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5092132253851975 | validation: 0.5604629208740931]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5008719673075186		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5008719673075186 | validation: 0.5529397076990239]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5048177764463917		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.5048177764463917 | validation: 0.5369167830692323]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47797273023596076		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.47797273023596076 | validation: 0.6212320438870771]
	TIME [epoch: 14.4 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.497874995922878		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.497874995922878 | validation: 0.9881465282766198]
	TIME [epoch: 14.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8008983511268656		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.8008983511268656 | validation: 0.6235689668829482]
	TIME [epoch: 14.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4882671958576409		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4882671958576409 | validation: 0.4960835926066647]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.535600691270118		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.535600691270118 | validation: 0.5109758315106818]
	TIME [epoch: 14.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4758682874303624		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.4758682874303624 | validation: 0.5315446922840331]
	TIME [epoch: 14.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4744063459152612		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.4744063459152612 | validation: 0.5255788037765711]
	TIME [epoch: 14.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46891420936286266		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.46891420936286266 | validation: 0.4794571670057683]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4256503395492108		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.4256503395492108 | validation: 0.5799175955907802]
	TIME [epoch: 14.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4519149267090433		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.4519149267090433 | validation: 0.47602881514763284]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4077738964158282		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.4077738964158282 | validation: 0.5203844144194024]
	TIME [epoch: 14.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4258253131895836		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.4258253131895836 | validation: 0.4701230704623619]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.408194739194142		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.408194739194142 | validation: 0.4547853698320985]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40091818940250645		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.40091818940250645 | validation: 0.5379015696107632]
	TIME [epoch: 14.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40886546228045284		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.40886546228045284 | validation: 0.4603906605368011]
	TIME [epoch: 14.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37333035956361926		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.37333035956361926 | validation: 0.48231941601873274]
	TIME [epoch: 14.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36860543821213965		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.36860543821213965 | validation: 0.39903145163997766]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3648228907884848		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.3648228907884848 | validation: 0.47550375913316156]
	TIME [epoch: 14.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8532005111950236		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.8532005111950236 | validation: 1.1885188620917202]
	TIME [epoch: 14.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9020748143668762		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.9020748143668762 | validation: 0.4650335108603887]
	TIME [epoch: 14.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4150231053667949		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4150231053667949 | validation: 0.42067406963173914]
	TIME [epoch: 14.4 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3715848855160778		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.3715848855160778 | validation: 0.4524569136965402]
	TIME [epoch: 14.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3562297366035603		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.3562297366035603 | validation: 0.43507163789571135]
	TIME [epoch: 14.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.343964840682052		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.343964840682052 | validation: 0.4212264569099179]
	TIME [epoch: 14.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3353256757070696		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.3353256757070696 | validation: 0.41087551815173584]
	TIME [epoch: 14.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3658826319803507		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.3658826319803507 | validation: 0.38216708209223393]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3212056659770424		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.3212056659770424 | validation: 0.4212631665009552]
	TIME [epoch: 14.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3296078819513256		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.3296078819513256 | validation: 0.4007668856674729]
	TIME [epoch: 14.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3238996268438693		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.3238996268438693 | validation: 0.38623119375525816]
	TIME [epoch: 14.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3175601497396223		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.3175601497396223 | validation: 0.4352656944550033]
	TIME [epoch: 14.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3165341349597862		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.3165341349597862 | validation: 0.35705867478446307]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30918813452053107		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.30918813452053107 | validation: 0.48854111743080103]
	TIME [epoch: 14.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3499715855304808		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.3499715855304808 | validation: 0.3618820221139887]
	TIME [epoch: 14.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3152708200477744		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.3152708200477744 | validation: 0.338612102242944]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30605353169381244		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.30605353169381244 | validation: 0.3818382553971354]
	TIME [epoch: 14.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3028651079130371		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.3028651079130371 | validation: 0.3705882827695164]
	TIME [epoch: 14.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30550651817197283		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.30550651817197283 | validation: 0.38785009187011527]
	TIME [epoch: 14.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29165167226884386		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.29165167226884386 | validation: 0.35983390969427625]
	TIME [epoch: 14.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2793983390774819		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.2793983390774819 | validation: 0.3227223814016147]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2747818053191271		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.2747818053191271 | validation: 0.39928272245879093]
	TIME [epoch: 14.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30574147305816823		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.30574147305816823 | validation: 0.3522058212953327]
	TIME [epoch: 14.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3746324027478085		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.3746324027478085 | validation: 0.4285676361856612]
	TIME [epoch: 14.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3197318607041095		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.3197318607041095 | validation: 0.33701291507471787]
	TIME [epoch: 14.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27605525657137875		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.27605525657137875 | validation: 0.5687996628499651]
	TIME [epoch: 14.4 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4620380624764121		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.4620380624764121 | validation: 0.6857928647091077]
	TIME [epoch: 14.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5333014436525305		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.5333014436525305 | validation: 0.34747645630651725]
	TIME [epoch: 14.4 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.275814637762153		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.275814637762153 | validation: 0.382293672820476]
	TIME [epoch: 14.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29456820536895967		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.29456820536895967 | validation: 0.3641159314220906]
	TIME [epoch: 14.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2961506799345826		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.2961506799345826 | validation: 0.4465895760085382]
	TIME [epoch: 14.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3018317667970939		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.3018317667970939 | validation: 0.38529523151543993]
	TIME [epoch: 14.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2876406689986035		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.2876406689986035 | validation: 1.0820031141945214]
	TIME [epoch: 14.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9250425174110732		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.9250425174110732 | validation: 1.4802000720357782]
	TIME [epoch: 14.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4484797690325673		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.4484797690325673 | validation: 1.5659907425328825]
	TIME [epoch: 14.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5187592610444156		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 1.5187592610444156 | validation: 1.4709882158245293]
	TIME [epoch: 14.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3716310724015797		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 1.3716310724015797 | validation: 1.416623971973614]
	TIME [epoch: 14.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4035296113625868		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 1.4035296113625868 | validation: 1.4054142148497917]
	TIME [epoch: 14.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4161888566897698		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.4161888566897698 | validation: 1.4026055074344415]
	TIME [epoch: 14.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3724120065762728		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 1.3724120065762728 | validation: 1.2283587545332748]
	TIME [epoch: 14.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2218236204528874		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 1.2218236204528874 | validation: 1.172403767327263]
	TIME [epoch: 14.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1161307678550998		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 1.1161307678550998 | validation: 1.07922985548154]
	TIME [epoch: 14.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9622071692587567		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.9622071692587567 | validation: 0.9608080305241855]
	TIME [epoch: 14.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7681987726463845		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.7681987726463845 | validation: 0.7626275297526086]
	TIME [epoch: 14.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6355188784884274		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.6355188784884274 | validation: 0.8718775624596425]
	TIME [epoch: 14.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7815735798078076		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.7815735798078076 | validation: 0.41880409052405787]
	TIME [epoch: 14.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3714341097781151		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.3714341097781151 | validation: 0.4430899712026081]
	TIME [epoch: 14.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3332519354866648		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.3332519354866648 | validation: 0.4973298036806652]
	TIME [epoch: 14.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5154261511293647		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.5154261511293647 | validation: 0.35650827048782774]
	TIME [epoch: 14.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3182767921526839		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.3182767921526839 | validation: 0.42330662428854854]
	TIME [epoch: 14.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31370311096411524		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.31370311096411524 | validation: 0.41706094707818125]
	TIME [epoch: 14.4 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3008136184072625		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.3008136184072625 | validation: 0.35942571730981115]
	TIME [epoch: 14.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2717954555317312		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.2717954555317312 | validation: 0.4656555149503538]
	TIME [epoch: 14.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44765354692365544		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.44765354692365544 | validation: 0.373831910685382]
	TIME [epoch: 14.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2885806345982434		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.2885806345982434 | validation: 0.42226594598962824]
	TIME [epoch: 14.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29633765936291967		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.29633765936291967 | validation: 0.35564338035614973]
	TIME [epoch: 14.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27940482338072065		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.27940482338072065 | validation: 0.3775815760269368]
	TIME [epoch: 14.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28033634743126445		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.28033634743126445 | validation: 0.37135104979131683]
	TIME [epoch: 14.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2656204373697664		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.2656204373697664 | validation: 0.31998589997127247]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25797014700046494		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.25797014700046494 | validation: 0.34278698260928614]
	TIME [epoch: 14.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2574175541493454		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.2574175541493454 | validation: 0.34355831345833443]
	TIME [epoch: 14.4 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48528987903121784		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.48528987903121784 | validation: 0.808973356812706]
	TIME [epoch: 14.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6647716746131043		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.6647716746131043 | validation: 0.3428792737764245]
	TIME [epoch: 14.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2947195918481723		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.2947195918481723 | validation: 0.44792503814352663]
	TIME [epoch: 14.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2996209412644189		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.2996209412644189 | validation: 0.38287951396691416]
	TIME [epoch: 14.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2715574125579626		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.2715574125579626 | validation: 0.3466744918743837]
	TIME [epoch: 14.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25430480237222497		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.25430480237222497 | validation: 0.3127771379996487]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2817067302780862		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.2817067302780862 | validation: 0.3090176383009625]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24333942346751924		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.24333942346751924 | validation: 0.34686676845219333]
	TIME [epoch: 14.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2581783875948327		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.2581783875948327 | validation: 0.3205574367176327]
	TIME [epoch: 14.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24293436791630996		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.24293436791630996 | validation: 0.3233218003709341]
	TIME [epoch: 14.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2362466175302328		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.2362466175302328 | validation: 0.3448276068771719]
	TIME [epoch: 14.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2793696401951272		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.2793696401951272 | validation: 0.3152209310915717]
	TIME [epoch: 14.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2567581409234453		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.2567581409234453 | validation: 0.34841222509678116]
	TIME [epoch: 14.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2519150783733467		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.2519150783733467 | validation: 0.30967561753175765]
	TIME [epoch: 14.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23330438495906872		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.23330438495906872 | validation: 0.2995194118092352]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22651090733891463		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.22651090733891463 | validation: 0.3026621546424962]
	TIME [epoch: 14.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23120609122953018		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.23120609122953018 | validation: 0.3004421191103316]
	TIME [epoch: 14.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4011628722595355		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.4011628722595355 | validation: 0.985006947869211]
	TIME [epoch: 14.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7364698939014034		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.7364698939014034 | validation: 0.7950744626660962]
	TIME [epoch: 14.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5044172188924521		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.5044172188924521 | validation: 0.4666776560638793]
	TIME [epoch: 14.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3058714808206012		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.3058714808206012 | validation: 0.30166410621476747]
	TIME [epoch: 14.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2129566207710924		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.2129566207710924 | validation: 0.27993184727303627]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21497598085436226		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.21497598085436226 | validation: 0.3164964146095013]
	TIME [epoch: 14.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22601313802612572		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.22601313802612572 | validation: 0.2700297755401588]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20855982152256922		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.20855982152256922 | validation: 0.28228937472781007]
	TIME [epoch: 14.5 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2904872790075319		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.2904872790075319 | validation: 0.4568233583903763]
	TIME [epoch: 14.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29042688571241115		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.29042688571241115 | validation: 0.2996268945780561]
	TIME [epoch: 14.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21804814777880954		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.21804814777880954 | validation: 0.2617985395690036]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2079374683243621		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.2079374683243621 | validation: 0.2537661490621276]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20858022584739028		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.20858022584739028 | validation: 0.26309472027552655]
	TIME [epoch: 14.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2010655036163675		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2010655036163675 | validation: 0.24847749830719013]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20621686221259783		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.20621686221259783 | validation: 0.26198552398516756]
	TIME [epoch: 14.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19667173282969846		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.19667173282969846 | validation: 0.2430653790459728]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21759118749001316		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.21759118749001316 | validation: 0.24634980605319506]
	TIME [epoch: 14.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22356605494687026		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.22356605494687026 | validation: 0.253289277047164]
	TIME [epoch: 14.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21789494728232117		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.21789494728232117 | validation: 0.2967245476732905]
	TIME [epoch: 14.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20269324801147467		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.20269324801147467 | validation: 0.470444851373021]
	TIME [epoch: 14.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37115650707757		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.37115650707757 | validation: 0.3013793439225054]
	TIME [epoch: 14.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26361230950554093		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.26361230950554093 | validation: 0.25749989497567355]
	TIME [epoch: 14.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19123407357091202		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.19123407357091202 | validation: 0.24066123688558322]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19866290744099627		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.19866290744099627 | validation: 0.24168852949310693]
	TIME [epoch: 14.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19125714774678293		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.19125714774678293 | validation: 0.28875602596448585]
	TIME [epoch: 14.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19833153177718213		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.19833153177718213 | validation: 0.24265618144541332]
	TIME [epoch: 14.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1741209301106585		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.1741209301106585 | validation: 0.24541760914365615]
	TIME [epoch: 14.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17870601498352834		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.17870601498352834 | validation: 0.24687982863226515]
	TIME [epoch: 14.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17660662060313853		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.17660662060313853 | validation: 0.23552580699556522]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17474210808059037		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.17474210808059037 | validation: 0.21250374945525544]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2685870809523963		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.2685870809523963 | validation: 0.2323877220623774]
	TIME [epoch: 14.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2250001418985031		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.2250001418985031 | validation: 0.2401361656115844]
	TIME [epoch: 14.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17733136530550325		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.17733136530550325 | validation: 0.22993913258905563]
	TIME [epoch: 14.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17328633644445096		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.17328633644445096 | validation: 0.2835373554268807]
	TIME [epoch: 14.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19383217233459818		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.19383217233459818 | validation: 0.22715931340073414]
	TIME [epoch: 14.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18234568090713907		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.18234568090713907 | validation: 0.25956904683588694]
	TIME [epoch: 14.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17563355923713808		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.17563355923713808 | validation: 0.22838096114032355]
	TIME [epoch: 14.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15633766323910725		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.15633766323910725 | validation: 0.20135678122432316]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20486861768072645		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.20486861768072645 | validation: 0.31131628954840784]
	TIME [epoch: 14.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18469804852442034		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.18469804852442034 | validation: 0.19999580327419064]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1619792546918788		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.1619792546918788 | validation: 0.19343742082154516]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30706874059044686		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.30706874059044686 | validation: 0.2018539961409232]
	TIME [epoch: 14.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18383458682234327		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.18383458682234327 | validation: 0.24913365541387666]
	TIME [epoch: 14.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16114081023759597		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.16114081023759597 | validation: 0.1959372085167675]
	TIME [epoch: 14.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1484075497783392		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.1484075497783392 | validation: 0.21717341389682565]
	TIME [epoch: 14.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16004076548217247		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.16004076548217247 | validation: 0.18825494754937216]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19047531027362158		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.19047531027362158 | validation: 0.1816757043471957]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16537430560146602		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.16537430560146602 | validation: 0.18586256048863772]
	TIME [epoch: 14.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15181456994278614		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.15181456994278614 | validation: 0.20159710655455002]
	TIME [epoch: 14.4 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17066533661876587		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.17066533661876587 | validation: 0.17830346151912402]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1440105728079151		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.1440105728079151 | validation: 0.28633364595989824]
	TIME [epoch: 14.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18313188088425536		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.18313188088425536 | validation: 0.1949502002078044]
	TIME [epoch: 14.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15729806364583093		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.15729806364583093 | validation: 0.20572828557841924]
	TIME [epoch: 14.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14442721333696304		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.14442721333696304 | validation: 0.18771815753288973]
	TIME [epoch: 14.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13864720695318739		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.13864720695318739 | validation: 0.1744592864597676]
	TIME [epoch: 14.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13046339027541362		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.13046339027541362 | validation: 0.176525924777874]
	TIME [epoch: 14.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1405452459306265		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.1405452459306265 | validation: 0.21303383545441898]
	TIME [epoch: 14.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14763950672777715		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.14763950672777715 | validation: 0.1572565819586509]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18326748776006851		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.18326748776006851 | validation: 0.16946218008744013]
	TIME [epoch: 14.4 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13833048796760863		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.13833048796760863 | validation: 0.24383393717539428]
	TIME [epoch: 14.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1538092728379642		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.1538092728379642 | validation: 0.16036677033698388]
	TIME [epoch: 14.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.146166327214171		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.146166327214171 | validation: 0.1856231858008811]
	TIME [epoch: 14.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13515180889760342		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.13515180889760342 | validation: 0.15735013600047718]
	TIME [epoch: 14.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13214815262652485		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.13214815262652485 | validation: 0.1931438297764906]
	TIME [epoch: 14.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1683889549986452		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.1683889549986452 | validation: 0.180900993217361]
	TIME [epoch: 14.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14433682570580958		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.14433682570580958 | validation: 0.1695476572754757]
	TIME [epoch: 14.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12201877330776573		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.12201877330776573 | validation: 0.19339934618220064]
	TIME [epoch: 14.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1303809157254302		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.1303809157254302 | validation: 0.1489473241969936]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12462086432153327		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.12462086432153327 | validation: 0.16044447092125125]
	TIME [epoch: 14.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12722626246910956		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.12722626246910956 | validation: 0.16085967532779255]
	TIME [epoch: 14.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16188353486895413		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.16188353486895413 | validation: 0.17916898383366364]
	TIME [epoch: 14.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13613348950366666		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.13613348950366666 | validation: 0.16350628235377324]
	TIME [epoch: 14.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1188444467734294		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.1188444467734294 | validation: 0.15400566240157973]
	TIME [epoch: 14.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1361061281726212		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.1361061281726212 | validation: 0.16226558997372703]
	TIME [epoch: 14.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1330344057300643		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.1330344057300643 | validation: 0.2187076609667411]
	TIME [epoch: 14.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.142757804494691		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.142757804494691 | validation: 0.15318729272879655]
	TIME [epoch: 14.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12024011293695838		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.12024011293695838 | validation: 0.2268252343024425]
	TIME [epoch: 14.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18782237125280818		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.18782237125280818 | validation: 0.16447486990887064]
	TIME [epoch: 14.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1295726730165375		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.1295726730165375 | validation: 0.1808439668852318]
	TIME [epoch: 14.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12105647323390989		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.12105647323390989 | validation: 0.15678835375604094]
	TIME [epoch: 14.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10972525356076016		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.10972525356076016 | validation: 0.1595586253305458]
	TIME [epoch: 14.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12277471823725702		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.12277471823725702 | validation: 0.14759595563421848]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1209689793715156		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.1209689793715156 | validation: 0.1505792811310087]
	TIME [epoch: 14.4 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11697178782454858		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.11697178782454858 | validation: 0.1391360648455244]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10921094784920896		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.10921094784920896 | validation: 0.15258374753079862]
	TIME [epoch: 14.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11059170616586683		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.11059170616586683 | validation: 0.1424455330143092]
	TIME [epoch: 14.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10684891304250886		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.10684891304250886 | validation: 0.22891670718507082]
	TIME [epoch: 14.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13365098737059378		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.13365098737059378 | validation: 0.13855500510864846]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10676727379184381		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.10676727379184381 | validation: 0.13469236848714244]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10445321325355883		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.10445321325355883 | validation: 0.13191537927660937]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10896293727338346		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.10896293727338346 | validation: 0.16450045287774853]
	TIME [epoch: 14.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10929520449914426		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.10929520449914426 | validation: 0.12842488112706246]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11433640102476286		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.11433640102476286 | validation: 0.13216012729621449]
	TIME [epoch: 14.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10526056817065824		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.10526056817065824 | validation: 0.15708479570946465]
	TIME [epoch: 14.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14854084765895964		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.14854084765895964 | validation: 0.13797513393641453]
	TIME [epoch: 14.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1326033228655887		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.1326033228655887 | validation: 0.15255432772946276]
	TIME [epoch: 14.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10758717552851836		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.10758717552851836 | validation: 0.12861816588762723]
	TIME [epoch: 14.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10920376319438044		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.10920376319438044 | validation: 0.15351903359318403]
	TIME [epoch: 14.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11421812419692462		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.11421812419692462 | validation: 0.13241215318834645]
	TIME [epoch: 14.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09809364425257003		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.09809364425257003 | validation: 0.167141145211703]
	TIME [epoch: 14.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10329300059900368		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.10329300059900368 | validation: 0.13079885405577318]
	TIME [epoch: 14.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09847903148735053		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.09847903148735053 | validation: 0.11860564113220967]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0963117724696553		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.0963117724696553 | validation: 0.4887261319862951]
	TIME [epoch: 14.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2912121953068937		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.2912121953068937 | validation: 0.16336005383711139]
	TIME [epoch: 14.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12279805604939087		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.12279805604939087 | validation: 0.11523543713535829]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13553880438105875		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.13553880438105875 | validation: 0.13130976586851445]
	TIME [epoch: 14.5 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09884110138124229		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.09884110138124229 | validation: 0.12257866660228495]
	TIME [epoch: 14.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09519041920018455		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.09519041920018455 | validation: 0.17713864946298796]
	TIME [epoch: 14.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11472955212201796		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.11472955212201796 | validation: 0.12707768452736654]
	TIME [epoch: 14.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10482998216536665		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.10482998216536665 | validation: 0.12500942143643165]
	TIME [epoch: 14.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09345111446065466		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.09345111446065466 | validation: 0.1410551663720353]
	TIME [epoch: 14.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0995478038967202		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.0995478038967202 | validation: 0.1203931338202675]
	TIME [epoch: 14.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09300084783659876		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.09300084783659876 | validation: 0.14286767648009566]
	TIME [epoch: 14.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10425466293433225		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.10425466293433225 | validation: 0.11977719885215117]
	TIME [epoch: 14.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09790034310368141		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.09790034310368141 | validation: 0.11664339264427111]
	TIME [epoch: 14.4 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08734562045712761		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.08734562045712761 | validation: 0.13312126320540824]
	TIME [epoch: 14.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09418327310043142		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.09418327310043142 | validation: 0.12771202594454395]
	TIME [epoch: 14.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0913626010811918		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.0913626010811918 | validation: 0.1290403232416417]
	TIME [epoch: 14.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12424559034983454		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.12424559034983454 | validation: 0.20110204432565332]
	TIME [epoch: 14.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15458484346486107		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.15458484346486107 | validation: 0.15671931791307403]
	TIME [epoch: 14.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10614282235394107		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.10614282235394107 | validation: 0.11134859644933326]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08954457639638023		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.08954457639638023 | validation: 0.13335995277966603]
	TIME [epoch: 14.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0929779101880856		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.0929779101880856 | validation: 0.10770619949158422]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1151450568659182		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.1151450568659182 | validation: 0.125696677862779]
	TIME [epoch: 14.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0962404240654804		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0962404240654804 | validation: 0.14860108485656523]
	TIME [epoch: 14.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0960803549696515		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0960803549696515 | validation: 0.11654344737740663]
	TIME [epoch: 14.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08411751983696433		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.08411751983696433 | validation: 0.1226634958596393]
	TIME [epoch: 14.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08414023918418359		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.08414023918418359 | validation: 0.1217395039962058]
	TIME [epoch: 14.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09678907180511268		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.09678907180511268 | validation: 0.11816445483364642]
	TIME [epoch: 14.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0875858216403726		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.0875858216403726 | validation: 0.18732517999159862]
	TIME [epoch: 14.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11395125752417949		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.11395125752417949 | validation: 0.11152032164125979]
	TIME [epoch: 14.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08267959960379154		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.08267959960379154 | validation: 0.12779113971713185]
	TIME [epoch: 14.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0805351903197668		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.0805351903197668 | validation: 0.1161751675107793]
	TIME [epoch: 14.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08908848255366617		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.08908848255366617 | validation: 0.11197923454721033]
	TIME [epoch: 14.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08268774465772029		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.08268774465772029 | validation: 0.1376531480164457]
	TIME [epoch: 14.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09373344707975602		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.09373344707975602 | validation: 0.111448123235615]
	TIME [epoch: 14.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08697218009442957		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.08697218009442957 | validation: 0.13838143802628608]
	TIME [epoch: 14.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09322931342753461		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.09322931342753461 | validation: 0.10994167793731383]
	TIME [epoch: 14.5 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08221602565723928		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.08221602565723928 | validation: 0.14166631156011028]
	TIME [epoch: 14.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0854538687656372		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.0854538687656372 | validation: 0.11185230209468063]
	TIME [epoch: 14.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09186475617500042		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.09186475617500042 | validation: 0.11049648256538867]
	TIME [epoch: 14.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07823679612753504		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.07823679612753504 | validation: 0.12844759219172036]
	TIME [epoch: 14.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08968881563455965		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.08968881563455965 | validation: 0.10826614868688254]
	TIME [epoch: 14.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08334237403133556		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.08334237403133556 | validation: 0.18395806624840405]
	TIME [epoch: 14.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1060492991825486		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.1060492991825486 | validation: 0.10636577928058273]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10176440044812915		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.10176440044812915 | validation: 0.11026379297001512]
	TIME [epoch: 14.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08489213455999797		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.08489213455999797 | validation: 0.10407483888591146]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08228317901531709		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.08228317901531709 | validation: 0.11539732031381161]
	TIME [epoch: 14.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08711474558202277		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.08711474558202277 | validation: 0.16549912076406215]
	TIME [epoch: 14.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09081798232570884		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.09081798232570884 | validation: 0.10071933340691136]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08011971974132975		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.08011971974132975 | validation: 0.11570418049219455]
	TIME [epoch: 14.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0758893151567315		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.0758893151567315 | validation: 0.11018500286891397]
	TIME [epoch: 14.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09150111639130332		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.09150111639130332 | validation: 0.10169781846758555]
	TIME [epoch: 14.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07580333115267103		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07580333115267103 | validation: 0.09792860229738894]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07836662949240099		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.07836662949240099 | validation: 0.10064137084026736]
	TIME [epoch: 14.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07378432159744604		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.07378432159744604 | validation: 0.09384179995525374]
	TIME [epoch: 14.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09846752140092274		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.09846752140092274 | validation: 0.10573657583616192]
	TIME [epoch: 14.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07834894899914055		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.07834894899914055 | validation: 0.12730042923714532]
	TIME [epoch: 14.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08623999237060262		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.08623999237060262 | validation: 0.09187207995811834]
	TIME [epoch: 14.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12272390845867087		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.12272390845867087 | validation: 0.23191293257284534]
	TIME [epoch: 14.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13622992704778683		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.13622992704778683 | validation: 0.10372879817136256]
	TIME [epoch: 14.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07855956527009478		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.07855956527009478 | validation: 0.10364409183166007]
	TIME [epoch: 132 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0734101788120667		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0734101788120667 | validation: 0.09132224986454042]
	TIME [epoch: 30.8 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0711349618819952		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.0711349618819952 | validation: 0.09371691225683715]
	TIME [epoch: 31 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06859460224095826		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.06859460224095826 | validation: 0.12320188424491879]
	TIME [epoch: 31 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09298921438554987		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.09298921438554987 | validation: 0.08873043180457524]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07018483845331573		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.07018483845331573 | validation: 0.09531221972181468]
	TIME [epoch: 30.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07508520207345186		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.07508520207345186 | validation: 0.12303721667574195]
	TIME [epoch: 30.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0793512136949501		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.0793512136949501 | validation: 0.11143854092129911]
	TIME [epoch: 31 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08950486742811403		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.08950486742811403 | validation: 0.09560744938247762]
	TIME [epoch: 31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07797800373406527		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.07797800373406527 | validation: 0.1196425056418802]
	TIME [epoch: 31 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08065645221245318		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.08065645221245318 | validation: 0.10187531329755668]
	TIME [epoch: 30.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07021642216255647		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.07021642216255647 | validation: 0.1112926125567564]
	TIME [epoch: 30.8 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07691141946321232		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.07691141946321232 | validation: 0.09800576461547916]
	TIME [epoch: 31 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06906962753472712		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.06906962753472712 | validation: 0.09524264211286086]
	TIME [epoch: 31 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07599286880806279		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.07599286880806279 | validation: 0.08936855888538923]
	TIME [epoch: 31 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06884353605760574		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06884353605760574 | validation: 0.1602899831195945]
	TIME [epoch: 31 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1099589663429982		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.1099589663429982 | validation: 0.10452003855127588]
	TIME [epoch: 31 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07189304751348592		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.07189304751348592 | validation: 0.09592724173103984]
	TIME [epoch: 30.9 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06533917092120108		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06533917092120108 | validation: 0.10469603298387567]
	TIME [epoch: 31 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08900257248080591		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.08900257248080591 | validation: 0.0891627020251578]
	TIME [epoch: 30.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0784845159237961		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.0784845159237961 | validation: 0.11793401809205557]
	TIME [epoch: 31 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07893689918501468		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.07893689918501468 | validation: 0.09390044177357271]
	TIME [epoch: 30.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0765274396894392		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.0765274396894392 | validation: 0.09948981538025603]
	TIME [epoch: 30.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07028983481732176		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.07028983481732176 | validation: 0.1299553542479781]
	TIME [epoch: 30.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08411309849611538		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.08411309849611538 | validation: 0.15289418653849696]
	TIME [epoch: 30.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15727563396198652		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.15727563396198652 | validation: 0.09539340131136816]
	TIME [epoch: 31 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07315675020733911		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.07315675020733911 | validation: 0.10941012078954285]
	TIME [epoch: 31 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0722775717667068		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.0722775717667068 | validation: 0.09858483205183215]
	TIME [epoch: 31 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06719613443086275		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.06719613443086275 | validation: 0.09616424616414633]
	TIME [epoch: 30.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0665830208195817		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.0665830208195817 | validation: 0.09441467189091275]
	TIME [epoch: 31 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06698883731269037		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.06698883731269037 | validation: 0.10085770218168458]
	TIME [epoch: 31 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06810373466526884		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.06810373466526884 | validation: 0.0908738327513357]
	TIME [epoch: 30.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06835878778674379		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.06835878778674379 | validation: 0.08986021193816847]
	TIME [epoch: 30.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06338220678427722		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06338220678427722 | validation: 0.09147731436100413]
	TIME [epoch: 30.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07033210378441046		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.07033210378441046 | validation: 0.09383697637853648]
	TIME [epoch: 30.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06729703719206973		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.06729703719206973 | validation: 0.11022076061849462]
	TIME [epoch: 30.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07013100689934904		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.07013100689934904 | validation: 0.08920170263425858]
	TIME [epoch: 30.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06127310402943847		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.06127310402943847 | validation: 0.08800578386496381]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06290361823737221		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06290361823737221 | validation: 0.08687646580505633]
	TIME [epoch: 30.8 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06343485896349899		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06343485896349899 | validation: 0.0842612250110103]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06551166241744161		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06551166241744161 | validation: 0.09327864218021933]
	TIME [epoch: 30.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06649977560309037		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.06649977560309037 | validation: 0.09473302316647954]
	TIME [epoch: 30.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06992559649455955		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.06992559649455955 | validation: 0.10924286326897166]
	TIME [epoch: 30.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0757968862602933		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.0757968862602933 | validation: 0.08807610595714861]
	TIME [epoch: 30.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06650156941372073		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06650156941372073 | validation: 0.09284337618735614]
	TIME [epoch: 30.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06355418561497925		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06355418561497925 | validation: 0.08186971168569956]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0680120940523889		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.0680120940523889 | validation: 0.08695963174971957]
	TIME [epoch: 30.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06875036299257217		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.06875036299257217 | validation: 0.09097497901558481]
	TIME [epoch: 30.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06820785073194037		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.06820785073194037 | validation: 0.08826026153320335]
	TIME [epoch: 30.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06154792982497988		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.06154792982497988 | validation: 0.10421185963717075]
	TIME [epoch: 30.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07005230875799931		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.07005230875799931 | validation: 0.08245610761734475]
	TIME [epoch: 30.8 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06451193700547236		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.06451193700547236 | validation: 0.0954210631152649]
	TIME [epoch: 30.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06109819644019179		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06109819644019179 | validation: 0.08135972284312541]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06706075423067491		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06706075423067491 | validation: 0.08147204237222748]
	TIME [epoch: 30.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05965601976987746		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.05965601976987746 | validation: 0.11058748659908375]
	TIME [epoch: 30.8 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06862197966362779		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.06862197966362779 | validation: 0.08069284996277015]
	TIME [epoch: 30.8 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06005577033482813		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.06005577033482813 | validation: 0.0957452739022689]
	TIME [epoch: 30.9 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06938331346248952		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.06938331346248952 | validation: 0.07699553030328941]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05869814355521935		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.05869814355521935 | validation: 0.11319079153258733]
	TIME [epoch: 30.8 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06987187906570748		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.06987187906570748 | validation: 0.07589722049746797]
	TIME [epoch: 30.8 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07301307770059767		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.07301307770059767 | validation: 0.1099778717903741]
	TIME [epoch: 31 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06699698082163451		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.06699698082163451 | validation: 0.07782999069765026]
	TIME [epoch: 30.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0574259872292814		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.0574259872292814 | validation: 0.1056632949844957]
	TIME [epoch: 31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06808605732281647		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06808605732281647 | validation: 0.08208593195679445]
	TIME [epoch: 31 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06332374011566766		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.06332374011566766 | validation: 0.07678429810278467]
	TIME [epoch: 30.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06750010494667441		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.06750010494667441 | validation: 0.07855281500201433]
	TIME [epoch: 30.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0687938007871521		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.0687938007871521 | validation: 0.10118795414621773]
	TIME [epoch: 31 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06817370284182168		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.06817370284182168 | validation: 0.07616448476685718]
	TIME [epoch: 31 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06629336083645188		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.06629336083645188 | validation: 0.09010295595715233]
	TIME [epoch: 30.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06213381810474346		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.06213381810474346 | validation: 0.08474278499099203]
	TIME [epoch: 31.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05927595817499885		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.05927595817499885 | validation: 0.07589009114335221]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056689942808179856		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.056689942808179856 | validation: 0.08309640238530702]
	TIME [epoch: 31 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0618203782050432		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.0618203782050432 | validation: 0.08243847840946192]
	TIME [epoch: 31.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056372682897763426		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.056372682897763426 | validation: 0.08068582204854431]
	TIME [epoch: 31.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061282754789818324		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.061282754789818324 | validation: 0.07714563595612166]
	TIME [epoch: 31.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055502600518361256		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.055502600518361256 | validation: 0.06976742923328969]
	TIME [epoch: 31.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05950919580397021		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.05950919580397021 | validation: 0.07310477694583094]
	TIME [epoch: 31.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058297426629895496		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.058297426629895496 | validation: 0.09766603329554092]
	TIME [epoch: 31.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06198194271071396		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.06198194271071396 | validation: 0.07505950285656482]
	TIME [epoch: 31.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05670395921252576		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.05670395921252576 | validation: 0.08960173668089065]
	TIME [epoch: 31.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05785994973480299		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05785994973480299 | validation: 0.07484566955916605]
	TIME [epoch: 31.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05321107320103276		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.05321107320103276 | validation: 0.09781961164474345]
	TIME [epoch: 31.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06380235563610866		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.06380235563610866 | validation: 0.07813005324327778]
	TIME [epoch: 31.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07934346715968683		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.07934346715968683 | validation: 0.06730491295335106]
	TIME [epoch: 31.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06131282438079556		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.06131282438079556 | validation: 0.09866785533875587]
	TIME [epoch: 31.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.062206431674112744		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.062206431674112744 | validation: 0.07947876891602607]
	TIME [epoch: 31.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06041612638052238		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.06041612638052238 | validation: 0.07554178910557861]
	TIME [epoch: 31.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06929628084809367		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.06929628084809367 | validation: 0.10468659860892565]
	TIME [epoch: 31.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06390659415697757		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.06390659415697757 | validation: 0.08209186100969379]
	TIME [epoch: 31.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055520323800895735		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.055520323800895735 | validation: 0.08155561117834614]
	TIME [epoch: 31.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05953693155335872		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.05953693155335872 | validation: 0.07295460842835381]
	TIME [epoch: 31.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053423554233970764		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.053423554233970764 | validation: 0.07055171375148879]
	TIME [epoch: 31.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057325954252055826		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.057325954252055826 | validation: 0.07787579388508438]
	TIME [epoch: 31.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05527521750424848		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.05527521750424848 | validation: 0.06813220824736048]
	TIME [epoch: 31.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05222789950673067		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.05222789950673067 | validation: 0.07476077231549137]
	TIME [epoch: 31.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05268950251805711		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.05268950251805711 | validation: 0.18328211329218294]
	TIME [epoch: 31.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13665136960391805		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.13665136960391805 | validation: 0.16891917207170898]
	TIME [epoch: 31.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09483552975957513		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.09483552975957513 | validation: 0.08151053002145918]
	TIME [epoch: 31.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06055463066703604		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.06055463066703604 | validation: 0.07219603774513565]
	TIME [epoch: 31.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05342465215868013		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.05342465215868013 | validation: 0.06468100266098323]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05320780177974259		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.05320780177974259 | validation: 0.07314879108865673]
	TIME [epoch: 31.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055550542248747115		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.055550542248747115 | validation: 0.07841382796657259]
	TIME [epoch: 31.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056318675579306435		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.056318675579306435 | validation: 0.0759924942379118]
	TIME [epoch: 31.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05153748710712493		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.05153748710712493 | validation: 0.07637979996592931]
	TIME [epoch: 31.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05390905620329573		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.05390905620329573 | validation: 0.07300822151216045]
	TIME [epoch: 31.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05232754707069661		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.05232754707069661 | validation: 0.0634988272585126]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05431555991845197		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.05431555991845197 | validation: 0.07753202200276813]
	TIME [epoch: 31.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05462727416051291		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.05462727416051291 | validation: 0.07461014673310944]
	TIME [epoch: 31.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05162027874246954		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.05162027874246954 | validation: 0.09138519488447501]
	TIME [epoch: 31.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055923879972108514		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.055923879972108514 | validation: 0.07133234958141017]
	TIME [epoch: 31.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05437612698093044		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.05437612698093044 | validation: 0.06780715338946348]
	TIME [epoch: 31.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049559118882711056		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.049559118882711056 | validation: 0.09235222239939822]
	TIME [epoch: 31.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05887574104525445		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.05887574104525445 | validation: 0.07055615654217878]
	TIME [epoch: 31.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04983193112660872		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.04983193112660872 | validation: 0.08304039589371688]
	TIME [epoch: 31.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05277844895626815		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.05277844895626815 | validation: 0.06910997729562009]
	TIME [epoch: 31.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052826592724600635		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.052826592724600635 | validation: 0.06819712188441439]
	TIME [epoch: 31.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04901266343506469		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.04901266343506469 | validation: 0.07804228089076119]
	TIME [epoch: 31.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05124259857273543		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.05124259857273543 | validation: 0.06760237243223166]
	TIME [epoch: 31.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04996477130476175		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.04996477130476175 | validation: 0.07763893708206347]
	TIME [epoch: 31.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05516666631783965		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.05516666631783965 | validation: 0.0817771059776409]
	TIME [epoch: 31.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0560061211363711		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.0560061211363711 | validation: 0.06652631394310146]
	TIME [epoch: 31.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0519028943587398		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0519028943587398 | validation: 0.06936755405954986]
	TIME [epoch: 31.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05432016451658893		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05432016451658893 | validation: 0.07166841930622551]
	TIME [epoch: 31.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050123348922181905		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.050123348922181905 | validation: 0.0633806004709854]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04845964965991677		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.04845964965991677 | validation: 0.07273821376295722]
	TIME [epoch: 31.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05292294485523359		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.05292294485523359 | validation: 0.07658791922463065]
	TIME [epoch: 31.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046393116459120295		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.046393116459120295 | validation: 0.0736318147361438]
	TIME [epoch: 31.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051521153220676914		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.051521153220676914 | validation: 0.06672495917060824]
	TIME [epoch: 31.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05572937248739686		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05572937248739686 | validation: 0.06531903941942864]
	TIME [epoch: 31.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06052009703145965		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.06052009703145965 | validation: 0.06813910434577634]
	TIME [epoch: 31.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06094665996929118		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.06094665996929118 | validation: 0.06624179890218905]
	TIME [epoch: 31.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05028207854452223		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05028207854452223 | validation: 0.08680926862871477]
	TIME [epoch: 31.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054027935700406385		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.054027935700406385 | validation: 0.06404894276438926]
	TIME [epoch: 31.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050424891460646576		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.050424891460646576 | validation: 0.08119737851531415]
	TIME [epoch: 31.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05911727916037092		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.05911727916037092 | validation: 0.0787194700294389]
	TIME [epoch: 31.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055235332264051445		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.055235332264051445 | validation: 0.07028412210740834]
	TIME [epoch: 31.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05344751443862368		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.05344751443862368 | validation: 0.07074186272229148]
	TIME [epoch: 31.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04666886643852519		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.04666886643852519 | validation: 0.06783655946901383]
	TIME [epoch: 31.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058407938199635724		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.058407938199635724 | validation: 0.07422087633147086]
	TIME [epoch: 31.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05168790946284924		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.05168790946284924 | validation: 0.07131461969114165]
	TIME [epoch: 31.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05012690029217471		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.05012690029217471 | validation: 0.07874908216362911]
	TIME [epoch: 31.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04921597510525731		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.04921597510525731 | validation: 0.06921271793049334]
	TIME [epoch: 31.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048156706242006325		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.048156706242006325 | validation: 0.07478445223068694]
	TIME [epoch: 31.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04799076283596167		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.04799076283596167 | validation: 0.07571618548208475]
	TIME [epoch: 31.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04753693561245059		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.04753693561245059 | validation: 0.06943588113565523]
	TIME [epoch: 31.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05058720904714546		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.05058720904714546 | validation: 0.07467095392579308]
	TIME [epoch: 31.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04989554463470422		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.04989554463470422 | validation: 0.07386287525047243]
	TIME [epoch: 31.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04946882149513267		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04946882149513267 | validation: 0.06475614563418101]
	TIME [epoch: 31.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04865370681717002		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.04865370681717002 | validation: 0.06840725426001053]
	TIME [epoch: 31.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04795896705584572		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.04795896705584572 | validation: 0.06648203005358025]
	TIME [epoch: 31.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049696218461713376		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.049696218461713376 | validation: 0.07500504178049615]
	TIME [epoch: 31.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051413777144954494		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.051413777144954494 | validation: 0.0724069237130808]
	TIME [epoch: 31.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04683323342760191		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.04683323342760191 | validation: 0.06590494981187611]
	TIME [epoch: 31.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04818285259868698		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.04818285259868698 | validation: 0.06950389686004184]
	TIME [epoch: 31.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04522145344904417		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.04522145344904417 | validation: 0.06855315220076467]
	TIME [epoch: 31.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045234836153925206		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.045234836153925206 | validation: 0.06652456645296374]
	TIME [epoch: 31.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04823777266519643		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.04823777266519643 | validation: 0.06743888938775361]
	TIME [epoch: 31.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04663200715405463		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.04663200715405463 | validation: 0.07939188662033836]
	TIME [epoch: 31.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06207711646915015		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.06207711646915015 | validation: 0.06397244422493042]
	TIME [epoch: 31.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04755120710832703		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.04755120710832703 | validation: 0.06469697111011319]
	TIME [epoch: 31.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04605159527663438		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.04605159527663438 | validation: 0.06776440368503422]
	TIME [epoch: 31.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04933581453884869		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.04933581453884869 | validation: 0.0661236013630976]
	TIME [epoch: 31.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0512969839201435		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.0512969839201435 | validation: 0.0627399202737806]
	TIME [epoch: 31.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04588160930124764		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.04588160930124764 | validation: 0.0633586933941077]
	TIME [epoch: 31.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04863995633142316		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.04863995633142316 | validation: 0.06638866647144681]
	TIME [epoch: 31.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04426865811232036		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.04426865811232036 | validation: 0.07252802589052383]
	TIME [epoch: 31.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04586525457535373		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.04586525457535373 | validation: 0.06726940540192645]
	TIME [epoch: 31.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0466892108474285		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.0466892108474285 | validation: 0.07185685855665092]
	TIME [epoch: 31.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06572515580298942		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.06572515580298942 | validation: 0.06581321699656031]
	TIME [epoch: 31.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058707423925077204		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.058707423925077204 | validation: 0.06046031923515567]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05084033968361819		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.05084033968361819 | validation: 0.06268861641429095]
	TIME [epoch: 31.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04467154958995925		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.04467154958995925 | validation: 0.06795275422778496]
	TIME [epoch: 31.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06037340059577854		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.06037340059577854 | validation: 0.07001259315966221]
	TIME [epoch: 31.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06003017145978784		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.06003017145978784 | validation: 0.06216629153600009]
	TIME [epoch: 31.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047781686764792615		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.047781686764792615 | validation: 0.0625769629964309]
	TIME [epoch: 31.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04920354675456822		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.04920354675456822 | validation: 0.0699040386699324]
	TIME [epoch: 31.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04891362363888867		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.04891362363888867 | validation: 0.06711557064599959]
	TIME [epoch: 31.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04760373079060951		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.04760373079060951 | validation: 0.0668407583280487]
	TIME [epoch: 31.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05151656894161292		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05151656894161292 | validation: 0.06271562407981036]
	TIME [epoch: 31.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048478297332611754		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.048478297332611754 | validation: 0.07171076318526023]
	TIME [epoch: 31.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050471998904648124		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.050471998904648124 | validation: 0.06285366024907917]
	TIME [epoch: 31.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04649187340016998		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.04649187340016998 | validation: 0.06968976935313295]
	TIME [epoch: 31.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047367577620285845		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.047367577620285845 | validation: 0.07138036391125295]
	TIME [epoch: 31.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0485700347292306		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.0485700347292306 | validation: 0.06304602979921324]
	TIME [epoch: 31.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0471414943279036		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.0471414943279036 | validation: 0.06258327027557933]
	TIME [epoch: 31.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048308145665925284		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.048308145665925284 | validation: 0.07223358809385637]
	TIME [epoch: 31.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05019960483956285		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.05019960483956285 | validation: 0.0677662648179147]
	TIME [epoch: 31.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04810327035343174		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.04810327035343174 | validation: 0.06188397714994223]
	TIME [epoch: 31.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04488545076547415		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.04488545076547415 | validation: 0.060549223176903325]
	TIME [epoch: 31.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04532847746821257		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.04532847746821257 | validation: 0.06423089639421102]
	TIME [epoch: 31.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04753173168957229		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.04753173168957229 | validation: 0.0706284892307573]
	TIME [epoch: 31.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05412612303136129		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.05412612303136129 | validation: 0.0706962017201475]
	TIME [epoch: 31.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046202072487838156		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.046202072487838156 | validation: 0.0618185421688378]
	TIME [epoch: 31.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04682078329115762		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.04682078329115762 | validation: 0.061263183393151156]
	TIME [epoch: 31.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046983145995094365		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.046983145995094365 | validation: 0.06264079930806019]
	TIME [epoch: 31.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04311081340659489		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.04311081340659489 | validation: 0.07253373297718078]
	TIME [epoch: 31.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05220621984248734		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.05220621984248734 | validation: 0.06956007349476466]
	TIME [epoch: 31.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04803639366341752		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.04803639366341752 | validation: 0.06936597008630478]
	TIME [epoch: 31.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056739072987882166		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.056739072987882166 | validation: 0.06343911101393249]
	TIME [epoch: 31.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04519015516366153		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.04519015516366153 | validation: 0.0626541638321432]
	TIME [epoch: 31.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04280768306796481		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.04280768306796481 | validation: 0.06459098683236682]
	TIME [epoch: 31.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04758869427312659		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.04758869427312659 | validation: 0.06866178724204529]
	TIME [epoch: 31.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04383947988382629		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.04383947988382629 | validation: 0.0676832531737885]
	TIME [epoch: 31.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04240961018225445		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.04240961018225445 | validation: 0.06685824740720273]
	TIME [epoch: 31.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043874407353289256		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.043874407353289256 | validation: 0.0645481068259153]
	TIME [epoch: 31.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04767790981174279		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.04767790981174279 | validation: 0.06784308214566336]
	TIME [epoch: 31.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04421168906396543		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.04421168906396543 | validation: 0.06803322096880392]
	TIME [epoch: 31.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05059251200807169		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.05059251200807169 | validation: 0.06504641839219027]
	TIME [epoch: 31.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047516339826973876		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.047516339826973876 | validation: 0.06731746797568348]
	TIME [epoch: 31.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04546402072429899		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.04546402072429899 | validation: 0.06669250625302363]
	TIME [epoch: 31.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04217732824659102		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.04217732824659102 | validation: 0.06741866976537524]
	TIME [epoch: 31.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04270859447830258		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.04270859447830258 | validation: 0.06657458785230178]
	TIME [epoch: 31.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04752695859971008		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.04752695859971008 | validation: 0.06900358094716512]
	TIME [epoch: 31.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04420204244870098		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.04420204244870098 | validation: 0.061169397647474005]
	TIME [epoch: 31.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04473229063634543		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.04473229063634543 | validation: 0.06340708277970922]
	TIME [epoch: 31.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04439550700983944		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.04439550700983944 | validation: 0.07239035137829015]
	TIME [epoch: 31.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04789091620345744		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.04789091620345744 | validation: 0.06704165423110241]
	TIME [epoch: 31.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045812159288869544		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.045812159288869544 | validation: 0.0664531252819501]
	TIME [epoch: 31.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04362073608833865		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.04362073608833865 | validation: 0.06102930306708487]
	TIME [epoch: 31.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0445823757510372		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.0445823757510372 | validation: 0.06217788428192518]
	TIME [epoch: 31.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04452039507513412		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.04452039507513412 | validation: 0.05811091151842135]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04390654188729573		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.04390654188729573 | validation: 0.06631087197587311]
	TIME [epoch: 30.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046186477328931025		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.046186477328931025 | validation: 0.060316665359579]
	TIME [epoch: 30.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04335663103230987		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.04335663103230987 | validation: 0.06807966019264554]
	TIME [epoch: 30.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0446129316358882		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.0446129316358882 | validation: 0.06034317038596313]
	TIME [epoch: 30.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04467514149331793		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.04467514149331793 | validation: 0.06296276346218346]
	TIME [epoch: 30.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04396845422417814		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.04396845422417814 | validation: 0.05986421092312991]
	TIME [epoch: 30.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04554027907264775		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.04554027907264775 | validation: 0.0710786722921662]
	TIME [epoch: 30.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04691918011460763		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.04691918011460763 | validation: 0.06184051908007748]
	TIME [epoch: 30.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04738970842498772		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.04738970842498772 | validation: 0.061812263219272294]
	TIME [epoch: 30.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04543409964730144		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.04543409964730144 | validation: 0.05294150502732464]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044122893524642104		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.044122893524642104 | validation: 0.06115393559526035]
	TIME [epoch: 30.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042536343827192305		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.042536343827192305 | validation: 0.06283612657454145]
	TIME [epoch: 31 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04633253589046259		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.04633253589046259 | validation: 0.055694248447169296]
	TIME [epoch: 30.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04270295436149331		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.04270295436149331 | validation: 0.06182640657104872]
	TIME [epoch: 30.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04127892811539054		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.04127892811539054 | validation: 0.0606910222620741]
	TIME [epoch: 30.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04179566506128589		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.04179566506128589 | validation: 0.061700909563225104]
	TIME [epoch: 31 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04391807490811639		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.04391807490811639 | validation: 0.06205831815603024]
	TIME [epoch: 31 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0452141844482082		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.0452141844482082 | validation: 0.06062868921133011]
	TIME [epoch: 31 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04321137425725434		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.04321137425725434 | validation: 0.059743434515650784]
	TIME [epoch: 31 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04441430669946911		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.04441430669946911 | validation: 0.05224281750031567]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04559310806264129		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.04559310806264129 | validation: 0.06639833491170298]
	TIME [epoch: 31 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042665273912712194		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.042665273912712194 | validation: 0.056734521311438746]
	TIME [epoch: 31 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04012166345912075		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.04012166345912075 | validation: 0.056658347178690706]
	TIME [epoch: 30.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04921985802714672		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.04921985802714672 | validation: 0.05565004915237673]
	TIME [epoch: 30.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040362584187012024		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.040362584187012024 | validation: 0.054854185705642135]
	TIME [epoch: 30.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04214970416915936		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.04214970416915936 | validation: 0.061984891848705884]
	TIME [epoch: 30.9 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042516473763272694		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.042516473763272694 | validation: 0.06354153215574679]
	TIME [epoch: 31 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04276359815978642		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.04276359815978642 | validation: 0.05845771905687193]
	TIME [epoch: 31 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045599094793917175		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.045599094793917175 | validation: 0.057759628611067496]
	TIME [epoch: 31 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044625493469720605		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.044625493469720605 | validation: 0.05701876805439783]
	TIME [epoch: 31 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04257043603816004		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.04257043603816004 | validation: 0.05938571409701441]
	TIME [epoch: 30.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041709487909351334		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.041709487909351334 | validation: 0.06502660964602543]
	TIME [epoch: 31 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04225418213627863		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.04225418213627863 | validation: 0.05763393404509869]
	TIME [epoch: 31 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04079536744326383		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.04079536744326383 | validation: 0.0608021759481435]
	TIME [epoch: 30.9 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042748646566893325		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.042748646566893325 | validation: 0.05975999921367286]
	TIME [epoch: 31.1 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04279124145386798		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.04279124145386798 | validation: 0.05928196418633602]
	TIME [epoch: 31.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04517761678203365		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.04517761678203365 | validation: 0.056828878640640235]
	TIME [epoch: 31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042591151232352915		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.042591151232352915 | validation: 0.06078546117909574]
	TIME [epoch: 31.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04520504549323342		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.04520504549323342 | validation: 0.054439632937830766]
	TIME [epoch: 31.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04367994993777036		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.04367994993777036 | validation: 0.05462034805885829]
	TIME [epoch: 31 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04197440521720137		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.04197440521720137 | validation: 0.058096698517104174]
	TIME [epoch: 31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03996293969569659		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.03996293969569659 | validation: 0.06129150697329902]
	TIME [epoch: 31 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04510535677079097		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.04510535677079097 | validation: 0.06348102760372397]
	TIME [epoch: 31 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04351687332704981		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.04351687332704981 | validation: 0.059640571511907936]
	TIME [epoch: 31 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044445066758025044		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.044445066758025044 | validation: 0.06356383000557808]
	TIME [epoch: 31 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04236783188746357		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.04236783188746357 | validation: 0.05665376722465192]
	TIME [epoch: 31 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04183994601215366		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.04183994601215366 | validation: 0.0542408396726513]
	TIME [epoch: 31.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04327216760243521		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.04327216760243521 | validation: 0.054835382148502976]
	TIME [epoch: 31 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04089131099324768		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.04089131099324768 | validation: 0.058678882149987115]
	TIME [epoch: 30.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039433627884756885		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.039433627884756885 | validation: 0.052128830657259644]
	TIME [epoch: 30.9 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04318896904727393		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.04318896904727393 | validation: 0.06356039343235287]
	TIME [epoch: 30.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045424940617595416		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.045424940617595416 | validation: 0.059214168842307784]
	TIME [epoch: 31 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04046474396713223		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.04046474396713223 | validation: 0.06761663202978417]
	TIME [epoch: 31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04401689812173358		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.04401689812173358 | validation: 0.058121662268574584]
	TIME [epoch: 31 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04165910167901361		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.04165910167901361 | validation: 0.06269768439271071]
	TIME [epoch: 31 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040917317366475275		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.040917317366475275 | validation: 0.05640994891465957]
	TIME [epoch: 31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04424474549449159		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.04424474549449159 | validation: 0.05365660343629104]
	TIME [epoch: 31 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04332064612500317		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.04332064612500317 | validation: 0.058413830514673595]
	TIME [epoch: 31 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04727238073711841		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.04727238073711841 | validation: 0.057567963487370546]
	TIME [epoch: 31 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04414102350935557		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.04414102350935557 | validation: 0.06099867337434446]
	TIME [epoch: 31.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04148057041456993		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.04148057041456993 | validation: 0.05908035548233599]
	TIME [epoch: 31 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044572023340783334		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.044572023340783334 | validation: 0.058557635757947014]
	TIME [epoch: 31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04106615322266953		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.04106615322266953 | validation: 0.06100490422448943]
	TIME [epoch: 31.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040972075747284245		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.040972075747284245 | validation: 0.05707490074230364]
	TIME [epoch: 31.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041564103039162636		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.041564103039162636 | validation: 0.05985812482456853]
	TIME [epoch: 31 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04113952819795845		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.04113952819795845 | validation: 0.05909475745895165]
	TIME [epoch: 30.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040238962936138496		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.040238962936138496 | validation: 0.05551179693350275]
	TIME [epoch: 31 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039780702024363124		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.039780702024363124 | validation: 0.060782941714426775]
	TIME [epoch: 31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041041886005578725		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.041041886005578725 | validation: 0.05758809829513434]
	TIME [epoch: 31.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04211532020791414		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.04211532020791414 | validation: 0.05655435587498119]
	TIME [epoch: 31 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0399838341369505		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.0399838341369505 | validation: 0.05345657255888164]
	TIME [epoch: 31 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04204870440973198		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.04204870440973198 | validation: 0.06379522796444978]
	TIME [epoch: 30.9 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04130650739989972		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.04130650739989972 | validation: 0.05495697084639015]
	TIME [epoch: 30.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04179334473699245		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.04179334473699245 | validation: 0.0589149027427967]
	TIME [epoch: 31 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0462003160547164		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.0462003160547164 | validation: 0.054417976638547375]
	TIME [epoch: 31 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04083274071867175		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.04083274071867175 | validation: 0.055399555726895144]
	TIME [epoch: 31 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04031346248642195		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.04031346248642195 | validation: 0.052697630845583904]
	TIME [epoch: 31 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0438643612860014		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.0438643612860014 | validation: 0.058439156523770165]
	TIME [epoch: 31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04163844281553456		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.04163844281553456 | validation: 0.055970545941865124]
	TIME [epoch: 30.9 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04249211450881634		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.04249211450881634 | validation: 0.055126098852043795]
	TIME [epoch: 31 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041255044599827935		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.041255044599827935 | validation: 0.061626007857524284]
	TIME [epoch: 31 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04077429697510013		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.04077429697510013 | validation: 0.05931853258226354]
	TIME [epoch: 31.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043866115728193524		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.043866115728193524 | validation: 0.06648346557024747]
	TIME [epoch: 31 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04107589073675044		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.04107589073675044 | validation: 0.05918051368290426]
	TIME [epoch: 31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041722323236623754		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.041722323236623754 | validation: 0.05430371144620355]
	TIME [epoch: 30.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042693047555196637		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.042693047555196637 | validation: 0.05258164058726755]
	TIME [epoch: 31 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04009006072687424		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.04009006072687424 | validation: 0.05497165300716034]
	TIME [epoch: 31 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04429022038297219		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.04429022038297219 | validation: 0.07196016638866479]
	TIME [epoch: 31 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04848415678781221		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.04848415678781221 | validation: 0.06599537452031633]
	TIME [epoch: 31 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0458962315217363		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.0458962315217363 | validation: 0.06332731591874663]
	TIME [epoch: 31.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04089868531918431		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.04089868531918431 | validation: 0.05869988253166319]
	TIME [epoch: 31.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039401934098808676		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.039401934098808676 | validation: 0.05710284454857206]
	TIME [epoch: 31 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03962150566855063		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.03962150566855063 | validation: 0.05916169860223493]
	TIME [epoch: 30.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03837297596559042		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.03837297596559042 | validation: 0.05336758542747455]
	TIME [epoch: 31.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04010812420417492		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.04010812420417492 | validation: 0.056093431677102304]
	TIME [epoch: 31.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04075277444593145		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.04075277444593145 | validation: 0.0552858005236162]
	TIME [epoch: 31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0404249586675577		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.0404249586675577 | validation: 0.05471812053455512]
	TIME [epoch: 31.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04488758121864167		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.04488758121864167 | validation: 0.05559442192872025]
	TIME [epoch: 31 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0413005812027624		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.0413005812027624 | validation: 0.06221822667812025]
	TIME [epoch: 31 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040690738797986695		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.040690738797986695 | validation: 0.05023934022816343]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04088524526912933		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.04088524526912933 | validation: 0.060226971726894854]
	TIME [epoch: 30.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0410621409058534		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.0410621409058534 | validation: 0.06169534960972725]
	TIME [epoch: 31 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04222742540532104		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.04222742540532104 | validation: 0.06349128530906922]
	TIME [epoch: 31.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04341970451445798		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.04341970451445798 | validation: 0.06186332954285034]
	TIME [epoch: 31.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04294608219914954		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.04294608219914954 | validation: 0.060023740544716375]
	TIME [epoch: 30.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04222238477237364		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.04222238477237364 | validation: 0.052512601173709594]
	TIME [epoch: 31 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045807290760009695		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.045807290760009695 | validation: 0.06269209817913565]
	TIME [epoch: 31 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047773587269450246		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.047773587269450246 | validation: 0.06875671466335846]
	TIME [epoch: 30.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04627185178551644		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.04627185178551644 | validation: 0.06330258899486914]
	TIME [epoch: 30.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046408959511930276		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.046408959511930276 | validation: 0.05523375736936187]
	TIME [epoch: 31 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041923434390964326		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.041923434390964326 | validation: 0.05418833760663636]
	TIME [epoch: 31 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04016560428327869		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.04016560428327869 | validation: 0.05847033859653786]
	TIME [epoch: 31 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042506905665037883		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.042506905665037883 | validation: 0.05332388558835502]
	TIME [epoch: 30.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03970586940793229		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.03970586940793229 | validation: 0.053974868170326554]
	TIME [epoch: 30.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03993206342392004		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.03993206342392004 | validation: 0.05715805396561118]
	TIME [epoch: 31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041643591084017335		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.041643591084017335 | validation: 0.05896158573360786]
	TIME [epoch: 31.1 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03995178788390316		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.03995178788390316 | validation: 0.05582251763038648]
	TIME [epoch: 30.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04240214962874472		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.04240214962874472 | validation: 0.05683910255518503]
	TIME [epoch: 31 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03887738713256118		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.03887738713256118 | validation: 0.054237928953824094]
	TIME [epoch: 31 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04061318463743743		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.04061318463743743 | validation: 0.05817096864233438]
	TIME [epoch: 31.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04161179334101191		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.04161179334101191 | validation: 0.05197359701326874]
	TIME [epoch: 31.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03771294369968406		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.03771294369968406 | validation: 0.05294107837448329]
	TIME [epoch: 31 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039100159165820936		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.039100159165820936 | validation: 0.05472894925637404]
	TIME [epoch: 30.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038647347413187955		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.038647347413187955 | validation: 0.05048121128839479]
	TIME [epoch: 31.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039077711097603204		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.039077711097603204 | validation: 0.055375553677404224]
	TIME [epoch: 31.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03813661953616376		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.03813661953616376 | validation: 0.05435117960285494]
	TIME [epoch: 31.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038884330731423146		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.038884330731423146 | validation: 0.05577604297160287]
	TIME [epoch: 31 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042015410607219444		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.042015410607219444 | validation: 0.05345716220899395]
	TIME [epoch: 31 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041922752551294004		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.041922752551294004 | validation: 0.056072392402598314]
	TIME [epoch: 31 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039380785840996774		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.039380785840996774 | validation: 0.05536929708261787]
	TIME [epoch: 31.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03937591267284009		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.03937591267284009 | validation: 0.053931230649278655]
	TIME [epoch: 31.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04111966515263819		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.04111966515263819 | validation: 0.05224109253842268]
	TIME [epoch: 31 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04018368063664227		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.04018368063664227 | validation: 0.054512750701215744]
	TIME [epoch: 30.9 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037272710992194896		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.037272710992194896 | validation: 0.05976075070424145]
	TIME [epoch: 31.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03845998585741786		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.03845998585741786 | validation: 0.05980019963067943]
	TIME [epoch: 31.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04056748296688261		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.04056748296688261 | validation: 0.05514971491061941]
	TIME [epoch: 31.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03896826395620394		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.03896826395620394 | validation: 0.05737409326772013]
	TIME [epoch: 30.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04013920955680658		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.04013920955680658 | validation: 0.05886184024001526]
	TIME [epoch: 31 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039524694252474235		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.039524694252474235 | validation: 0.05370295681160674]
	TIME [epoch: 31.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04103985924855999		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.04103985924855999 | validation: 0.05844147946093186]
	TIME [epoch: 31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03835927460966897		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.03835927460966897 | validation: 0.049240786393267066]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040869687176898754		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.040869687176898754 | validation: 0.058098742368490976]
	TIME [epoch: 31.1 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03874432195461565		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.03874432195461565 | validation: 0.05350481344832532]
	TIME [epoch: 31.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03638106554700319		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.03638106554700319 | validation: 0.05951254222619125]
	TIME [epoch: 31.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04224691302231511		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.04224691302231511 | validation: 0.052829609695274275]
	TIME [epoch: 31.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03947186371994033		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.03947186371994033 | validation: 0.053018747228995466]
	TIME [epoch: 31 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03971235432862379		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.03971235432862379 | validation: 0.06074990724377909]
	TIME [epoch: 31 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044730756716845996		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.044730756716845996 | validation: 0.06080168934769823]
	TIME [epoch: 31.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04077782683508692		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.04077782683508692 | validation: 0.053884231105152815]
	TIME [epoch: 31.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0377570169391355		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.0377570169391355 | validation: 0.05363846101137687]
	TIME [epoch: 31.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038957886981488096		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.038957886981488096 | validation: 0.05052219121989437]
	TIME [epoch: 31.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03834374779452204		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.03834374779452204 | validation: 0.053710706186613444]
	TIME [epoch: 31.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04079742397633633		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.04079742397633633 | validation: 0.05014536791103677]
	TIME [epoch: 31 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04057118665244456		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.04057118665244456 | validation: 0.05323447048699567]
	TIME [epoch: 31 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04023431363433513		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.04023431363433513 | validation: 0.04944519855388207]
	TIME [epoch: 31.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041846710571997105		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.041846710571997105 | validation: 0.05305769886683705]
	TIME [epoch: 31.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0416303633424151		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.0416303633424151 | validation: 0.05406146099010392]
	TIME [epoch: 31.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04015306350911362		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.04015306350911362 | validation: 0.055441430122074545]
	TIME [epoch: 31.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04275178782336111		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.04275178782336111 | validation: 0.053644701781548114]
	TIME [epoch: 31.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041107677618272866		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.041107677618272866 | validation: 0.05967630435399194]
	TIME [epoch: 31.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03872527585233547		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.03872527585233547 | validation: 0.0547725272508111]
	TIME [epoch: 31.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04005810629741996		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.04005810629741996 | validation: 0.0489193326153008]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03696749089178296		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.03696749089178296 | validation: 0.05650293359723311]
	TIME [epoch: 31.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03935695168845606		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.03935695168845606 | validation: 0.06061999456438211]
	TIME [epoch: 31.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04057234174071214		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.04057234174071214 | validation: 0.05586350829909856]
	TIME [epoch: 31.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04055142357372512		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.04055142357372512 | validation: 0.05468525102879325]
	TIME [epoch: 31.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04093517154853776		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.04093517154853776 | validation: 0.04906019198020613]
	TIME [epoch: 31.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03958153473875656		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.03958153473875656 | validation: 0.05599883083552372]
	TIME [epoch: 31.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03991142402048295		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.03991142402048295 | validation: 0.06115017810382384]
	TIME [epoch: 31.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03854307389871496		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.03854307389871496 | validation: 0.055678246871670695]
	TIME [epoch: 31.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03911596884723933		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.03911596884723933 | validation: 0.054689974524814056]
	TIME [epoch: 31.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0390430815573323		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.0390430815573323 | validation: 0.05509220704819215]
	TIME [epoch: 31.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03963195772529931		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.03963195772529931 | validation: 0.05521599914113571]
	TIME [epoch: 31.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036752226966213636		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.036752226966213636 | validation: 0.055104358420386836]
	TIME [epoch: 31.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03948569279311015		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.03948569279311015 | validation: 0.05261795076584189]
	TIME [epoch: 31.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0384490766019069		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.0384490766019069 | validation: 0.05719593704328179]
	TIME [epoch: 31.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03813796247430782		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.03813796247430782 | validation: 0.05325795426130412]
	TIME [epoch: 31.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0379772101246775		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.0379772101246775 | validation: 0.05499388559909984]
	TIME [epoch: 31.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040085197188571814		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.040085197188571814 | validation: 0.05426995533050594]
	TIME [epoch: 31.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039542420833519484		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.039542420833519484 | validation: 0.05357335215176469]
	TIME [epoch: 31.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041729804533918904		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.041729804533918904 | validation: 0.05859554875108658]
	TIME [epoch: 31.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036991143077843194		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.036991143077843194 | validation: 0.05572259177878408]
	TIME [epoch: 31.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04143991179013684		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.04143991179013684 | validation: 0.05444519588955002]
	TIME [epoch: 31.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04109187722419601		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.04109187722419601 | validation: 0.051468530854318864]
	TIME [epoch: 31.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044942306538226805		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.044942306538226805 | validation: 0.059688648733403564]
	TIME [epoch: 31.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04387716426823984		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.04387716426823984 | validation: 0.05597826110817973]
	TIME [epoch: 31.2 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03877900169990177		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.03877900169990177 | validation: 0.05753233946566361]
	TIME [epoch: 31.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04155584676978981		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.04155584676978981 | validation: 0.050662595008561954]
	TIME [epoch: 31.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03989913932929861		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.03989913932929861 | validation: 0.04924626286306594]
	TIME [epoch: 31.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03816022127358834		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.03816022127358834 | validation: 0.0545615318253909]
	TIME [epoch: 31.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037322203345328354		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.037322203345328354 | validation: 0.05411244104039528]
	TIME [epoch: 31.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0406276708155388		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.0406276708155388 | validation: 0.051688626636781844]
	TIME [epoch: 31.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03931980839902355		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.03931980839902355 | validation: 0.05136517258347037]
	TIME [epoch: 31.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03946323187568835		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.03946323187568835 | validation: 0.051022783052291645]
	TIME [epoch: 31.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03667156206322391		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.03667156206322391 | validation: 0.05392409892242083]
	TIME [epoch: 31.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0383622143798788		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.0383622143798788 | validation: 0.05665867571260829]
	TIME [epoch: 31.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03910860869332558		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.03910860869332558 | validation: 0.05453441242092584]
	TIME [epoch: 31.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0392013056853934		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.0392013056853934 | validation: 0.05953855442938834]
	TIME [epoch: 31.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03855343558343338		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.03855343558343338 | validation: 0.05871982848503507]
	TIME [epoch: 31.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03732051457838302		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.03732051457838302 | validation: 0.05482685750724652]
	TIME [epoch: 31.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03780716422378064		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.03780716422378064 | validation: 0.0511087689111168]
	TIME [epoch: 31.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038911694618989426		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.038911694618989426 | validation: 0.05772334916418034]
	TIME [epoch: 31.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0397016271200352		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.0397016271200352 | validation: 0.048726027099981126]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03735087597040128		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.03735087597040128 | validation: 0.05346091886819724]
	TIME [epoch: 31.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03801276535456223		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.03801276535456223 | validation: 0.06049086709969442]
	TIME [epoch: 31.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042738878713039705		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.042738878713039705 | validation: 0.059024007897715114]
	TIME [epoch: 31.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036833534022326524		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.036833534022326524 | validation: 0.055806245003593984]
	TIME [epoch: 31.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036756865975822096		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.036756865975822096 | validation: 0.05182758175349165]
	TIME [epoch: 31.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037783589138284765		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.037783589138284765 | validation: 0.05537522474834172]
	TIME [epoch: 31.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04124856218080576		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.04124856218080576 | validation: 0.0597519834708059]
	TIME [epoch: 31.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037361065232140465		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.037361065232140465 | validation: 0.05032943336985546]
	TIME [epoch: 31.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03989881498487381		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.03989881498487381 | validation: 0.05441158954960323]
	TIME [epoch: 31.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041034754159199674		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.041034754159199674 | validation: 0.058718934654018375]
	TIME [epoch: 31.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03970096505006883		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.03970096505006883 | validation: 0.052656783970241094]
	TIME [epoch: 31.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04005592242560274		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.04005592242560274 | validation: 0.05449269006077426]
	TIME [epoch: 31.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04053852005858628		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.04053852005858628 | validation: 0.0529836091212785]
	TIME [epoch: 31.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03858457851719932		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.03858457851719932 | validation: 0.05668909053677516]
	TIME [epoch: 31.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03855011493780239		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.03855011493780239 | validation: 0.05599993348559956]
	TIME [epoch: 31.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04008595326940512		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.04008595326940512 | validation: 0.05123226309587825]
	TIME [epoch: 31.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037551573348994616		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.037551573348994616 | validation: 0.052779381903085044]
	TIME [epoch: 31.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038823404925010324		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.038823404925010324 | validation: 0.05405571882293787]
	TIME [epoch: 31.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039929646026881635		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.039929646026881635 | validation: 0.05171761197432212]
	TIME [epoch: 31.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0369084270757427		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.0369084270757427 | validation: 0.0485205042941918]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03756853716631899		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.03756853716631899 | validation: 0.05280192894676519]
	TIME [epoch: 31.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04078053758639616		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.04078053758639616 | validation: 0.04877559990021133]
	TIME [epoch: 31.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03836249139195752		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.03836249139195752 | validation: 0.05275191908580046]
	TIME [epoch: 31.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03719279488614709		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.03719279488614709 | validation: 0.06020923878771709]
	TIME [epoch: 31.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040850706893014455		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.040850706893014455 | validation: 0.057717289260573804]
	TIME [epoch: 31.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036773890767177866		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.036773890767177866 | validation: 0.055716239754615995]
	TIME [epoch: 31.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03769547631409216		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.03769547631409216 | validation: 0.048404104678899836]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0347356227283868		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.0347356227283868 | validation: 0.04935567815724674]
	TIME [epoch: 31.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03842323504447943		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.03842323504447943 | validation: 0.052451454963886104]
	TIME [epoch: 31.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036118403074736995		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.036118403074736995 | validation: 0.05534992134850469]
	TIME [epoch: 31.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03537550501085302		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.03537550501085302 | validation: 0.058882148117096994]
	TIME [epoch: 31.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038038251046595764		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.038038251046595764 | validation: 0.051012016730038905]
	TIME [epoch: 31.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03924733633158309		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.03924733633158309 | validation: 0.05481848541402632]
	TIME [epoch: 31.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0386313068410714		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.0386313068410714 | validation: 0.05286694827311718]
	TIME [epoch: 31.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039406318616126194		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.039406318616126194 | validation: 0.05053020112319955]
	TIME [epoch: 31.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03948181862004306		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.03948181862004306 | validation: 0.0508462776504891]
	TIME [epoch: 31.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038045815826160345		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.038045815826160345 | validation: 0.05566723255358039]
	TIME [epoch: 31.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039438937374198305		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.039438937374198305 | validation: 0.061274602817320604]
	TIME [epoch: 31.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04193298698981848		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.04193298698981848 | validation: 0.05409837670189421]
	TIME [epoch: 31.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039267296596699544		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.039267296596699544 | validation: 0.05463945424973855]
	TIME [epoch: 31.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03911997438181698		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.03911997438181698 | validation: 0.052169382987272764]
	TIME [epoch: 31.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03829792685334245		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.03829792685334245 | validation: 0.05866562775679978]
	TIME [epoch: 31.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03903439545768818		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.03903439545768818 | validation: 0.055507169684617066]
	TIME [epoch: 31.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0393148944339301		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.0393148944339301 | validation: 0.05241449853633309]
	TIME [epoch: 31.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03713883993072735		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.03713883993072735 | validation: 0.053504555578026136]
	TIME [epoch: 31.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03596622983971105		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.03596622983971105 | validation: 0.05202420017823706]
	TIME [epoch: 31.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03898430632735704		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.03898430632735704 | validation: 0.05325584377568813]
	TIME [epoch: 31.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03802954601513126		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.03802954601513126 | validation: 0.05390976831705006]
	TIME [epoch: 31.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03646625260723923		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.03646625260723923 | validation: 0.04846240209899228]
	TIME [epoch: 31.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03858650387525189		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.03858650387525189 | validation: 0.051541477013145244]
	TIME [epoch: 31.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03866650785180118		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.03866650785180118 | validation: 0.0505476095166324]
	TIME [epoch: 31.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035889097242361764		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.035889097242361764 | validation: 0.05138209209112941]
	TIME [epoch: 31.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03597507602812326		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.03597507602812326 | validation: 0.05217095685237168]
	TIME [epoch: 31.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03827176294765021		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.03827176294765021 | validation: 0.05310280037878179]
	TIME [epoch: 31.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040238170450007285		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.040238170450007285 | validation: 0.05188732093669018]
	TIME [epoch: 31.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04041097533668127		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.04041097533668127 | validation: 0.055349643636439974]
	TIME [epoch: 31.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039026721818422136		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.039026721818422136 | validation: 0.055713205394674704]
	TIME [epoch: 31.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0405282885791137		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.0405282885791137 | validation: 0.05595092032940924]
	TIME [epoch: 31.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040724375359247104		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.040724375359247104 | validation: 0.06222567224478901]
	TIME [epoch: 31.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04074533656948943		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.04074533656948943 | validation: 0.05828071935260115]
	TIME [epoch: 31.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03947308740372757		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.03947308740372757 | validation: 0.060815619068927365]
	TIME [epoch: 31.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04043400797378091		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.04043400797378091 | validation: 0.05348282913413134]
	TIME [epoch: 31.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03875758344684453		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.03875758344684453 | validation: 0.05225746408144338]
	TIME [epoch: 31.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03715982496363553		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.03715982496363553 | validation: 0.05194483477476473]
	TIME [epoch: 31.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04133929079127372		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.04133929079127372 | validation: 0.05347124595762862]
	TIME [epoch: 31.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03488222836380417		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.03488222836380417 | validation: 0.05039453514967878]
	TIME [epoch: 31.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0358009967831411		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.0358009967831411 | validation: 0.04858463593766661]
	TIME [epoch: 31.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03686726726910756		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.03686726726910756 | validation: 0.04816727814128756]
	TIME [epoch: 31.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0381899259350808		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.0381899259350808 | validation: 0.054713107389386575]
	TIME [epoch: 31.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03900823491670614		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.03900823491670614 | validation: 0.051983716493300226]
	TIME [epoch: 31.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03774183714515261		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.03774183714515261 | validation: 0.05803233051392385]
	TIME [epoch: 31.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03666417679595078		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.03666417679595078 | validation: 0.05483535064570738]
	TIME [epoch: 31.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03720657093685664		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.03720657093685664 | validation: 0.051697109937129615]
	TIME [epoch: 31.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03818830629201139		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.03818830629201139 | validation: 0.05042881015009104]
	TIME [epoch: 31.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03787869102411445		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.03787869102411445 | validation: 0.05475633111716094]
	TIME [epoch: 31.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03921854420692247		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.03921854420692247 | validation: 0.053089039709264485]
	TIME [epoch: 31.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0389019024315287		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.0389019024315287 | validation: 0.05115282731445812]
	TIME [epoch: 165 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035266110349389784		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.035266110349389784 | validation: 0.05061770874342675]
	TIME [epoch: 64 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03794257810748605		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.03794257810748605 | validation: 0.05224324404184888]
	TIME [epoch: 64.1 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03952654201799333		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.03952654201799333 | validation: 0.05182258641112787]
	TIME [epoch: 63.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0400546223922985		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.0400546223922985 | validation: 0.054854960786395596]
	TIME [epoch: 64.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038558384639438725		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.038558384639438725 | validation: 0.05555394373103633]
	TIME [epoch: 63.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038683644090079135		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.038683644090079135 | validation: 0.05277466228390513]
	TIME [epoch: 64 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03913263684395367		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.03913263684395367 | validation: 0.051158643597252224]
	TIME [epoch: 64.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03952577803315652		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.03952577803315652 | validation: 0.05273097598823287]
	TIME [epoch: 64.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0390142181084642		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.0390142181084642 | validation: 0.05423800758728903]
	TIME [epoch: 64 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04039389606031953		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.04039389606031953 | validation: 0.05287832911687323]
	TIME [epoch: 64.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03669386423537327		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.03669386423537327 | validation: 0.04815199779377424]
	TIME [epoch: 64.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039050426695816834		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.039050426695816834 | validation: 0.052014984329270646]
	TIME [epoch: 64.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0400458006524132		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.0400458006524132 | validation: 0.052601650980245857]
	TIME [epoch: 64.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03908428289590514		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.03908428289590514 | validation: 0.051892546237403726]
	TIME [epoch: 64.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03871456351391124		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.03871456351391124 | validation: 0.054199748700580336]
	TIME [epoch: 64.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039707999677248404		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.039707999677248404 | validation: 0.053054416586144605]
	TIME [epoch: 64.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035511722730261194		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.035511722730261194 | validation: 0.052576458787329056]
	TIME [epoch: 64.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0377435387867166		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.0377435387867166 | validation: 0.05673014049826071]
	TIME [epoch: 64.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036173689767878964		[learning rate: 8.7096e-06]
	Learning Rate: 8.70964e-06
	LOSS [training: 0.036173689767878964 | validation: 0.055524011984176516]
	TIME [epoch: 64.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040852179323600094		[learning rate: 8.6481e-06]
	Learning Rate: 8.64815e-06
	LOSS [training: 0.040852179323600094 | validation: 0.05315207205862935]
	TIME [epoch: 64.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041618207767668725		[learning rate: 8.5871e-06]
	Learning Rate: 8.5871e-06
	LOSS [training: 0.041618207767668725 | validation: 0.051690432961825544]
	TIME [epoch: 64.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036987407162062905		[learning rate: 8.5265e-06]
	Learning Rate: 8.52647e-06
	LOSS [training: 0.036987407162062905 | validation: 0.05574848370897532]
	TIME [epoch: 64.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040159475579462506		[learning rate: 8.4663e-06]
	Learning Rate: 8.46627e-06
	LOSS [training: 0.040159475579462506 | validation: 0.05472209371943892]
	TIME [epoch: 64.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04344603030809291		[learning rate: 8.4065e-06]
	Learning Rate: 8.40651e-06
	LOSS [training: 0.04344603030809291 | validation: 0.05185567756643157]
	TIME [epoch: 64.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04029712713232891		[learning rate: 8.3472e-06]
	Learning Rate: 8.34716e-06
	LOSS [training: 0.04029712713232891 | validation: 0.05186470075194654]
	TIME [epoch: 64.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03568767824600291		[learning rate: 8.2882e-06]
	Learning Rate: 8.28823e-06
	LOSS [training: 0.03568767824600291 | validation: 0.0553143369761685]
	TIME [epoch: 64.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040564724993751605		[learning rate: 8.2297e-06]
	Learning Rate: 8.22972e-06
	LOSS [training: 0.040564724993751605 | validation: 0.05358469110894586]
	TIME [epoch: 64.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038327052402871535		[learning rate: 8.1716e-06]
	Learning Rate: 8.17162e-06
	LOSS [training: 0.038327052402871535 | validation: 0.0568638509975964]
	TIME [epoch: 64.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03826009766110734		[learning rate: 8.1139e-06]
	Learning Rate: 8.11392e-06
	LOSS [training: 0.03826009766110734 | validation: 0.05238312462506739]
	TIME [epoch: 64.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0396692322802584		[learning rate: 8.0566e-06]
	Learning Rate: 8.05664e-06
	LOSS [training: 0.0396692322802584 | validation: 0.04874905286000163]
	TIME [epoch: 64.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03646971553565475		[learning rate: 7.9998e-06]
	Learning Rate: 7.99976e-06
	LOSS [training: 0.03646971553565475 | validation: 0.05804907842294156]
	TIME [epoch: 64.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037773985146101		[learning rate: 7.9433e-06]
	Learning Rate: 7.94328e-06
	LOSS [training: 0.037773985146101 | validation: 0.05309225515179027]
	TIME [epoch: 64.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03741858414111761		[learning rate: 7.8872e-06]
	Learning Rate: 7.8872e-06
	LOSS [training: 0.03741858414111761 | validation: 0.048136650942659745]
	TIME [epoch: 64.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04057563055454237		[learning rate: 7.8315e-06]
	Learning Rate: 7.83153e-06
	LOSS [training: 0.04057563055454237 | validation: 0.0524777516318109]
	TIME [epoch: 63.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035319595076705845		[learning rate: 7.7762e-06]
	Learning Rate: 7.77624e-06
	LOSS [training: 0.035319595076705845 | validation: 0.049552309421738855]
	TIME [epoch: 63.9 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03732996554147875		[learning rate: 7.7213e-06]
	Learning Rate: 7.72133e-06
	LOSS [training: 0.03732996554147875 | validation: 0.05258433207548328]
	TIME [epoch: 64.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0371933742053161		[learning rate: 7.6668e-06]
	Learning Rate: 7.66683e-06
	LOSS [training: 0.0371933742053161 | validation: 0.05255159162081929]
	TIME [epoch: 64.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038854562430596276		[learning rate: 7.6127e-06]
	Learning Rate: 7.6127e-06
	LOSS [training: 0.038854562430596276 | validation: 0.05572039036765426]
	TIME [epoch: 64.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036766268961727785		[learning rate: 7.559e-06]
	Learning Rate: 7.55895e-06
	LOSS [training: 0.036766268961727785 | validation: 0.052055316148441194]
	TIME [epoch: 64.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03892576580072568		[learning rate: 7.5056e-06]
	Learning Rate: 7.50559e-06
	LOSS [training: 0.03892576580072568 | validation: 0.046582651653177355]
	TIME [epoch: 64.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_1041.pth
	Model improved!!!
EPOCH 1042/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03786063427109991		[learning rate: 7.4526e-06]
	Learning Rate: 7.4526e-06
	LOSS [training: 0.03786063427109991 | validation: 0.055252223322289254]
	TIME [epoch: 64.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03797028221978459		[learning rate: 7.4e-06]
	Learning Rate: 7.39998e-06
	LOSS [training: 0.03797028221978459 | validation: 0.05034894199435811]
	TIME [epoch: 64.1 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03779260543024513		[learning rate: 7.3477e-06]
	Learning Rate: 7.34774e-06
	LOSS [training: 0.03779260543024513 | validation: 0.05583881990474176]
	TIME [epoch: 64.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038561760809590055		[learning rate: 7.2959e-06]
	Learning Rate: 7.29587e-06
	LOSS [training: 0.038561760809590055 | validation: 0.053797127031559526]
	TIME [epoch: 64.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037797162697265005		[learning rate: 7.2444e-06]
	Learning Rate: 7.24436e-06
	LOSS [training: 0.037797162697265005 | validation: 0.05094680791887605]
	TIME [epoch: 64.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03816462621606366		[learning rate: 7.1932e-06]
	Learning Rate: 7.19322e-06
	LOSS [training: 0.03816462621606366 | validation: 0.05077878638890812]
	TIME [epoch: 63.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039221196516943044		[learning rate: 7.1424e-06]
	Learning Rate: 7.14244e-06
	LOSS [training: 0.039221196516943044 | validation: 0.05201573881343222]
	TIME [epoch: 64.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0404714176766572		[learning rate: 7.092e-06]
	Learning Rate: 7.09201e-06
	LOSS [training: 0.0404714176766572 | validation: 0.04717459857867529]
	TIME [epoch: 64.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038402568603872214		[learning rate: 7.0419e-06]
	Learning Rate: 7.04194e-06
	LOSS [training: 0.038402568603872214 | validation: 0.05263324035255201]
	TIME [epoch: 64.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0349252379734345		[learning rate: 6.9922e-06]
	Learning Rate: 6.99222e-06
	LOSS [training: 0.0349252379734345 | validation: 0.05204861939507738]
	TIME [epoch: 64.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036363148536953015		[learning rate: 6.9429e-06]
	Learning Rate: 6.94286e-06
	LOSS [training: 0.036363148536953015 | validation: 0.05291964323071485]
	TIME [epoch: 64.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038403253720041955		[learning rate: 6.8938e-06]
	Learning Rate: 6.89385e-06
	LOSS [training: 0.038403253720041955 | validation: 0.04707184470042289]
	TIME [epoch: 64.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03619621770376792		[learning rate: 6.8452e-06]
	Learning Rate: 6.84518e-06
	LOSS [training: 0.03619621770376792 | validation: 0.05242362235981836]
	TIME [epoch: 64.2 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03639361531192945		[learning rate: 6.7969e-06]
	Learning Rate: 6.79685e-06
	LOSS [training: 0.03639361531192945 | validation: 0.05034308261895733]
	TIME [epoch: 64.2 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03816746081852462		[learning rate: 6.7489e-06]
	Learning Rate: 6.74887e-06
	LOSS [training: 0.03816746081852462 | validation: 0.05093641269437277]
	TIME [epoch: 64.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03880244604645463		[learning rate: 6.7012e-06]
	Learning Rate: 6.70122e-06
	LOSS [training: 0.03880244604645463 | validation: 0.053135157921198196]
	TIME [epoch: 64.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03820794944782054		[learning rate: 6.6539e-06]
	Learning Rate: 6.65391e-06
	LOSS [training: 0.03820794944782054 | validation: 0.04930391445090701]
	TIME [epoch: 64.2 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03816516984317804		[learning rate: 6.6069e-06]
	Learning Rate: 6.60694e-06
	LOSS [training: 0.03816516984317804 | validation: 0.05077100455128633]
	TIME [epoch: 64.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035632822639041696		[learning rate: 6.5603e-06]
	Learning Rate: 6.56029e-06
	LOSS [training: 0.035632822639041696 | validation: 0.05390931196913548]
	TIME [epoch: 64.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037114396537721		[learning rate: 6.514e-06]
	Learning Rate: 6.51398e-06
	LOSS [training: 0.037114396537721 | validation: 0.05409328220491183]
	TIME [epoch: 64.1 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035867748768406364		[learning rate: 6.468e-06]
	Learning Rate: 6.46799e-06
	LOSS [training: 0.035867748768406364 | validation: 0.050174398920088605]
	TIME [epoch: 64.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03871454273253651		[learning rate: 6.4223e-06]
	Learning Rate: 6.42233e-06
	LOSS [training: 0.03871454273253651 | validation: 0.053025608518677506]
	TIME [epoch: 64.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0375335401274124		[learning rate: 6.377e-06]
	Learning Rate: 6.37698e-06
	LOSS [training: 0.0375335401274124 | validation: 0.04478842064188999]
	TIME [epoch: 64.3 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240813_194114/states/model_phi1_2c_v_mmd1_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039483972879871085		[learning rate: 6.332e-06]
	Learning Rate: 6.33197e-06
	LOSS [training: 0.039483972879871085 | validation: 0.05521658250369485]
	TIME [epoch: 64.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03448500603052768		[learning rate: 6.2873e-06]
	Learning Rate: 6.28726e-06
	LOSS [training: 0.03448500603052768 | validation: 0.050086720727429236]
	TIME [epoch: 64.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038509860986335354		[learning rate: 6.2429e-06]
	Learning Rate: 6.24288e-06
	LOSS [training: 0.038509860986335354 | validation: 0.05383218228821919]
	TIME [epoch: 64.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03796929027823073		[learning rate: 6.1988e-06]
	Learning Rate: 6.1988e-06
	LOSS [training: 0.03796929027823073 | validation: 0.051783100048013314]
	TIME [epoch: 64.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03733737370615248		[learning rate: 6.155e-06]
	Learning Rate: 6.15504e-06
	LOSS [training: 0.03733737370615248 | validation: 0.05367302163395674]
	TIME [epoch: 64.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03716074201475955		[learning rate: 6.1116e-06]
	Learning Rate: 6.11159e-06
	LOSS [training: 0.03716074201475955 | validation: 0.05523161639511799]
	TIME [epoch: 64.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037811117573632036		[learning rate: 6.0684e-06]
	Learning Rate: 6.06844e-06
	LOSS [training: 0.037811117573632036 | validation: 0.05254172178277792]
	TIME [epoch: 64.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03738653310286591		[learning rate: 6.0256e-06]
	Learning Rate: 6.0256e-06
	LOSS [training: 0.03738653310286591 | validation: 0.05334664472594]
	TIME [epoch: 64.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0368342898452069		[learning rate: 5.9831e-06]
	Learning Rate: 5.98306e-06
	LOSS [training: 0.0368342898452069 | validation: 0.0533106784300767]
	TIME [epoch: 64.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03935868493579575		[learning rate: 5.9408e-06]
	Learning Rate: 5.94082e-06
	LOSS [training: 0.03935868493579575 | validation: 0.05033671044193471]
	TIME [epoch: 64.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03917535351046188		[learning rate: 5.8989e-06]
	Learning Rate: 5.89888e-06
	LOSS [training: 0.03917535351046188 | validation: 0.0508314710326582]
	TIME [epoch: 64.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03952986759542944		[learning rate: 5.8572e-06]
	Learning Rate: 5.85723e-06
	LOSS [training: 0.03952986759542944 | validation: 0.05301505940029469]
	TIME [epoch: 64.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036840432324108274		[learning rate: 5.8159e-06]
	Learning Rate: 5.81588e-06
	LOSS [training: 0.036840432324108274 | validation: 0.05212179004906297]
	TIME [epoch: 64.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03755581004330211		[learning rate: 5.7748e-06]
	Learning Rate: 5.77482e-06
	LOSS [training: 0.03755581004330211 | validation: 0.04867649288809029]
	TIME [epoch: 64.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03748292090737598		[learning rate: 5.7341e-06]
	Learning Rate: 5.73405e-06
	LOSS [training: 0.03748292090737598 | validation: 0.053423783365002676]
	TIME [epoch: 64.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03608072129857791		[learning rate: 5.6936e-06]
	Learning Rate: 5.69357e-06
	LOSS [training: 0.03608072129857791 | validation: 0.05227324268455076]
	TIME [epoch: 64.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03772692246037749		[learning rate: 5.6534e-06]
	Learning Rate: 5.65337e-06
	LOSS [training: 0.03772692246037749 | validation: 0.054005282100010744]
	TIME [epoch: 64.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03642049857683889		[learning rate: 5.6135e-06]
	Learning Rate: 5.61346e-06
	LOSS [training: 0.03642049857683889 | validation: 0.053989590981463886]
	TIME [epoch: 64.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04019496938418751		[learning rate: 5.5738e-06]
	Learning Rate: 5.57383e-06
	LOSS [training: 0.04019496938418751 | validation: 0.050729308545643885]
	TIME [epoch: 64.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03704828475409966		[learning rate: 5.5345e-06]
	Learning Rate: 5.53448e-06
	LOSS [training: 0.03704828475409966 | validation: 0.06242673014867661]
	TIME [epoch: 64.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03746977119105473		[learning rate: 5.4954e-06]
	Learning Rate: 5.49541e-06
	LOSS [training: 0.03746977119105473 | validation: 0.05615323718350865]
	TIME [epoch: 64.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03689244274113465		[learning rate: 5.4566e-06]
	Learning Rate: 5.45661e-06
	LOSS [training: 0.03689244274113465 | validation: 0.05378916667527879]
	TIME [epoch: 64.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04095092181948602		[learning rate: 5.4181e-06]
	Learning Rate: 5.41809e-06
	LOSS [training: 0.04095092181948602 | validation: 0.054718631016415886]
	TIME [epoch: 64.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03700885102490721		[learning rate: 5.3798e-06]
	Learning Rate: 5.37984e-06
	LOSS [training: 0.03700885102490721 | validation: 0.05309624580523769]
	TIME [epoch: 64.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03758696127034801		[learning rate: 5.3419e-06]
	Learning Rate: 5.34186e-06
	LOSS [training: 0.03758696127034801 | validation: 0.05445672196170373]
	TIME [epoch: 63.9 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03740689690964272		[learning rate: 5.3041e-06]
	Learning Rate: 5.30415e-06
	LOSS [training: 0.03740689690964272 | validation: 0.05114460281144855]
	TIME [epoch: 64.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0376171895060753		[learning rate: 5.2667e-06]
	Learning Rate: 5.2667e-06
	LOSS [training: 0.0376171895060753 | validation: 0.052466198967887324]
	TIME [epoch: 64.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03873331209261027		[learning rate: 5.2295e-06]
	Learning Rate: 5.22952e-06
	LOSS [training: 0.03873331209261027 | validation: 0.05372862368548388]
	TIME [epoch: 64.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04297813120726841		[learning rate: 5.1926e-06]
	Learning Rate: 5.1926e-06
	LOSS [training: 0.04297813120726841 | validation: 0.055994435047046]
	TIME [epoch: 64.2 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03928258279409805		[learning rate: 5.1559e-06]
	Learning Rate: 5.15594e-06
	LOSS [training: 0.03928258279409805 | validation: 0.05471987831543166]
	TIME [epoch: 64.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04162247008588019		[learning rate: 5.1195e-06]
	Learning Rate: 5.11954e-06
	LOSS [training: 0.04162247008588019 | validation: 0.05223431888293423]
	TIME [epoch: 64.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03749977000410948		[learning rate: 5.0834e-06]
	Learning Rate: 5.0834e-06
	LOSS [training: 0.03749977000410948 | validation: 0.046472330267292]
	TIME [epoch: 64.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03798358873741125		[learning rate: 5.0475e-06]
	Learning Rate: 5.04751e-06
	LOSS [training: 0.03798358873741125 | validation: 0.05660542768612138]
	TIME [epoch: 64.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03802486965012458		[learning rate: 5.0119e-06]
	Learning Rate: 5.01188e-06
	LOSS [training: 0.03802486965012458 | validation: 0.05675567598241456]
	TIME [epoch: 64.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03760342881462936		[learning rate: 4.9765e-06]
	Learning Rate: 4.97649e-06
	LOSS [training: 0.03760342881462936 | validation: 0.04985852946430824]
	TIME [epoch: 64.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0384603253276994		[learning rate: 4.9414e-06]
	Learning Rate: 4.94136e-06
	LOSS [training: 0.0384603253276994 | validation: 0.05550108376414038]
	TIME [epoch: 64.2 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03962311170492949		[learning rate: 4.9065e-06]
	Learning Rate: 4.90647e-06
	LOSS [training: 0.03962311170492949 | validation: 0.05957093150267987]
	TIME [epoch: 64.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03777515660246987		[learning rate: 4.8718e-06]
	Learning Rate: 4.87183e-06
	LOSS [training: 0.03777515660246987 | validation: 0.055094485830162465]
	TIME [epoch: 64.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03936295499960836		[learning rate: 4.8374e-06]
	Learning Rate: 4.83744e-06
	LOSS [training: 0.03936295499960836 | validation: 0.05789470485488793]
	TIME [epoch: 64.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037861841707924795		[learning rate: 4.8033e-06]
	Learning Rate: 4.80329e-06
	LOSS [training: 0.037861841707924795 | validation: 0.055628090763516026]
	TIME [epoch: 64.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04173754174436432		[learning rate: 4.7694e-06]
	Learning Rate: 4.76938e-06
	LOSS [training: 0.04173754174436432 | validation: 0.0570668901873938]
	TIME [epoch: 64.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03809251530758813		[learning rate: 4.7357e-06]
	Learning Rate: 4.73571e-06
	LOSS [training: 0.03809251530758813 | validation: 0.05181597350040528]
	TIME [epoch: 64.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04075037181479191		[learning rate: 4.7023e-06]
	Learning Rate: 4.70227e-06
	LOSS [training: 0.04075037181479191 | validation: 0.04880020928991814]
	TIME [epoch: 64.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038467936176177796		[learning rate: 4.6691e-06]
	Learning Rate: 4.66908e-06
	LOSS [training: 0.038467936176177796 | validation: 0.05325806532514095]
	TIME [epoch: 64.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04062599401324149		[learning rate: 4.6361e-06]
	Learning Rate: 4.63611e-06
	LOSS [training: 0.04062599401324149 | validation: 0.050012329744974715]
	TIME [epoch: 64.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038354558379964654		[learning rate: 4.6034e-06]
	Learning Rate: 4.60338e-06
	LOSS [training: 0.038354558379964654 | validation: 0.05055850663280823]
	TIME [epoch: 64.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03807307045883859		[learning rate: 4.5709e-06]
	Learning Rate: 4.57088e-06
	LOSS [training: 0.03807307045883859 | validation: 0.05615542943610056]
	TIME [epoch: 64.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03644271166448591		[learning rate: 4.5386e-06]
	Learning Rate: 4.53861e-06
	LOSS [training: 0.03644271166448591 | validation: 0.051984016872337904]
	TIME [epoch: 64.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03708778603138048		[learning rate: 4.5066e-06]
	Learning Rate: 4.50657e-06
	LOSS [training: 0.03708778603138048 | validation: 0.05739012138464068]
	TIME [epoch: 64.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03828641097814731		[learning rate: 4.4748e-06]
	Learning Rate: 4.47475e-06
	LOSS [training: 0.03828641097814731 | validation: 0.055587150541368605]
	TIME [epoch: 64.4 sec]
EPOCH 1115/2000:
	Training over batches...
