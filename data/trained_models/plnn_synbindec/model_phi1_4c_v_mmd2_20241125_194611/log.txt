Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2254578880

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.767033716344886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.767033716344886 | validation: 5.036397816684716]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.12933881134283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.12933881134283 | validation: 4.53017246376261]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9518807268816865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9518807268816865 | validation: 4.735734666116798]
	TIME [epoch: 2.86 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.960421332723961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.960421332723961 | validation: 4.627002976744]
	TIME [epoch: 2.86 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.836288651673273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.836288651673273 | validation: 4.307121174318937]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.57912584466552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.57912584466552 | validation: 4.132831054400948]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.477010753360154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.477010753360154 | validation: 4.118862360616035]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.400088685511196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.400088685511196 | validation: 4.278464898691129]
	TIME [epoch: 2.86 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.513476186850035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.513476186850035 | validation: 4.311431678531112]
	TIME [epoch: 2.86 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.66784656106986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.66784656106986 | validation: 3.955857227541457]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.196064560206838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.196064560206838 | validation: 3.9600805639173178]
	TIME [epoch: 2.84 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.194535851773044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.194535851773044 | validation: 3.768062255575973]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.09955160092977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.09955160092977 | validation: 3.7494804135767694]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.045584948881125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.045584948881125 | validation: 3.7018104086246995]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.998853084447657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.998853084447657 | validation: 3.660927410154308]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9421889331314266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9421889331314266 | validation: 3.6148148669095947]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8726596021245827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8726596021245827 | validation: 3.5550335384849734]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.888613675278267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.888613675278267 | validation: 3.4516021344534495]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7996138594455227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7996138594455227 | validation: 3.6639090650743102]
	TIME [epoch: 2.86 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8762225315321417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8762225315321417 | validation: 3.3662935779259096]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.250360647687295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.250360647687295 | validation: 3.0763481341436565]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9250868489405026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9250868489405026 | validation: 2.990735925159344]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0744807797632587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0744807797632587 | validation: 2.2961114832491556]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3885971482919444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3885971482919444 | validation: 2.1189623337800807]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.047791092914595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047791092914595 | validation: 1.5878595889856992]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5789030802231503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5789030802231503 | validation: 1.8600766783964433]
	TIME [epoch: 2.86 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9761820424833532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9761820424833532 | validation: 1.6453667159425365]
	TIME [epoch: 2.85 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6145313539637618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6145313539637618 | validation: 1.4565088303695952]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4046272043005406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4046272043005406 | validation: 1.3165108667690881]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.348062835199674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.348062835199674 | validation: 1.1972839188107327]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0894180775014883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0894180775014883 | validation: 1.2755023752911008]
	TIME [epoch: 2.84 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1835235677952969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1835235677952969 | validation: 1.1675300017914656]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049423039876509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.049423039876509 | validation: 1.1226114365995128]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0696835226179682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0696835226179682 | validation: 1.2218151276056983]
	TIME [epoch: 2.85 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0685123961958858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0685123961958858 | validation: 1.0302228963636322]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0657022240749967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0657022240749967 | validation: 1.2587145815580112]
	TIME [epoch: 2.86 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0979079794450208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0979079794450208 | validation: 1.0933216281019396]
	TIME [epoch: 2.86 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0097475991461076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0097475991461076 | validation: 1.0030063170257901]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.045932389074601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.045932389074601 | validation: 1.2146466096679267]
	TIME [epoch: 2.85 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0771925296687708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0771925296687708 | validation: 1.020535741856088]
	TIME [epoch: 2.85 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9760242116325683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9760242116325683 | validation: 1.0621949192292768]
	TIME [epoch: 2.86 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2584500526850204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2584500526850204 | validation: 1.412457416068283]
	TIME [epoch: 2.85 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3595170505139522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3595170505139522 | validation: 0.9895598166340648]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.05139812597683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.05139812597683 | validation: 1.0315679001863078]
	TIME [epoch: 2.86 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142648131896313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1142648131896313 | validation: 0.9989877879402388]
	TIME [epoch: 2.85 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0039981851189816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0039981851189816 | validation: 0.9959678713325772]
	TIME [epoch: 2.85 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.015593432629838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.015593432629838 | validation: 0.9074382169025927]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9382299992505463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9382299992505463 | validation: 0.9098823124421043]
	TIME [epoch: 2.85 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0055683173407683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0055683173407683 | validation: 0.9610045945277514]
	TIME [epoch: 2.86 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9997046813621847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9997046813621847 | validation: 0.8630041614928583]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325559415129342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9325559415129342 | validation: 0.9323469177285539]
	TIME [epoch: 2.86 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642447606555996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9642447606555996 | validation: 0.8708273258286753]
	TIME [epoch: 2.85 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9392207757267577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9392207757267577 | validation: 0.8667249688604489]
	TIME [epoch: 2.85 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9237214184076169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9237214184076169 | validation: 0.8522057814305726]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077317300629616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9077317300629616 | validation: 0.8731359778403966]
	TIME [epoch: 2.85 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9912875931023344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9912875931023344 | validation: 1.1031438694494813]
	TIME [epoch: 2.85 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0896282993406754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0896282993406754 | validation: 0.9049402014542139]
	TIME [epoch: 2.84 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0724907023707677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0724907023707677 | validation: 0.8724042538762075]
	TIME [epoch: 2.85 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280111300334133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280111300334133 | validation: 0.8789940136429909]
	TIME [epoch: 2.85 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039955060244279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9039955060244279 | validation: 0.8194777435530907]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248398388807008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248398388807008 | validation: 0.8510764979294294]
	TIME [epoch: 2.85 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933387225472464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8933387225472464 | validation: 0.8438705776915754]
	TIME [epoch: 2.85 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9148760831562952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9148760831562952 | validation: 0.9150179669337279]
	TIME [epoch: 2.85 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9685413685930138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9685413685930138 | validation: 0.7968393758571292]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218978475057863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9218978475057863 | validation: 0.9016844380976319]
	TIME [epoch: 2.86 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448715128808798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9448715128808798 | validation: 0.8006072234650727]
	TIME [epoch: 2.85 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9426080384768173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9426080384768173 | validation: 0.8451897240478949]
	TIME [epoch: 2.84 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242514516345415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9242514516345415 | validation: 0.7911513173554253]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743835786821614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743835786821614 | validation: 0.8079076399756048]
	TIME [epoch: 2.86 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990749285657248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8990749285657248 | validation: 0.8211705349982403]
	TIME [epoch: 2.86 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9383510787847005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9383510787847005 | validation: 0.896488770322899]
	TIME [epoch: 2.85 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9613645145472688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9613645145472688 | validation: 0.7613470728683245]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956785637692656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956785637692656 | validation: 0.8360506843834458]
	TIME [epoch: 2.86 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.900798230581645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.900798230581645 | validation: 0.795321334136426]
	TIME [epoch: 2.86 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068656322709009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068656322709009 | validation: 0.8093427948643556]
	TIME [epoch: 2.85 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830007445259596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8830007445259596 | validation: 0.7498691578440427]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813466932480967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813466932480967 | validation: 0.7952120048377926]
	TIME [epoch: 2.86 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850722889909036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8850722889909036 | validation: 0.7816164768674715]
	TIME [epoch: 2.85 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781943979025164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8781943979025164 | validation: 0.7897914127321285]
	TIME [epoch: 2.85 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9201078735688742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9201078735688742 | validation: 0.9794803541748891]
	TIME [epoch: 2.85 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0400451406684623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0400451406684623 | validation: 0.8210150015605241]
	TIME [epoch: 2.85 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013786855290108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.013786855290108 | validation: 0.7844171752899888]
	TIME [epoch: 2.85 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874155014908384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.874155014908384 | validation: 0.8443643504696537]
	TIME [epoch: 2.85 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921151493197063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.921151493197063 | validation: 0.7592148151449337]
	TIME [epoch: 2.85 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937155694379592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937155694379592 | validation: 0.7964093326648612]
	TIME [epoch: 2.85 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8882797898828588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8882797898828588 | validation: 0.7573138684902985]
	TIME [epoch: 2.85 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889906530404565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889906530404565 | validation: 0.8201715136816742]
	TIME [epoch: 2.85 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9103815065158607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9103815065158607 | validation: 0.767287169722116]
	TIME [epoch: 2.85 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243259008013438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9243259008013438 | validation: 0.7422338682108802]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8690215691999962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8690215691999962 | validation: 0.768886404091466]
	TIME [epoch: 2.86 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657334267208779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657334267208779 | validation: 0.7723789362040571]
	TIME [epoch: 2.85 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005986731480162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005986731480162 | validation: 0.851903860475786]
	TIME [epoch: 2.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640294389390869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9640294389390869 | validation: 0.8041867132842868]
	TIME [epoch: 2.85 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386217561847124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9386217561847124 | validation: 0.7792463995351343]
	TIME [epoch: 2.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034989363393837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9034989363393837 | validation: 0.8106621517415352]
	TIME [epoch: 2.86 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907211804957195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8907211804957195 | validation: 0.747988320448229]
	TIME [epoch: 2.85 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861201553548383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861201553548383 | validation: 0.795273198483968]
	TIME [epoch: 2.86 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8898454930435001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8898454930435001 | validation: 0.768562421197901]
	TIME [epoch: 2.85 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9203725918107927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9203725918107927 | validation: 0.7787209965715651]
	TIME [epoch: 2.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016608116846077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016608116846077 | validation: 0.7501542340998104]
	TIME [epoch: 2.85 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811212454352628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811212454352628 | validation: 0.7467389875136806]
	TIME [epoch: 2.85 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766339713757622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8766339713757622 | validation: 0.7820684165689076]
	TIME [epoch: 2.85 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311810493620835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9311810493620835 | validation: 0.8063034040199266]
	TIME [epoch: 2.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9532377114870929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9532377114870929 | validation: 0.7907803173549987]
	TIME [epoch: 2.86 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440411519040852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9440411519040852 | validation: 0.7476651693219929]
	TIME [epoch: 2.86 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684579043134184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8684579043134184 | validation: 0.73139640414551]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758578647451397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758578647451397 | validation: 0.7386012884264402]
	TIME [epoch: 2.85 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705485640820905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8705485640820905 | validation: 0.7356036016986837]
	TIME [epoch: 2.91 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701054328187442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8701054328187442 | validation: 0.794577505777199]
	TIME [epoch: 2.85 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9452401557147775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9452401557147775 | validation: 0.7433557373188798]
	TIME [epoch: 2.85 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861262296910022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861262296910022 | validation: 0.825434508021424]
	TIME [epoch: 2.85 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010500491486165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010500491486165 | validation: 0.7806480987916453]
	TIME [epoch: 2.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998253399528433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998253399528433 | validation: 0.8964357964684583]
	TIME [epoch: 2.84 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0318086189839577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0318086189839577 | validation: 0.7781133669949241]
	TIME [epoch: 2.85 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929326226784367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.929326226784367 | validation: 0.816371778964601]
	TIME [epoch: 2.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9296446511550147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9296446511550147 | validation: 0.7255541575129032]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732681798089368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8732681798089368 | validation: 0.734538909500499]
	TIME [epoch: 2.86 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8820846624298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8820846624298 | validation: 0.735686339878987]
	TIME [epoch: 2.85 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694738250538094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8694738250538094 | validation: 0.7407658725144087]
	TIME [epoch: 2.85 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831864698452304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831864698452304 | validation: 0.7807484286349585]
	TIME [epoch: 2.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9586928675106842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9586928675106842 | validation: 0.7504381935824052]
	TIME [epoch: 2.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713839160907715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713839160907715 | validation: 0.7236644843765124]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528222610224373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528222610224373 | validation: 0.7332338778815978]
	TIME [epoch: 2.85 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869130458862626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869130458862626 | validation: 0.7294892554321665]
	TIME [epoch: 2.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876734405779546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876734405779546 | validation: 0.7875677504974931]
	TIME [epoch: 2.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980592079502796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9980592079502796 | validation: 0.7345725405171062]
	TIME [epoch: 2.85 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8807366761847979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8807366761847979 | validation: 0.7552278016146143]
	TIME [epoch: 2.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865248305893417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8865248305893417 | validation: 0.756085154680012]
	TIME [epoch: 2.85 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9195468988015904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9195468988015904 | validation: 0.7648447408174128]
	TIME [epoch: 2.85 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879380543701599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879380543701599 | validation: 0.7168722888134669]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007442597438745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9007442597438745 | validation: 0.7297525882718873]
	TIME [epoch: 2.86 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648102832460791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8648102832460791 | validation: 0.718368331408736]
	TIME [epoch: 2.86 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8548067521861359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8548067521861359 | validation: 0.7115382144278103]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84704147295556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84704147295556 | validation: 0.7520955978998567]
	TIME [epoch: 2.86 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8924607356209984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8924607356209984 | validation: 0.7674441593601373]
	TIME [epoch: 2.86 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504385631946562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9504385631946562 | validation: 0.7177156844155275]
	TIME [epoch: 2.86 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556845888631862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556845888631862 | validation: 0.7248228934728931]
	TIME [epoch: 2.85 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549070484072957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8549070484072957 | validation: 0.7859379653496275]
	TIME [epoch: 2.85 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9449684260600326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9449684260600326 | validation: 0.744256314367091]
	TIME [epoch: 2.86 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724778340577706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724778340577706 | validation: 0.7222678920727623]
	TIME [epoch: 2.86 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584344680782641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584344680782641 | validation: 0.7152670417005496]
	TIME [epoch: 2.86 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613621371264002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8613621371264002 | validation: 0.7438435061768325]
	TIME [epoch: 2.86 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760638665089223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760638665089223 | validation: 0.824697842635953]
	TIME [epoch: 2.86 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9849052474337319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9849052474337319 | validation: 0.8022753400863547]
	TIME [epoch: 2.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194722871446317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9194722871446317 | validation: 0.8011516612619506]
	TIME [epoch: 2.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342373850049359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9342373850049359 | validation: 0.7106159860374549]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460669968941139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8460669968941139 | validation: 0.7717363412272986]
	TIME [epoch: 2.85 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883480693167716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883480693167716 | validation: 0.7328613356114051]
	TIME [epoch: 2.84 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681660557591292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681660557591292 | validation: 0.7179270244807742]
	TIME [epoch: 2.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420892658103455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420892658103455 | validation: 0.7872282824236955]
	TIME [epoch: 2.85 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9559912521240045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9559912521240045 | validation: 0.746148588008321]
	TIME [epoch: 2.84 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866382393310553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.866382393310553 | validation: 0.7206881523074122]
	TIME [epoch: 2.84 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844179069114234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844179069114234 | validation: 0.7475199155617732]
	TIME [epoch: 2.84 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8914457040712048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8914457040712048 | validation: 0.7582254046725234]
	TIME [epoch: 2.84 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764242603073104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8764242603073104 | validation: 0.7372912528356077]
	TIME [epoch: 2.84 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012695640100605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012695640100605 | validation: 0.8911998570277755]
	TIME [epoch: 2.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9707018150476643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9707018150476643 | validation: 0.8453879748167603]
	TIME [epoch: 2.85 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9907356053798989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9907356053798989 | validation: 0.8018558012024928]
	TIME [epoch: 2.85 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194036155115061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194036155115061 | validation: 1.2821448779422462]
	TIME [epoch: 2.85 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.463000309731471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.463000309731471 | validation: 0.8028479193535043]
	TIME [epoch: 2.84 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9635710859855947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9635710859855947 | validation: 0.8238946495421182]
	TIME [epoch: 2.84 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640590296615154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9640590296615154 | validation: 0.7974173967417223]
	TIME [epoch: 2.84 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335060834047338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9335060834047338 | validation: 0.715600406051017]
	TIME [epoch: 2.84 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597524484708147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597524484708147 | validation: 0.7519394494246403]
	TIME [epoch: 2.84 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860128839779683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860128839779683 | validation: 0.7151584128510842]
	TIME [epoch: 2.84 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517141774287126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8517141774287126 | validation: 0.7093313817028197]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520110478214216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520110478214216 | validation: 0.7242468520048548]
	TIME [epoch: 2.86 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667587670807927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667587670807927 | validation: 0.7297631874721202]
	TIME [epoch: 2.85 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795297850514159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8795297850514159 | validation: 0.7118042587587832]
	TIME [epoch: 2.86 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.863041687913955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.863041687913955 | validation: 0.7231221006513904]
	TIME [epoch: 2.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533151776336311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533151776336311 | validation: 0.7192665107042538]
	TIME [epoch: 2.86 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459893400812101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8459893400812101 | validation: 0.71013219211574]
	TIME [epoch: 2.86 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419235187997799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8419235187997799 | validation: 0.7330696010782733]
	TIME [epoch: 2.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526712321747172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526712321747172 | validation: 0.7074776744510732]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8436026607871888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8436026607871888 | validation: 0.7685836842601671]
	TIME [epoch: 2.86 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9061051956285677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9061051956285677 | validation: 0.7439165219079311]
	TIME [epoch: 2.86 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152474371425958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152474371425958 | validation: 0.743567712960568]
	TIME [epoch: 2.86 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862553437918831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.862553437918831 | validation: 0.7751132812169608]
	TIME [epoch: 2.86 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934667749070088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934667749070088 | validation: 0.7300321650596877]
	TIME [epoch: 2.86 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869901870599662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869901870599662 | validation: 0.7919922314177525]
	TIME [epoch: 2.86 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926532646379033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926532646379033 | validation: 0.8141351222717343]
	TIME [epoch: 2.86 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9948481062363278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9948481062363278 | validation: 0.7166368145855663]
	TIME [epoch: 2.86 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.847244529103987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.847244529103987 | validation: 0.7916602551905207]
	TIME [epoch: 2.86 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245197369443426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245197369443426 | validation: 0.7342320440363701]
	TIME [epoch: 2.86 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054878605161798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9054878605161798 | validation: 0.7119684416761145]
	TIME [epoch: 2.86 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8354361157411075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8354361157411075 | validation: 0.7806699093509647]
	TIME [epoch: 2.86 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072331133710304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072331133710304 | validation: 0.7119440434240277]
	TIME [epoch: 2.86 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642385081423529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642385081423529 | validation: 0.7818726098764897]
	TIME [epoch: 2.86 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833920222846419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8833920222846419 | validation: 0.8094570440186506]
	TIME [epoch: 2.86 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066527450692613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9066527450692613 | validation: 0.709649682609189]
	TIME [epoch: 2.86 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812327589817506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812327589817506 | validation: 0.6938077664015841]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909508633450785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909508633450785 | validation: 5.743380344750936]
	TIME [epoch: 2.86 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.778333539660665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.778333539660665 | validation: 2.85188830795475]
	TIME [epoch: 2.86 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9996854508046624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9996854508046624 | validation: 2.3235688538558805]
	TIME [epoch: 2.86 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.56280638762512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.56280638762512 | validation: 1.3675612365616439]
	TIME [epoch: 2.86 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5943724270392818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5943724270392818 | validation: 0.8441741586817217]
	TIME [epoch: 2.86 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9634073030601076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9634073030601076 | validation: 0.9331580121481764]
	TIME [epoch: 2.86 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0814325040462385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0814325040462385 | validation: 0.8201743056923491]
	TIME [epoch: 2.86 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.961559762013128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.961559762013128 | validation: 0.7999788400736437]
	TIME [epoch: 2.86 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026474306021317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026474306021317 | validation: 0.7907360901061672]
	TIME [epoch: 2.86 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774644664903758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774644664903758 | validation: 0.7474219290855479]
	TIME [epoch: 181 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732999181347217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8732999181347217 | validation: 0.7517641167633861]
	TIME [epoch: 6.15 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646775859454721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646775859454721 | validation: 0.7497561183066015]
	TIME [epoch: 6.13 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861459499791482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.861459499791482 | validation: 0.7404476036192842]
	TIME [epoch: 6.12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859770576782771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859770576782771 | validation: 0.7418110371475675]
	TIME [epoch: 6.14 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545502273600437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545502273600437 | validation: 0.7270171551367168]
	TIME [epoch: 6.13 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505346948806655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505346948806655 | validation: 0.7308503228358894]
	TIME [epoch: 6.15 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8485030583634741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8485030583634741 | validation: 0.7314881783277963]
	TIME [epoch: 6.13 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529473710310006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8529473710310006 | validation: 0.7380115303846346]
	TIME [epoch: 6.14 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515994467598625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515994467598625 | validation: 0.732833712141217]
	TIME [epoch: 6.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448246622471718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448246622471718 | validation: 0.728026993523299]
	TIME [epoch: 6.13 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486474028317065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8486474028317065 | validation: 0.754360211675794]
	TIME [epoch: 6.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425550246619116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425550246619116 | validation: 0.7305404181303091]
	TIME [epoch: 6.13 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496932533504287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496932533504287 | validation: 0.7333718056630019]
	TIME [epoch: 6.13 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501232540601477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501232540601477 | validation: 0.7605949888297152]
	TIME [epoch: 6.14 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531255226631297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531255226631297 | validation: 0.7207004607894898]
	TIME [epoch: 6.13 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8567869213753184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8567869213753184 | validation: 0.7549240769692469]
	TIME [epoch: 6.13 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500309225808583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8500309225808583 | validation: 0.7196077028183587]
	TIME [epoch: 6.13 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568417528932376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568417528932376 | validation: 0.7041667648002332]
	TIME [epoch: 6.14 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844531468295712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844531468295712 | validation: 0.7552750768793799]
	TIME [epoch: 6.13 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598830038464186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8598830038464186 | validation: 0.777018266010927]
	TIME [epoch: 6.12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9548174964707231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9548174964707231 | validation: 0.7572913728240929]
	TIME [epoch: 6.13 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747823022766406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747823022766406 | validation: 0.7550297554513187]
	TIME [epoch: 6.13 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906887194796772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906887194796772 | validation: 0.6968394053879069]
	TIME [epoch: 6.13 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493661267988452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8493661267988452 | validation: 0.7199077204830107]
	TIME [epoch: 6.13 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845703724968597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.845703724968597 | validation: 0.7186339350035273]
	TIME [epoch: 6.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581240184058403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8581240184058403 | validation: 0.7071777535105261]
	TIME [epoch: 6.12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730771484882923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730771484882923 | validation: 0.783066246819662]
	TIME [epoch: 6.12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825663368434932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8825663368434932 | validation: 0.7326004268706671]
	TIME [epoch: 6.12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009321607670827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009321607670827 | validation: 0.7356392156458387]
	TIME [epoch: 6.14 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859936958954027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859936958954027 | validation: 0.7811876574409419]
	TIME [epoch: 6.13 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603772534882026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603772534882026 | validation: 0.701114377165529]
	TIME [epoch: 6.13 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8395810399490259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8395810399490259 | validation: 0.7600043806706819]
	TIME [epoch: 6.13 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838529915055024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8838529915055024 | validation: 0.7213610802198638]
	TIME [epoch: 6.13 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070944637049302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9070944637049302 | validation: 0.7085449410073021]
	TIME [epoch: 6.13 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237888169152647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8237888169152647 | validation: 0.7358375143840298]
	TIME [epoch: 6.12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8801607992741245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8801607992741245 | validation: 0.7426149961154792]
	TIME [epoch: 6.12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308026064642175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9308026064642175 | validation: 0.7396869355040956]
	TIME [epoch: 6.12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329876148177212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329876148177212 | validation: 0.7845274376948139]
	TIME [epoch: 6.13 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194765226681966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9194765226681966 | validation: 0.7141991480995769]
	TIME [epoch: 6.13 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185798300811665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185798300811665 | validation: 0.7000591932907557]
	TIME [epoch: 6.12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231488602393648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231488602393648 | validation: 0.6802113513298063]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822184421619762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822184421619762 | validation: 0.7356779936476201]
	TIME [epoch: 6.13 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320576627673382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320576627673382 | validation: 0.7777479247378896]
	TIME [epoch: 6.12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484141398264524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8484141398264524 | validation: 0.7590489618528468]
	TIME [epoch: 6.13 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809595494210337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809595494210337 | validation: 4.088447801455676]
	TIME [epoch: 6.12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.551800173995177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.551800173995177 | validation: 0.9214737251297422]
	TIME [epoch: 6.12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9147478025694143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9147478025694143 | validation: 0.7219254572442607]
	TIME [epoch: 6.12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055357821596933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055357821596933 | validation: 0.7379921026410798]
	TIME [epoch: 6.13 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277649392367061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277649392367061 | validation: 0.7389487403248097]
	TIME [epoch: 6.13 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067947132142564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067947132142564 | validation: 0.7221434032836973]
	TIME [epoch: 6.13 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980548411563939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980548411563939 | validation: 0.7107350936881472]
	TIME [epoch: 6.12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702114876398976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7702114876398976 | validation: 0.705330721898712]
	TIME [epoch: 6.13 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458764524270288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7458764524270288 | validation: 0.8080496543937725]
	TIME [epoch: 6.12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661423104804717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661423104804717 | validation: 0.8242381253101257]
	TIME [epoch: 6.12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342730166795481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8342730166795481 | validation: 0.8421265345773462]
	TIME [epoch: 6.12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9126569026816435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9126569026816435 | validation: 0.7918726945092781]
	TIME [epoch: 6.12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9694827904951996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9694827904951996 | validation: 0.7537296828448925]
	TIME [epoch: 6.13 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613422112574122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8613422112574122 | validation: 0.7590102899626331]
	TIME [epoch: 6.13 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652987335218422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8652987335218422 | validation: 0.6941231593554731]
	TIME [epoch: 6.13 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888678013667665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7888678013667665 | validation: 0.6938870538060874]
	TIME [epoch: 6.12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835988557998854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7835988557998854 | validation: 0.6864639066373199]
	TIME [epoch: 6.12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733181298021793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733181298021793 | validation: 0.9291054684903476]
	TIME [epoch: 6.13 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142671018866733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9142671018866733 | validation: 1.2006206307292002]
	TIME [epoch: 6.12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366372186305496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.366372186305496 | validation: 0.7688608715157385]
	TIME [epoch: 6.13 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714382518317902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714382518317902 | validation: 0.8554964774525806]
	TIME [epoch: 6.12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0134289445824185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0134289445824185 | validation: 0.7687078294099048]
	TIME [epoch: 6.13 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025290965552898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025290965552898 | validation: 0.7588073928122637]
	TIME [epoch: 6.12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736596905734494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8736596905734494 | validation: 0.7394487272188225]
	TIME [epoch: 6.13 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243724318631906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8243724318631906 | validation: 0.722475560042256]
	TIME [epoch: 6.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187008493274153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8187008493274153 | validation: 0.7200239267010877]
	TIME [epoch: 6.13 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270040526658946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270040526658946 | validation: 0.7226897271755863]
	TIME [epoch: 6.12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169452707790273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169452707790273 | validation: 0.7106771081485905]
	TIME [epoch: 6.12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107821595610842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8107821595610842 | validation: 0.6741263103223221]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034101223534873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8034101223534873 | validation: 0.6803818120198809]
	TIME [epoch: 6.12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940625865683151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7940625865683151 | validation: 0.6880139203186888]
	TIME [epoch: 6.12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814672119164686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814672119164686 | validation: 0.6759073880357775]
	TIME [epoch: 6.13 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664594612761977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664594612761977 | validation: 0.6649623208062465]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364027617180436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364027617180436 | validation: 0.6172140571908371]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740675192764151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6740675192764151 | validation: 0.7339658496182604]
	TIME [epoch: 6.12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165765119575167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165765119575167 | validation: 1.0919289318281036]
	TIME [epoch: 6.12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2226136092883053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2226136092883053 | validation: 1.0028107341726908]
	TIME [epoch: 6.13 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2185365826109094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2185365826109094 | validation: 1.001275638683042]
	TIME [epoch: 6.13 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2208152692706158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2208152692706158 | validation: 0.7116094460211473]
	TIME [epoch: 6.12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505605048612642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505605048612642 | validation: 0.9112964295087006]
	TIME [epoch: 6.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0161734151327455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0161734151327455 | validation: 0.757563757922763]
	TIME [epoch: 6.12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856557478026676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8856557478026676 | validation: 0.7388489945822357]
	TIME [epoch: 6.12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698512727906819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8698512727906819 | validation: 0.7150557907068943]
	TIME [epoch: 6.13 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074631495543926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8074631495543926 | validation: 0.6961624145384473]
	TIME [epoch: 6.12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944002761628671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7944002761628671 | validation: 0.6472461270304951]
	TIME [epoch: 6.13 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642815557291002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642815557291002 | validation: 0.6372287856245094]
	TIME [epoch: 6.13 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126661901208882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126661901208882 | validation: 0.8024217324792867]
	TIME [epoch: 6.12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059428103268564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8059428103268564 | validation: 1.2536767640726545]
	TIME [epoch: 6.13 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4273648607031733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4273648607031733 | validation: 0.7458933906914234]
	TIME [epoch: 6.12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365930034231397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365930034231397 | validation: 0.8534897211819139]
	TIME [epoch: 6.12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0006110740756395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0006110740756395 | validation: 0.7543074413739927]
	TIME [epoch: 6.12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984711253659206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984711253659206 | validation: 0.7521869614298078]
	TIME [epoch: 6.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587671511271782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8587671511271782 | validation: 0.7389456850826305]
	TIME [epoch: 6.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308989680307778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8308989680307778 | validation: 0.7289464563359173]
	TIME [epoch: 6.13 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865295667157221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.865295667157221 | validation: 0.7153915446028747]
	TIME [epoch: 6.12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176779174140808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176779174140808 | validation: 0.7085843997552312]
	TIME [epoch: 6.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353645649020891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353645649020891 | validation: 0.6802096203684617]
	TIME [epoch: 6.12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207052103996683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207052103996683 | validation: 0.6922850791333423]
	TIME [epoch: 6.13 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941119269810354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941119269810354 | validation: 0.6958722994220032]
	TIME [epoch: 6.12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876638156743985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876638156743985 | validation: 0.6715190545149039]
	TIME [epoch: 6.12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732880937045975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7732880937045975 | validation: 0.6713256491052878]
	TIME [epoch: 6.13 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574491769003197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574491769003197 | validation: 0.6244603478969464]
	TIME [epoch: 6.12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058850556133213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058850556133213 | validation: 1.563227898197742]
	TIME [epoch: 6.13 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5248195518253096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5248195518253096 | validation: 1.7921971304539512]
	TIME [epoch: 6.12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.003082863002342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.003082863002342 | validation: 1.1246576520745215]
	TIME [epoch: 6.13 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2846431830409761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2846431830409761 | validation: 0.7783096864150925]
	TIME [epoch: 6.13 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9051737845171186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9051737845171186 | validation: 0.825934437415615]
	TIME [epoch: 6.13 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9649413634865559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9649413634865559 | validation: 0.7376821631108665]
	TIME [epoch: 6.12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499939453170963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499939453170963 | validation: 0.7502842035467836]
	TIME [epoch: 6.12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8801475558132086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8801475558132086 | validation: 0.7233920501448721]
	TIME [epoch: 6.13 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252936759490166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8252936759490166 | validation: 0.7007011925806857]
	TIME [epoch: 6.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156430109228242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8156430109228242 | validation: 0.697954252853119]
	TIME [epoch: 6.14 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049014147231658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049014147231658 | validation: 0.7032743138144072]
	TIME [epoch: 6.13 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021204399339067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021204399339067 | validation: 0.7119614378759679]
	TIME [epoch: 6.13 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017217298697003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8017217298697003 | validation: 0.7039157307569414]
	TIME [epoch: 6.13 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913745627267139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7913745627267139 | validation: 0.6770576603176097]
	TIME [epoch: 6.13 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910295379088719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7910295379088719 | validation: 0.6703976552175203]
	TIME [epoch: 6.12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728938777001395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7728938777001395 | validation: 0.6792997982593705]
	TIME [epoch: 6.13 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745863957996093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745863957996093 | validation: 0.6449095647607512]
	TIME [epoch: 6.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314464581554222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7314464581554222 | validation: 0.7572780351375513]
	TIME [epoch: 6.14 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737316282074411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7737316282074411 | validation: 1.1432023272222922]
	TIME [epoch: 6.13 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455556149450606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2455556149450606 | validation: 0.8015113977459871]
	TIME [epoch: 6.13 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9526633422371437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9526633422371437 | validation: 0.8249959545520167]
	TIME [epoch: 6.13 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950903462120782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.950903462120782 | validation: 0.6517259223814762]
	TIME [epoch: 6.12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092248415315072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092248415315072 | validation: 0.7088562599781195]
	TIME [epoch: 6.13 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716568984568552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.716568984568552 | validation: 1.4469026116969161]
	TIME [epoch: 6.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3989170107840727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3989170107840727 | validation: 1.0389163147884382]
	TIME [epoch: 6.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344319715641336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1344319715641336 | validation: 0.755386324178283]
	TIME [epoch: 6.12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920227147929656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920227147929656 | validation: 0.7298075817353101]
	TIME [epoch: 6.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610736374731245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8610736374731245 | validation: 0.7035536534005509]
	TIME [epoch: 6.12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150057641247032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150057641247032 | validation: 0.7300809041852201]
	TIME [epoch: 6.12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845643859675954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.845643859675954 | validation: 0.7101770851387006]
	TIME [epoch: 6.12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002216628337729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002216628337729 | validation: 0.6667826583282436]
	TIME [epoch: 6.12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966861111467336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966861111467336 | validation: 0.6766782053881937]
	TIME [epoch: 6.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656619854769369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656619854769369 | validation: 0.6437523240846401]
	TIME [epoch: 6.12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403667994740946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403667994740946 | validation: 0.6024168625708809]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887085159911612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887085159911612 | validation: 0.7485275551608145]
	TIME [epoch: 6.1 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553276597366637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553276597366637 | validation: 1.0008082893661274]
	TIME [epoch: 6.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0645169392166727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0645169392166727 | validation: 0.6358228904569568]
	TIME [epoch: 6.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688163819691936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688163819691936 | validation: 0.6689270810062751]
	TIME [epoch: 6.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751606579532529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751606579532529 | validation: 0.6690306839623444]
	TIME [epoch: 6.09 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882359764925293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882359764925293 | validation: 0.7229690800508211]
	TIME [epoch: 6.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719515814482485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719515814482485 | validation: 0.8716966994836955]
	TIME [epoch: 6.09 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504055299550178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9504055299550178 | validation: 0.6541701037407441]
	TIME [epoch: 6.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637696950444223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637696950444223 | validation: 0.6616337005836572]
	TIME [epoch: 6.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400980719959164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7400980719959164 | validation: 0.6311040149430563]
	TIME [epoch: 6.12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492922869961291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6492922869961291 | validation: 0.7777610695095629]
	TIME [epoch: 6.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623629680506171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623629680506171 | validation: 0.9911772851433261]
	TIME [epoch: 6.11 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047852130696279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.047852130696279 | validation: 0.6633977695305688]
	TIME [epoch: 6.11 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7381681371924208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7381681371924208 | validation: 0.6750307263465701]
	TIME [epoch: 6.12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622329613305401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7622329613305401 | validation: 0.6034551272262846]
	TIME [epoch: 6.12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546258311062545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6546258311062545 | validation: 0.5859743730312308]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6039716196203856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6039716196203856 | validation: 0.7408056170709898]
	TIME [epoch: 6.13 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668283793203383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668283793203383 | validation: 0.9999655994165227]
	TIME [epoch: 6.13 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0847357244011868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0847357244011868 | validation: 0.7516328979600726]
	TIME [epoch: 6.13 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149815657281811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149815657281811 | validation: 0.793526113359991]
	TIME [epoch: 6.12 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397677592225735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397677592225735 | validation: 0.707279979592163]
	TIME [epoch: 6.12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224400197818102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8224400197818102 | validation: 0.6801363938799142]
	TIME [epoch: 6.11 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600341256208182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600341256208182 | validation: 0.5608482700162462]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632973371448498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632973371448498 | validation: 1.0016021029039028]
	TIME [epoch: 6.13 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9396757649808712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9396757649808712 | validation: 1.0763766580889613]
	TIME [epoch: 6.12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1956381632428514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1956381632428514 | validation: 0.7816412167815212]
	TIME [epoch: 6.13 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730240161484099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730240161484099 | validation: 0.7430058474685713]
	TIME [epoch: 6.13 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603497949116948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8603497949116948 | validation: 0.7442147173459995]
	TIME [epoch: 6.12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267901804653544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8267901804653544 | validation: 0.6735336947294579]
	TIME [epoch: 6.13 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739326654434559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739326654434559 | validation: 0.5639522244381879]
	TIME [epoch: 6.13 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133305597028416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6133305597028416 | validation: 0.6359519261228898]
	TIME [epoch: 6.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788043862050784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6788043862050784 | validation: 0.9621710142742345]
	TIME [epoch: 6.12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0174208602733374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0174208602733374 | validation: 0.595271301999129]
	TIME [epoch: 6.13 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6087945605793652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6087945605793652 | validation: 0.6751321069698212]
	TIME [epoch: 6.13 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714937014699598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714937014699598 | validation: 0.7380938765429939]
	TIME [epoch: 6.14 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896242112518289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896242112518289 | validation: 0.5810291676791276]
	TIME [epoch: 6.13 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361610616719662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361610616719662 | validation: 0.6470120285236961]
	TIME [epoch: 6.12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787463851380354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6787463851380354 | validation: 0.7343628051670454]
	TIME [epoch: 6.13 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828488070594645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7828488070594645 | validation: 0.5276657855914344]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5706078179999983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5706078179999983 | validation: 0.5883011619152082]
	TIME [epoch: 6.09 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237448675784674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6237448675784674 | validation: 0.7583168479703479]
	TIME [epoch: 6.09 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8124743054216399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8124743054216399 | validation: 0.5823145201941691]
	TIME [epoch: 6.09 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660158729035099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660158729035099 | validation: 0.6298090062320626]
	TIME [epoch: 6.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6929539876521761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6929539876521761 | validation: 0.540826883350689]
	TIME [epoch: 6.13 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694722270478301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5694722270478301 | validation: 0.6030035149641283]
	TIME [epoch: 6.13 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5949408809574709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5949408809574709 | validation: 0.737357860133861]
	TIME [epoch: 6.13 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933110824146633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933110824146633 | validation: 0.5291694051326301]
	TIME [epoch: 6.13 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139806586256568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139806586256568 | validation: 0.5937983467512586]
	TIME [epoch: 6.13 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218724070241065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218724070241065 | validation: 0.8156659441683373]
	TIME [epoch: 6.13 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862626023460831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862626023460831 | validation: 0.5555435165080755]
	TIME [epoch: 6.13 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6114335403594283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6114335403594283 | validation: 0.8082966809877871]
	TIME [epoch: 6.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261818234075261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8261818234075261 | validation: 0.6667811875624899]
	TIME [epoch: 6.13 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379181284355767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379181284355767 | validation: 0.5934287645214863]
	TIME [epoch: 6.13 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607161004274855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6607161004274855 | validation: 0.513941345571311]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617225038186492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5617225038186492 | validation: 0.5163062177413823]
	TIME [epoch: 6.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5297838081837587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5297838081837587 | validation: 0.6823846610412716]
	TIME [epoch: 6.09 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127558245388838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7127558245388838 | validation: 0.5102855442662219]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5194929703397851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5194929703397851 | validation: 0.5424580327965227]
	TIME [epoch: 6.13 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679569179747168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679569179747168 | validation: 0.4690234208678362]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4895282284604661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4895282284604661 | validation: 0.46788630509651974]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.481107492318948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.481107492318948 | validation: 0.6844704994685985]
	TIME [epoch: 6.14 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685660912454486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6685660912454486 | validation: 0.8119913910678964]
	TIME [epoch: 6.13 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877046777283334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8877046777283334 | validation: 0.564998579339502]
	TIME [epoch: 6.14 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028117014011842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6028117014011842 | validation: 0.6868737786762544]
	TIME [epoch: 6.11 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946922701777346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946922701777346 | validation: 0.6062076403828858]
	TIME [epoch: 6.1 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6841039969689982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6841039969689982 | validation: 0.4608468695865419]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500557551025155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.500557551025155 | validation: 0.6910304618879823]
	TIME [epoch: 6.13 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521603899454043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521603899454043 | validation: 0.6593103793806416]
	TIME [epoch: 6.14 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966305903938234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966305903938234 | validation: 0.5128747674670407]
	TIME [epoch: 6.13 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5462811272338449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5462811272338449 | validation: 0.44700742719497727]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5042384248657279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5042384248657279 | validation: 0.4090418757822441]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109301075561352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4109301075561352 | validation: 0.44496158617745485]
	TIME [epoch: 6.14 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44169035115443445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44169035115443445 | validation: 0.5442722487917947]
	TIME [epoch: 6.11 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5840161311712658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5840161311712658 | validation: 0.43655091319300116]
	TIME [epoch: 6.11 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4599525950429119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4599525950429119 | validation: 0.37692109109072486]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39037507715999764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39037507715999764 | validation: 0.3766815438170757]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37449858026765165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37449858026765165 | validation: 0.5852035070830937]
	TIME [epoch: 6.11 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335845720684795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335845720684795 | validation: 0.5719881404192099]
	TIME [epoch: 6.11 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6199326439419551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6199326439419551 | validation: 0.4788097814877878]
	TIME [epoch: 6.11 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482712728769548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5482712728769548 | validation: 0.42706309104283185]
	TIME [epoch: 6.12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46595537285671884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46595537285671884 | validation: 0.5441045534209032]
	TIME [epoch: 6.11 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5242560525588191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5242560525588191 | validation: 0.6071331800573522]
	TIME [epoch: 6.12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592184642820343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592184642820343 | validation: 0.37375474472960124]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44493443682117517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44493443682117517 | validation: 0.6280900582592829]
	TIME [epoch: 6.11 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6107511409641764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6107511409641764 | validation: 0.5879517695115428]
	TIME [epoch: 6.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6020318464714766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6020318464714766 | validation: 0.3097929908803707]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33361788095161227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33361788095161227 | validation: 0.3303347384064798]
	TIME [epoch: 6.11 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34123883533772204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34123883533772204 | validation: 0.45125013206803577]
	TIME [epoch: 6.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45019292108511044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45019292108511044 | validation: 0.374962903656636]
	TIME [epoch: 6.1 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35023174875185165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35023174875185165 | validation: 0.36796463094911014]
	TIME [epoch: 6.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3989677477036207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3989677477036207 | validation: 0.3624876529525062]
	TIME [epoch: 6.1 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3540918670615675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3540918670615675 | validation: 0.4665539387880522]
	TIME [epoch: 6.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49729854583266153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49729854583266153 | validation: 0.30487485604292613]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30528909901440165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30528909901440165 | validation: 0.3905595663454253]
	TIME [epoch: 6.11 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38147160624869286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38147160624869286 | validation: 0.6424506285300613]
	TIME [epoch: 6.11 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7095567035596386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7095567035596386 | validation: 0.2736086240104708]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914800160017399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2914800160017399 | validation: 0.6651286006591743]
	TIME [epoch: 6.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6039972737337436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6039972737337436 | validation: 0.6256644375041072]
	TIME [epoch: 6.09 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031414998436405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7031414998436405 | validation: 0.6354214851129191]
	TIME [epoch: 6.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230907645937613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6230907645937613 | validation: 0.3096722713181314]
	TIME [epoch: 6.09 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3559694355951645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3559694355951645 | validation: 0.38437821331555977]
	TIME [epoch: 6.12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42485769225361153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42485769225361153 | validation: 0.37833335991534917]
	TIME [epoch: 6.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32612014098276393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32612014098276393 | validation: 0.2632373339221647]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052777619587375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3052777619587375 | validation: 0.2833485859060488]
	TIME [epoch: 6.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830666957448061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830666957448061 | validation: 0.2538267327809943]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29259829724148284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29259829724148284 | validation: 0.3479100260401304]
	TIME [epoch: 6.12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31529724907429374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31529724907429374 | validation: 0.3232920538819313]
	TIME [epoch: 6.12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3640099910432659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3640099910432659 | validation: 0.32759923002916175]
	TIME [epoch: 6.11 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28367606933587103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28367606933587103 | validation: 0.25509849717096744]
	TIME [epoch: 6.13 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29777525190138465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29777525190138465 | validation: 0.30657475963560255]
	TIME [epoch: 6.12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27504444761425995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27504444761425995 | validation: 0.3119735147418472]
	TIME [epoch: 6.13 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3633296860690693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3633296860690693 | validation: 0.24678498993234302]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23464702133075271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23464702133075271 | validation: 0.198820069731982]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2115211517193999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2115211517193999 | validation: 0.27201012734113056]
	TIME [epoch: 6.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24903829377986228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24903829377986228 | validation: 0.4153634108998011]
	TIME [epoch: 6.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709704864970756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4709704864970756 | validation: 0.2429420454533943]
	TIME [epoch: 6.11 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27503186121658874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27503186121658874 | validation: 0.8602560671646403]
	TIME [epoch: 6.12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490466250466762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7490466250466762 | validation: 0.49803829909926733]
	TIME [epoch: 6.13 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5639102347393362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5639102347393362 | validation: 0.2813461335847037]
	TIME [epoch: 6.12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29885894925873163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29885894925873163 | validation: 0.7698550294982889]
	TIME [epoch: 6.13 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684918233986442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684918233986442 | validation: 0.3714345634678281]
	TIME [epoch: 6.12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4036346531713473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4036346531713473 | validation: 0.31389988821505815]
	TIME [epoch: 6.13 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278765222422675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3278765222422675 | validation: 0.46321021943691054]
	TIME [epoch: 6.12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4422240098512352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4422240098512352 | validation: 0.3311266658088251]
	TIME [epoch: 6.12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35988767162529883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35988767162529883 | validation: 0.23706905261028616]
	TIME [epoch: 6.13 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585249644116094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2585249644116094 | validation: 0.20288983350985365]
	TIME [epoch: 6.13 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21239308748530722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21239308748530722 | validation: 0.19157452759212482]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1966296985284042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1966296985284042 | validation: 0.1591233093415043]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19877935169017769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19877935169017769 | validation: 0.19993456012015545]
	TIME [epoch: 6.11 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19401663919553375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19401663919553375 | validation: 0.15436762792623251]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849539103832167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849539103832167 | validation: 0.1611892831818619]
	TIME [epoch: 6.11 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17534948932222783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17534948932222783 | validation: 0.22981256060715458]
	TIME [epoch: 6.11 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208528006528897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2208528006528897 | validation: 0.47636231349237834]
	TIME [epoch: 6.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654585314480419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5654585314480419 | validation: 0.26378215867304716]
	TIME [epoch: 6.12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32741571925014273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32741571925014273 | validation: 0.6528912174324273]
	TIME [epoch: 6.13 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5414955558532494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5414955558532494 | validation: 0.589367223081933]
	TIME [epoch: 6.12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729937418327486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729937418327486 | validation: 0.663353672012211]
	TIME [epoch: 6.13 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8539569653928811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8539569653928811 | validation: 0.6235621624697812]
	TIME [epoch: 6.11 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968755323585307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7968755323585307 | validation: 0.4534001006834009]
	TIME [epoch: 6.12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673143739017306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5673143739017306 | validation: 1.037633990939918]
	TIME [epoch: 6.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675931518334712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8675931518334712 | validation: 0.4723541379478263]
	TIME [epoch: 6.11 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5286459700150742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5286459700150742 | validation: 0.34053856198153576]
	TIME [epoch: 6.12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32020726738552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32020726738552 | validation: 0.6803970385072009]
	TIME [epoch: 6.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5713413155724688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5713413155724688 | validation: 0.3473080654027621]
	TIME [epoch: 6.13 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117132573701792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117132573701792 | validation: 0.39722135656900015]
	TIME [epoch: 6.12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3073673719207675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3073673719207675 | validation: 0.2046076999058511]
	TIME [epoch: 6.12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24220958395718634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24220958395718634 | validation: 0.20133149424303795]
	TIME [epoch: 6.12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21270008005219787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21270008005219787 | validation: 0.1956256068622074]
	TIME [epoch: 6.12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24317513502477517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24317513502477517 | validation: 0.2825283837768915]
	TIME [epoch: 6.11 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26123151258818544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26123151258818544 | validation: 0.21284799119120723]
	TIME [epoch: 6.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2606408471549391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2606408471549391 | validation: 0.21785230967241653]
	TIME [epoch: 6.12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2015528326726903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2015528326726903 | validation: 0.17020861085799585]
	TIME [epoch: 6.13 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20021639933440583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20021639933440583 | validation: 0.30728172606838156]
	TIME [epoch: 6.12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26368054695458537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26368054695458537 | validation: 0.3031550911476879]
	TIME [epoch: 6.13 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33915824743005074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33915824743005074 | validation: 0.1817548693044233]
	TIME [epoch: 6.12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19224082523780195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19224082523780195 | validation: 0.20638633787703445]
	TIME [epoch: 6.12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18694956552835001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18694956552835001 | validation: 0.31760544040020194]
	TIME [epoch: 6.12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32585403878007774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32585403878007774 | validation: 0.22319044485614753]
	TIME [epoch: 6.12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1877108395705029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1877108395705029 | validation: 0.13113058642525752]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17015880264569774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17015880264569774 | validation: 0.17080396335846434]
	TIME [epoch: 190 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14986938797278035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14986938797278035 | validation: 0.13818508465044907]
	TIME [epoch: 13 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728254975438332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728254975438332 | validation: 0.37610543001145913]
	TIME [epoch: 13 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30235086370645803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30235086370645803 | validation: 0.47591584438180806]
	TIME [epoch: 13 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594733919132037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.594733919132037 | validation: 0.4327242330275851]
	TIME [epoch: 13 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5136048130647513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5136048130647513 | validation: 0.21084689967621723]
	TIME [epoch: 13 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23500364920900474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23500364920900474 | validation: 1.502819551342215]
	TIME [epoch: 13 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3071532559736445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3071532559736445 | validation: 0.3226872940141727]
	TIME [epoch: 13 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3895478268016045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3895478268016045 | validation: 0.15820686343780002]
	TIME [epoch: 13 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16405227688025228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16405227688025228 | validation: 0.28392379504309273]
	TIME [epoch: 13 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23151970742757408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23151970742757408 | validation: 0.3030671052571545]
	TIME [epoch: 13 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33191142181678795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33191142181678795 | validation: 0.23074483684898348]
	TIME [epoch: 13 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22681787327426897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22681787327426897 | validation: 0.17968901702048226]
	TIME [epoch: 13 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19425407859716234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19425407859716234 | validation: 0.29542108234436254]
	TIME [epoch: 13 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2541333642171945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2541333642171945 | validation: 0.2546663885603139]
	TIME [epoch: 13 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017830116331624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3017830116331624 | validation: 0.6360350550207308]
	TIME [epoch: 13 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5449577086239973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5449577086239973 | validation: 0.19120931860310975]
	TIME [epoch: 13 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2439545207316627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2439545207316627 | validation: 0.19372437182757174]
	TIME [epoch: 13 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19416407780085235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19416407780085235 | validation: 0.2052402099022581]
	TIME [epoch: 13 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.190662549345563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.190662549345563 | validation: 0.13428166348146162]
	TIME [epoch: 13 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16719928464129793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16719928464129793 | validation: 0.24010573797133797]
	TIME [epoch: 13 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19896242146022158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19896242146022158 | validation: 0.24324646252764837]
	TIME [epoch: 13 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30308381027898984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30308381027898984 | validation: 1.2341984727638704]
	TIME [epoch: 13 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9417111811437303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9417111811437303 | validation: 0.26348107090799816]
	TIME [epoch: 13 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25671544184213635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25671544184213635 | validation: 0.12212175345710885]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638074413855336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638074413855336 | validation: 0.129473456459899]
	TIME [epoch: 13 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1710127961587225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1710127961587225 | validation: 0.12651141065218569]
	TIME [epoch: 13 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1585489807759879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1585489807759879 | validation: 0.1687742049434301]
	TIME [epoch: 13 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15556621220061106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15556621220061106 | validation: 0.1635305727542104]
	TIME [epoch: 13 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22738343081912987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22738343081912987 | validation: 0.3487682939642932]
	TIME [epoch: 13 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28319421388828886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28319421388828886 | validation: 0.22322406596060074]
	TIME [epoch: 13 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29697348081326136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29697348081326136 | validation: 0.1609785273254338]
	TIME [epoch: 13 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14745837742399232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14745837742399232 | validation: 0.1351809725523505]
	TIME [epoch: 13 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1340788855362091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1340788855362091 | validation: 0.2107067449502738]
	TIME [epoch: 13 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524234071701218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1524234071701218 | validation: 0.2198404046268003]
	TIME [epoch: 13 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26202161692506337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26202161692506337 | validation: 0.22833021004616239]
	TIME [epoch: 13 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17434665877430408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17434665877430408 | validation: 0.11037302020372236]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12890666737705914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12890666737705914 | validation: 0.24969209716062962]
	TIME [epoch: 13 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185025898235803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.185025898235803 | validation: 0.3638361844159901]
	TIME [epoch: 13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211384527680799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4211384527680799 | validation: 0.26447217354433816]
	TIME [epoch: 13 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2460784489350327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2460784489350327 | validation: 0.12528495157503747]
	TIME [epoch: 13 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15270252712621066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15270252712621066 | validation: 0.2128527699341678]
	TIME [epoch: 13 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463989965332606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1463989965332606 | validation: 0.1686804007839232]
	TIME [epoch: 13 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178800781623585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178800781623585 | validation: 0.33562880979861365]
	TIME [epoch: 13 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23084679668122973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23084679668122973 | validation: 0.2863815087479477]
	TIME [epoch: 13 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491163325836989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3491163325836989 | validation: 0.15980585826118437]
	TIME [epoch: 13 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17475822018755685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17475822018755685 | validation: 0.6177440541216791]
	TIME [epoch: 13 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019566598883517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5019566598883517 | validation: 0.36772813585972886]
	TIME [epoch: 13 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43427438028761345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43427438028761345 | validation: 0.39342225545073245]
	TIME [epoch: 13 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43212427206714815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43212427206714815 | validation: 0.1942377638502035]
	TIME [epoch: 13 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19527719589886428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19527719589886428 | validation: 0.18741775594895338]
	TIME [epoch: 13 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15965505004959024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15965505004959024 | validation: 0.12661576787798376]
	TIME [epoch: 13 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1671496736288797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1671496736288797 | validation: 0.3738085905136195]
	TIME [epoch: 13 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754053694519869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2754053694519869 | validation: 0.3271827014813198]
	TIME [epoch: 13 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36952104063369384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36952104063369384 | validation: 0.5455457188335141]
	TIME [epoch: 13 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5775124566534778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5775124566534778 | validation: 0.47982308968289644]
	TIME [epoch: 13 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48492012341108093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48492012341108093 | validation: 0.25428176876902403]
	TIME [epoch: 13 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27979647485391584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27979647485391584 | validation: 0.22664739118557195]
	TIME [epoch: 13 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23474876436440198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23474876436440198 | validation: 0.19464076740080916]
	TIME [epoch: 13 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1811142686370922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1811142686370922 | validation: 0.1627165918928224]
	TIME [epoch: 13 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1636012822309158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1636012822309158 | validation: 0.13386618717386703]
	TIME [epoch: 13 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.158508264433629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.158508264433629 | validation: 0.20213736416859873]
	TIME [epoch: 13 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1621804727461342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1621804727461342 | validation: 0.20804897341258194]
	TIME [epoch: 13 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575716234951274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2575716234951274 | validation: 0.43491658200265154]
	TIME [epoch: 13 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31102085009140723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31102085009140723 | validation: 0.2561795489654582]
	TIME [epoch: 13 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29738296071699055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29738296071699055 | validation: 0.12453480825364012]
	TIME [epoch: 13 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14456911772145864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14456911772145864 | validation: 0.43217589974020215]
	TIME [epoch: 13 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30346378291893983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30346378291893983 | validation: 0.3451501675108064]
	TIME [epoch: 13 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3934714697790856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3934714697790856 | validation: 0.5353860433827423]
	TIME [epoch: 13 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366055631000763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5366055631000763 | validation: 0.4897046356194807]
	TIME [epoch: 13 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5613076529096734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5613076529096734 | validation: 0.24532477466415986]
	TIME [epoch: 13 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915571931695206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2915571931695206 | validation: 2.5587520806367183]
	TIME [epoch: 13 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.341575411194076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.341575411194076 | validation: 2.0944486562113833]
	TIME [epoch: 13 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7082311363615992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7082311363615992 | validation: 0.817028508071285]
	TIME [epoch: 13 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6185373937556473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6185373937556473 | validation: 0.5675006102804167]
	TIME [epoch: 13 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5590851989564907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590851989564907 | validation: 0.3266503623927719]
	TIME [epoch: 13 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3206974733855685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3206974733855685 | validation: 0.28596311494814547]
	TIME [epoch: 13 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25004733905725035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25004733905725035 | validation: 0.2164117128230721]
	TIME [epoch: 13 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.186337732395452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.186337732395452 | validation: 0.15978615720451206]
	TIME [epoch: 13 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17889987960973688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17889987960973688 | validation: 0.1587218711909374]
	TIME [epoch: 13 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16155606024861804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16155606024861804 | validation: 0.1358189561197373]
	TIME [epoch: 13 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15139212704316907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15139212704316907 | validation: 0.12601650365122122]
	TIME [epoch: 13 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14761758839428724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14761758839428724 | validation: 0.15777152417801832]
	TIME [epoch: 13 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14932656368158814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14932656368158814 | validation: 0.1292450801230048]
	TIME [epoch: 13 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13882099711262855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13882099711262855 | validation: 0.13761339683342663]
	TIME [epoch: 13 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1433991091930551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1433991091930551 | validation: 0.12020686670503444]
	TIME [epoch: 13 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13690720683033142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13690720683033142 | validation: 0.1572495639624022]
	TIME [epoch: 13 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13464022609208973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13464022609208973 | validation: 0.11297431595257278]
	TIME [epoch: 13 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14253794396823571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14253794396823571 | validation: 0.21249609413989462]
	TIME [epoch: 13 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15478658896499853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15478658896499853 | validation: 0.14823811147663063]
	TIME [epoch: 13 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19628292516812218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19628292516812218 | validation: 0.21077187383999052]
	TIME [epoch: 13 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14947046730572985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14947046730572985 | validation: 0.10384982103411043]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13167703981556161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13167703981556161 | validation: 0.16483353864094227]
	TIME [epoch: 13 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12325962939522496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12325962939522496 | validation: 0.15663050233279663]
	TIME [epoch: 13 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17065028082603273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17065028082603273 | validation: 0.3012259402773083]
	TIME [epoch: 13 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22463686891900267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22463686891900267 | validation: 0.22802086901727853]
	TIME [epoch: 13 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2753653343739575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2753653343739575 | validation: 0.18266194727302618]
	TIME [epoch: 13 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16308755119469015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16308755119469015 | validation: 0.2768819166907147]
	TIME [epoch: 13 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22891348576227052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22891348576227052 | validation: 0.21077065183134344]
	TIME [epoch: 13 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26829437516616594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26829437516616594 | validation: 0.16893812917360435]
	TIME [epoch: 13 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439175594086704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439175594086704 | validation: 0.16065820604986275]
	TIME [epoch: 13 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127961069121486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.127961069121486 | validation: 0.12996983167534015]
	TIME [epoch: 13 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16310105349951393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16310105349951393 | validation: 1.6152168245015588]
	TIME [epoch: 13 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8052695944126151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8052695944126151 | validation: 0.5236632115235653]
	TIME [epoch: 13 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763045889189258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5763045889189258 | validation: 0.388152736824798]
	TIME [epoch: 13 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5120702893700354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5120702893700354 | validation: 0.27242460322851375]
	TIME [epoch: 13 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34549477312538485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34549477312538485 | validation: 0.2586685443562264]
	TIME [epoch: 13 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2631052283580752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2631052283580752 | validation: 0.20265572339499782]
	TIME [epoch: 13 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1926935565736115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1926935565736115 | validation: 0.1906588060530886]
	TIME [epoch: 13 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19080040072593243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19080040072593243 | validation: 0.1466975254197629]
	TIME [epoch: 13 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14950501010995637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14950501010995637 | validation: 0.12065973627019431]
	TIME [epoch: 13 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685878526776566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685878526776566 | validation: 0.2102020387149156]
	TIME [epoch: 13 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599802597483755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1599802597483755 | validation: 0.12813027117804096]
	TIME [epoch: 13 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440058207885416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1440058207885416 | validation: 0.14192335572805995]
	TIME [epoch: 13 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12098917510111867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12098917510111867 | validation: 0.1132124619454644]
	TIME [epoch: 13 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11199722189362689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11199722189362689 | validation: 0.10618459576803428]
	TIME [epoch: 13 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11235930130813801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11235930130813801 | validation: 0.10741401177966026]
	TIME [epoch: 13 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10920039002518805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10920039002518805 | validation: 0.14220358019321516]
	TIME [epoch: 13 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11579783239917829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11579783239917829 | validation: 0.11019424999851718]
	TIME [epoch: 13 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14630418786962351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14630418786962351 | validation: 0.37701741340314826]
	TIME [epoch: 13 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25353445272259295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25353445272259295 | validation: 0.3169168871484775]
	TIME [epoch: 13 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3525483064405724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3525483064405724 | validation: 0.26263670944763357]
	TIME [epoch: 13 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25787867848937046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25787867848937046 | validation: 0.2996462313661658]
	TIME [epoch: 13 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20111569792038742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20111569792038742 | validation: 0.13047448191489677]
	TIME [epoch: 13 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17257237732656183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17257237732656183 | validation: 0.1367679895280936]
	TIME [epoch: 13 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297875872785157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1297875872785157 | validation: 0.11528991026891414]
	TIME [epoch: 13 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12545181903207706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12545181903207706 | validation: 0.25459921491059273]
	TIME [epoch: 13 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18202893402596476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18202893402596476 | validation: 0.27993224995472604]
	TIME [epoch: 13 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30988736541006523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30988736541006523 | validation: 0.17833404710595643]
	TIME [epoch: 13 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17483470679258864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17483470679258864 | validation: 0.63653992135426]
	TIME [epoch: 13 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47962237976191896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47962237976191896 | validation: 0.19409206675364224]
	TIME [epoch: 13 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21741818570859295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21741818570859295 | validation: 0.14587778117087383]
	TIME [epoch: 13 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1673159259873335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1673159259873335 | validation: 0.28222740624319703]
	TIME [epoch: 13 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1845357945804311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1845357945804311 | validation: 0.181290784689645]
	TIME [epoch: 13 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2094909189726421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2094909189726421 | validation: 0.222272521574785]
	TIME [epoch: 13 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17749923500064602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17749923500064602 | validation: 0.12227879471078139]
	TIME [epoch: 13 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477926475423551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477926475423551 | validation: 0.4114242525246157]
	TIME [epoch: 13 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24403082502443504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24403082502443504 | validation: 0.15727156743305548]
	TIME [epoch: 13 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17604256083387654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17604256083387654 | validation: 0.12754646443932288]
	TIME [epoch: 13 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13594535117260517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13594535117260517 | validation: 0.08489327715055969]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11470222090574436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11470222090574436 | validation: 0.13999080665348665]
	TIME [epoch: 13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11323076563506021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11323076563506021 | validation: 0.1124341633506651]
	TIME [epoch: 13 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10347136156484586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10347136156484586 | validation: 0.08982265979921092]
	TIME [epoch: 13 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695675463448424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10695675463448424 | validation: 0.11042383855207544]
	TIME [epoch: 13 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12256162189368197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12256162189368197 | validation: 0.36896937955753667]
	TIME [epoch: 13 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477211071587891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3477211071587891 | validation: 0.20348567794921643]
	TIME [epoch: 13 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2672434059014916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2672434059014916 | validation: 0.23884690862646082]
	TIME [epoch: 13 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071809811145449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2071809811145449 | validation: 0.2157211912876428]
	TIME [epoch: 13 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15912063855850517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15912063855850517 | validation: 0.14111712133113982]
	TIME [epoch: 13 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551717266816151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551717266816151 | validation: 0.3363230819523689]
	TIME [epoch: 13 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2257683252893887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2257683252893887 | validation: 0.22904155985451125]
	TIME [epoch: 13 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665455463965564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2665455463965564 | validation: 0.13776592598390056]
	TIME [epoch: 13 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11728811811932655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11728811811932655 | validation: 0.24662621458649672]
	TIME [epoch: 13 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14905789280498485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14905789280498485 | validation: 0.18091372989611754]
	TIME [epoch: 13 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20031850771352738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20031850771352738 | validation: 0.14331851861468722]
	TIME [epoch: 13 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11612067767126678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11612067767126678 | validation: 0.11101719361103918]
	TIME [epoch: 13 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354095453692113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09354095453692113 | validation: 0.08424325690483865]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09360763799116936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09360763799116936 | validation: 0.12475301337804577]
	TIME [epoch: 13 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09298330953620841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09298330953620841 | validation: 0.09519440603398417]
	TIME [epoch: 13 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10318526298068183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10318526298068183 | validation: 0.5376943774209467]
	TIME [epoch: 13 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767444935839617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3767444935839617 | validation: 0.413836970183181]
	TIME [epoch: 13 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068052310524165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5068052310524165 | validation: 1.0172293024937957]
	TIME [epoch: 13 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1622465878399042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1622465878399042 | validation: 1.3675093971671595]
	TIME [epoch: 13 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5195003510844742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5195003510844742 | validation: 1.5120081018126463]
	TIME [epoch: 13 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7298499403080765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7298499403080765 | validation: 1.5964658008093755]
	TIME [epoch: 13 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.812962861628781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.812962861628781 | validation: 1.678468974477962]
	TIME [epoch: 13 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.875201938490547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.875201938490547 | validation: 1.709118125281348]
	TIME [epoch: 13 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9162361316148009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9162361316148009 | validation: 1.6831085975172613]
	TIME [epoch: 13 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9632298317575432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9632298317575432 | validation: 1.6913644615404246]
	TIME [epoch: 13 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.974659151713302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.974659151713302 | validation: 1.7388523411912864]
	TIME [epoch: 13 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9772570777732226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9772570777732226 | validation: 1.6927248100049568]
	TIME [epoch: 13 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.013011515773569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.013011515773569 | validation: 1.6991643438148385]
	TIME [epoch: 13 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9893587174304594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9893587174304594 | validation: 1.7186777210570925]
	TIME [epoch: 13 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.017347636912937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.017347636912937 | validation: 1.7139257931022733]
	TIME [epoch: 13 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.019556837546007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.019556837546007 | validation: 1.7512787856010168]
	TIME [epoch: 13 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0923646812309915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0923646812309915 | validation: 1.73434834015781]
	TIME [epoch: 13 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.028093056470483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.028093056470483 | validation: 1.7824405077213983]
	TIME [epoch: 13 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1047714684070393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1047714684070393 | validation: 1.7282494866058586]
	TIME [epoch: 13 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.036192132746951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.036192132746951 | validation: 1.67760682575436]
	TIME [epoch: 13 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.042857912251952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.042857912251952 | validation: 1.708913407232695]
	TIME [epoch: 13 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.02574821299239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.02574821299239 | validation: 1.6726612632284743]
	TIME [epoch: 13 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9993921510267831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9993921510267831 | validation: 1.7147130582053614]
	TIME [epoch: 13 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0422897954892703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0422897954892703 | validation: 1.6624266458030683]
	TIME [epoch: 13 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0385236252340637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0385236252340637 | validation: 1.7387615695901648]
	TIME [epoch: 13 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0383071856356687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0383071856356687 | validation: 1.6962896767702658]
	TIME [epoch: 13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.056000069252129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.056000069252129 | validation: 1.7112399868331332]
	TIME [epoch: 13 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0629085508449574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0629085508449574 | validation: 1.683774822508915]
	TIME [epoch: 13 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0409327675056113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0409327675056113 | validation: 1.7116709236539869]
	TIME [epoch: 13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.08758645958926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.08758645958926 | validation: 1.7198322250890614]
	TIME [epoch: 13 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0678766048378416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0678766048378416 | validation: 1.7583861811989323]
	TIME [epoch: 13 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.117590840133679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.117590840133679 | validation: 1.7055687130195538]
	TIME [epoch: 13 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.054994681951507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.054994681951507 | validation: 1.6483433284563984]
	TIME [epoch: 13 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0600480132324646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0600480132324646 | validation: 1.6742162399418488]
	TIME [epoch: 13 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0076598442752864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0076598442752864 | validation: 1.6012461990987934]
	TIME [epoch: 13 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0043807552251627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0043807552251627 | validation: 1.633690959794381]
	TIME [epoch: 13 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0042083356418696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0042083356418696 | validation: 1.655938313391393]
	TIME [epoch: 13 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9863607401356143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9863607401356143 | validation: 1.614002464411806]
	TIME [epoch: 13 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0355280379840086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0355280379840086 | validation: 1.6409384690983957]
	TIME [epoch: 13 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0378790130284186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0378790130284186 | validation: 1.6487374252287161]
	TIME [epoch: 13 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.044617270613988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.044617270613988 | validation: 1.6684631597983897]
	TIME [epoch: 13 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0477981493952404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0477981493952404 | validation: 1.6346446459594934]
	TIME [epoch: 13 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0807534817734235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0807534817734235 | validation: 1.6079318916264722]
	TIME [epoch: 13 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0438759866060447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0438759866060447 | validation: 1.6578234990811396]
	TIME [epoch: 13 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0640921063998987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0640921063998987 | validation: 1.64089369235911]
	TIME [epoch: 13 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0236246798544575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0236246798544575 | validation: 1.6103698731687741]
	TIME [epoch: 13 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0367113882677605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0367113882677605 | validation: 1.6005708365657054]
	TIME [epoch: 13 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0287755996049066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0287755996049066 | validation: 1.5866582233151982]
	TIME [epoch: 13 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0102916965182387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0102916965182387 | validation: 1.6038090290169429]
	TIME [epoch: 13 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0689611117161557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0689611117161557 | validation: 1.5853135474069842]
	TIME [epoch: 13 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0427206403626306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0427206403626306 | validation: 1.5935016636577595]
	TIME [epoch: 13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0613311658385554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0613311658385554 | validation: 1.5579396852179794]
	TIME [epoch: 13 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0160497148526466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0160497148526466 | validation: 1.6096265544248671]
	TIME [epoch: 13 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.079609734784707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.079609734784707 | validation: 1.5722695620185405]
	TIME [epoch: 13 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.043307198369111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.043307198369111 | validation: 1.550600837944527]
	TIME [epoch: 13 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0652469016885595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0652469016885595 | validation: 1.540858505603307]
	TIME [epoch: 13 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0202598376259586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0202598376259586 | validation: 1.5103117407989555]
	TIME [epoch: 13 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9612359984175067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9612359984175067 | validation: 1.6711398030519455]
	TIME [epoch: 13 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.837719560431024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.837719560431024 | validation: 1.3117020637837964]
	TIME [epoch: 13 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4195723646215228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4195723646215228 | validation: 1.2414304826751468]
	TIME [epoch: 13 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.319945757746076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.319945757746076 | validation: 1.0004017501833677]
	TIME [epoch: 13 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205239725785298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.205239725785298 | validation: 0.824014158457916]
	TIME [epoch: 13 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9178789674502598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9178789674502598 | validation: 1.3878711474115777]
	TIME [epoch: 13 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5620104163241524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5620104163241524 | validation: 0.9476370454552225]
	TIME [epoch: 13 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9586757117204432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9586757117204432 | validation: 1.0882545420170813]
	TIME [epoch: 13 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.168537008853767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.168537008853767 | validation: 0.5928152323048227]
	TIME [epoch: 13 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537629558630213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537629558630213 | validation: 0.7723594302100925]
	TIME [epoch: 13 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268615227780892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268615227780892 | validation: 0.4005592350760232]
	TIME [epoch: 13 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813772969771942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4813772969771942 | validation: 0.4347164261293557]
	TIME [epoch: 13 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4859487261768157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4859487261768157 | validation: 0.37510785325570045]
	TIME [epoch: 13 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40475146665118017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40475146665118017 | validation: 0.3389460473231699]
	TIME [epoch: 13 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40686581915341863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40686581915341863 | validation: 0.3514465511491429]
	TIME [epoch: 13 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3706685209503887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3706685209503887 | validation: 0.39285100619996494]
	TIME [epoch: 13 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46233855447012717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46233855447012717 | validation: 0.3364002630438158]
	TIME [epoch: 13 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34342999737638286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34342999737638286 | validation: 0.3866745485270551]
	TIME [epoch: 13 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44239539310195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44239539310195 | validation: 0.3149362134735626]
	TIME [epoch: 13 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33747328104607943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33747328104607943 | validation: 0.3339407884537245]
	TIME [epoch: 13 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3033954300252924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3033954300252924 | validation: 0.2826788584142394]
	TIME [epoch: 13 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30254149863068697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30254149863068697 | validation: 0.5748273329923839]
	TIME [epoch: 13 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4698121014627671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4698121014627671 | validation: 0.39658749613690686]
	TIME [epoch: 13 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038918286124374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5038918286124374 | validation: 0.3948714608170561]
	TIME [epoch: 13 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49142595114752047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49142595114752047 | validation: 0.3143697482080709]
	TIME [epoch: 13 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3525421992303339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3525421992303339 | validation: 0.3961969043939991]
	TIME [epoch: 13 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35261470398663247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35261470398663247 | validation: 0.3553186056371712]
	TIME [epoch: 13 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39198257826322536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39198257826322536 | validation: 0.2512547074551028]
	TIME [epoch: 13 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646666321819459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2646666321819459 | validation: 0.29646621742390994]
	TIME [epoch: 13 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728286494399096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728286494399096 | validation: 0.2834838094496144]
	TIME [epoch: 13 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183825318577122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183825318577122 | validation: 0.6037040278654784]
	TIME [epoch: 13 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5017652837492631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5017652837492631 | validation: 0.2987117159174985]
	TIME [epoch: 13 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649019159188884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3649019159188884 | validation: 0.21945239403258215]
	TIME [epoch: 13 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25217746376482564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25217746376482564 | validation: 0.28659420305748967]
	TIME [epoch: 13 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26782637409125387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26782637409125387 | validation: 0.22821226875923425]
	TIME [epoch: 13 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.226285042875869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.226285042875869 | validation: 0.20286999214718637]
	TIME [epoch: 13 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2187152248718511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2187152248718511 | validation: 0.35712632051207915]
	TIME [epoch: 13 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28241230602009315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28241230602009315 | validation: 0.30913863274510134]
	TIME [epoch: 13 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36462804034838525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36462804034838525 | validation: 1.1422123286965697]
	TIME [epoch: 13 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.201855577088175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.201855577088175 | validation: 0.8171806837706989]
	TIME [epoch: 13 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889241699054025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889241699054025 | validation: 0.40082604030599767]
	TIME [epoch: 13 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489833036261429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.489833036261429 | validation: 0.20759222833113455]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_194611/states/model_phi1_4c_v_mmd2_758.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6351.139 seconds.
