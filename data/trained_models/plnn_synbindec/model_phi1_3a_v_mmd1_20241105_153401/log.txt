Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/basic/data_phi1_3a/training', validation_data='data/training_data/basic/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3688536562

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9204346619264845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9204346619264845 | validation: 4.549166915959003]
	TIME [epoch: 249 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.281232920988221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.281232920988221 | validation: 4.420578254784796]
	TIME [epoch: 0.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.765316793457405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.765316793457405 | validation: 5.334644063126995]
	TIME [epoch: 0.69 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.296387511901072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.296387511901072 | validation: 4.2552639709033135]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6393231065691602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6393231065691602 | validation: 4.219737232629352]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5145198297386635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5145198297386635 | validation: 4.137050423609245]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.453091270068818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.453091270068818 | validation: 4.1002942312977]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2639358115716535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2639358115716535 | validation: 4.046767032301524]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.12501016426328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.12501016426328 | validation: 3.7949006852714735]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0443377413254837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0443377413254837 | validation: 3.853716689556629]
	TIME [epoch: 0.691 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.050586263162497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.050586263162497 | validation: 3.6065440008363128]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.132600819346877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.132600819346877 | validation: 3.335319909873944]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.670822771198763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.670822771198763 | validation: 3.6065251299905543]
	TIME [epoch: 0.692 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8215618169746914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8215618169746914 | validation: 2.9551659787303]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.490647955796945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.490647955796945 | validation: 2.711215741097309]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.297355799920394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.297355799920394 | validation: 2.45928941954257]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8716675549349446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8716675549349446 | validation: 1.4492082903351315]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4145044575993355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4145044575993355 | validation: 3.6725074335832346]
	TIME [epoch: 0.691 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.702915179532364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.702915179532364 | validation: 1.2804268568848067]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3917164479655093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3917164479655093 | validation: 1.5675424281184558]
	TIME [epoch: 0.691 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3017502649928243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3017502649928243 | validation: 1.7639003198527918]
	TIME [epoch: 0.691 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.297347541111858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.297347541111858 | validation: 1.3296984706114792]
	TIME [epoch: 0.69 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1403114354082404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1403114354082404 | validation: 1.3891166366546575]
	TIME [epoch: 0.689 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144718485175742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1144718485175742 | validation: 1.3545772052504472]
	TIME [epoch: 0.689 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0741371974663994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0741371974663994 | validation: 1.3867984022526492]
	TIME [epoch: 0.692 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0572942560392222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0572942560392222 | validation: 1.3639301477117747]
	TIME [epoch: 0.689 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0390859777815387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0390859777815387 | validation: 1.3019336977228073]
	TIME [epoch: 0.695 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0494174058703043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0494174058703043 | validation: 1.5525135940614654]
	TIME [epoch: 0.688 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.286927664191647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.286927664191647 | validation: 1.4237749164580518]
	TIME [epoch: 0.688 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3872192537690837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3872192537690837 | validation: 1.5991659013548762]
	TIME [epoch: 0.689 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1973893463350178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1973893463350178 | validation: 1.323122179203509]
	TIME [epoch: 0.688 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0403241652771897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0403241652771897 | validation: 1.2806376249065445]
	TIME [epoch: 0.688 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1978191136279823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1978191136279823 | validation: 1.4248004597955246]
	TIME [epoch: 0.689 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1810693008020754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1810693008020754 | validation: 1.3736005839261223]
	TIME [epoch: 0.688 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0756711341330896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0756711341330896 | validation: 1.3190868936370732]
	TIME [epoch: 0.688 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022414213727482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.022414213727482 | validation: 1.2270192296126952]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0357744474952033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0357744474952033 | validation: 1.2620527202554337]
	TIME [epoch: 0.692 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0016440427712565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0016440427712565 | validation: 1.2884900853676264]
	TIME [epoch: 0.689 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.999041867539543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.999041867539543 | validation: 1.2545511081721716]
	TIME [epoch: 0.688 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9807219273833048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9807219273833048 | validation: 1.267501757320071]
	TIME [epoch: 0.691 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940129477144883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9940129477144883 | validation: 1.2641411192914316]
	TIME [epoch: 0.689 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0266575679184247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0266575679184247 | validation: 1.3379445453483971]
	TIME [epoch: 0.688 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0577763785419219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0577763785419219 | validation: 1.2438572025883743]
	TIME [epoch: 0.688 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9909747846364155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9909747846364155 | validation: 1.2227237683429932]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597914041075751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9597914041075751 | validation: 1.1773012948587391]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410331975186188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9410331975186188 | validation: 1.1798873117924853]
	TIME [epoch: 0.691 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950434966857566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.950434966857566 | validation: 1.1785486204998417]
	TIME [epoch: 0.69 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9667645459520244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9667645459520244 | validation: 1.2432261010869596]
	TIME [epoch: 0.689 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.028320759057911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.028320759057911 | validation: 1.175969944452755]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9381817354125772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9381817354125772 | validation: 1.147627749886771]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218052248585018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9218052248585018 | validation: 1.1238112683646377]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242389623932761		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9242389623932761 | validation: 1.1916795623326262]
	TIME [epoch: 0.691 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9571911910321448		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9571911910321448 | validation: 1.1647576981032004]
	TIME [epoch: 0.688 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0423430505600115		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.0423430505600115 | validation: 1.2692709750853508]
	TIME [epoch: 0.686 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.062330929303087		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.062330929303087 | validation: 1.1041730143168926]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9324587782073809		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.9324587782073809 | validation: 1.0879014197546946]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9112808405276676		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9112808405276676 | validation: 1.1606610744579986]
	TIME [epoch: 0.691 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462723981991908		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9462723981991908 | validation: 1.1183882685931443]
	TIME [epoch: 0.689 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951136577836389		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.951136577836389 | validation: 1.1853277620967226]
	TIME [epoch: 0.687 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9924505053189583		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9924505053189583 | validation: 1.0938410872927695]
	TIME [epoch: 0.686 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918984737137341		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.918984737137341 | validation: 1.0493909708760005]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958556014582054		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.8958556014582054 | validation: 1.053215389238332]
	TIME [epoch: 0.69 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8895937177563884		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.8895937177563884 | validation: 1.0399020606337523]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8844706517459335		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.8844706517459335 | validation: 1.0531936288768387]
	TIME [epoch: 0.689 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887071879857195		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.887071879857195 | validation: 1.0132374533073532]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821989524843548		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.8821989524843548 | validation: 1.0607581626356841]
	TIME [epoch: 0.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910281157050883		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.8910281157050883 | validation: 1.0156528575416008]
	TIME [epoch: 0.689 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012441685837675		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.9012441685837675 | validation: 1.1106990021997452]
	TIME [epoch: 0.686 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9369439744609926		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9369439744609926 | validation: 1.0345881070237213]
	TIME [epoch: 0.686 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9415676406016048		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9415676406016048 | validation: 1.1411052447528538]
	TIME [epoch: 0.686 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603769038604175		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.9603769038604175 | validation: 1.060209615272348]
	TIME [epoch: 0.686 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9415268207059583		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.9415268207059583 | validation: 1.1697346906194441]
	TIME [epoch: 0.685 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0238512994496978		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.0238512994496978 | validation: 1.058254654606148]
	TIME [epoch: 0.686 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277328111942191		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9277328111942191 | validation: 0.9941803058081983]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757579871558745		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8757579871558745 | validation: 0.99125320169187]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836721908169741		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8836721908169741 | validation: 1.0132232750541177]
	TIME [epoch: 0.693 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883828341784357		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8883828341784357 | validation: 1.035904289892695]
	TIME [epoch: 0.692 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892887522753427		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8892887522753427 | validation: 0.9807458119835698]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758165337595231		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.8758165337595231 | validation: 0.9743897453525392]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655810006401368		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8655810006401368 | validation: 0.9668984152304223]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626961783200026		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8626961783200026 | validation: 0.9951549693439363]
	TIME [epoch: 0.689 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731139471647934		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.8731139471647934 | validation: 0.9941659365587683]
	TIME [epoch: 0.688 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89853990310066		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.89853990310066 | validation: 1.1205310884615602]
	TIME [epoch: 0.689 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9901624520767223		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.9901624520767223 | validation: 1.0141549660625826]
	TIME [epoch: 0.69 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9477019872890423		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.9477019872890423 | validation: 1.0502526373110423]
	TIME [epoch: 0.689 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9138944313251115		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.9138944313251115 | validation: 0.9787058181430741]
	TIME [epoch: 0.688 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959821574378549		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8959821574378549 | validation: 1.0255116480340476]
	TIME [epoch: 0.688 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9094265368436264		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.9094265368436264 | validation: 0.9720781354907025]
	TIME [epoch: 0.688 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766645508442928		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8766645508442928 | validation: 0.9730457866518765]
	TIME [epoch: 0.688 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85924012992489		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.85924012992489 | validation: 0.9605115340486933]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558090116053628		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8558090116053628 | validation: 0.9515802086791277]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642386631033574		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8642386631033574 | validation: 0.9785148218426003]
	TIME [epoch: 0.689 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734553706808117		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8734553706808117 | validation: 1.0841163103373888]
	TIME [epoch: 0.689 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9289694163667156		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9289694163667156 | validation: 1.0095229165121606]
	TIME [epoch: 0.688 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9415074412190938		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.9415074412190938 | validation: 1.0564609905674118]
	TIME [epoch: 0.687 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202085694251059		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9202085694251059 | validation: 0.9591243080097049]
	TIME [epoch: 0.687 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8563996412817556		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8563996412817556 | validation: 0.9555995873048826]
	TIME [epoch: 0.687 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604943051600096		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8604943051600096 | validation: 0.9961599338590071]
	TIME [epoch: 0.688 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881030596211705		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.881030596211705 | validation: 0.9552844551320883]
	TIME [epoch: 0.687 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996901417450347		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8996901417450347 | validation: 0.9981837084589675]
	TIME [epoch: 0.687 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202956853791255		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.9202956853791255 | validation: 0.9734749904016852]
	TIME [epoch: 0.691 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597293355809396		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8597293355809396 | validation: 0.9430512394386468]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469450823069122		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8469450823069122 | validation: 0.9582493142747268]
	TIME [epoch: 0.692 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8561803285671934		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8561803285671934 | validation: 0.9954718152389648]
	TIME [epoch: 0.693 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997785190896701		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8997785190896701 | validation: 1.1306539675968044]
	TIME [epoch: 0.69 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0006677279223022		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.0006677279223022 | validation: 0.9615975444675451]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614168308489204		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8614168308489204 | validation: 0.9508851711821124]
	TIME [epoch: 0.686 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844884970489129		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.844884970489129 | validation: 0.9493017919946937]
	TIME [epoch: 0.689 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866832256728416		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.866832256728416 | validation: 0.9389998102600491]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85357211602608		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.85357211602608 | validation: 0.9620129396196504]
	TIME [epoch: 0.693 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495480020647597		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8495480020647597 | validation: 0.9398531570182929]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8396583901610392		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8396583901610392 | validation: 0.9369863662232707]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839887887585844		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.839887887585844 | validation: 0.9646222027714604]
	TIME [epoch: 0.691 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788876552161594		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8788876552161594 | validation: 0.9858780840972076]
	TIME [epoch: 0.691 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9533117408713964		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9533117408713964 | validation: 1.1312448415717886]
	TIME [epoch: 0.69 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035109359929904		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.035109359929904 | validation: 0.9342577818936081]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472769855916054		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8472769855916054 | validation: 0.9511382889867463]
	TIME [epoch: 0.696 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8787583988653072		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8787583988653072 | validation: 0.9527398214028259]
	TIME [epoch: 0.693 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580936781572297		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8580936781572297 | validation: 0.9176132678352517]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353292374418314		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8353292374418314 | validation: 0.926026729202575]
	TIME [epoch: 0.691 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437656213366003		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8437656213366003 | validation: 0.9490622570222378]
	TIME [epoch: 0.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382735457762527		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8382735457762527 | validation: 0.9124070029587119]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250349062757907		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8250349062757907 | validation: 0.9218964455717202]
	TIME [epoch: 0.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81920811129599		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.81920811129599 | validation: 0.9138568533501976]
	TIME [epoch: 0.686 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244854042533188		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8244854042533188 | validation: 0.926508342575011]
	TIME [epoch: 0.685 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368735407839017		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8368735407839017 | validation: 0.991304087772091]
	TIME [epoch: 0.685 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992927532552095		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8992927532552095 | validation: 1.185822582913058]
	TIME [epoch: 0.685 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0622998750898571		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.0622998750898571 | validation: 0.977413840822954]
	TIME [epoch: 0.685 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9128374277748236		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.9128374277748236 | validation: 0.9746617124625955]
	TIME [epoch: 0.686 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.932683354371245		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.932683354371245 | validation: 0.9405810560814466]
	TIME [epoch: 0.687 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432500852258847		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8432500852258847 | validation: 0.9419895856185868]
	TIME [epoch: 0.687 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744830388569534		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8744830388569534 | validation: 0.9317922452178511]
	TIME [epoch: 0.688 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353187004444718		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8353187004444718 | validation: 0.8925117024547293]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227989550917159		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8227989550917159 | validation: 0.9120212713351035]
	TIME [epoch: 0.689 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220547564874263		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8220547564874263 | validation: 0.8973825469487612]
	TIME [epoch: 0.686 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114101292664291		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8114101292664291 | validation: 0.8949007340246841]
	TIME [epoch: 0.686 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096144926694663		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8096144926694663 | validation: 0.8846904535671621]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103572660448856		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8103572660448856 | validation: 0.8771501281216683]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8053781021101611		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8053781021101611 | validation: 0.9021201219495523]
	TIME [epoch: 0.692 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108698390493152		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8108698390493152 | validation: 0.9868224090545633]
	TIME [epoch: 0.69 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410165524506968		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.9410165524506968 | validation: 1.2489837516596936]
	TIME [epoch: 0.688 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1597623979752727		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1597623979752727 | validation: 0.9073343356925293]
	TIME [epoch: 0.688 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188315024500669		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8188315024500669 | validation: 0.9497817556720257]
	TIME [epoch: 0.688 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200740295882143		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.9200740295882143 | validation: 0.939784780951415]
	TIME [epoch: 0.688 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434078478468746		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8434078478468746 | validation: 0.9091536934421042]
	TIME [epoch: 0.688 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207828821907094		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8207828821907094 | validation: 0.8896883870204809]
	TIME [epoch: 0.691 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260665723723309		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8260665723723309 | validation: 0.8972097030981532]
	TIME [epoch: 0.689 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169550261727179		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8169550261727179 | validation: 0.8953214342873445]
	TIME [epoch: 0.687 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8191769616421004		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8191769616421004 | validation: 0.8918538077035759]
	TIME [epoch: 0.687 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809976024762397		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.809976024762397 | validation: 0.8959428166400504]
	TIME [epoch: 0.687 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060772981190215		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8060772981190215 | validation: 0.8806619103743536]
	TIME [epoch: 0.688 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047894903961857		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8047894903961857 | validation: 0.8824833209984251]
	TIME [epoch: 0.687 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802925741699986		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.802925741699986 | validation: 0.8703423494441783]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915362648013501		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7915362648013501 | validation: 0.8868226039395339]
	TIME [epoch: 0.69 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099351575756072		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8099351575756072 | validation: 0.9454919261407382]
	TIME [epoch: 0.689 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017763818274653		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.9017763818274653 | validation: 1.081913528700026]
	TIME [epoch: 0.688 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0340893802341458		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0340893802341458 | validation: 0.8602663694835914]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039201138282235		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8039201138282235 | validation: 0.8766171971053013]
	TIME [epoch: 0.691 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218409167729055		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8218409167729055 | validation: 0.9123402625046803]
	TIME [epoch: 0.689 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324802683751341		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8324802683751341 | validation: 0.8816739813871126]
	TIME [epoch: 0.692 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990870312376818		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7990870312376818 | validation: 0.8556891065425472]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966689615675959		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7966689615675959 | validation: 0.8824557849275785]
	TIME [epoch: 0.691 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952862530059517		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7952862530059517 | validation: 0.8529524114003216]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933768086810777		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7933768086810777 | validation: 0.853964407596386]
	TIME [epoch: 0.691 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907699410044279		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7907699410044279 | validation: 0.8403210904253013]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970727423782346		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7970727423782346 | validation: 0.9018972348409884]
	TIME [epoch: 0.689 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375075589788881		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8375075589788881 | validation: 0.9276610911733352]
	TIME [epoch: 0.687 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9119059027712473		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9119059027712473 | validation: 0.9875682733570518]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242988556988055		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.9242988556988055 | validation: 0.8629622952223411]
	TIME [epoch: 0.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007650105993612		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8007650105993612 | validation: 0.8744970829578378]
	TIME [epoch: 0.69 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130255699512068		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8130255699512068 | validation: 0.8626211923347633]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804870284710008		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.804870284710008 | validation: 0.8411278018112172]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794968575362055		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7794968575362055 | validation: 0.835024923958938]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944218295994459		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7944218295994459 | validation: 0.8396487322786226]
	TIME [epoch: 0.689 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844162480760893		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7844162480760893 | validation: 0.8481238457270486]
	TIME [epoch: 0.693 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780335206133881		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.780335206133881 | validation: 0.833844366580313]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834181723620535		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7834181723620535 | validation: 0.8254441747756469]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742431882599278		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7742431882599278 | validation: 0.8164825153617162]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792522754446503		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.792522754446503 | validation: 0.9840301897472721]
	TIME [epoch: 0.693 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980524656183099		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8980524656183099 | validation: 0.9806501532760947]
	TIME [epoch: 0.693 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9992175470524918		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9992175470524918 | validation: 0.8948823035967544]
	TIME [epoch: 0.692 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646072072843677		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8646072072843677 | validation: 0.8484753399809446]
	TIME [epoch: 0.689 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903462427730027		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7903462427730027 | validation: 0.8529665128017231]
	TIME [epoch: 0.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293358223147198		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8293358223147198 | validation: 0.8324638906579548]
	TIME [epoch: 0.692 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864968297768392		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7864968297768392 | validation: 0.8343939350893648]
	TIME [epoch: 0.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810973761944001		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7810973761944001 | validation: 0.8139140442466103]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865178321518315		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7865178321518315 | validation: 0.8162106480545571]
	TIME [epoch: 0.691 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760254460585031		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7760254460585031 | validation: 0.8302240711096535]
	TIME [epoch: 0.693 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695998085548891		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7695998085548891 | validation: 0.8219835492820486]
	TIME [epoch: 0.692 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669408912369922		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7669408912369922 | validation: 0.8221847354866532]
	TIME [epoch: 0.691 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697900943818428		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7697900943818428 | validation: 0.8335393461642321]
	TIME [epoch: 0.691 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939483767398907		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7939483767398907 | validation: 0.843160642464325]
	TIME [epoch: 0.691 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428116843393889		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8428116843393889 | validation: 0.9808893869730089]
	TIME [epoch: 0.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343645215268488		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9343645215268488 | validation: 0.824492896800281]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994054500084868		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7994054500084868 | validation: 0.7971762405738423]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769363617180808		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.769363617180808 | validation: 0.8142140632791819]
	TIME [epoch: 0.692 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715828742919143		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7715828742919143 | validation: 0.802421679363388]
	TIME [epoch: 0.692 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734871343414493		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7734871343414493 | validation: 0.8082171213081946]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684542162886752		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7684542162886752 | validation: 0.7721807355278267]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761469754087148		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.761469754087148 | validation: 0.8112286705052585]
	TIME [epoch: 0.689 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668533588067581		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7668533588067581 | validation: 0.7830088684889094]
	TIME [epoch: 263 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669145458277167		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7669145458277167 | validation: 0.8300239451522646]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991905701416911		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7991905701416911 | validation: 0.8593569387595621]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813875347868384		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8813875347868384 | validation: 0.9102072912591492]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876082281061824		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.876082281061824 | validation: 0.7959782254280475]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664962576079912		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7664962576079912 | validation: 0.7975985385966927]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766785735569498		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7766785735569498 | validation: 0.8294871041073671]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822028703151156		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7822028703151156 | validation: 0.7757836229921309]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582109064597918		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7582109064597918 | validation: 0.780571382162496]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547058722181863		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7547058722181863 | validation: 0.7709714339595233]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538553547728907		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7538553547728907 | validation: 0.7629319781365285]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501386406307166		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7501386406307166 | validation: 0.7427379577561964]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468433171022073		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7468433171022073 | validation: 0.7667564538994436]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431286131222684		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7431286131222684 | validation: 0.7377485639023013]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574447915111836		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7574447915111836 | validation: 0.8567022779297511]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325609500674283		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8325609500674283 | validation: 0.9558519492986893]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0029343605373708		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0029343605373708 | validation: 0.8307322495468954]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095412491729786		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.8095412491729786 | validation: 0.7990663741360673]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7674606643813999		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7674606643813999 | validation: 0.8004546743264502]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924588044329522		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7924588044329522 | validation: 0.7804706031745449]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7555715675241137		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.7555715675241137 | validation: 0.7632064954724068]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741186783748372		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.741186783748372 | validation: 0.758465095830053]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452123741372093		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7452123741372093 | validation: 0.7593829463746564]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505223234844306		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7505223234844306 | validation: 0.7428525955169967]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739597188568377		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.739597188568377 | validation: 0.7564499342673113]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475794830363683		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7475794830363683 | validation: 0.7546666000213004]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771726330850225		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.771726330850225 | validation: 0.8261487027835195]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162080860360817		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8162080860360817 | validation: 0.7946372724863888]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085429664869735		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.8085429664869735 | validation: 0.7612469652343057]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620743353338355		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7620743353338355 | validation: 0.7417632785200656]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403565961808467		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7403565961808467 | validation: 0.7423747222415139]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447332739885099		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7447332739885099 | validation: 0.7424459864816502]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415350107480407		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7415350107480407 | validation: 0.7343191282389614]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517188498381264		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7517188498381264 | validation: 0.7630191273211427]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757891609379717		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7757891609379717 | validation: 0.7682962643763186]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984959079599142		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7984959079599142 | validation: 0.7994599499777669]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898402631750325		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7898402631750325 | validation: 0.738824554686043]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7381928283516386		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7381928283516386 | validation: 0.7162977469922174]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221665618817403		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7221665618817403 | validation: 0.7192597540099458]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266035872347084		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7266035872347084 | validation: 0.7106951350853157]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249351929303631		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7249351929303631 | validation: 0.719868629428298]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347899445172636		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7347899445172636 | validation: 0.7166331713870262]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7604096175990506		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7604096175990506 | validation: 0.7918802971907252]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004916945092844		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8004916945092844 | validation: 0.764794263687949]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846931891382317		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.7846931891382317 | validation: 0.7265195679968866]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7375608145471039		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7375608145471039 | validation: 0.7053114007161972]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149779997801347		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7149779997801347 | validation: 0.7084017038166137]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194703891909499		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7194703891909499 | validation: 0.6976381453868532]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712183934832168		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.712183934832168 | validation: 0.6972287588448798]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720637743995749		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.720637743995749 | validation: 0.7168000761348626]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438066038314795		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7438066038314795 | validation: 0.7459879779572777]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792267683186549		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.792267683186549 | validation: 0.7684447370465538]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851754211930987		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7851754211930987 | validation: 0.7099840283215815]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311831567602564		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7311831567602564 | validation: 0.697086321516377]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063083039144149		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7063083039144149 | validation: 0.6738876594002093]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009923845022238		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7009923845022238 | validation: 0.6827932698006433]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7121759462443514		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7121759462443514 | validation: 0.6964482527308646]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252061580043287		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7252061580043287 | validation: 0.7190361702415587]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577408206984961		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7577408206984961 | validation: 0.7284811418350139]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7561002526039458		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7561002526039458 | validation: 0.701064514162638]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184879590601889		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7184879590601889 | validation: 0.6769604209000122]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011735045178725		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7011735045178725 | validation: 0.656823664995897]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692671168062187		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.692671168062187 | validation: 0.6688455054986218]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904205854496341		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6904205854496341 | validation: 0.661981150367089]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694659309404783		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.694659309404783 | validation: 0.658211075076393]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896689863711356		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6896689863711356 | validation: 0.6602483276817204]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070104574492586		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7070104574492586 | validation: 0.689790663842226]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348387898288722		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7348387898288722 | validation: 0.765777482620435]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169400208422812		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8169400208422812 | validation: 0.7581927900961802]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655578104387079		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7655578104387079 | validation: 0.6853064139049376]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001133003311096		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7001133003311096 | validation: 0.6706600445505521]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859938544876966		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6859938544876966 | validation: 0.6413629558425935]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988962154228525		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.6988962154228525 | validation: 0.6527884456317934]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706292704265054		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.706292704265054 | validation: 0.6627989786414004]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7039495984230035		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7039495984230035 | validation: 0.6629419614907299]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034705815050252		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7034705815050252 | validation: 0.6547149622761808]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002905994583348		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7002905994583348 | validation: 0.6462106146750128]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908940209082414		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6908940209082414 | validation: 0.6365602188789544]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6821883243338673		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6821883243338673 | validation: 0.6401105494022472]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840952240686521		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6840952240686521 | validation: 0.6434252932410862]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955070739813916		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6955070739813916 | validation: 0.6558668952458327]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080892494073947		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7080892494073947 | validation: 0.6523266759825399]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150523060871031		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7150523060871031 | validation: 0.6610022767927177]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969824631343972		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6969824631343972 | validation: 0.6444067989032649]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812113841840431		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6812113841840431 | validation: 0.6323676293299766]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771865554910643		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6771865554910643 | validation: 0.6248713943799177]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680784258626191		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6680784258626191 | validation: 0.6346962171742265]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670573019143468		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.670573019143468 | validation: 0.6223984670421272]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824752499271339		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6824752499271339 | validation: 0.6416322945354727]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924694756023476		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6924694756023476 | validation: 0.6649623739817779]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100845151351828		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7100845151351828 | validation: 0.6544384238322363]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015950375246507		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7015950375246507 | validation: 0.6234219296972943]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776014159339969		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6776014159339969 | validation: 0.5920891190451784]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596286821037662		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6596286821037662 | validation: 0.5896635025351709]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654037330634666		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.654037330634666 | validation: 0.5856496692077852]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604774407783168		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6604774407783168 | validation: 0.5840775989528274]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485696299837034		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6485696299837034 | validation: 0.5913264511584163]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589004397727942		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6589004397727942 | validation: 0.6202370991695605]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6742277258836983		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.6742277258836983 | validation: 0.6483749686202936]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352585704740875		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.7352585704740875 | validation: 0.6837578197330054]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477710718065425		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7477710718065425 | validation: 0.6009818874410802]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629735141486833		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6629735141486833 | validation: 0.5765520823065046]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509487013156092		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6509487013156092 | validation: 0.5992631601928837]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694158296780242		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6694158296780242 | validation: 0.5900592061053145]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661369153201517		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.661369153201517 | validation: 0.6001359486628217]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561442872049815		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6561442872049815 | validation: 0.5715214503089648]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417521809116465		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6417521809116465 | validation: 0.575923179787677]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488208788379407		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6488208788379407 | validation: 0.5759231975793206]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398739123176372		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6398739123176372 | validation: 0.5678198792858872]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443782611282853		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6443782611282853 | validation: 0.5722258648649824]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346070172163434		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6346070172163434 | validation: 0.5693409131782702]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393305210013744		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6393305210013744 | validation: 0.5575567965425303]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254834427671934		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6254834427671934 | validation: 0.575484452191331]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635276074088628		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.635276074088628 | validation: 0.5778904929671207]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323433861556961		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6323433861556961 | validation: 0.5847698349424538]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344898225359942		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6344898225359942 | validation: 0.642915138672215]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214713558922988		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7214713558922988 | validation: 0.7937977957971961]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8466743623694679		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8466743623694679 | validation: 0.6364840706870694]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534712941756419		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6534712941756419 | validation: 0.6017915461985321]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673298506733909		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6673298506733909 | validation: 0.579281565765324]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505256296728981		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6505256296728981 | validation: 0.5412473314212102]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188266588808199		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6188266588808199 | validation: 0.5519021700151171]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249369038204009		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6249369038204009 | validation: 0.580656217255436]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246872454982961		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6246872454982961 | validation: 0.542877578665783]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237630101383522		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6237630101383522 | validation: 0.542902754386182]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263948808147594		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6263948808147594 | validation: 0.5367125635890162]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188144634064505		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6188144634064505 | validation: 0.5621507451220552]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129710575019303		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6129710575019303 | validation: 0.5656276296939093]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178252679851802		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6178252679851802 | validation: 0.5832891980901572]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325484059194402		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6325484059194402 | validation: 0.5759508741721774]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495109299908068		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6495109299908068 | validation: 0.6250972946250268]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650510536742518		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6650510536742518 | validation: 0.5436711785787817]
	TIME [epoch: 1.98 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605323228510211		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.605323228510211 | validation: 0.5319552427452584]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048655973523361		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6048655973523361 | validation: 0.5188410544161391]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994408405002654		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.5994408405002654 | validation: 0.5372267027151977]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932089821150155		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5932089821150155 | validation: 0.5653767038898009]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6064336993188584		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6064336993188584 | validation: 0.5406337333045765]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6071946376569164		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6071946376569164 | validation: 0.5624617608959163]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613429774515977		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.613429774515977 | validation: 0.5612788976912905]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264590840533568		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6264590840533568 | validation: 0.5889834890181369]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333687942488584		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6333687942488584 | validation: 0.5543903185601855]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593292975002318		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.593292975002318 | validation: 0.5419808272505969]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843831061288955		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5843831061288955 | validation: 0.522272434009419]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857081498294937		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5857081498294937 | validation: 0.5541315574531436]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5890511123945971		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5890511123945971 | validation: 0.5303599226205375]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5805567815667952		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5805567815667952 | validation: 0.5433008504303227]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881001426869054		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5881001426869054 | validation: 0.5570773935247181]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6134657108827007		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6134657108827007 | validation: 0.6057046740257159]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424130168513638		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6424130168513638 | validation: 0.5359227169459607]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5669742491777106		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5669742491777106 | validation: 0.52209538088821]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632126283976157		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.5632126283976157 | validation: 0.5383611490578787]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779679057193333		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5779679057193333 | validation: 0.5377352547357493]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5649328385844834		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5649328385844834 | validation: 0.5333515938500742]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5696624777904781		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.5696624777904781 | validation: 0.5285952108097254]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5683041688675108		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5683041688675108 | validation: 0.5326774543957006]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708208003426661		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5708208003426661 | validation: 0.5297474159140482]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523141274039332		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5523141274039332 | validation: 0.5263554565280715]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568962676510725		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.568962676510725 | validation: 0.5076724281120296]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5446107905655929		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.5446107905655929 | validation: 0.5218567227666976]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487695732219026		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5487695732219026 | validation: 0.5186642166053297]
	TIME [epoch: 1.37 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447412521828159		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5447412521828159 | validation: 0.5694926661479054]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815633461584467		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5815633461584467 | validation: 0.4887906785468936]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5272110180239918		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5272110180239918 | validation: 0.49434129898870727]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166651074013148		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5166651074013148 | validation: 0.4897883202722675]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5074533081877439		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5074533081877439 | validation: 0.5053237781360916]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5095198575348386		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5095198575348386 | validation: 0.5247165117007881]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415248692802896		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.5415248692802896 | validation: 0.5692071421233813]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.586501221695985		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.586501221695985 | validation: 0.6251859567530805]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401273392952433		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6401273392952433 | validation: 0.5566571235981662]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5591253267008284		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5591253267008284 | validation: 0.550930645586847]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5653000151415536		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5653000151415536 | validation: 0.5437797254169823]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5378388031567751		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5378388031567751 | validation: 0.4738029204464952]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982886994450814		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.4982886994450814 | validation: 0.4964937500147862]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122444478970951		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5122444478970951 | validation: 0.49922132737595293]
	TIME [epoch: 1.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5091426109609628		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5091426109609628 | validation: 0.48924697950355533]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49840900720928755		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.49840900720928755 | validation: 0.5174914579926234]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5043812931662373		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5043812931662373 | validation: 0.48884478765594896]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.479884323204177		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.479884323204177 | validation: 0.4790603756639897]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4758669052569472		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.4758669052569472 | validation: 0.475425789979092]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4893160114182079		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.4893160114182079 | validation: 0.5377091521881658]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5254917455641486		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5254917455641486 | validation: 0.49682079691399567]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5118010280039922		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5118010280039922 | validation: 0.5650291741757105]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390127834098399		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5390127834098399 | validation: 0.4877622321390523]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47823271601110284		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.47823271601110284 | validation: 0.47948980680910025]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45919597357709296		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.45919597357709296 | validation: 0.48061382415797427]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46899686062514434		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.46899686062514434 | validation: 0.4814297618980982]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027450189332197		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5027450189332197 | validation: 0.6181832981480948]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585296265194729		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.585296265194729 | validation: 0.4941804905132647]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47684467446369455		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.47684467446369455 | validation: 0.5694489146137256]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5834310244556314		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5834310244556314 | validation: 0.5920466289945597]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5743619755807509		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5743619755807509 | validation: 0.5266651828044878]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49752180821707503		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.49752180821707503 | validation: 0.5169032880543301]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072228693972768		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5072228693972768 | validation: 0.5009704967018357]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4598871698172766		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.4598871698172766 | validation: 0.4852737180494933]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45376705555616254		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.45376705555616254 | validation: 0.47211007608876643]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46804368654842576		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.46804368654842576 | validation: 0.5018327421199914]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4757483593803717		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4757483593803717 | validation: 0.45596674729666753]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4483550302861366		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.4483550302861366 | validation: 0.4610917191469037]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4368310916864248		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.4368310916864248 | validation: 0.4870415617138273]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4421713206715525		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.4421713206715525 | validation: 0.47890537182144083]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4525049356021664		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4525049356021664 | validation: 0.4955454846452459]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47232647039154424		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.47232647039154424 | validation: 0.4675735978436551]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44417078985859915		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.44417078985859915 | validation: 0.49155131694057225]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45356329508503435		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.45356329508503435 | validation: 0.46020938966463754]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4514953036908615		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.4514953036908615 | validation: 0.5417697672208853]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011114376710425		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5011114376710425 | validation: 0.45572786602663196]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289259570202723		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4289259570202723 | validation: 0.4702324177683494]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42432768131777865		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.42432768131777865 | validation: 0.49033037590154765]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4702003733821615		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.4702003733821615 | validation: 0.46773590317927677]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45465152065317316		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.45465152065317316 | validation: 0.5330925185601167]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4731024702958049		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.4731024702958049 | validation: 0.4478425891409121]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42348905429493827		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.42348905429493827 | validation: 0.4640420868647801]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4038979233481475		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.4038979233481475 | validation: 0.46551041005336097]
	TIME [epoch: 1.37 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42447178759697834		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.42447178759697834 | validation: 0.5201076700851317]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4865941569765503		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.4865941569765503 | validation: 0.44856110827885526]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4022003792706383		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4022003792706383 | validation: 0.4602532222389636]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953241908969787		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3953241908969787 | validation: 0.44650628031314865]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4043779714254701		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.4043779714254701 | validation: 0.4799149946096506]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4405787947614552		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.4405787947614552 | validation: 0.4976383059588411]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.468444153824051		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.468444153824051 | validation: 0.5465491925100208]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49597315004678166		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.49597315004678166 | validation: 0.4537774288975226]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39237946178204314		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.39237946178204314 | validation: 0.4832463491984828]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4628843537264794		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.4628843537264794 | validation: 0.5046102668232519]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46844590908444816		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.46844590908444816 | validation: 0.4415393975440325]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920806367368042		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.3920806367368042 | validation: 0.5002619224283482]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5023565471541707		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.5023565471541707 | validation: 0.5036835970631575]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44821203763322476		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.44821203763322476 | validation: 0.45330340974855776]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393383537116549		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.393383537116549 | validation: 0.46186780710008785]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43393031483659467		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.43393031483659467 | validation: 0.4574102766664644]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41255474556060107		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.41255474556060107 | validation: 0.4406677943368674]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37177381448620944		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.37177381448620944 | validation: 0.41214226036993096]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37232930174946605		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.37232930174946605 | validation: 0.42562196510086825]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37278980529847744		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.37278980529847744 | validation: 0.4372942804368391]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3881917203073886		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.3881917203073886 | validation: 0.44911426525297293]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39951190335240866		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.39951190335240866 | validation: 0.44125854264374165]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36294849789785133		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.36294849789785133 | validation: 0.41093693611727056]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35741878017800305		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.35741878017800305 | validation: 0.4170565659191383]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35717062000512717		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.35717062000512717 | validation: 0.41827448382933946]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3828576424837153		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.3828576424837153 | validation: 0.5432122276269972]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49046287844113073		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.49046287844113073 | validation: 0.3879687105986962]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477163711065051		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.3477163711065051 | validation: 0.45721212876565576]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41546187720030475		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.41546187720030475 | validation: 0.511016442673254]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48829908129080907		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.48829908129080907 | validation: 0.39251731788089295]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3467206596563934		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.3467206596563934 | validation: 0.48678687066639625]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619821215297555		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4619821215297555 | validation: 0.46670269218704824]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42914254474646935		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.42914254474646935 | validation: 0.39380894150151047]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.340378712212411		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.340378712212411 | validation: 0.4782019684961135]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4467888915647413		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.4467888915647413 | validation: 0.43348584796925]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3681227701531347		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.3681227701531347 | validation: 0.3781830400227849]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336123469149417		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.336123469149417 | validation: 0.38566361185187464]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34599452905169026		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.34599452905169026 | validation: 0.4622796719191376]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42060733470311307		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.42060733470311307 | validation: 0.3927914453040421]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32872211628646575		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.32872211628646575 | validation: 0.3874160438173406]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33230159680642274		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.33230159680642274 | validation: 0.3698145866339482]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32970257986365936		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.32970257986365936 | validation: 0.3837052299821277]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3389534206357637		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.3389534206357637 | validation: 0.4653794537488815]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41439159931362135		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.41439159931362135 | validation: 0.3493892525629403]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3141145584006407		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.3141145584006407 | validation: 0.3637226937942237]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323819028217347		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.323819028217347 | validation: 0.4335163736330368]
	TIME [epoch: 1.37 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780948979967005		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.3780948979967005 | validation: 0.38835291283559087]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3537899458110727		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.3537899458110727 | validation: 0.37406476910120046]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34303588313308		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.34303588313308 | validation: 0.3568168314501774]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30556122550640075		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.30556122550640075 | validation: 0.3650591136843073]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036349682553071		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.3036349682553071 | validation: 0.3603510946791664]
	TIME [epoch: 1.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.331234604921227		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.331234604921227 | validation: 0.4096055047990266]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792956312616291		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.3792956312616291 | validation: 0.3321883333401145]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925572780302349		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.2925572780302349 | validation: 0.3369772392970568]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2942721129433717		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2942721129433717 | validation: 0.36902048707583046]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3212658237618026		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.3212658237618026 | validation: 0.3987090091233878]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39796904560232904		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.39796904560232904 | validation: 0.3938935820476]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35236741818038925		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.35236741818038925 | validation: 0.31770444059013325]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28289650499844216		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.28289650499844216 | validation: 0.31135914601987796]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27960994951944046		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.27960994951944046 | validation: 0.32734029125042213]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27558678934203523		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.27558678934203523 | validation: 0.31147157753543286]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27621692343785276		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.27621692343785276 | validation: 0.3264073053908064]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775184385589316		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2775184385589316 | validation: 0.3476941047971403]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3499166111819239		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.3499166111819239 | validation: 0.45771871544625836]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4145414279368086		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.4145414279368086 | validation: 0.2937329827390111]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2627784498486373		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.2627784498486373 | validation: 0.350242137175764]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390226993105206		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.3390226993105206 | validation: 0.42327926251653986]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014540360611204		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.4014540360611204 | validation: 0.30904683394160193]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2565758301137416		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.2565758301137416 | validation: 0.3244881728546735]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32240457387541077		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.32240457387541077 | validation: 0.2886896796884017]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25074223323338457		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.25074223323338457 | validation: 0.28242994356215034]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25675252201791404		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.25675252201791404 | validation: 0.28983744138363127]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2911389100744571		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2911389100744571 | validation: 0.31021308551526316]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.273711697606		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.273711697606 | validation: 0.2997802243788453]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29757677169542224		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.29757677169542224 | validation: 0.38839985312145286]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35459707326276074		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.35459707326276074 | validation: 0.3410667575933737]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3180662794880229		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.3180662794880229 | validation: 0.2688609416248855]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2287982941107007		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.2287982941107007 | validation: 0.29539022812292365]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24905512951494935		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.24905512951494935 | validation: 0.31910927607800127]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2959278808205469		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.2959278808205469 | validation: 0.30273554380987777]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2580206394848561		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.2580206394848561 | validation: 0.26536100562195647]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2637501931759511		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.2637501931759511 | validation: 0.2727295085562997]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23134212113996364		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.23134212113996364 | validation: 0.26230130411283936]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2300031977804618		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.2300031977804618 | validation: 0.2697557264513497]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2300732093749847		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2300732093749847 | validation: 0.2324209855648104]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21800775124851388		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.21800775124851388 | validation: 0.2501244825746353]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21700025611016954		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.21700025611016954 | validation: 0.2323792342382837]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.213133212041326		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.213133212041326 | validation: 0.2728111894531608]
	TIME [epoch: 266 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23649644500664357		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.23649644500664357 | validation: 0.32279284443432066]
	TIME [epoch: 2.71 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32329129930605727		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.32329129930605727 | validation: 0.3152423750325828]
	TIME [epoch: 2.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693951060547938		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.2693951060547938 | validation: 0.23100997224172573]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2133784401600768		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.2133784401600768 | validation: 0.24739294560242314]
	TIME [epoch: 2.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20492480598386778		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.20492480598386778 | validation: 0.23459174917462217]
	TIME [epoch: 2.71 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23276249378652053		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.23276249378652053 | validation: 0.31984773397401445]
	TIME [epoch: 2.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2735019203258927		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.2735019203258927 | validation: 0.25654639389842376]
	TIME [epoch: 2.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574356230985322		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.2574356230985322 | validation: 0.2840300629525184]
	TIME [epoch: 2.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24547448970828434		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.24547448970828434 | validation: 0.24857745474557147]
	TIME [epoch: 2.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23735134492463827		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.23735134492463827 | validation: 0.25515506312576325]
	TIME [epoch: 2.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2164278133980751		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.2164278133980751 | validation: 0.2526022238202476]
	TIME [epoch: 2.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24911814344892705		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.24911814344892705 | validation: 0.30117031540244366]
	TIME [epoch: 2.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613820070552171		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.2613820070552171 | validation: 0.25665708576408186]
	TIME [epoch: 2.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603147800448303		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2603147800448303 | validation: 0.2177357003974065]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17856566521521708		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.17856566521521708 | validation: 0.2714871657140609]
	TIME [epoch: 2.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20172975288667933		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.20172975288667933 | validation: 0.1959814007874037]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18987056729843146		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.18987056729843146 | validation: 0.23683102321014038]
	TIME [epoch: 2.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19388231902316264		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.19388231902316264 | validation: 0.20822889622201865]
	TIME [epoch: 2.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17943673730645568		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.17943673730645568 | validation: 0.20144626915827335]
	TIME [epoch: 2.69 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17262778488760286		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.17262778488760286 | validation: 0.19921998359259244]
	TIME [epoch: 2.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18112382154235757		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.18112382154235757 | validation: 0.17749102090567284]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741321963757871		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.1741321963757871 | validation: 0.2433672605369612]
	TIME [epoch: 2.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18080481896555717		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.18080481896555717 | validation: 0.36583548011161615]
	TIME [epoch: 2.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3736856904801835		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.3736856904801835 | validation: 0.25259267988235556]
	TIME [epoch: 2.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20408913772716525		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.20408913772716525 | validation: 0.23054848050878263]
	TIME [epoch: 2.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21239521156395913		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.21239521156395913 | validation: 0.2742535136274739]
	TIME [epoch: 2.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23485607359397456		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.23485607359397456 | validation: 0.21277217999333964]
	TIME [epoch: 2.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19216727626126934		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.19216727626126934 | validation: 0.18269548497481922]
	TIME [epoch: 2.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15928287420299095		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.15928287420299095 | validation: 0.21128827231850195]
	TIME [epoch: 2.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17461540319756336		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.17461540319756336 | validation: 0.1832937848194962]
	TIME [epoch: 2.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051217176079838		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.16051217176079838 | validation: 0.2423219437333941]
	TIME [epoch: 2.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20004401985797393		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.20004401985797393 | validation: 0.3652137330808027]
	TIME [epoch: 2.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259002444634381		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.3259002444634381 | validation: 0.17784660538286523]
	TIME [epoch: 2.69 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15556529952811424		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.15556529952811424 | validation: 0.28470336215417585]
	TIME [epoch: 2.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2373339690077203		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.2373339690077203 | validation: 0.2054740820904426]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19987914669155615		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.19987914669155615 | validation: 0.19961589404485874]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15103128262979756		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.15103128262979756 | validation: 0.18514207470629243]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506531389390083		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.1506531389390083 | validation: 0.179405946745391]
	TIME [epoch: 2.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16023337541063043		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.16023337541063043 | validation: 0.20237362267386808]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16036897622354523		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16036897622354523 | validation: 0.17968492414134757]
	TIME [epoch: 2.69 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479044393225299		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.1479044393225299 | validation: 0.1842256961322365]
	TIME [epoch: 2.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492212432362048		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1492212432362048 | validation: 0.19461976565330316]
	TIME [epoch: 2.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19905742502297263		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.19905742502297263 | validation: 0.260867879635444]
	TIME [epoch: 2.69 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20686293178362702		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.20686293178362702 | validation: 0.19919601014180366]
	TIME [epoch: 2.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20778856236226823		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.20778856236226823 | validation: 0.19435618000161148]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15011716456438873		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.15011716456438873 | validation: 0.2035045134901734]
	TIME [epoch: 2.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19173718728885994		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.19173718728885994 | validation: 0.2267501785369758]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18348634632299762		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.18348634632299762 | validation: 0.16026600638261357]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14648478635963294		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.14648478635963294 | validation: 0.21274210796622645]
	TIME [epoch: 2.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649520966025277		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.1649520966025277 | validation: 0.18717426384888608]
	TIME [epoch: 2.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17155183764319634		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.17155183764319634 | validation: 0.19536635418991288]
	TIME [epoch: 2.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635873971842944		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1635873971842944 | validation: 0.16657047466588326]
	TIME [epoch: 2.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15263280098691362		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.15263280098691362 | validation: 0.2330697864897543]
	TIME [epoch: 2.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18162349416747664		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.18162349416747664 | validation: 0.1729709183951493]
	TIME [epoch: 2.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629002896076478		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.1629002896076478 | validation: 0.18358232851848852]
	TIME [epoch: 2.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13829635187808983		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.13829635187808983 | validation: 0.14524865255726302]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13532740990207545		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.13532740990207545 | validation: 0.17407081165479255]
	TIME [epoch: 2.71 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13751718169451813		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.13751718169451813 | validation: 0.15501397169248907]
	TIME [epoch: 2.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12572307721071627		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.12572307721071627 | validation: 0.1509150396694591]
	TIME [epoch: 2.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11898330284457628		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.11898330284457628 | validation: 0.14428866812582766]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11917209191999956		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.11917209191999956 | validation: 0.1524523185834279]
	TIME [epoch: 2.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266258070151615		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1266258070151615 | validation: 0.14230446804046584]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13044624376990338		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.13044624376990338 | validation: 0.24997795085063182]
	TIME [epoch: 2.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18436347194415212		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.18436347194415212 | validation: 0.3657974871335533]
	TIME [epoch: 2.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35104225269549744		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.35104225269549744 | validation: 0.15325584745029985]
	TIME [epoch: 2.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12496475982157688		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.12496475982157688 | validation: 0.45187454147673667]
	TIME [epoch: 2.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4198443530206551		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4198443530206551 | validation: 0.14232978500789342]
	TIME [epoch: 2.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362920207608631		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.12362920207608631 | validation: 0.20524271772494163]
	TIME [epoch: 2.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19424075488743825		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.19424075488743825 | validation: 0.19143147333657684]
	TIME [epoch: 2.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459357278405344		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.1459357278405344 | validation: 0.5298390709750129]
	TIME [epoch: 2.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4855249259412648		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.4855249259412648 | validation: 0.23649941629010307]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2069358421543647		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.2069358421543647 | validation: 0.29360529666593566]
	TIME [epoch: 2.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28885412947418204		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.28885412947418204 | validation: 0.15751418690774405]
	TIME [epoch: 2.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12907416881184325		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.12907416881184325 | validation: 0.2466821674938879]
	TIME [epoch: 2.69 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18256047640775896		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.18256047640775896 | validation: 0.14040799138143775]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152964695724532		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.1152964695724532 | validation: 0.14775006545497757]
	TIME [epoch: 2.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12271719076793482		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.12271719076793482 | validation: 0.14882405342524607]
	TIME [epoch: 2.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262669531592589		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.12262669531592589 | validation: 0.14194244290966387]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11349905408362707		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.11349905408362707 | validation: 0.1441746542525937]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12479089012677948		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.12479089012677948 | validation: 0.20720473809065054]
	TIME [epoch: 2.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517484420690242		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1517484420690242 | validation: 0.13495298417345222]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1263287046761393		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1263287046761393 | validation: 0.14267715426509972]
	TIME [epoch: 2.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11135859907091707		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.11135859907091707 | validation: 0.13758760764232622]
	TIME [epoch: 2.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967570295943552		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.10967570295943552 | validation: 0.131480169000427]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10359571476539543		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.10359571476539543 | validation: 0.12930696608704323]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10526417873951571		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.10526417873951571 | validation: 0.11857980456097438]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067024360854308		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.11067024360854308 | validation: 0.190741160464213]
	TIME [epoch: 2.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14319566039787052		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.14319566039787052 | validation: 0.1640658331641223]
	TIME [epoch: 2.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15658408715897484		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.15658408715897484 | validation: 0.2628571107007351]
	TIME [epoch: 2.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20596371465858732		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.20596371465858732 | validation: 0.18787283302981247]
	TIME [epoch: 2.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1817681229236153		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.1817681229236153 | validation: 0.13156398593693377]
	TIME [epoch: 2.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11231026819709844		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.11231026819709844 | validation: 0.15263727223236506]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1130230841194777		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.1130230841194777 | validation: 0.13811137781696686]
	TIME [epoch: 2.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13819316441360183		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.13819316441360183 | validation: 0.13892736075083092]
	TIME [epoch: 2.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10557397438450887		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.10557397438450887 | validation: 0.13799003480056843]
	TIME [epoch: 2.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09813853116440271		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.09813853116440271 | validation: 0.12843397227887635]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572857637073742		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.09572857637073742 | validation: 0.12420504510209361]
	TIME [epoch: 2.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09447719057436345		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.09447719057436345 | validation: 0.15245280733523597]
	TIME [epoch: 2.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688848252669075		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.10688848252669075 | validation: 0.12615967938874093]
	TIME [epoch: 2.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318961639344661		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.12318961639344661 | validation: 0.17223296347795436]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13055271522516462		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.13055271522516462 | validation: 0.1335336999707149]
	TIME [epoch: 2.69 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581619989401322		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.11581619989401322 | validation: 0.28495113678438794]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23110928060916439		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.23110928060916439 | validation: 0.1382615725745643]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11882373601614721		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.11882373601614721 | validation: 0.11701828662911865]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10456359917481832		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10456359917481832 | validation: 0.20410495084236324]
	TIME [epoch: 2.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491235682379587		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.1491235682379587 | validation: 0.18172539485404218]
	TIME [epoch: 2.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18155360907805967		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.18155360907805967 | validation: 0.12104178085276907]
	TIME [epoch: 2.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303528337060589		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.09303528337060589 | validation: 0.21563754021561155]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1918574548405021		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.1918574548405021 | validation: 0.14424011815078006]
	TIME [epoch: 2.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10316386375174018		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.10316386375174018 | validation: 0.15752236577884193]
	TIME [epoch: 2.69 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11814935062382935		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.11814935062382935 | validation: 0.12188277708570142]
	TIME [epoch: 2.69 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1122897412558904		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.1122897412558904 | validation: 0.11911902557376144]
	TIME [epoch: 2.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09218035148174764		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.09218035148174764 | validation: 0.12631909058896246]
	TIME [epoch: 2.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09188324180784076		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.09188324180784076 | validation: 0.10972990163244219]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09136269254096256		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.09136269254096256 | validation: 0.10797398354957995]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608345437601057		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.08608345437601057 | validation: 0.11143400253062888]
	TIME [epoch: 2.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09138246429545575		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.09138246429545575 | validation: 0.11407928012948687]
	TIME [epoch: 2.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09998218155183222		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.09998218155183222 | validation: 0.15870998946107187]
	TIME [epoch: 2.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292286905826801		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.1292286905826801 | validation: 0.11677972831359118]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12636394364074438		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.12636394364074438 | validation: 0.13814075471925663]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888175533215861		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.09888175533215861 | validation: 0.10300938032659655]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08899192841274557		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.08899192841274557 | validation: 0.11249576187238182]
	TIME [epoch: 2.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08748184364516785		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.08748184364516785 | validation: 0.10519078654861323]
	TIME [epoch: 2.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08863920515089856		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08863920515089856 | validation: 0.1294995774194637]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572598106101544		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.09572598106101544 | validation: 0.2504852712266566]
	TIME [epoch: 2.69 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21440934120772884		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.21440934120772884 | validation: 0.14794371766920553]
	TIME [epoch: 2.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10685177500225279		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10685177500225279 | validation: 0.09976740227860204]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08290007537053612		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.08290007537053612 | validation: 0.10319229069907276]
	TIME [epoch: 2.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09619829420095911		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.09619829420095911 | validation: 0.1297625196177513]
	TIME [epoch: 2.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446365131994554		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.09446365131994554 | validation: 0.1184271317194877]
	TIME [epoch: 2.69 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10778230425719869		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.10778230425719869 | validation: 0.11379857212916407]
	TIME [epoch: 2.69 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08984818813546497		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.08984818813546497 | validation: 0.10458893148208613]
	TIME [epoch: 2.69 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467785515633051		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.08467785515633051 | validation: 0.10560409668796611]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314824779698742		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.08314824779698742 | validation: 0.09870404636056834]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09433604439819834		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.09433604439819834 | validation: 0.16525878202605626]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532722582953855		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.11532722582953855 | validation: 0.1391510271996028]
	TIME [epoch: 2.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12028769249643205		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.12028769249643205 | validation: 0.1578346183566462]
	TIME [epoch: 2.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.113450589457729		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.113450589457729 | validation: 0.08945568282361681]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816090917545773		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.0816090917545773 | validation: 0.09561388628449768]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783257051768199		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.0783257051768199 | validation: 0.10948484738017157]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792236589369936		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.07792236589369936 | validation: 0.10170546791919133]
	TIME [epoch: 2.69 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09347924755613014		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.09347924755613014 | validation: 0.13317908704952675]
	TIME [epoch: 2.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09593520484255366		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.09593520484255366 | validation: 0.11476556116949849]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691026229514766		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.10691026229514766 | validation: 0.14764785461894106]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690446452491167		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.09690446452491167 | validation: 0.11899104967839297]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09521266198643488		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.09521266198643488 | validation: 0.1100683983839969]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07639832358784622		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.07639832358784622 | validation: 0.1225481739351301]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08741761203095032		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.08741761203095032 | validation: 0.1026011127117763]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09292681447668272		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09292681447668272 | validation: 0.12534395358009429]
	TIME [epoch: 2.69 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361568303261002		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.08361568303261002 | validation: 0.08859472519330346]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822253578507861		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.0822253578507861 | validation: 0.10658830894790672]
	TIME [epoch: 2.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798771367505766		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.0798771367505766 | validation: 0.10610575076495393]
	TIME [epoch: 2.69 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090524163671072		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.090524163671072 | validation: 0.1326589201652124]
	TIME [epoch: 2.69 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09943858275231965		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09943858275231965 | validation: 0.09876721023326203]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618848124439343		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.09618848124439343 | validation: 0.10665635336808404]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468597383229413		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.07468597383229413 | validation: 0.08114175529465849]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07610939061842859		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07610939061842859 | validation: 0.10470152175908079]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07331628967138336		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.07331628967138336 | validation: 0.1003048095994424]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545033824727354		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.08545033824727354 | validation: 0.12119965896067726]
	TIME [epoch: 2.69 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08689246297312427		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.08689246297312427 | validation: 0.08918456753243514]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538535872451582		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.07538535872451582 | validation: 0.10233761429234478]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07523013338137671		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.07523013338137671 | validation: 0.12181983912994998]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11040472571246365		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.11040472571246365 | validation: 0.16049915142735563]
	TIME [epoch: 2.69 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11727297679799005		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.11727297679799005 | validation: 0.16002676870705237]
	TIME [epoch: 2.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1432811615024688		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1432811615024688 | validation: 0.09008352290722013]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902206995086976		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.06902206995086976 | validation: 0.1400180793969177]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09360992033795648		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.09360992033795648 | validation: 0.09107624119531449]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08153156886094765		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08153156886094765 | validation: 0.07358322698882931]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06301150528709111		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06301150528709111 | validation: 0.10252775475495467]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929191734187734		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06929191734187734 | validation: 0.08580549743233604]
	TIME [epoch: 2.69 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06619750431531858		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06619750431531858 | validation: 0.08896319124346877]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06384125133126481		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.06384125133126481 | validation: 0.08409033061759381]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309760100428308		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.06309760100428308 | validation: 0.07522327983490573]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453777433803777		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.06453777433803777 | validation: 0.08572211751553623]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860985150815682		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05860985150815682 | validation: 0.08529722735661767]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06579738872100613		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06579738872100613 | validation: 0.19159368623267725]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15346626714374056		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.15346626714374056 | validation: 0.13273617287029071]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787824166800033		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.09787824166800033 | validation: 0.07233476055524042]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06631295587879243		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06631295587879243 | validation: 0.07833838505301616]
	TIME [epoch: 2.69 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06369892569821042		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06369892569821042 | validation: 0.08420837977861884]
	TIME [epoch: 2.69 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458142721524288		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06458142721524288 | validation: 0.07286755248600278]
	TIME [epoch: 2.69 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551618475359847		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.06551618475359847 | validation: 0.09318652022522805]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905112475558564		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06905112475558564 | validation: 0.08054673732431927]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531044156127258		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07531044156127258 | validation: 0.10759237855536652]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07546469258398038		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07546469258398038 | validation: 0.08003083824553801]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06736101935966367		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.06736101935966367 | validation: 0.09231032712827635]
	TIME [epoch: 2.69 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06129023856111851		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.06129023856111851 | validation: 0.08026869142155227]
	TIME [epoch: 2.69 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061695108731108984		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.061695108731108984 | validation: 0.09999769536012612]
	TIME [epoch: 2.69 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846545681065852		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06846545681065852 | validation: 0.08764301648901639]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08719040875122477		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.08719040875122477 | validation: 0.11801000267341706]
	TIME [epoch: 2.69 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08612602511798483		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.08612602511798483 | validation: 0.1321673774463343]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10525219246078095		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.10525219246078095 | validation: 0.07852431369103546]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060988682239772		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06060988682239772 | validation: 0.0888163793071207]
	TIME [epoch: 2.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06176978184641534		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.06176978184641534 | validation: 0.0815584160070013]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06613376645086651		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06613376645086651 | validation: 0.088614702937848]
	TIME [epoch: 2.69 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062269270851493114		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.062269270851493114 | validation: 0.0724873188367982]
	TIME [epoch: 2.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055278615175641		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.055278615175641 | validation: 0.07552391801720332]
	TIME [epoch: 2.69 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054980230467114366		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.054980230467114366 | validation: 0.09079621069619549]
	TIME [epoch: 2.69 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05674015516976623		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.05674015516976623 | validation: 0.07794940981050036]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881219034397464		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.05881219034397464 | validation: 0.0792184378841596]
	TIME [epoch: 2.69 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056799748503059216		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.056799748503059216 | validation: 0.06846025621186813]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05968508429100837		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.05968508429100837 | validation: 0.1283219919864462]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08312588064246658		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.08312588064246658 | validation: 0.09960256610724122]
	TIME [epoch: 2.69 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09981121238249513		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09981121238249513 | validation: 0.11322519532341545]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06909994716813465		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06909994716813465 | validation: 0.0704432112657297]
	TIME [epoch: 2.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360314370874029		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06360314370874029 | validation: 0.07935312353155895]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945805585815378		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05945805585815378 | validation: 0.06554334367823995]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05227483899732658		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.05227483899732658 | validation: 0.07290958436226384]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05297902222504398		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.05297902222504398 | validation: 0.07406463275117232]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05381219999794139		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.05381219999794139 | validation: 0.06658039336858944]
	TIME [epoch: 2.68 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05603121775139659		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05603121775139659 | validation: 0.10040525313425652]
	TIME [epoch: 2.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244101206965967		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06244101206965967 | validation: 0.08587512170841848]
	TIME [epoch: 2.68 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07589986630022003		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.07589986630022003 | validation: 0.1095541904722496]
	TIME [epoch: 2.68 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190902893754746		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.07190902893754746 | validation: 0.07079285268060712]
	TIME [epoch: 2.68 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06731119853700329		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06731119853700329 | validation: 0.0885690976053001]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056139600304573246		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.056139600304573246 | validation: 0.06149920395282134]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05323345920644261		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.05323345920644261 | validation: 0.06336186450268828]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853531618439032		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.04853531618439032 | validation: 0.08356759898787094]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202961173479517		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05202961173479517 | validation: 0.07200350375428062]
	TIME [epoch: 2.69 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258156963447205		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06258156963447205 | validation: 0.12008318986081974]
	TIME [epoch: 2.68 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795858605033961		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.0795858605033961 | validation: 0.1021025782480902]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08479133433640212		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.08479133433640212 | validation: 0.07801615433431273]
	TIME [epoch: 2.68 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05379000802533415		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.05379000802533415 | validation: 0.06585230093213915]
	TIME [epoch: 2.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04944176845702071		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.04944176845702071 | validation: 0.06498975014579982]
	TIME [epoch: 2.68 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054822769099578644		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.054822769099578644 | validation: 0.09173313073814438]
	TIME [epoch: 2.68 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570915363319734		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.0570915363319734 | validation: 0.0630698021832176]
	TIME [epoch: 2.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048772280430848344		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.048772280430848344 | validation: 0.06956231477562827]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691320869517232		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.04691320869517232 | validation: 0.08847039143698741]
	TIME [epoch: 2.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07360067251301784		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.07360067251301784 | validation: 0.10727610295692491]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387165878708307		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.07387165878708307 | validation: 0.05621562897692836]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05786035152870189		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05786035152870189 | validation: 0.0686885391277184]
	TIME [epoch: 2.69 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04742954746192773		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.04742954746192773 | validation: 0.06258438524821341]
	TIME [epoch: 2.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047410602141168025		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.047410602141168025 | validation: 0.053490551410949695]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05236538811844372		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.05236538811844372 | validation: 0.07477794192392062]
	TIME [epoch: 2.69 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055267406480351014		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.055267406480351014 | validation: 0.06114966853203777]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714184686713081		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.05714184686713081 | validation: 0.08006888002402518]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055746991248758156		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.055746991248758156 | validation: 0.057948119528956156]
	TIME [epoch: 2.69 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05361925037034963		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.05361925037034963 | validation: 0.08212704967825102]
	TIME [epoch: 2.69 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658655711030292		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05658655711030292 | validation: 0.046361513443486935]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04976767849492078		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.04976767849492078 | validation: 0.07362170692968051]
	TIME [epoch: 2.69 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863916662006872		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.04863916662006872 | validation: 0.05355643832044965]
	TIME [epoch: 2.69 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458826845439428		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.0458826845439428 | validation: 0.06939385781900402]
	TIME [epoch: 2.69 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602259820014646		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04602259820014646 | validation: 0.056346054466233156]
	TIME [epoch: 2.69 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045700263911328386		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.045700263911328386 | validation: 0.08590488610295989]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05468138855610967		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05468138855610967 | validation: 0.07625981697243067]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802483599861094		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06802483599861094 | validation: 0.12026481712968846]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08428043042731258		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.08428043042731258 | validation: 0.07171737638954148]
	TIME [epoch: 2.69 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06344643201183056		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.06344643201183056 | validation: 0.1311979802593289]
	TIME [epoch: 2.69 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10301528923412155		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.10301528923412155 | validation: 0.05412889781114401]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051185580173011114		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.051185580173011114 | validation: 0.06510229237808832]
	TIME [epoch: 2.68 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05215477583714555		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05215477583714555 | validation: 0.0682012984172589]
	TIME [epoch: 2.68 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04571457520511583		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.04571457520511583 | validation: 0.06450034640997308]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316356652872837		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.04316356652872837 | validation: 0.05768292750812747]
	TIME [epoch: 2.69 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044379735317155396		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.044379735317155396 | validation: 0.057056230458721595]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04385574520463121		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04385574520463121 | validation: 0.05922334837971013]
	TIME [epoch: 2.69 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043105274924764556		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.043105274924764556 | validation: 0.05519977572617524]
	TIME [epoch: 2.68 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0502629632867129		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.0502629632867129 | validation: 0.060560709546014474]
	TIME [epoch: 2.68 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042853967239465114		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.042853967239465114 | validation: 0.05362319710960733]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04242234793609678		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.04242234793609678 | validation: 0.05840040516380072]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062524365634802		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.04062524365634802 | validation: 0.060484187675579074]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042732997778834936		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.042732997778834936 | validation: 0.047738877790366306]
	TIME [epoch: 2.69 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04099875252028356		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.04099875252028356 | validation: 0.05891377952402772]
	TIME [epoch: 2.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04091410322786094		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.04091410322786094 | validation: 0.054205510234694]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04355432535091564		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.04355432535091564 | validation: 0.05910488552654978]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04101078978891967		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.04101078978891967 | validation: 0.059724596991038985]
	TIME [epoch: 2.69 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04290592546694028		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.04290592546694028 | validation: 0.07538540059229006]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859541915456017		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.04859541915456017 | validation: 0.09540855551541325]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720103061908891		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.07720103061908891 | validation: 0.07931501613899826]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609164299619336		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05609164299619336 | validation: 0.051972286173439144]
	TIME [epoch: 2.69 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961387905726704		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.03961387905726704 | validation: 0.0522034173560489]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04117557548890242		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04117557548890242 | validation: 0.06287836540103371]
	TIME [epoch: 2.69 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043350064884298226		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.043350064884298226 | validation: 0.05277937811044218]
	TIME [epoch: 2.69 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344639946649279		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.04344639946649279 | validation: 0.06419829787127608]
	TIME [epoch: 2.68 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04524562403926982		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.04524562403926982 | validation: 0.053964640573226565]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999434710013346		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.04999434710013346 | validation: 0.07802929223044536]
	TIME [epoch: 2.69 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053001651634834834		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.053001651634834834 | validation: 0.04825091712703289]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049208166005773046		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.049208166005773046 | validation: 0.07408345437858284]
	TIME [epoch: 2.69 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04455971165063362		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.04455971165063362 | validation: 0.047105204748418034]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040287084280127		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.040287084280127 | validation: 0.053838071571485596]
	TIME [epoch: 2.69 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888946495425277		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.03888946495425277 | validation: 0.043518492565043455]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036714415008186466		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.036714415008186466 | validation: 0.04276881474784345]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038470645558116544		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.038470645558116544 | validation: 0.05212468989448105]
	TIME [epoch: 2.69 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868002100687521		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.03868002100687521 | validation: 0.05220048509682021]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03789561881378025		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.03789561881378025 | validation: 0.04320361544483206]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712449848589385		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.03712449848589385 | validation: 0.05384752399238793]
	TIME [epoch: 2.69 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702339965939774		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.03702339965939774 | validation: 0.04750272264559931]
	TIME [epoch: 2.68 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574735000442086		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.03574735000442086 | validation: 0.07389407903193594]
	TIME [epoch: 2.69 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051435223106249595		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.051435223106249595 | validation: 0.09264590199322911]
	TIME [epoch: 2.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08156849842989175		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.08156849842989175 | validation: 0.07939495692642841]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048813807413002785		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.048813807413002785 | validation: 0.048472173928093665]
	TIME [epoch: 2.68 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03710686212253527		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.03710686212253527 | validation: 0.0494182335092493]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036834068830097204		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.036834068830097204 | validation: 0.059269304896553014]
	TIME [epoch: 2.68 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038394998818862894		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.038394998818862894 | validation: 0.0538707040855303]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047651615079383		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04047651615079383 | validation: 0.051303672485477285]
	TIME [epoch: 2.68 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549644578277859		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.03549644578277859 | validation: 0.043801468258919174]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036558559318355395		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.036558559318355395 | validation: 0.053246515713232606]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03633041797469411		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.03633041797469411 | validation: 0.04549689501295403]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038472361540798555		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.038472361540798555 | validation: 0.05778285189633106]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04367972222989396		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04367972222989396 | validation: 0.06849837938604822]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573427028706917		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.05573427028706917 | validation: 0.0763406646751301]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289418474566326		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.05289418474566326 | validation: 0.046580405162822254]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0426524951233562		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.0426524951233562 | validation: 0.04516035663406215]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352571997170041		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.0352571997170041 | validation: 0.05559019621064343]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034908509459137774		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.034908509459137774 | validation: 0.040103665824274944]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438989775236809		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.03438989775236809 | validation: 0.04573791164384622]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03457781447025476		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.03457781447025476 | validation: 0.04478299116070384]
	TIME [epoch: 2.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0336618780180566		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.0336618780180566 | validation: 0.04205262910362205]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386220218978726		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.03386220218978726 | validation: 0.047931679260156415]
	TIME [epoch: 2.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491043429613538		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.03491043429613538 | validation: 0.04468506000271302]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035742001275392336		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.035742001275392336 | validation: 0.062262990379390606]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039108486723651505		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.039108486723651505 | validation: 0.05517576117046561]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04898647947189125		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04898647947189125 | validation: 0.0777490803816081]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052968143246855136		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.052968143246855136 | validation: 0.04225422695059378]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348790025678521		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.04348790025678521 | validation: 0.05329335869028236]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038683792098296026		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.038683792098296026 | validation: 0.04448472919018607]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037121685508420925		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.037121685508420925 | validation: 0.04794245083022762]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03640133600106653		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.03640133600106653 | validation: 0.03245131038829007]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03270379911889173		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.03270379911889173 | validation: 0.045571276217476114]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338676011380779		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.03338676011380779 | validation: 0.03779284651969416]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031811324622458226		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.031811324622458226 | validation: 0.03932119642256728]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032809009039075215		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.032809009039075215 | validation: 0.038560943131329094]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03188044241428817		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.03188044241428817 | validation: 0.04007246484427676]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03086019373439778		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.03086019373439778 | validation: 0.04622370328407976]
	TIME [epoch: 2.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03132777145500865		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.03132777145500865 | validation: 0.03850900419176387]
	TIME [epoch: 2.69 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030507871765924535		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.030507871765924535 | validation: 0.03948753987969989]
	TIME [epoch: 2.69 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194064522192853		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.03194064522192853 | validation: 0.04615695841745296]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162751485275989		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.03162751485275989 | validation: 0.037562990445022544]
	TIME [epoch: 2.69 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03784431951236841		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.03784431951236841 | validation: 0.10312674938987139]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06949153029916501		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.06949153029916501 | validation: 0.07169657768700453]
	TIME [epoch: 2.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690661141432971		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06690661141432971 | validation: 0.038562530019640724]
	TIME [epoch: 2.71 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031990252404271224		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.031990252404271224 | validation: 0.047853645908719204]
	TIME [epoch: 2.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037772715839054935		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.037772715839054935 | validation: 0.044553169063525216]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03679929568230549		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.03679929568230549 | validation: 0.03838676300886498]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031262368797342024		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.031262368797342024 | validation: 0.045308653891305464]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03182180851942675		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03182180851942675 | validation: 0.028524751554074242]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032186044676990225		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.032186044676990225 | validation: 0.037954217274685335]
	TIME [epoch: 2.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030356430666324614		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.030356430666324614 | validation: 0.034019784122686456]
	TIME [epoch: 2.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031463491598540595		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.031463491598540595 | validation: 0.03676671544077808]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030520360337343313		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.030520360337343313 | validation: 0.04240967947298608]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030561136951364448		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.030561136951364448 | validation: 0.03136114222665259]
	TIME [epoch: 2.68 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030594591256838558		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.030594591256838558 | validation: 0.05692438438620151]
	TIME [epoch: 2.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902203565492481		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.03902203565492481 | validation: 0.05094550148025084]
	TIME [epoch: 2.68 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453390294809056		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.0453390294809056 | validation: 0.052658470959734804]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03535476716066262		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.03535476716066262 | validation: 0.035439368504151414]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030748220869198192		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.030748220869198192 | validation: 0.04351183295936656]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029063895113796815		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.029063895113796815 | validation: 0.034546434908633745]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027656182266769212		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.027656182266769212 | validation: 0.03676641053569144]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02902073619426499		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.02902073619426499 | validation: 0.03558557268017422]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02981853581784612		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.02981853581784612 | validation: 0.043283949152933016]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029534715998683413		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.029534715998683413 | validation: 0.041386962000047246]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03062841997219712		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.03062841997219712 | validation: 0.03572412192764648]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02805281239395087		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.02805281239395087 | validation: 0.04066349049250745]
	TIME [epoch: 2.68 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029408964468648827		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.029408964468648827 | validation: 0.03423152701450252]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034004046026683554		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.034004046026683554 | validation: 0.08391040781232242]
	TIME [epoch: 2.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051008205952193915		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.051008205952193915 | validation: 0.07801272472245613]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06898278148053029		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.06898278148053029 | validation: 0.04297237451374449]
	TIME [epoch: 2.68 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029548375322601527		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.029548375322601527 | validation: 0.047457866279887656]
	TIME [epoch: 2.68 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03618160742341769		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.03618160742341769 | validation: 0.040331105909096034]
	TIME [epoch: 2.68 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383239281856096		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.03383239281856096 | validation: 0.03802828327877807]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029680153172246513		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.029680153172246513 | validation: 0.033472733811176804]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02721178917454268		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.02721178917454268 | validation: 0.03320790865589625]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03067167481731009		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.03067167481731009 | validation: 0.039393529694378045]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030407512217482193		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.030407512217482193 | validation: 0.03128242591764315]
	TIME [epoch: 2.68 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028079168051758023		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.028079168051758023 | validation: 0.04070489676483225]
	TIME [epoch: 2.68 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02721258899407717		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.02721258899407717 | validation: 0.037502439676200844]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0299793231378147		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.0299793231378147 | validation: 0.059112442390571764]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037434416065444676		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.037434416065444676 | validation: 0.03975308133065635]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041383458918949546		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.041383458918949546 | validation: 0.04167702372825211]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02953117078401534		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.02953117078401534 | validation: 0.03300584554046047]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02960822660565717		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.02960822660565717 | validation: 0.03884538260537582]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313475328426696		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0313475328426696 | validation: 0.04341607515391546]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028468060396161787		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.028468060396161787 | validation: 0.03127284901692526]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028115505317485294		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.028115505317485294 | validation: 0.037814197056283444]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02685599352819574		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.02685599352819574 | validation: 0.03298910377063794]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02720833155245458		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.02720833155245458 | validation: 0.03405680647815562]
	TIME [epoch: 2.68 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028174713886068247		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.028174713886068247 | validation: 0.03397892535725874]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028972683214521355		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.028972683214521355 | validation: 0.05952758920222807]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663828141265993		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03663828141265993 | validation: 0.05198263753973101]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591343975287904		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.04591343975287904 | validation: 0.04718201886729179]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326689926347229		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.0326689926347229 | validation: 0.033429346866728514]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027326801958530142		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.027326801958530142 | validation: 0.02942470105905287]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03120819404153705		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.03120819404153705 | validation: 0.04283322644586453]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02731336669051011		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.02731336669051011 | validation: 0.03333188439482029]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02652541407294651		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.02652541407294651 | validation: 0.03535264482516574]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027411157295553733		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.027411157295553733 | validation: 0.03317036870148778]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0275460873347106		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.0275460873347106 | validation: 0.02512341752662846]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764625385454936		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.02764625385454936 | validation: 0.027247688001395015]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750121662692029		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.02750121662692029 | validation: 0.02811819263338712]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024413339007039625		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.024413339007039625 | validation: 0.04253624318195151]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028943922520112332		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.028943922520112332 | validation: 0.03543077185576409]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868993331910749		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03868993331910749 | validation: 0.07215173930733709]
	TIME [epoch: 2.68 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048720106519510045		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.048720106519510045 | validation: 0.03458285295015679]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03940239144045461		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03940239144045461 | validation: 0.0357925998038274]
	TIME [epoch: 2.68 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026050282460975974		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.026050282460975974 | validation: 0.04036284017192074]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024959598357742436		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.024959598357742436 | validation: 0.035312116336461946]
	TIME [epoch: 2.68 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030706082679179396		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.030706082679179396 | validation: 0.03709438565774958]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025937982748915924		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.025937982748915924 | validation: 0.027225524760092358]
	TIME [epoch: 2.68 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0247835797372643		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0247835797372643 | validation: 0.03203979760510409]
	TIME [epoch: 2.68 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026094707178982005		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.026094707178982005 | validation: 0.02892869003224296]
	TIME [epoch: 2.68 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025374000887422945		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.025374000887422945 | validation: 0.029905397733148577]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635076973834686		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.02635076973834686 | validation: 0.03721209879101293]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02529741306202087		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.02529741306202087 | validation: 0.02913778299186594]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025638168725333407		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.025638168725333407 | validation: 0.029351335478874998]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024522061289618913		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.024522061289618913 | validation: 0.02863733508687923]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02479690470954112		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.02479690470954112 | validation: 0.04316845007056671]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02658024722542791		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.02658024722542791 | validation: 0.027695159509103975]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027047857749270942		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.027047857749270942 | validation: 0.04546816795066949]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03447447559833654		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03447447559833654 | validation: 0.042146450834805005]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436102516676245		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.04436102516676245 | validation: 0.038502219379389024]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028001436700453786		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.028001436700453786 | validation: 0.035039634005454325]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026216649034608103		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.026216649034608103 | validation: 0.02888022691300579]
	TIME [epoch: 2.68 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02808745045113823		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.02808745045113823 | validation: 0.025493979623733592]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024600358060049992		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.024600358060049992 | validation: 0.02492177335235627]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024305403508295277		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.024305403508295277 | validation: 0.02799585541571391]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025875961581612136		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.025875961581612136 | validation: 0.035700707861132816]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025995240893250605		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.025995240893250605 | validation: 0.031121030077093]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033343938391171085		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.033343938391171085 | validation: 0.05893460759350105]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029705579129979454		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.029705579129979454 | validation: 0.024708512284262886]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02385647340991036		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.02385647340991036 | validation: 0.024440683149689436]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024537898957574827		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.024537898957574827 | validation: 0.045988755696664024]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030062341454529067		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.030062341454529067 | validation: 0.030323005307322567]
	TIME [epoch: 2.68 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028054022995490924		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.028054022995490924 | validation: 0.04032422393905993]
	TIME [epoch: 2.68 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025502487128844138		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.025502487128844138 | validation: 0.02759809049464595]
	TIME [epoch: 2.68 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026085817842722785		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.026085817842722785 | validation: 0.031518363816175815]
	TIME [epoch: 2.68 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024652306961906092		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.024652306961906092 | validation: 0.024644763348129622]
	TIME [epoch: 2.68 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025547959094686625		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.025547959094686625 | validation: 0.03677281861322087]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02370871778477398		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.02370871778477398 | validation: 0.023988725602081193]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021874182591555373		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.021874182591555373 | validation: 0.02830454015282325]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02426486980607812		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.02426486980607812 | validation: 0.02734923492820597]
	TIME [epoch: 2.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024804608118315256		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.024804608118315256 | validation: 0.03000640843183322]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024658941060849395		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.024658941060849395 | validation: 0.033675934111308425]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030845774061911635		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.030845774061911635 | validation: 0.05097582929234948]
	TIME [epoch: 2.68 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03056563469337701		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03056563469337701 | validation: 0.027954046100804715]
	TIME [epoch: 2.68 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027630555878739145		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.027630555878739145 | validation: 0.03804296613660531]
	TIME [epoch: 2.68 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024592040391419587		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.024592040391419587 | validation: 0.02902657148327425]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02385319844764214		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.02385319844764214 | validation: 0.030448528641025853]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022734108626094995		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.022734108626094995 | validation: 0.027162639181544518]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024190183334785793		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.024190183334785793 | validation: 0.02996071945440833]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024341808187837356		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.024341808187837356 | validation: 0.025942977593467587]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02503945092566478		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.02503945092566478 | validation: 0.02667032457173317]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023358628718181583		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.023358628718181583 | validation: 0.03314352026748121]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021698832698494482		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.021698832698494482 | validation: 0.025477013024675668]
	TIME [epoch: 2.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024547536611866514		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.024547536611866514 | validation: 0.0297391132283958]
	TIME [epoch: 2.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02547044917509081		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.02547044917509081 | validation: 0.03781865497978649]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04253557614062926		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.04253557614062926 | validation: 0.05750024924738502]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03985178827114815		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.03985178827114815 | validation: 0.0277022302193301]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023074231016886323		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.023074231016886323 | validation: 0.025868799108728243]
	TIME [epoch: 2.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02317214862614014		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.02317214862614014 | validation: 0.027815608958762325]
	TIME [epoch: 2.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022723253248815516		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.022723253248815516 | validation: 0.027565167812225844]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022822311749374498		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.022822311749374498 | validation: 0.02955631480373326]
	TIME [epoch: 2.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0225942109577664		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0225942109577664 | validation: 0.030105242232594972]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023672108219372753		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.023672108219372753 | validation: 0.0295850325967162]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025208049309774886		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.025208049309774886 | validation: 0.03770126339619153]
	TIME [epoch: 2.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026924557347179127		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.026924557347179127 | validation: 0.02660751681083175]
	TIME [epoch: 2.68 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02305309845896239		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.02305309845896239 | validation: 0.031315433266240346]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0228967081364818		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.0228967081364818 | validation: 0.025947438455924017]
	TIME [epoch: 2.68 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023064789953949633		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.023064789953949633 | validation: 0.027872450569920933]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02651634874955504		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.02651634874955504 | validation: 0.039450063838387325]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026547570603906417		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.026547570603906417 | validation: 0.023559731301144665]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025526552819834354		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.025526552819834354 | validation: 0.03050458975391996]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023795757523292674		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.023795757523292674 | validation: 0.025463824179038103]
	TIME [epoch: 2.68 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023054916080526233		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.023054916080526233 | validation: 0.03163587734133049]
	TIME [epoch: 2.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025692916491411495		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.025692916491411495 | validation: 0.025584919518449746]
	TIME [epoch: 2.68 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021986636415143982		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.021986636415143982 | validation: 0.026628345460309757]
	TIME [epoch: 2.68 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021799918469309957		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.021799918469309957 | validation: 0.02611995222186001]
	TIME [epoch: 2.68 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020518166640686026		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.020518166640686026 | validation: 0.024988587353272203]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022172808246805384		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.022172808246805384 | validation: 0.02590914186575943]
	TIME [epoch: 2.68 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022786663901219573		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.022786663901219573 | validation: 0.030508761727907386]
	TIME [epoch: 2.68 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023017422476489732		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.023017422476489732 | validation: 0.020316683002636966]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02329732745718081		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.02329732745718081 | validation: 0.03970365051190458]
	TIME [epoch: 2.68 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025304785122811983		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.025304785122811983 | validation: 0.034874836012052206]
	TIME [epoch: 2.68 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129090678519265		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.03129090678519265 | validation: 0.044235178491042464]
	TIME [epoch: 2.68 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03139836804975019		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03139836804975019 | validation: 0.027168577017819356]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026761529510202494		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.026761529510202494 | validation: 0.03350136506746231]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02319895141792732		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.02319895141792732 | validation: 0.025007707064068525]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022338124604900877		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.022338124604900877 | validation: 0.01922768425814605]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021661468103019433		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.021661468103019433 | validation: 0.030756101390404314]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022253899317640383		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.022253899317640383 | validation: 0.02399241413247402]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02429018058574664		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.02429018058574664 | validation: 0.037507482237549794]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024376852447827908		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.024376852447827908 | validation: 0.02542606118636704]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024627120855959448		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.024627120855959448 | validation: 0.03373035670434615]
	TIME [epoch: 2.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022555382944133145		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.022555382944133145 | validation: 0.026173497172976973]
	TIME [epoch: 2.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021536415481008903		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.021536415481008903 | validation: 0.021003178520990887]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022033168417743318		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.022033168417743318 | validation: 0.0298724201843176]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024386580572128197		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.024386580572128197 | validation: 0.02160338574754239]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023870848260232096		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.023870848260232096 | validation: 0.03154299735820117]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02267396962005757		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.02267396962005757 | validation: 0.02117975416567492]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024787504558041303		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.024787504558041303 | validation: 0.023356298814355997]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020668569864745096		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.020668569864745096 | validation: 0.02825371574682832]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02150860965934587		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.02150860965934587 | validation: 0.02247676840753083]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02253709739891552		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.02253709739891552 | validation: 0.025084471358222834]
	TIME [epoch: 2.68 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020274629725048583		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.020274629725048583 | validation: 0.020049403484684782]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022486367116419066		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.022486367116419066 | validation: 0.03598814602000723]
	TIME [epoch: 2.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02536214530679537		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.02536214530679537 | validation: 0.03339930383633329]
	TIME [epoch: 2.68 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032264646107018985		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.032264646107018985 | validation: 0.04154516126214223]
	TIME [epoch: 2.68 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02350615352314714		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.02350615352314714 | validation: 0.021307600551465612]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01904663355498456		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.01904663355498456 | validation: 0.021786888357855933]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021044584188036463		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.021044584188036463 | validation: 0.02789884181505592]
	TIME [epoch: 2.68 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021537982953320326		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.021537982953320326 | validation: 0.02187522316451849]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020616334671915624		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.020616334671915624 | validation: 0.02340938992427832]
	TIME [epoch: 274 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020837468900287717		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.020837468900287717 | validation: 0.029485505535771152]
	TIME [epoch: 5.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02216756523415869		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.02216756523415869 | validation: 0.02020246194897085]
	TIME [epoch: 5.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024035216706952564		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.024035216706952564 | validation: 0.023445200384244382]
	TIME [epoch: 5.75 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021264605060577584		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.021264605060577584 | validation: 0.019316335787483555]
	TIME [epoch: 5.75 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020485214938174054		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.020485214938174054 | validation: 0.03564652916355549]
	TIME [epoch: 5.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022684185050942496		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.022684185050942496 | validation: 0.020289797583884023]
	TIME [epoch: 5.76 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022447261975020127		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.022447261975020127 | validation: 0.025879784840736282]
	TIME [epoch: 5.75 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022882253658004285		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.022882253658004285 | validation: 0.026647620386870963]
	TIME [epoch: 5.75 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023284165095902348		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.023284165095902348 | validation: 0.031250387861972206]
	TIME [epoch: 5.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021141383646386697		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.021141383646386697 | validation: 0.022074232051968352]
	TIME [epoch: 5.76 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02244385849156749		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.02244385849156749 | validation: 0.026370885445842457]
	TIME [epoch: 5.75 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023880250047145397		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.023880250047145397 | validation: 0.02893289314598109]
	TIME [epoch: 5.76 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02188778568371037		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.02188778568371037 | validation: 0.0240267884627925]
	TIME [epoch: 5.75 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019775506702440264		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.019775506702440264 | validation: 0.02542369639429743]
	TIME [epoch: 5.75 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01987576173046468		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.01987576173046468 | validation: 0.021976623431009396]
	TIME [epoch: 5.75 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01965225620193652		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.01965225620193652 | validation: 0.024770811455146948]
	TIME [epoch: 5.75 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020252712536279912		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.020252712536279912 | validation: 0.030476379509200707]
	TIME [epoch: 5.75 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062123271204861		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.02062123271204861 | validation: 0.01913478984536786]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1019.pth
	Model improved!!!
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021303899101526982		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.021303899101526982 | validation: 0.033612377081392864]
	TIME [epoch: 5.74 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029945899477855937		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.029945899477855937 | validation: 0.025105764862681026]
	TIME [epoch: 5.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02973153792616156		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.02973153792616156 | validation: 0.027500230881028983]
	TIME [epoch: 5.76 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020679803849997357		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.020679803849997357 | validation: 0.03351128299455687]
	TIME [epoch: 5.75 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021689858945130567		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.021689858945130567 | validation: 0.025209211584825156]
	TIME [epoch: 5.76 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021831469162814313		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.021831469162814313 | validation: 0.026420417194630105]
	TIME [epoch: 5.75 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021364006782713777		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.021364006782713777 | validation: 0.020404863061894138]
	TIME [epoch: 5.76 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01978329320904645		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.01978329320904645 | validation: 0.019528224254829474]
	TIME [epoch: 5.76 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021438307531276388		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.021438307531276388 | validation: 0.022324632672579732]
	TIME [epoch: 5.75 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019362407096870468		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.019362407096870468 | validation: 0.021606389182975645]
	TIME [epoch: 5.75 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01940350870649715		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.01940350870649715 | validation: 0.0308007392793403]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02218005368344133		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.02218005368344133 | validation: 0.021295821957116358]
	TIME [epoch: 5.75 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171495947993802		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.02171495947993802 | validation: 0.027688167109287606]
	TIME [epoch: 5.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022284940085285712		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.022284940085285712 | validation: 0.02045812084228107]
	TIME [epoch: 5.76 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02205062758145207		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.02205062758145207 | validation: 0.024035161204414648]
	TIME [epoch: 5.75 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02239039880742488		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.02239039880742488 | validation: 0.023162780478724013]
	TIME [epoch: 5.76 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020505910083972435		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.020505910083972435 | validation: 0.020699139751462006]
	TIME [epoch: 5.76 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019572623217386356		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.019572623217386356 | validation: 0.0234761624922547]
	TIME [epoch: 5.75 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01992008921134779		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.01992008921134779 | validation: 0.02059775909222979]
	TIME [epoch: 5.76 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01856248057283056		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.01856248057283056 | validation: 0.021616988788144553]
	TIME [epoch: 5.75 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022357606608255982		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.022357606608255982 | validation: 0.023727812448729707]
	TIME [epoch: 5.76 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021503436441417802		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.021503436441417802 | validation: 0.032573836161592006]
	TIME [epoch: 5.75 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020795786461497757		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.020795786461497757 | validation: 0.021315050160546346]
	TIME [epoch: 5.76 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022760764016489504		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.022760764016489504 | validation: 0.036494806039148564]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02314037987716628		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.02314037987716628 | validation: 0.019948781083450242]
	TIME [epoch: 5.75 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022827143394186847		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.022827143394186847 | validation: 0.018756522142262727]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1045.pth
	Model improved!!!
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01867087590900313		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.01867087590900313 | validation: 0.02449000613659327]
	TIME [epoch: 5.75 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019552232864576427		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.019552232864576427 | validation: 0.023786188328196924]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02060483801656949		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.02060483801656949 | validation: 0.0248460459518708]
	TIME [epoch: 5.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01994023816670223		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.01994023816670223 | validation: 0.0257052822885138]
	TIME [epoch: 5.75 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971909235587879		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.01971909235587879 | validation: 0.022967329402615477]
	TIME [epoch: 5.74 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01860153736910467		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.01860153736910467 | validation: 0.019749388507753176]
	TIME [epoch: 5.75 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018376847655324647		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.018376847655324647 | validation: 0.020820451631787414]
	TIME [epoch: 5.75 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019927161380386214		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.019927161380386214 | validation: 0.023611866548245725]
	TIME [epoch: 5.76 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019747162847899603		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.019747162847899603 | validation: 0.019345916660660007]
	TIME [epoch: 5.75 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01976870097498724		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.01976870097498724 | validation: 0.019467921579219456]
	TIME [epoch: 5.75 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024885733904051133		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.024885733904051133 | validation: 0.03896694129431871]
	TIME [epoch: 5.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026042176997203476		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.026042176997203476 | validation: 0.018383486553782726]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243635155674837		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.02243635155674837 | validation: 0.026235839517770983]
	TIME [epoch: 5.74 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02007959204666511		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.02007959204666511 | validation: 0.02109300025426117]
	TIME [epoch: 5.75 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019726387853916966		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.019726387853916966 | validation: 0.01874395097202754]
	TIME [epoch: 5.75 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01930156308561825		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.01930156308561825 | validation: 0.024820698250847206]
	TIME [epoch: 5.75 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019518429368216687		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.019518429368216687 | validation: 0.025727867537946644]
	TIME [epoch: 5.75 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019870746905071252		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.019870746905071252 | validation: 0.02297722948504984]
	TIME [epoch: 5.75 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020129291844110702		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.020129291844110702 | validation: 0.02148565413509007]
	TIME [epoch: 5.76 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017220243230644763		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.017220243230644763 | validation: 0.02115160083403535]
	TIME [epoch: 5.75 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01886851430011263		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.01886851430011263 | validation: 0.016560746835478504]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1066.pth
	Model improved!!!
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019874651289156647		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.019874651289156647 | validation: 0.022753746959956272]
	TIME [epoch: 5.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020384226625312935		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.020384226625312935 | validation: 0.019976961266283528]
	TIME [epoch: 5.76 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019569545547092452		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.019569545547092452 | validation: 0.026397803395373884]
	TIME [epoch: 5.76 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01936008806211709		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.01936008806211709 | validation: 0.023208419091751156]
	TIME [epoch: 5.75 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02042587069025484		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.02042587069025484 | validation: 0.022228349819050326]
	TIME [epoch: 5.75 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02101345966701711		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.02101345966701711 | validation: 0.017557512413928316]
	TIME [epoch: 5.75 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021245060540918424		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.021245060540918424 | validation: 0.02746812334041653]
	TIME [epoch: 5.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020666130842160763		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.020666130842160763 | validation: 0.02198208791547922]
	TIME [epoch: 5.76 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021855273102747957		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.021855273102747957 | validation: 0.037570005981062]
	TIME [epoch: 5.75 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024789592750676173		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.024789592750676173 | validation: 0.02254418208394844]
	TIME [epoch: 5.75 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021806528906935334		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.021806528906935334 | validation: 0.021219360133583692]
	TIME [epoch: 5.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01838997480001208		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.01838997480001208 | validation: 0.022900938102434484]
	TIME [epoch: 5.75 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019518200711082307		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.019518200711082307 | validation: 0.02037059489326682]
	TIME [epoch: 5.76 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020986051248631555		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.020986051248631555 | validation: 0.022325798307871444]
	TIME [epoch: 5.75 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01947375017075034		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.01947375017075034 | validation: 0.022413289056120257]
	TIME [epoch: 5.75 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01886639550093863		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.01886639550093863 | validation: 0.021598019615974913]
	TIME [epoch: 5.75 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017311338663725088		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.017311338663725088 | validation: 0.015031324080166431]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1083.pth
	Model improved!!!
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018878425130972418		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.018878425130972418 | validation: 0.021118427706931033]
	TIME [epoch: 5.75 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018933289591658773		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.018933289591658773 | validation: 0.020876617089360696]
	TIME [epoch: 5.76 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020732254751685414		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.020732254751685414 | validation: 0.02324744373038029]
	TIME [epoch: 5.75 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019431117134135506		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.019431117134135506 | validation: 0.023744458354211913]
	TIME [epoch: 5.76 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020567770632511025		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.020567770632511025 | validation: 0.019516874046895435]
	TIME [epoch: 5.76 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019316251397030942		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.019316251397030942 | validation: 0.03296422274112961]
	TIME [epoch: 5.76 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020629154507148178		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.020629154507148178 | validation: 0.020096839285701412]
	TIME [epoch: 5.76 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024227523998440206		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.024227523998440206 | validation: 0.027647162370183234]
	TIME [epoch: 5.76 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02000092321557851		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.02000092321557851 | validation: 0.02394071945750021]
	TIME [epoch: 5.76 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017593560565328684		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.017593560565328684 | validation: 0.02052217385400006]
	TIME [epoch: 5.75 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01898710893117636		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.01898710893117636 | validation: 0.022224180414197877]
	TIME [epoch: 5.76 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01835892476054151		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.01835892476054151 | validation: 0.021843414453263866]
	TIME [epoch: 5.75 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01991931235623661		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.01991931235623661 | validation: 0.021040735257423244]
	TIME [epoch: 5.75 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020499107414184634		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.020499107414184634 | validation: 0.026529000077104698]
	TIME [epoch: 5.76 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020058278527048046		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.020058278527048046 | validation: 0.014819310407830267]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1098.pth
	Model improved!!!
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018468419583222376		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.018468419583222376 | validation: 0.01705294512968339]
	TIME [epoch: 5.75 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018844010913783828		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.018844010913783828 | validation: 0.01955689103618102]
	TIME [epoch: 5.76 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018942474344639065		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.018942474344639065 | validation: 0.019895933757045094]
	TIME [epoch: 5.75 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020731551308665025		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.020731551308665025 | validation: 0.0197209323901794]
	TIME [epoch: 5.76 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018796817828642585		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.018796817828642585 | validation: 0.02293481259005352]
	TIME [epoch: 5.75 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018907273379204264		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.018907273379204264 | validation: 0.021531805408193258]
	TIME [epoch: 5.77 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018389485505143827		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.018389485505143827 | validation: 0.026675155693818432]
	TIME [epoch: 5.75 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020006386140657643		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.020006386140657643 | validation: 0.01630476021231242]
	TIME [epoch: 5.76 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018873757004029452		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.018873757004029452 | validation: 0.01834334096824991]
	TIME [epoch: 5.75 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018050452871470637		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.018050452871470637 | validation: 0.023390082599241732]
	TIME [epoch: 5.76 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016494845426128106		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.016494845426128106 | validation: 0.01744258109073868]
	TIME [epoch: 5.75 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017682931117763704		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.017682931117763704 | validation: 0.019369309955464764]
	TIME [epoch: 5.76 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018871954010045808		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.018871954010045808 | validation: 0.020580055534714037]
	TIME [epoch: 5.75 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019491813239276		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.02019491813239276 | validation: 0.02328674527516012]
	TIME [epoch: 5.75 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01875282576415436		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.01875282576415436 | validation: 0.016675993836754072]
	TIME [epoch: 5.75 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02123854022165166		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.02123854022165166 | validation: 0.03024642359121096]
	TIME [epoch: 5.76 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021317769714366372		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.021317769714366372 | validation: 0.021967335770461612]
	TIME [epoch: 5.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021616805123395255		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.021616805123395255 | validation: 0.023837673301716267]
	TIME [epoch: 5.76 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017953631937095458		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.017953631937095458 | validation: 0.01872121162863281]
	TIME [epoch: 5.75 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018601028669436147		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.018601028669436147 | validation: 0.019334661408425387]
	TIME [epoch: 5.76 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02015736522726187		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02015736522726187 | validation: 0.02540525475692051]
	TIME [epoch: 5.76 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01994781895586483		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.01994781895586483 | validation: 0.02048247570544473]
	TIME [epoch: 5.76 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017927502659811086		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.017927502659811086 | validation: 0.021182484237276224]
	TIME [epoch: 5.76 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019142152735905672		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.019142152735905672 | validation: 0.021775751055768407]
	TIME [epoch: 5.88 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01774919737662061		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.01774919737662061 | validation: 0.016201802648576142]
	TIME [epoch: 5.76 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018677693194209827		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.018677693194209827 | validation: 0.016557604633135183]
	TIME [epoch: 5.76 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017861749659374867		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.017861749659374867 | validation: 0.022276719043743815]
	TIME [epoch: 5.76 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020087131416246764		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.020087131416246764 | validation: 0.019636548247338206]
	TIME [epoch: 5.75 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02259711497818393		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.02259711497818393 | validation: 0.026111867888903902]
	TIME [epoch: 5.75 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01995907238135684		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.01995907238135684 | validation: 0.02413345095862838]
	TIME [epoch: 5.76 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01795601580131058		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.01795601580131058 | validation: 0.01939384449768193]
	TIME [epoch: 5.75 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02013391199222244		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.02013391199222244 | validation: 0.0178427776332655]
	TIME [epoch: 5.76 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01783343874673817		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.01783343874673817 | validation: 0.0196840113599513]
	TIME [epoch: 5.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018452519151663783		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.018452519151663783 | validation: 0.01747991279093516]
	TIME [epoch: 5.76 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017668456189073232		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.017668456189073232 | validation: 0.019374832487719675]
	TIME [epoch: 5.76 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017460477429806828		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.017460477429806828 | validation: 0.016211386356393243]
	TIME [epoch: 5.75 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020018409585759785		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.020018409585759785 | validation: 0.026926454150735746]
	TIME [epoch: 5.76 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016752252566345584		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.016752252566345584 | validation: 0.015369592989813175]
	TIME [epoch: 5.76 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018189620347825956		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.018189620347825956 | validation: 0.0162271830042847]
	TIME [epoch: 5.76 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018891575819064668		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.018891575819064668 | validation: 0.024588190741087736]
	TIME [epoch: 5.76 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022253192129388536		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.022253192129388536 | validation: 0.023459555864430794]
	TIME [epoch: 5.75 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02021297084919852		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.02021297084919852 | validation: 0.022517225645490344]
	TIME [epoch: 5.75 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01678712110169912		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.01678712110169912 | validation: 0.01882643309235332]
	TIME [epoch: 5.76 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016659780184248386		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.016659780184248386 | validation: 0.01956912268689627]
	TIME [epoch: 5.76 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017126947211828218		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.017126947211828218 | validation: 0.022040292918652917]
	TIME [epoch: 5.76 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01789710158275545		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.01789710158275545 | validation: 0.017430787536327947]
	TIME [epoch: 5.75 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016891312407985564		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.016891312407985564 | validation: 0.018461428448791558]
	TIME [epoch: 5.76 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018085570104467662		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.018085570104467662 | validation: 0.02146213916909516]
	TIME [epoch: 5.76 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01741100544371243		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.01741100544371243 | validation: 0.01957304713860866]
	TIME [epoch: 5.75 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017707042264312613		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.017707042264312613 | validation: 0.0174340600878851]
	TIME [epoch: 5.76 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019189717881493536		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.019189717881493536 | validation: 0.027767490819935486]
	TIME [epoch: 5.75 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019290510934115816		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.019290510934115816 | validation: 0.0192302348899436]
	TIME [epoch: 5.76 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01842944753809444		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.01842944753809444 | validation: 0.018495635064567313]
	TIME [epoch: 5.75 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017624477873467218		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.017624477873467218 | validation: 0.017453172816908304]
	TIME [epoch: 5.76 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017078532988051284		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.017078532988051284 | validation: 0.02503437126361793]
	TIME [epoch: 5.75 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017064269653199717		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.017064269653199717 | validation: 0.017501894352593927]
	TIME [epoch: 5.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016045562354517224		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.016045562354517224 | validation: 0.022901649494472927]
	TIME [epoch: 5.75 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951428912880107		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.01951428912880107 | validation: 0.02155910889723206]
	TIME [epoch: 5.76 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020249182129770515		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.020249182129770515 | validation: 0.020897503954536337]
	TIME [epoch: 5.75 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018154691934817925		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.018154691934817925 | validation: 0.025529572594753663]
	TIME [epoch: 5.75 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01919506074210669		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.01919506074210669 | validation: 0.020348364965262263]
	TIME [epoch: 5.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171888741660986		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.02171888741660986 | validation: 0.021119212050692077]
	TIME [epoch: 5.76 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018131474667768807		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.018131474667768807 | validation: 0.02417783316725123]
	TIME [epoch: 5.76 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016567279814216768		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.016567279814216768 | validation: 0.01697854015287582]
	TIME [epoch: 5.77 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018722572152869165		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.018722572152869165 | validation: 0.01835858770618956]
	TIME [epoch: 5.76 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017694339852058896		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.017694339852058896 | validation: 0.01770589889023192]
	TIME [epoch: 5.76 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018544084489879875		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.018544084489879875 | validation: 0.020337520121034083]
	TIME [epoch: 5.75 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018565496346128058		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.018565496346128058 | validation: 0.02047977319742339]
	TIME [epoch: 5.76 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01829850717557855		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.01829850717557855 | validation: 0.021622578712624366]
	TIME [epoch: 5.75 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017223219966819504		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.017223219966819504 | validation: 0.0169785519830206]
	TIME [epoch: 5.75 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01745149205082721		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.01745149205082721 | validation: 0.01927210699738935]
	TIME [epoch: 5.75 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01859073170258842		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.01859073170258842 | validation: 0.023008380804023523]
	TIME [epoch: 5.76 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01727181855796261		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.01727181855796261 | validation: 0.022349647530174856]
	TIME [epoch: 5.75 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019281860879825502		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.019281860879825502 | validation: 0.024307885812130104]
	TIME [epoch: 5.76 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01752873080999538		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.01752873080999538 | validation: 0.021409196246254016]
	TIME [epoch: 5.75 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017143221338127745		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.017143221338127745 | validation: 0.017522218131325224]
	TIME [epoch: 5.75 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01614854644288804		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.01614854644288804 | validation: 0.018355052739209936]
	TIME [epoch: 5.75 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01778727268084861		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.01778727268084861 | validation: 0.021329665152090494]
	TIME [epoch: 5.75 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01823063075777914		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.01823063075777914 | validation: 0.018068645882537065]
	TIME [epoch: 5.76 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0169575794630104		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.0169575794630104 | validation: 0.02241905415321844]
	TIME [epoch: 5.75 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01587770391100429		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.01587770391100429 | validation: 0.019747859790928808]
	TIME [epoch: 5.75 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016332779452383747		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.016332779452383747 | validation: 0.016124481341801413]
	TIME [epoch: 5.76 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01731567584919277		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.01731567584919277 | validation: 0.023116869066833835]
	TIME [epoch: 5.76 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017521439059300453		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.017521439059300453 | validation: 0.017935849740807325]
	TIME [epoch: 5.76 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0184031187825768		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.0184031187825768 | validation: 0.02780263354178781]
	TIME [epoch: 5.75 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017819702930670037		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.017819702930670037 | validation: 0.020888571458577983]
	TIME [epoch: 5.76 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018351197890630725		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.018351197890630725 | validation: 0.024657204521331695]
	TIME [epoch: 5.75 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018118831732178077		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.018118831732178077 | validation: 0.0179446847547447]
	TIME [epoch: 5.76 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017658512991217547		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.017658512991217547 | validation: 0.02217812492751033]
	TIME [epoch: 5.75 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016318821678330816		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.016318821678330816 | validation: 0.021467540412473337]
	TIME [epoch: 5.76 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01560584435287221		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.01560584435287221 | validation: 0.020141492232769887]
	TIME [epoch: 5.75 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016286578834165354		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.016286578834165354 | validation: 0.01975612389948487]
	TIME [epoch: 5.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01650901235350983		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.01650901235350983 | validation: 0.016435606144298145]
	TIME [epoch: 5.76 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017963781542873352		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.017963781542873352 | validation: 0.028350749726257174]
	TIME [epoch: 5.76 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018458921292188263		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.018458921292188263 | validation: 0.02520003621189685]
	TIME [epoch: 5.76 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01899286274686651		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.01899286274686651 | validation: 0.023813270450228963]
	TIME [epoch: 5.75 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015117580703936255		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.015117580703936255 | validation: 0.017754164473290514]
	TIME [epoch: 5.75 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017615494481368052		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.017615494481368052 | validation: 0.01742195462819724]
	TIME [epoch: 5.75 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016048073255447097		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.016048073255447097 | validation: 0.019079059188117316]
	TIME [epoch: 5.77 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01787930559300046		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.01787930559300046 | validation: 0.016936325185518932]
	TIME [epoch: 5.76 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015126333425009336		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.015126333425009336 | validation: 0.017612079214451827]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_153401/states/model_phi1_3a_v_mmd1_1199.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4207.312 seconds.
