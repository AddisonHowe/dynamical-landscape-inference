Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 617787268

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.855975814405467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.855975814405467 | validation: 4.8740995772396225]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.267301907214407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.267301907214407 | validation: 4.4352436094462595]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.603253174328337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.603253174328337 | validation: 4.902872609111771]
	TIME [epoch: 1.42 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.215832898782584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.215832898782584 | validation: 5.314644147691708]
	TIME [epoch: 1.42 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.7350391336551825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7350391336551825 | validation: 3.6111258344355024]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8745701674862216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8745701674862216 | validation: 4.17457385306065]
	TIME [epoch: 1.43 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.326367664921947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.326367664921947 | validation: 3.657671895755656]
	TIME [epoch: 1.43 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.775247225073605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.775247225073605 | validation: 3.6132788260752577]
	TIME [epoch: 1.42 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7907847567227977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7907847567227977 | validation: 3.6733962131765825]
	TIME [epoch: 1.42 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.760165013511097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.760165013511097 | validation: 3.4667755487583367]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.653584655938866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.653584655938866 | validation: 3.498907309212939]
	TIME [epoch: 1.42 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6081461942679613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6081461942679613 | validation: 3.41242223035357]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5750032271726937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5750032271726937 | validation: 3.468787022288108]
	TIME [epoch: 1.42 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.560824464317576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.560824464317576 | validation: 3.3746705606820373]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5389085581758195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5389085581758195 | validation: 3.4296746412221513]
	TIME [epoch: 1.43 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5263064474381465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5263064474381465 | validation: 3.32353294306487]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4916079951245105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4916079951245105 | validation: 3.353630665278739]
	TIME [epoch: 1.43 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.470023756293421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.470023756293421 | validation: 3.2775324890795448]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4447458786921903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4447458786921903 | validation: 3.293757013727866]
	TIME [epoch: 1.43 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4285519425531623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4285519425531623 | validation: 3.2342807278534167]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.416696948386337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.416696948386337 | validation: 3.2791610865092973]
	TIME [epoch: 1.43 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.41953794968656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.41953794968656 | validation: 3.2135461358972126]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.414312986352172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.414312986352172 | validation: 3.2881199761560382]
	TIME [epoch: 1.43 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.432743416908321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.432743416908321 | validation: 3.18711730594819]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3853330188949116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3853330188949116 | validation: 3.208502215859058]
	TIME [epoch: 1.43 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3632112399394747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3632112399394747 | validation: 3.1243598656516496]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3183307702889047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3183307702889047 | validation: 3.1342322891982453]
	TIME [epoch: 1.43 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3002558632257295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3002558632257295 | validation: 3.086782256484243]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2803056366423244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2803056366423244 | validation: 3.1101876048933894]
	TIME [epoch: 1.43 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2846639194841694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2846639194841694 | validation: 3.0837156838534074]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.294741763486033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.294741763486033 | validation: 3.155551405434668]
	TIME [epoch: 1.43 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.34154808390692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.34154808390692 | validation: 3.0498427589242474]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.269039180705086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.269039180705086 | validation: 3.0513550109109433]
	TIME [epoch: 1.42 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2376100042124025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2376100042124025 | validation: 2.9874812592815294]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.200101196658793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.200101196658793 | validation: 2.9981195959681193]
	TIME [epoch: 1.43 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1888927545442547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1888927545442547 | validation: 2.963931246724247]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1752311997957317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1752311997957317 | validation: 2.9893820449764883]
	TIME [epoch: 1.42 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.179569750173743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.179569750173743 | validation: 2.955438956666801]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1617594334746553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1617594334746553 | validation: 2.9843834013070683]
	TIME [epoch: 1.43 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.16606680675421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.16606680675421 | validation: 2.928177111695133]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1044115686231812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1044115686231812 | validation: 2.918752774278679]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.060705218448114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.060705218448114 | validation: 2.8686736191293734]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.983764899915392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.983764899915392 | validation: 2.7600994840473363]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8842322422752646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8842322422752646 | validation: 2.563254020506837]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6107356471484673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6107356471484673 | validation: 2.1852850705650733]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4119753748330606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4119753748330606 | validation: 1.7964737020818728]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485439101123354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.485439101123354 | validation: 3.3873917817527515]
	TIME [epoch: 1.43 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2644309301856347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2644309301856347 | validation: 1.1461305064489546]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986670016615955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9986670016615955 | validation: 1.7753235682148139]
	TIME [epoch: 1.43 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5712222443895372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5712222443895372 | validation: 1.2730841315850903]
	TIME [epoch: 1.43 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149464552416317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.149464552416317 | validation: 0.9915227306435818]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9526960527512551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9526960527512551 | validation: 1.1023117886625855]
	TIME [epoch: 1.43 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759275529853189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759275529853189 | validation: 1.0295315059305528]
	TIME [epoch: 1.43 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426422615690017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426422615690017 | validation: 0.9366121921530287]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960985115263016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960985115263016 | validation: 0.9595239692117886]
	TIME [epoch: 1.43 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087262803493865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087262803493865 | validation: 0.9693234361390882]
	TIME [epoch: 1.43 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986140912959293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986140912959293 | validation: 0.9193240846313414]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821813698464112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7821813698464112 | validation: 0.9463196692949349]
	TIME [epoch: 1.43 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921947799257082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7921947799257082 | validation: 0.943103320037881]
	TIME [epoch: 1.43 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840197043748063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7840197043748063 | validation: 0.9402031732459695]
	TIME [epoch: 1.43 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775186081199621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775186081199621 | validation: 0.962370154281051]
	TIME [epoch: 1.43 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766103040889405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7766103040889405 | validation: 0.916612095951926]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736063560153725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736063560153725 | validation: 0.9530566266721368]
	TIME [epoch: 1.43 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786119466874009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786119466874009 | validation: 0.9201872367570605]
	TIME [epoch: 1.43 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746247649047843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746247649047843 | validation: 0.9606665152606738]
	TIME [epoch: 1.43 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769675568120101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769675568120101 | validation: 0.8958149844286702]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811376996720749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811376996720749 | validation: 1.0359823285193157]
	TIME [epoch: 1.43 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869269522532005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869269522532005 | validation: 0.9340012101252463]
	TIME [epoch: 1.43 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8482760579485569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8482760579485569 | validation: 1.073574764665704]
	TIME [epoch: 1.43 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794991365343537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794991365343537 | validation: 1.0920936583564849]
	TIME [epoch: 1.43 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016044034350381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016044034350381 | validation: 0.8997448279822905]
	TIME [epoch: 1.43 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086563349144873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8086563349144873 | validation: 0.9889947637237481]
	TIME [epoch: 1.42 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77559779005076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77559779005076 | validation: 0.8642229672939387]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718218924099361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7718218924099361 | validation: 1.047783095393689]
	TIME [epoch: 1.43 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949698982188804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7949698982188804 | validation: 0.8800519893411753]
	TIME [epoch: 1.43 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815836601198339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815836601198339 | validation: 1.1113502181298949]
	TIME [epoch: 1.44 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8143602271368766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8143602271368766 | validation: 0.8884622810969252]
	TIME [epoch: 1.43 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014720937833181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8014720937833181 | validation: 0.9861074139638938]
	TIME [epoch: 1.43 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739783446177642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739783446177642 | validation: 0.8761192570575533]
	TIME [epoch: 1.43 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670442093171479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670442093171479 | validation: 0.9294325410556784]
	TIME [epoch: 1.82 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617468768853536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617468768853536 | validation: 0.8831570547106969]
	TIME [epoch: 1.43 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549112641482265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549112641482265 | validation: 0.931382185689607]
	TIME [epoch: 1.43 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7579875828204525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7579875828204525 | validation: 0.8814848957282225]
	TIME [epoch: 1.43 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871017695479979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7871017695479979 | validation: 1.2271437586750726]
	TIME [epoch: 1.43 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9741805085419654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9741805085419654 | validation: 0.9493155455297331]
	TIME [epoch: 1.43 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573526234001655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573526234001655 | validation: 0.9697526174292369]
	TIME [epoch: 1.43 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087987001886817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087987001886817 | validation: 1.0660429666647961]
	TIME [epoch: 1.43 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810652272347225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810652272347225 | validation: 0.9587699501683536]
	TIME [epoch: 1.43 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814278223627767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814278223627767 | validation: 1.1524333183213666]
	TIME [epoch: 1.43 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508475313774571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508475313774571 | validation: 0.8799891901180174]
	TIME [epoch: 1.43 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911940375841044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911940375841044 | validation: 0.9005690934131565]
	TIME [epoch: 1.43 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606810000703701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606810000703701 | validation: 0.8926560216853017]
	TIME [epoch: 1.43 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611829071579086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611829071579086 | validation: 0.8475316079650972]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582356431823447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582356431823447 | validation: 0.9428746625576381]
	TIME [epoch: 1.44 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662700705633095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7662700705633095 | validation: 0.8596284997280347]
	TIME [epoch: 1.43 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653888206741624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653888206741624 | validation: 0.9549607312839286]
	TIME [epoch: 1.43 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741559388945685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7741559388945685 | validation: 0.8894343435402372]
	TIME [epoch: 1.43 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057310306467826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057310306467826 | validation: 1.0779000755403683]
	TIME [epoch: 1.43 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049330818741536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049330818741536 | validation: 1.0448311563218675]
	TIME [epoch: 1.43 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815465933688157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815465933688157 | validation: 0.9806829865770621]
	TIME [epoch: 1.43 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717120287247713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8717120287247713 | validation: 1.0771706375508108]
	TIME [epoch: 1.43 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084112292124315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8084112292124315 | validation: 0.849302980573333]
	TIME [epoch: 1.43 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981426872484044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7981426872484044 | validation: 1.0288218762347654]
	TIME [epoch: 1.43 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918615189467905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918615189467905 | validation: 0.8530876535347872]
	TIME [epoch: 1.43 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667442541901616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7667442541901616 | validation: 0.9229967764317843]
	TIME [epoch: 1.43 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609376007590041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7609376007590041 | validation: 0.8662242343159335]
	TIME [epoch: 1.42 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697334066126393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7697334066126393 | validation: 0.9874206278713699]
	TIME [epoch: 1.43 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833506061158144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7833506061158144 | validation: 0.8936200566353034]
	TIME [epoch: 1.43 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161539110943962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161539110943962 | validation: 1.0251509559504133]
	TIME [epoch: 1.43 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192009025936426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192009025936426 | validation: 0.9368246458071638]
	TIME [epoch: 1.43 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260491343769911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260491343769911 | validation: 0.9152134275702187]
	TIME [epoch: 1.43 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738255945404936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738255945404936 | validation: 0.917983058076878]
	TIME [epoch: 1.43 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573547392570268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573547392570268 | validation: 0.8439660188334971]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722761832394079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722761832394079 | validation: 1.1763431034662275]
	TIME [epoch: 1.43 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688235800248768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688235800248768 | validation: 0.8718550711320735]
	TIME [epoch: 1.43 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380037696821923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8380037696821923 | validation: 1.0803538044651937]
	TIME [epoch: 1.43 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266226525778315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266226525778315 | validation: 0.8823872017922894]
	TIME [epoch: 1.43 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511561680095856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511561680095856 | validation: 0.8603907406157818]
	TIME [epoch: 1.43 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673560702301001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673560702301001 | validation: 0.9891840157238807]
	TIME [epoch: 1.43 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853846155380994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853846155380994 | validation: 0.8841423639514787]
	TIME [epoch: 1.43 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956377445560344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7956377445560344 | validation: 1.0691542970467693]
	TIME [epoch: 1.43 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275714732620568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8275714732620568 | validation: 0.9889348599417938]
	TIME [epoch: 1.43 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893390408264492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893390408264492 | validation: 0.8975662222586354]
	TIME [epoch: 1.43 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906337313937237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906337313937237 | validation: 0.9480726661968873]
	TIME [epoch: 1.43 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673532165412784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673532165412784 | validation: 0.8401721513061983]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645810226659561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645810226659561 | validation: 0.9651504016145112]
	TIME [epoch: 1.43 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711601144580346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711601144580346 | validation: 0.8820684283297592]
	TIME [epoch: 1.43 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602703156575175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602703156575175 | validation: 0.9075883524174695]
	TIME [epoch: 1.43 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611501169619291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611501169619291 | validation: 0.9247410446575558]
	TIME [epoch: 1.43 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917640623837412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7917640623837412 | validation: 0.9862014003917159]
	TIME [epoch: 1.43 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434972278196046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8434972278196046 | validation: 0.9538891729265808]
	TIME [epoch: 1.43 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304695354912415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304695354912415 | validation: 0.9747811709753317]
	TIME [epoch: 1.43 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805079944937517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805079944937517 | validation: 0.9004612541240697]
	TIME [epoch: 1.43 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618119635312549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618119635312549 | validation: 0.8822389675945216]
	TIME [epoch: 1.43 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746689807483501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746689807483501 | validation: 0.9699469637761544]
	TIME [epoch: 1.43 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648993122380974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648993122380974 | validation: 0.8525422476195086]
	TIME [epoch: 1.43 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820352925818325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820352925818325 | validation: 1.3316914828475128]
	TIME [epoch: 1.43 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516132049551672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9516132049551672 | validation: 0.8894176509787659]
	TIME [epoch: 1.43 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787291508797682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787291508797682 | validation: 0.9008328067450564]
	TIME [epoch: 1.43 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151182869289391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151182869289391 | validation: 1.0313479663279688]
	TIME [epoch: 1.43 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222226677750811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222226677750811 | validation: 0.9033245282307917]
	TIME [epoch: 1.43 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618817132999811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618817132999811 | validation: 0.8534521034984279]
	TIME [epoch: 1.43 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700375574057071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700375574057071 | validation: 0.9713594287760838]
	TIME [epoch: 1.43 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769946525025266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769946525025266 | validation: 0.8555349269053996]
	TIME [epoch: 1.43 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668446317054537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668446317054537 | validation: 0.9182531679541035]
	TIME [epoch: 1.43 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743350452993132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743350452993132 | validation: 0.9148032392870743]
	TIME [epoch: 1.43 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967257111200367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967257111200367 | validation: 0.9366429131081863]
	TIME [epoch: 1.43 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249679964780122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8249679964780122 | validation: 0.9712786642701986]
	TIME [epoch: 1.43 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288281167460667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8288281167460667 | validation: 0.9331170729143365]
	TIME [epoch: 1.43 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76930538523335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76930538523335 | validation: 0.8644646056876131]
	TIME [epoch: 1.43 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464423235680328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464423235680328 | validation: 0.9535849048973382]
	TIME [epoch: 1.43 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759856733246683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759856733246683 | validation: 0.8286719644658899]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757098537261325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757098537261325 | validation: 1.1248174107596243]
	TIME [epoch: 1.43 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203572843706611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8203572843706611 | validation: 0.8482817758239801]
	TIME [epoch: 1.43 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769003432810704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769003432810704 | validation: 0.9867600061066999]
	TIME [epoch: 1.43 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691657716494262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691657716494262 | validation: 0.8542749792244395]
	TIME [epoch: 1.43 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548352907793483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548352907793483 | validation: 0.9699846136752175]
	TIME [epoch: 1.43 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620536139354371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7620536139354371 | validation: 0.8733262234343961]
	TIME [epoch: 1.43 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019362679560875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019362679560875 | validation: 1.063962830041629]
	TIME [epoch: 1.43 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828613819596994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.828613819596994 | validation: 0.9797105365816414]
	TIME [epoch: 1.43 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752674495588003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752674495588003 | validation: 0.8657935372691397]
	TIME [epoch: 1.44 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828057719539477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7828057719539477 | validation: 1.0708135371018839]
	TIME [epoch: 1.43 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092512042770987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8092512042770987 | validation: 0.8511445750554479]
	TIME [epoch: 1.43 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711952383178485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711952383178485 | validation: 0.9237862900396125]
	TIME [epoch: 1.43 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483387712655816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7483387712655816 | validation: 0.8750493742442912]
	TIME [epoch: 1.43 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477482132521922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7477482132521922 | validation: 0.8699856043935166]
	TIME [epoch: 1.43 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749028823116711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749028823116711 | validation: 0.9155282323743541]
	TIME [epoch: 1.43 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529524286679594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529524286679594 | validation: 0.8812589100255036]
	TIME [epoch: 1.43 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731972449350571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731972449350571 | validation: 1.0413968176480528]
	TIME [epoch: 1.43 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454514193373669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8454514193373669 | validation: 0.9180765946818966]
	TIME [epoch: 1.43 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375875876667256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8375875876667256 | validation: 0.9731138225227449]
	TIME [epoch: 1.43 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971899660753927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971899660753927 | validation: 0.899984493902087]
	TIME [epoch: 1.43 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545310588028238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545310588028238 | validation: 0.8643907797231741]
	TIME [epoch: 1.43 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761690673254609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761690673254609 | validation: 1.0495196612986253]
	TIME [epoch: 1.43 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874816668725132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7874816668725132 | validation: 0.8846228110277681]
	TIME [epoch: 1.43 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157995896456057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8157995896456057 | validation: 1.049285715669401]
	TIME [epoch: 1.43 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919880001123872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7919880001123872 | validation: 0.873629753634456]
	TIME [epoch: 1.43 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666917648836726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7666917648836726 | validation: 0.8863049368154194]
	TIME [epoch: 1.43 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525935275736051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525935275736051 | validation: 0.8926419904972053]
	TIME [epoch: 1.43 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75033788697159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75033788697159 | validation: 0.8537853708574641]
	TIME [epoch: 1.43 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7510414530430733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7510414530430733 | validation: 0.9761867325495666]
	TIME [epoch: 1.43 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757811454745956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757811454745956 | validation: 0.8680896450519617]
	TIME [epoch: 1.43 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796632594489324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.796632594489324 | validation: 0.9986308770261435]
	TIME [epoch: 1.43 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338596722997977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8338596722997977 | validation: 0.9941392307308523]
	TIME [epoch: 1.43 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905469244740521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7905469244740521 | validation: 0.8834272340040443]
	TIME [epoch: 1.43 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980344631466458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980344631466458 | validation: 1.0331736677958672]
	TIME [epoch: 1.43 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776331026679205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776331026679205 | validation: 0.8512130917656011]
	TIME [epoch: 1.43 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445855576363357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445855576363357 | validation: 0.8757483401214572]
	TIME [epoch: 1.43 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739493562294874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739493562294874 | validation: 0.9006475841894972]
	TIME [epoch: 1.43 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409997717929349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409997717929349 | validation: 0.8845687796424904]
	TIME [epoch: 1.43 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532135636131639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532135636131639 | validation: 0.861276996447752]
	TIME [epoch: 1.43 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668783152204232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668783152204232 | validation: 1.0170177097577955]
	TIME [epoch: 1.43 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167895787007156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167895787007156 | validation: 0.8946902211553538]
	TIME [epoch: 1.43 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131794628290548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8131794628290548 | validation: 0.9817159102655154]
	TIME [epoch: 1.43 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899402173035438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7899402173035438 | validation: 0.9005680396206774]
	TIME [epoch: 1.43 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746308620922091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746308620922091 | validation: 0.8388244132767817]
	TIME [epoch: 1.43 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399275277402737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399275277402737 | validation: 0.9453893501028277]
	TIME [epoch: 1.43 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.742303900629762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.742303900629762 | validation: 0.8183596548770972]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569186506667842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7569186506667842 | validation: 1.188203298258079]
	TIME [epoch: 1.43 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328056033031994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328056033031994 | validation: 0.902380381273315]
	TIME [epoch: 1.43 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309077012555541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309077012555541 | validation: 0.9582531024366405]
	TIME [epoch: 179 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686583918673945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7686583918673945 | validation: 0.9774674319237193]
	TIME [epoch: 2.86 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741927872679666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7741927872679666 | validation: 0.8312833847802497]
	TIME [epoch: 2.83 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661041109093293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661041109093293 | validation: 0.9050973328134306]
	TIME [epoch: 2.83 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487986513624099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487986513624099 | validation: 0.8636037970531995]
	TIME [epoch: 2.82 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392547741415184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7392547741415184 | validation: 0.856927467103049]
	TIME [epoch: 2.83 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403118355218311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403118355218311 | validation: 0.9605485730126665]
	TIME [epoch: 2.83 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654401597306251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654401597306251 | validation: 0.8876635075762853]
	TIME [epoch: 2.83 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055651316223389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055651316223389 | validation: 0.9810722688836772]
	TIME [epoch: 2.83 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837597608739032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837597608739032 | validation: 0.9260266451870027]
	TIME [epoch: 2.83 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616415324206963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616415324206963 | validation: 0.8310802720774625]
	TIME [epoch: 2.83 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778128570426046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778128570426046 | validation: 1.050958310097967]
	TIME [epoch: 2.83 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963246123002129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963246123002129 | validation: 0.829345455819622]
	TIME [epoch: 2.83 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461757954542227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461757954542227 | validation: 0.8517259426429405]
	TIME [epoch: 2.83 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258210370089455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258210370089455 | validation: 0.9044440447503004]
	TIME [epoch: 2.83 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335562720062244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7335562720062244 | validation: 0.8258436870405581]
	TIME [epoch: 2.83 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363557205935427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363557205935427 | validation: 1.01180556611636]
	TIME [epoch: 2.83 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589995104600006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589995104600006 | validation: 0.8591738025922665]
	TIME [epoch: 2.83 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718643530862136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7718643530862136 | validation: 1.006907373471679]
	TIME [epoch: 2.82 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757291239819291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757291239819291 | validation: 0.8664413088254665]
	TIME [epoch: 2.82 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756525989283122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756525989283122 | validation: 0.9099688727730462]
	TIME [epoch: 2.83 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912548663525898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7912548663525898 | validation: 1.0419518332509654]
	TIME [epoch: 2.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706559344150193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706559344150193 | validation: 0.8420398173956866]
	TIME [epoch: 2.83 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625633384036625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7625633384036625 | validation: 0.8404812015829484]
	TIME [epoch: 2.83 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331611885440481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331611885440481 | validation: 0.9307831804385828]
	TIME [epoch: 2.83 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304620175979298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7304620175979298 | validation: 0.819767460538235]
	TIME [epoch: 2.83 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323880930668404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7323880930668404 | validation: 0.8819544182758587]
	TIME [epoch: 2.83 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266024281277378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266024281277378 | validation: 0.8614868976552442]
	TIME [epoch: 2.83 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7255031716152012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7255031716152012 | validation: 0.8661059251008837]
	TIME [epoch: 2.83 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336121104748881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336121104748881 | validation: 0.8840408704721672]
	TIME [epoch: 2.83 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714871144597824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714871144597824 | validation: 0.9950161561028414]
	TIME [epoch: 2.83 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468945177304429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8468945177304429 | validation: 0.9139372741276169]
	TIME [epoch: 2.83 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967036524968972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967036524968972 | validation: 0.951483255967786]
	TIME [epoch: 2.83 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475616062931663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475616062931663 | validation: 0.8265897220950622]
	TIME [epoch: 2.83 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253439758896024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253439758896024 | validation: 0.9719082986459079]
	TIME [epoch: 2.83 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363854912808222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363854912808222 | validation: 0.810212224781613]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444605364116751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444605364116751 | validation: 1.0330295995785066]
	TIME [epoch: 2.83 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540649132620866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7540649132620866 | validation: 0.8201755163372696]
	TIME [epoch: 2.83 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628222724542326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7628222724542326 | validation: 0.9696679188811761]
	TIME [epoch: 2.82 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287220422796156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7287220422796156 | validation: 0.813219616609393]
	TIME [epoch: 2.83 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291581346542861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291581346542861 | validation: 0.9627561371656193]
	TIME [epoch: 2.83 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730332773750221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7730332773750221 | validation: 0.8924005133193555]
	TIME [epoch: 2.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172443039120768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172443039120768 | validation: 0.9705231893996706]
	TIME [epoch: 2.83 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982794737774285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982794737774285 | validation: 0.9082874588294573]
	TIME [epoch: 2.82 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274602180649018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274602180649018 | validation: 0.8155670083386032]
	TIME [epoch: 2.83 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133854506529059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7133854506529059 | validation: 0.8529177919571727]
	TIME [epoch: 2.83 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087683507238337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087683507238337 | validation: 0.8765463509957903]
	TIME [epoch: 2.82 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711073872657628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711073872657628 | validation: 0.8415732598324486]
	TIME [epoch: 2.83 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722799714693335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722799714693335 | validation: 0.9436693464981278]
	TIME [epoch: 2.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412288200333476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412288200333476 | validation: 0.9041090298281994]
	TIME [epoch: 2.83 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814798355502913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814798355502913 | validation: 0.9286720874229734]
	TIME [epoch: 2.83 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939193721291804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939193721291804 | validation: 0.964549732274584]
	TIME [epoch: 2.83 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693068851557433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7693068851557433 | validation: 0.7947101361610867]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207060496533104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7207060496533104 | validation: 0.9243817450895085]
	TIME [epoch: 2.83 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177365641166108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7177365641166108 | validation: 0.7848271940579181]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331194715086753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331194715086753 | validation: 0.9843896006212678]
	TIME [epoch: 2.83 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291936486903319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291936486903319 | validation: 0.7903730377439057]
	TIME [epoch: 2.83 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297779293158786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297779293158786 | validation: 0.8968333022471273]
	TIME [epoch: 2.82 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040255307574329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040255307574329 | validation: 0.8258098619683119]
	TIME [epoch: 2.82 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983600191692186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983600191692186 | validation: 0.8245451773349765]
	TIME [epoch: 2.82 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7090385610105802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7090385610105802 | validation: 0.9677478883427159]
	TIME [epoch: 2.83 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820759037036963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820759037036963 | validation: 0.9654734656569866]
	TIME [epoch: 2.83 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9246345248941539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9246345248941539 | validation: 0.9018262922256559]
	TIME [epoch: 2.83 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729821688349597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729821688349597 | validation: 0.8699122347846133]
	TIME [epoch: 2.83 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943706352509696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943706352509696 | validation: 0.8272440315866068]
	TIME [epoch: 2.83 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721643724946441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721643724946441 | validation: 0.8826424454529657]
	TIME [epoch: 2.83 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125727564471227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125727564471227 | validation: 0.8437378305935095]
	TIME [epoch: 2.83 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964001972059733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964001972059733 | validation: 0.8076119727416675]
	TIME [epoch: 2.83 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895423496401738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6895423496401738 | validation: 0.825782488651345]
	TIME [epoch: 2.83 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896390863324163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896390863324163 | validation: 0.7860042794139184]
	TIME [epoch: 2.82 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001166094665772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001166094665772 | validation: 0.9959993420699809]
	TIME [epoch: 2.82 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464503377026048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464503377026048 | validation: 0.8759121446924972]
	TIME [epoch: 2.83 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356047216137158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356047216137158 | validation: 0.8802540447076933]
	TIME [epoch: 2.83 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123918151953171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7123918151953171 | validation: 0.9841743755050617]
	TIME [epoch: 2.83 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246011703337353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246011703337353 | validation: 0.7910766519031976]
	TIME [epoch: 2.82 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384136987824735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8384136987824735 | validation: 0.9747101249444011]
	TIME [epoch: 2.82 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421694697095736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421694697095736 | validation: 0.8963070267890356]
	TIME [epoch: 2.82 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003367105088103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003367105088103 | validation: 0.7938721514469407]
	TIME [epoch: 2.83 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003690220130158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003690220130158 | validation: 0.8273032723349769]
	TIME [epoch: 2.83 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864695863880534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864695863880534 | validation: 0.8007384744254304]
	TIME [epoch: 2.83 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769641991235384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6769641991235384 | validation: 0.8070707103446157]
	TIME [epoch: 2.82 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817967972535215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817967972535215 | validation: 0.8324510373948151]
	TIME [epoch: 2.82 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923473508306391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923473508306391 | validation: 0.86918257705564]
	TIME [epoch: 2.83 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586695825020496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586695825020496 | validation: 1.05487840851527]
	TIME [epoch: 2.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9182013082055324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9182013082055324 | validation: 0.8470028913925399]
	TIME [epoch: 2.83 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433420503003686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433420503003686 | validation: 0.8262123152279767]
	TIME [epoch: 2.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810782252809381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810782252809381 | validation: 0.8426436531374666]
	TIME [epoch: 2.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809219373124887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809219373124887 | validation: 0.7898868803032282]
	TIME [epoch: 2.83 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924578512303677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6924578512303677 | validation: 0.8476888598466474]
	TIME [epoch: 2.83 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851891696605381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851891696605381 | validation: 0.81524746911583]
	TIME [epoch: 2.82 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797345749744461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6797345749744461 | validation: 0.8354025974023973]
	TIME [epoch: 2.82 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6845532261878627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6845532261878627 | validation: 0.8282617008845524]
	TIME [epoch: 2.82 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099901446220014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099901446220014 | validation: 0.8951266470414873]
	TIME [epoch: 2.82 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527358966096567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7527358966096567 | validation: 0.8810585933087381]
	TIME [epoch: 2.83 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580517392833468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580517392833468 | validation: 0.9058436101675174]
	TIME [epoch: 2.83 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186328717446397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186328717446397 | validation: 0.795222794996373]
	TIME [epoch: 2.83 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759092916964251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6759092916964251 | validation: 0.8404982816677695]
	TIME [epoch: 2.83 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664959213968786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664959213968786 | validation: 0.7673971243636406]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663805309998425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.663805309998425 | validation: 0.9352784221615646]
	TIME [epoch: 2.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6930249419764869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6930249419764869 | validation: 0.7936853154465464]
	TIME [epoch: 2.82 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962457972201047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962457972201047 | validation: 1.0130710457955268]
	TIME [epoch: 2.82 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7230434971340288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7230434971340288 | validation: 0.7772670472729261]
	TIME [epoch: 2.83 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721457712640883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721457712640883 | validation: 0.8321632405828403]
	TIME [epoch: 2.82 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910424608722319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910424608722319 | validation: 0.8932911573214334]
	TIME [epoch: 2.83 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546424309694105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546424309694105 | validation: 0.8953252916572396]
	TIME [epoch: 2.83 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7411586355959078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7411586355959078 | validation: 0.7721380257390555]
	TIME [epoch: 2.82 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915245035608335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915245035608335 | validation: 0.8278899059677474]
	TIME [epoch: 2.83 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587060326363797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6587060326363797 | validation: 0.7553797103346697]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528858841540666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528858841540666 | validation: 0.7658062940646222]
	TIME [epoch: 2.82 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469948608065511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6469948608065511 | validation: 0.7793831505153778]
	TIME [epoch: 2.82 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418362980754247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6418362980754247 | validation: 0.7198613482462992]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565010569848214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565010569848214 | validation: 0.8849308905170368]
	TIME [epoch: 2.81 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672612329386686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672612329386686 | validation: 0.7547422916249343]
	TIME [epoch: 2.81 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144938492447566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7144938492447566 | validation: 0.9924444399728838]
	TIME [epoch: 2.81 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135395731611103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7135395731611103 | validation: 0.8937911051845705]
	TIME [epoch: 2.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115022587749091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7115022587749091 | validation: 0.8112093317239553]
	TIME [epoch: 2.81 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684621520256969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8684621520256969 | validation: 0.9674973564830229]
	TIME [epoch: 2.81 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173991261946102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173991261946102 | validation: 0.791338597730467]
	TIME [epoch: 2.81 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64304478409479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.64304478409479 | validation: 0.7714999637251051]
	TIME [epoch: 2.81 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364395470656511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364395470656511 | validation: 0.8200784168171792]
	TIME [epoch: 2.81 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389412484403161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389412484403161 | validation: 0.7717326779235142]
	TIME [epoch: 2.81 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424497756486056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6424497756486056 | validation: 0.833452673650827]
	TIME [epoch: 2.82 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988059078316693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6988059078316693 | validation: 0.919412520651079]
	TIME [epoch: 2.81 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893471643233991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893471643233991 | validation: 0.8715495768927103]
	TIME [epoch: 2.81 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005509591919181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7005509591919181 | validation: 0.7626324725611884]
	TIME [epoch: 2.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346581092855095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346581092855095 | validation: 0.8507106340906753]
	TIME [epoch: 2.81 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370677503616782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370677503616782 | validation: 0.7301670670060867]
	TIME [epoch: 2.81 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254731213759386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254731213759386 | validation: 0.8299451941373175]
	TIME [epoch: 2.82 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329117449590517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6329117449590517 | validation: 0.7160594227527088]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352640574843648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6352640574843648 | validation: 0.865696548959026]
	TIME [epoch: 2.83 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629265305895787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6629265305895787 | validation: 0.8438165221263862]
	TIME [epoch: 2.83 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157283949708899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7157283949708899 | validation: 0.8326060514349148]
	TIME [epoch: 2.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787424400769742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787424400769742 | validation: 0.9346611274979569]
	TIME [epoch: 2.83 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906741658684439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6906741658684439 | validation: 0.7213599915077633]
	TIME [epoch: 2.83 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290149527505865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6290149527505865 | validation: 0.7603755016982511]
	TIME [epoch: 2.83 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249949148943817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6249949148943817 | validation: 0.8022866041424923]
	TIME [epoch: 2.83 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327540539569997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6327540539569997 | validation: 0.7037136811809579]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489616336835465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6489616336835465 | validation: 0.829637705877866]
	TIME [epoch: 2.82 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6693167254099046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6693167254099046 | validation: 0.7663102567036145]
	TIME [epoch: 2.81 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862778169743081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6862778169743081 | validation: 0.8047426989438682]
	TIME [epoch: 2.81 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505936397475045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6505936397475045 | validation: 0.8472773385293556]
	TIME [epoch: 2.81 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6220192404209554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6220192404209554 | validation: 0.7217333844852326]
	TIME [epoch: 2.81 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6039425498787594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6039425498787594 | validation: 0.7467984409980984]
	TIME [epoch: 2.81 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974077948307838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5974077948307838 | validation: 0.7336505194053604]
	TIME [epoch: 2.81 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140675002773694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6140675002773694 | validation: 0.8126065086816281]
	TIME [epoch: 2.81 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818345498592335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6818345498592335 | validation: 0.8739552584112001]
	TIME [epoch: 2.81 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7039854551655604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7039854551655604 | validation: 0.712452332990249]
	TIME [epoch: 2.81 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859349888577164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6859349888577164 | validation: 0.8487716583809704]
	TIME [epoch: 2.81 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6055627341838031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6055627341838031 | validation: 0.7072766281928067]
	TIME [epoch: 2.83 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786177743329826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786177743329826 | validation: 0.7009560058668147]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5733285582676866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5733285582676866 | validation: 0.7601410847011616]
	TIME [epoch: 2.82 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6072255666510975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6072255666510975 | validation: 0.7466629983772173]
	TIME [epoch: 2.82 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246320115839018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246320115839018 | validation: 0.834204815125845]
	TIME [epoch: 2.81 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276992886480549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6276992886480549 | validation: 0.7415068163938163]
	TIME [epoch: 2.81 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664765866867697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5664765866867697 | validation: 0.6491214635779539]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5481224802956388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5481224802956388 | validation: 0.8280920145047521]
	TIME [epoch: 2.84 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933100292177365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933100292177365 | validation: 0.6350694826718428]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6930198420542505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6930198420542505 | validation: 0.8238629750259485]
	TIME [epoch: 2.83 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5768372849803843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5768372849803843 | validation: 0.7040141457097326]
	TIME [epoch: 2.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367297872251743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5367297872251743 | validation: 0.6236887344484593]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5671921802948217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5671921802948217 | validation: 0.8261381165439876]
	TIME [epoch: 2.83 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441722772752482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441722772752482 | validation: 0.8267949328176956]
	TIME [epoch: 2.83 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207922860865492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7207922860865492 | validation: 0.7301714745954925]
	TIME [epoch: 2.82 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444152969483718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5444152969483718 | validation: 0.7152218699035721]
	TIME [epoch: 2.82 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157947133529003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5157947133529003 | validation: 0.6459777986934536]
	TIME [epoch: 2.82 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305090185938542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5305090185938542 | validation: 0.7608082864914834]
	TIME [epoch: 2.83 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61106143678796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61106143678796 | validation: 0.8365093376649642]
	TIME [epoch: 2.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190289125089712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190289125089712 | validation: 0.8065626250582574]
	TIME [epoch: 2.83 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373292930171065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5373292930171065 | validation: 0.7169771041836187]
	TIME [epoch: 2.83 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270471541830649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5270471541830649 | validation: 0.6427787935235226]
	TIME [epoch: 2.83 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500113995368491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.500113995368491 | validation: 0.6934158421302945]
	TIME [epoch: 2.83 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416984066054302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5416984066054302 | validation: 0.7117561955037801]
	TIME [epoch: 2.83 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305234980950003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6305234980950003 | validation: 0.7646054155821954]
	TIME [epoch: 2.83 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5205553068368645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5205553068368645 | validation: 0.5630350912073602]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4812428849233954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4812428849233954 | validation: 0.748621124372904]
	TIME [epoch: 2.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5043815325802791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5043815325802791 | validation: 0.5318560075671521]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069327709734884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6069327709734884 | validation: 0.7465883931859887]
	TIME [epoch: 2.83 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48204849069059846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48204849069059846 | validation: 0.5734441242306753]
	TIME [epoch: 2.83 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42665020416599425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42665020416599425 | validation: 0.5569722191272831]
	TIME [epoch: 2.82 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40628910248662553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40628910248662553 | validation: 0.634599181545707]
	TIME [epoch: 2.83 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40710442330212054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40710442330212054 | validation: 0.5195971008908261]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330923394339658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330923394339658 | validation: 0.7995775730859442]
	TIME [epoch: 2.82 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4667912086071408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4667912086071408 | validation: 0.48898821082878413]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45896479717915045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45896479717915045 | validation: 0.6501585911280641]
	TIME [epoch: 2.83 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3838066534570301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3838066534570301 | validation: 0.6520740710469547]
	TIME [epoch: 2.83 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226422591392448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6226422591392448 | validation: 0.8198647078095155]
	TIME [epoch: 2.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277482371990457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6277482371990457 | validation: 0.7503046674044701]
	TIME [epoch: 2.83 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49915666763775074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49915666763775074 | validation: 0.6127454364903276]
	TIME [epoch: 2.83 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687351854914722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687351854914722 | validation: 0.6078742340298469]
	TIME [epoch: 2.83 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4744164388373644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4744164388373644 | validation: 0.6804015201752528]
	TIME [epoch: 2.83 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5690168389734583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5690168389734583 | validation: 0.6948135158025401]
	TIME [epoch: 2.83 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948703197064255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3948703197064255 | validation: 0.5848483849611876]
	TIME [epoch: 2.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38741924980727066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38741924980727066 | validation: 0.47557340072424115]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4731385108638074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4731385108638074 | validation: 0.7616752727438725]
	TIME [epoch: 2.83 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4577553809768283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4577553809768283 | validation: 0.47184833159947703]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36462317784966514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36462317784966514 | validation: 0.5647351458847506]
	TIME [epoch: 2.83 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227711654563447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227711654563447 | validation: 0.47556450375010545]
	TIME [epoch: 2.83 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30987447098565746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30987447098565746 | validation: 0.5656495896318083]
	TIME [epoch: 2.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259035775245414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259035775245414 | validation: 0.6142623880120412]
	TIME [epoch: 2.82 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.410699787725778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.410699787725778 | validation: 0.5537740965768383]
	TIME [epoch: 2.82 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5549799676432379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5549799676432379 | validation: 0.6542814526547943]
	TIME [epoch: 2.83 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3357479383689359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3357479383689359 | validation: 0.5574704085879526]
	TIME [epoch: 2.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36086848257867216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36086848257867216 | validation: 0.6221377228307794]
	TIME [epoch: 2.82 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454898076568403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5454898076568403 | validation: 0.7420889163192675]
	TIME [epoch: 2.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718025191227781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3718025191227781 | validation: 0.46983913732598115]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886557909998245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3886557909998245 | validation: 0.6327429732963387]
	TIME [epoch: 2.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3745875236657389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3745875236657389 | validation: 0.5083725195022607]
	TIME [epoch: 2.83 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038091477698404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3038091477698404 | validation: 0.46210864546325975]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3318506049647884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3318506049647884 | validation: 0.7720126794950052]
	TIME [epoch: 2.82 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5393640501895867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5393640501895867 | validation: 0.475852599169641]
	TIME [epoch: 2.81 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35135700799942227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35135700799942227 | validation: 0.5980480180868663]
	TIME [epoch: 2.81 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143267157958965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143267157958965 | validation: 0.4754617779376192]
	TIME [epoch: 2.81 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32582585828799454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32582585828799454 | validation: 0.670059113269643]
	TIME [epoch: 2.81 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40572692371808755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40572692371808755 | validation: 0.4639321381298615]
	TIME [epoch: 2.81 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35478642344043054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35478642344043054 | validation: 0.628677386893653]
	TIME [epoch: 2.81 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3251765833383665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3251765833383665 | validation: 0.43429567653686196]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3125263179696222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3125263179696222 | validation: 0.6861256197631844]
	TIME [epoch: 2.83 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35452111903908473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35452111903908473 | validation: 0.4065211579519028]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35767551514013746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35767551514013746 | validation: 0.7047085492598981]
	TIME [epoch: 2.81 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34206908121455776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34206908121455776 | validation: 0.4289403515432693]
	TIME [epoch: 2.82 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34529817610507324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34529817610507324 | validation: 0.6458848000135107]
	TIME [epoch: 2.81 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961343449434243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961343449434243 | validation: 0.5418646592886505]
	TIME [epoch: 2.81 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3359338426243646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3359338426243646 | validation: 0.6499221287737751]
	TIME [epoch: 2.81 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5513674452021674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5513674452021674 | validation: 0.5268797501751178]
	TIME [epoch: 2.81 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27140881722793064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27140881722793064 | validation: 0.6521305087878388]
	TIME [epoch: 2.81 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4483528307945545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4483528307945545 | validation: 0.6685529421832572]
	TIME [epoch: 2.81 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4767609939949813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4767609939949813 | validation: 0.6864872390448999]
	TIME [epoch: 2.82 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3172105080890649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3172105080890649 | validation: 0.5335080656562877]
	TIME [epoch: 2.82 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3684246205712387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3684246205712387 | validation: 0.5266015284331642]
	TIME [epoch: 2.82 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3874635472148414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3874635472148414 | validation: 0.6920212244962571]
	TIME [epoch: 2.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.329462615986494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.329462615986494 | validation: 0.4278159202762458]
	TIME [epoch: 2.81 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2986877756381022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2986877756381022 | validation: 0.5271775298615488]
	TIME [epoch: 2.82 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26160704842299987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26160704842299987 | validation: 0.47375059398390207]
	TIME [epoch: 2.81 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23689623867305698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23689623867305698 | validation: 0.4633774965033114]
	TIME [epoch: 2.81 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2312392403993295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2312392403993295 | validation: 0.5487758715719918]
	TIME [epoch: 2.81 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2461744958895487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2461744958895487 | validation: 0.4562236268726033]
	TIME [epoch: 2.81 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45169047126427847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45169047126427847 | validation: 0.8233410907120247]
	TIME [epoch: 2.81 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.554498438055563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.554498438055563 | validation: 0.5965947719458345]
	TIME [epoch: 2.82 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24873319896275115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24873319896275115 | validation: 0.38407801289692267]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4719273408433331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4719273408433331 | validation: 0.6095228309361612]
	TIME [epoch: 2.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3343182451984009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3343182451984009 | validation: 0.7780147621940346]
	TIME [epoch: 2.83 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40153437189235947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40153437189235947 | validation: 0.5235900516848603]
	TIME [epoch: 2.83 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205674494883338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3205674494883338 | validation: 0.4667524965553]
	TIME [epoch: 2.83 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35527086334379504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35527086334379504 | validation: 0.6924335432778291]
	TIME [epoch: 2.82 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30486497860990897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30486497860990897 | validation: 0.4484149357030428]
	TIME [epoch: 2.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2344982329799979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2344982329799979 | validation: 0.4892418301230472]
	TIME [epoch: 2.83 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22726992107261204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22726992107261204 | validation: 0.5365815710119678]
	TIME [epoch: 2.83 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22829753169051104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22829753169051104 | validation: 0.5185587999816764]
	TIME [epoch: 2.83 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23632579025528577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23632579025528577 | validation: 0.5064211356306136]
	TIME [epoch: 2.83 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039845523607906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4039845523607906 | validation: 0.7243292372812395]
	TIME [epoch: 2.83 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487304093454962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.487304093454962 | validation: 0.6551188583782968]
	TIME [epoch: 2.83 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629708049888558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2629708049888558 | validation: 0.4125246068064372]
	TIME [epoch: 2.83 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3945476932184747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3945476932184747 | validation: 0.6029288450232596]
	TIME [epoch: 2.82 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31832060359070286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31832060359070286 | validation: 0.7237217550484989]
	TIME [epoch: 2.83 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500808285213263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3500808285213263 | validation: 0.5026859683746614]
	TIME [epoch: 2.82 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22571287668694026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22571287668694026 | validation: 0.4482371419642512]
	TIME [epoch: 2.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22572008237751995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22572008237751995 | validation: 0.5442241424462528]
	TIME [epoch: 2.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21847111521748885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21847111521748885 | validation: 0.5110016525111621]
	TIME [epoch: 2.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24277207397327963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24277207397327963 | validation: 0.5375156608756737]
	TIME [epoch: 2.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46329228967226477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46329228967226477 | validation: 0.5448303074464401]
	TIME [epoch: 2.83 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22464783850765419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22464783850765419 | validation: 0.4626161310520955]
	TIME [epoch: 2.83 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2051965774413472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2051965774413472 | validation: 0.5867259538888124]
	TIME [epoch: 2.83 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21490541726280785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21490541726280785 | validation: 0.6077035474959277]
	TIME [epoch: 2.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5356011419330927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5356011419330927 | validation: 0.5844697982626096]
	TIME [epoch: 2.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2892132940688713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2892132940688713 | validation: 0.7464219470273491]
	TIME [epoch: 2.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.545226412820277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.545226412820277 | validation: 0.47822309555182874]
	TIME [epoch: 2.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248584379519938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5248584379519938 | validation: 0.6187990760780145]
	TIME [epoch: 2.83 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5474982902356221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5474982902356221 | validation: 0.596126869838586]
	TIME [epoch: 2.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611979505538315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2611979505538315 | validation: 0.6267538232681736]
	TIME [epoch: 2.83 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4106063356151611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4106063356151611 | validation: 0.4796949785486785]
	TIME [epoch: 2.83 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.271106651964151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.271106651964151 | validation: 0.5438755505844524]
	TIME [epoch: 2.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23482650913266598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23482650913266598 | validation: 0.4945642861810283]
	TIME [epoch: 2.82 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22265604233676178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22265604233676178 | validation: 0.46260341625683116]
	TIME [epoch: 2.82 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219996044683055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.219996044683055 | validation: 0.507269503104436]
	TIME [epoch: 2.82 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20876074886919063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20876074886919063 | validation: 0.4576431910020753]
	TIME [epoch: 2.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2964177880701391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2964177880701391 | validation: 0.6881439403731391]
	TIME [epoch: 2.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3700160225608306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3700160225608306 | validation: 0.43654035812541325]
	TIME [epoch: 2.83 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2175640709602675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2175640709602675 | validation: 0.5210086210447165]
	TIME [epoch: 2.83 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19519664470910364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19519664470910364 | validation: 0.48980036857406106]
	TIME [epoch: 2.83 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1945432063713557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1945432063713557 | validation: 0.4565529877329002]
	TIME [epoch: 2.83 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273656699470501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2273656699470501 | validation: 0.7462568893273409]
	TIME [epoch: 2.83 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4807798890252596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4807798890252596 | validation: 0.6447464452845609]
	TIME [epoch: 2.83 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24818142131515813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24818142131515813 | validation: 0.4198506780828801]
	TIME [epoch: 2.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2977233187065589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2977233187065589 | validation: 0.765411408321908]
	TIME [epoch: 2.83 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41816762431724797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41816762431724797 | validation: 0.5363302945476357]
	TIME [epoch: 2.83 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19857547109607931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19857547109607931 | validation: 0.4537699429156809]
	TIME [epoch: 2.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3460387230239941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3460387230239941 | validation: 0.6323647554343705]
	TIME [epoch: 2.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30784832303730186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30784832303730186 | validation: 0.6634936978419929]
	TIME [epoch: 2.83 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24945948331145573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24945948331145573 | validation: 0.44137368866281385]
	TIME [epoch: 2.83 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26159637031899796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26159637031899796 | validation: 0.5950586094949877]
	TIME [epoch: 2.83 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26083613374469133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26083613374469133 | validation: 0.5776302715619094]
	TIME [epoch: 2.83 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23157030230142503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23157030230142503 | validation: 0.498389842511745]
	TIME [epoch: 2.83 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21560083233229754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21560083233229754 | validation: 0.5856912376196751]
	TIME [epoch: 2.83 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24341020029636934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24341020029636934 | validation: 0.5560725500507232]
	TIME [epoch: 2.83 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365678806646055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2365678806646055 | validation: 0.5180441625488685]
	TIME [epoch: 2.83 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2517003621463937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2517003621463937 | validation: 0.6478278983175021]
	TIME [epoch: 2.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27204883901269505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27204883901269505 | validation: 0.5910364969399787]
	TIME [epoch: 2.83 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38430961960391574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38430961960391574 | validation: 1.3726382785402147]
	TIME [epoch: 2.83 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4041947250141658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4041947250141658 | validation: 0.7130274912844832]
	TIME [epoch: 2.83 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4875651871840437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4875651871840437 | validation: 0.7486944263482834]
	TIME [epoch: 2.82 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36901557847966926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36901557847966926 | validation: 0.8074790103003183]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136902602091621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4136902602091621 | validation: 0.571414345349245]
	TIME [epoch: 6.08 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33601040115907027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33601040115907027 | validation: 0.4922961814595693]
	TIME [epoch: 6.06 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26616118371396974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26616118371396974 | validation: 0.5303887749761527]
	TIME [epoch: 6.06 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710613061627954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2710613061627954 | validation: 0.5336320013223375]
	TIME [epoch: 6.07 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21546961124763686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21546961124763686 | validation: 0.49523164069413084]
	TIME [epoch: 6.07 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22939118684168056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22939118684168056 | validation: 0.471575040932356]
	TIME [epoch: 6.07 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20259266614315669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20259266614315669 | validation: 0.5142627736799641]
	TIME [epoch: 6.06 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20709940787130812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20709940787130812 | validation: 0.46436386435015625]
	TIME [epoch: 6.06 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19366866125718457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19366866125718457 | validation: 0.5190023905428561]
	TIME [epoch: 6.06 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19276089636841715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19276089636841715 | validation: 0.48803674395393487]
	TIME [epoch: 6.07 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2044926055589511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2044926055589511 | validation: 0.5919573358027793]
	TIME [epoch: 6.05 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24436081915686467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24436081915686467 | validation: 0.4680521229962903]
	TIME [epoch: 6.06 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669203379942482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2669203379942482 | validation: 0.7216456704564388]
	TIME [epoch: 6.06 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3387022953063606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3387022953063606 | validation: 0.44879052214626025]
	TIME [epoch: 6.07 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18445584705488188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18445584705488188 | validation: 0.47367752114479117]
	TIME [epoch: 6.06 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1995643327809194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1995643327809194 | validation: 0.6789977499252582]
	TIME [epoch: 6.07 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914837905513713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2914837905513713 | validation: 0.4875441468900218]
	TIME [epoch: 6.06 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709488330485474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2709488330485474 | validation: 0.5956559584127555]
	TIME [epoch: 6.06 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21817571941450997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21817571941450997 | validation: 0.4930976551459712]
	TIME [epoch: 6.05 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30416681238779314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30416681238779314 | validation: 0.58764377442457]
	TIME [epoch: 6.06 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3128260197532766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3128260197532766 | validation: 0.7302060244632147]
	TIME [epoch: 6.06 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28442189019798053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28442189019798053 | validation: 0.4466160460825572]
	TIME [epoch: 6.07 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20735498352565762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20735498352565762 | validation: 0.4665855391620959]
	TIME [epoch: 6.06 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19960772147385863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19960772147385863 | validation: 0.5461751805426571]
	TIME [epoch: 6.07 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2253794243302151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2253794243302151 | validation: 0.5563705894845258]
	TIME [epoch: 6.07 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2494341378359487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2494341378359487 | validation: 0.5584469295275966]
	TIME [epoch: 6.07 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347598811671712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2347598811671712 | validation: 0.6301783695745959]
	TIME [epoch: 6.06 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499865546399615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6499865546399615 | validation: 0.6455496293807379]
	TIME [epoch: 6.06 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616968601259563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6616968601259563 | validation: 0.46786395500949696]
	TIME [epoch: 6.05 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193774891549694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3193774891549694 | validation: 0.6752561663782509]
	TIME [epoch: 6.07 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659202868656162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3659202868656162 | validation: 0.6187582446598929]
	TIME [epoch: 6.06 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25381801493193634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25381801493193634 | validation: 0.47947633732499606]
	TIME [epoch: 6.06 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25223650627353145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25223650627353145 | validation: 0.5836447917681243]
	TIME [epoch: 6.07 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266935873060527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266935873060527 | validation: 0.47026062157629034]
	TIME [epoch: 6.06 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19785869477287407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19785869477287407 | validation: 0.49078332857189955]
	TIME [epoch: 6.07 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20707322481344712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20707322481344712 | validation: 0.5454795818438263]
	TIME [epoch: 6.07 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20535041010293234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20535041010293234 | validation: 0.5221579023259285]
	TIME [epoch: 6.06 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18930146270417347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18930146270417347 | validation: 0.44999721784706614]
	TIME [epoch: 6.06 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25367521335417886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25367521335417886 | validation: 0.8190341812829683]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_191255/states/model_phi1_4b_v_mmd2_540.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1946.687 seconds.
