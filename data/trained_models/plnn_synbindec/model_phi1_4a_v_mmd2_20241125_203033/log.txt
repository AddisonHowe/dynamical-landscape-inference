Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3367584395

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.926009919753268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.926009919753268 | validation: 3.5187610598212617]
	TIME [epoch: 175 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.346452413992262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346452413992262 | validation: 3.8635845669087545]
	TIME [epoch: 0.812 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.102658526627206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.102658526627206 | validation: 2.524731390473424]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3090523343524145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3090523343524145 | validation: 2.560248870698077]
	TIME [epoch: 0.719 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.085741476246345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085741476246345 | validation: 3.1290112665359433]
	TIME [epoch: 0.716 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8959562385112076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8959562385112076 | validation: 2.831688478892909]
	TIME [epoch: 0.714 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.744163053419586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.744163053419586 | validation: 2.8341568923829104]
	TIME [epoch: 0.72 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.554546114512699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.554546114512699 | validation: 2.5252322183751015]
	TIME [epoch: 0.714 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3869847363253074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3869847363253074 | validation: 2.3425099933463183]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1860792978862817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1860792978862817 | validation: 2.250175309587695]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8795960437699324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8795960437699324 | validation: 1.4071205063353334]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1107639051548315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1107639051548315 | validation: 2.4487307615166793]
	TIME [epoch: 0.711 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9328811765424176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9328811765424176 | validation: 5.965317572795918]
	TIME [epoch: 0.762 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.809461085083253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.809461085083253 | validation: 3.6327436259887707]
	TIME [epoch: 0.718 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.663907036052107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.663907036052107 | validation: 2.5211648971166505]
	TIME [epoch: 0.713 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.706909214333698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.706909214333698 | validation: 1.059605202561803]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6253885608408352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6253885608408352 | validation: 1.0795890173725569]
	TIME [epoch: 0.713 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6900028995089724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6900028995089724 | validation: 1.0828452038436156]
	TIME [epoch: 0.709 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3707944079520067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3707944079520067 | validation: 1.3564401934559727]
	TIME [epoch: 0.71 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3928823943834947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3928823943834947 | validation: 1.0954909643870545]
	TIME [epoch: 0.711 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4779353627534138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4779353627534138 | validation: 0.8713358579166542]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2977487771474765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2977487771474765 | validation: 1.1204147182757775]
	TIME [epoch: 0.718 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2374758468174532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2374758468174532 | validation: 0.8805808504685713]
	TIME [epoch: 0.718 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969829042401678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1969829042401678 | validation: 0.9165828637858375]
	TIME [epoch: 0.713 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1739535732418236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1739535732418236 | validation: 1.0096332040131923]
	TIME [epoch: 0.712 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1661906634937729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1661906634937729 | validation: 0.7361067684767004]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1555151869441795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1555151869441795 | validation: 0.8222841331820746]
	TIME [epoch: 0.72 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1376967916032186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1376967916032186 | validation: 1.0172705567068283]
	TIME [epoch: 0.717 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1511251652658026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1511251652658026 | validation: 0.7292228983647597]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2193158828876687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2193158828876687 | validation: 0.9073829986062028]
	TIME [epoch: 0.719 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677611962243607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2677611962243607 | validation: 1.1795805290194956]
	TIME [epoch: 0.717 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.291123757593683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.291123757593683 | validation: 0.657332140342499]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1188405001382529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1188405001382529 | validation: 0.8130579588780936]
	TIME [epoch: 0.724 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057057555142694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.057057555142694 | validation: 0.9446828843581051]
	TIME [epoch: 0.719 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630171651059626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0630171651059626 | validation: 0.6128627487098716]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985553548575742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0985553548575742 | validation: 1.104946643178806]
	TIME [epoch: 0.718 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0949077360759136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0949077360759136 | validation: 0.5584548375982509]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1600703596858761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1600703596858761 | validation: 1.2264245936370501]
	TIME [epoch: 0.719 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1244399718578124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1244399718578124 | validation: 0.5880831212446384]
	TIME [epoch: 0.715 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2040917331928447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2040917331928447 | validation: 0.8332809099173851]
	TIME [epoch: 0.714 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0951869272700356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0951869272700356 | validation: 1.1453246960466892]
	TIME [epoch: 0.713 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130697144330865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.130697144330865 | validation: 0.6505806663305131]
	TIME [epoch: 0.714 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0483075098565988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0483075098565988 | validation: 0.6606513718331097]
	TIME [epoch: 0.715 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010978274260891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010978274260891 | validation: 0.969523612199279]
	TIME [epoch: 0.716 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121760046684634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0121760046684634 | validation: 0.5806879160118174]
	TIME [epoch: 0.716 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022308013945389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.022308013945389 | validation: 0.800279058416439]
	TIME [epoch: 0.717 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9582751870178592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9582751870178592 | validation: 0.90213951735136]
	TIME [epoch: 0.717 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.973190565715384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.973190565715384 | validation: 0.6578789595430079]
	TIME [epoch: 0.715 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0280982815251622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0280982815251622 | validation: 1.1267271109321935]
	TIME [epoch: 0.713 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079550998824189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.079550998824189 | validation: 0.650493366895544]
	TIME [epoch: 0.715 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0167146904928586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0167146904928586 | validation: 0.7695475088156387]
	TIME [epoch: 0.714 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580602376394203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580602376394203 | validation: 0.8708637275720877]
	TIME [epoch: 0.713 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9401443526324994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9401443526324994 | validation: 0.5560474215405957]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0051148702952037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0051148702952037 | validation: 1.1080330992320624]
	TIME [epoch: 0.717 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0345441854923751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0345441854923751 | validation: 0.5686556353885436]
	TIME [epoch: 0.716 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1960880343333815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1960880343333815 | validation: 0.9928678919562952]
	TIME [epoch: 0.715 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9625045774986738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9625045774986738 | validation: 0.8398149985715598]
	TIME [epoch: 0.719 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782237916536314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9782237916536314 | validation: 0.6753488364301798]
	TIME [epoch: 0.718 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862604286536036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9862604286536036 | validation: 0.7157481032224249]
	TIME [epoch: 0.717 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9076620512663494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9076620512663494 | validation: 0.9077694350524217]
	TIME [epoch: 0.717 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9119637311059967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9119637311059967 | validation: 0.5797744829795669]
	TIME [epoch: 0.716 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412993923648116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9412993923648116 | validation: 0.9682535353575372]
	TIME [epoch: 0.718 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380213020401768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9380213020401768 | validation: 0.6770735018896248]
	TIME [epoch: 0.716 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9588475393371291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9588475393371291 | validation: 0.9582694060832246]
	TIME [epoch: 0.716 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9825429919727608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9825429919727608 | validation: 0.6885549350226339]
	TIME [epoch: 0.716 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342880801342082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9342880801342082 | validation: 0.8259794371496894]
	TIME [epoch: 0.716 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163397597897288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9163397597897288 | validation: 0.697797345940947]
	TIME [epoch: 0.716 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963990135158266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8963990135158266 | validation: 0.8239123679808128]
	TIME [epoch: 0.716 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89493509301275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.89493509301275 | validation: 0.6629900693147109]
	TIME [epoch: 0.715 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9024064758373624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9024064758373624 | validation: 0.9512171199491377]
	TIME [epoch: 0.716 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287654854227688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9287654854227688 | validation: 0.6144431510436082]
	TIME [epoch: 0.715 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.953209236322208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.953209236322208 | validation: 1.0397869918267335]
	TIME [epoch: 0.716 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614395657698355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9614395657698355 | validation: 0.5311230980332381]
	TIME [epoch: 0.721 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9897109149711796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9897109149711796 | validation: 0.9790550961939772]
	TIME [epoch: 0.717 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705743163377544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9705743163377544 | validation: 0.6457335427988031]
	TIME [epoch: 0.713 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9787525454745568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9787525454745568 | validation: 0.7065245378290066]
	TIME [epoch: 0.713 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830101827188189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8830101827188189 | validation: 0.9179120901977291]
	TIME [epoch: 0.713 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944302345154952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944302345154952 | validation: 0.6206070210926768]
	TIME [epoch: 0.712 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926439689308324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926439689308324 | validation: 0.8311039594990621]
	TIME [epoch: 0.712 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8715269621911831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8715269621911831 | validation: 0.6696441119319249]
	TIME [epoch: 0.713 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547322267713088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547322267713088 | validation: 0.8172382610426528]
	TIME [epoch: 0.712 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737092478834302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737092478834302 | validation: 0.7514129299436941]
	TIME [epoch: 0.712 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224688784121569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224688784121569 | validation: 1.0558927617499636]
	TIME [epoch: 0.714 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.06517276184482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.06517276184482 | validation: 0.6789062131642676]
	TIME [epoch: 0.718 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817798838589149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8817798838589149 | validation: 0.7138227312339259]
	TIME [epoch: 0.716 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545929395977617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545929395977617 | validation: 0.7642929086703837]
	TIME [epoch: 0.713 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509880020483034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8509880020483034 | validation: 0.5841649864978975]
	TIME [epoch: 0.712 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875821405959755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875821405959755 | validation: 1.0734271287564037]
	TIME [epoch: 0.714 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0082732681313455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0082732681313455 | validation: 0.5854226033190972]
	TIME [epoch: 0.715 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1592221288232893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1592221288232893 | validation: 0.9087512897760948]
	TIME [epoch: 0.713 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911521055661803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911521055661803 | validation: 0.9237509325179624]
	TIME [epoch: 0.713 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0066407839054845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0066407839054845 | validation: 0.6795079298303716]
	TIME [epoch: 0.713 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9383344094777115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9383344094777115 | validation: 0.6707343646744854]
	TIME [epoch: 0.713 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475176483258642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8475176483258642 | validation: 0.825451255916801]
	TIME [epoch: 0.712 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825644175218547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8825644175218547 | validation: 0.6619718311376351]
	TIME [epoch: 0.712 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872973251239651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872973251239651 | validation: 0.6905070211693296]
	TIME [epoch: 0.713 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367033971713186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8367033971713186 | validation: 0.8525964298046435]
	TIME [epoch: 0.715 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591407326934183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591407326934183 | validation: 0.6477211456880414]
	TIME [epoch: 0.718 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.903200147230374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.903200147230374 | validation: 1.1795803254584016]
	TIME [epoch: 0.717 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006442890806601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006442890806601 | validation: 0.6461529425058545]
	TIME [epoch: 0.718 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9071533652440742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9071533652440742 | validation: 0.8452761210186099]
	TIME [epoch: 0.718 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810489084780716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8810489084780716 | validation: 0.6773926728331148]
	TIME [epoch: 0.715 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486854791209646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8486854791209646 | validation: 0.648503589711085]
	TIME [epoch: 0.717 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850060525311275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850060525311275 | validation: 0.7893672986017953]
	TIME [epoch: 0.718 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645177381103752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645177381103752 | validation: 0.6436859651244594]
	TIME [epoch: 0.716 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265252401787456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9265252401787456 | validation: 0.8295170053089364]
	TIME [epoch: 0.717 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956072948163458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956072948163458 | validation: 0.6956972566452528]
	TIME [epoch: 0.716 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899992756581317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.899992756581317 | validation: 0.7124923062033661]
	TIME [epoch: 0.716 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661983129078114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661983129078114 | validation: 1.1905109250666701]
	TIME [epoch: 0.716 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.008768345925169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.008768345925169 | validation: 0.6033741144467415]
	TIME [epoch: 0.719 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9513730923755988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9513730923755988 | validation: 0.9559901618486042]
	TIME [epoch: 0.719 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900030325864638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8900030325864638 | validation: 0.6060966395713293]
	TIME [epoch: 0.717 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8366262156612245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8366262156612245 | validation: 0.692788418550875]
	TIME [epoch: 0.716 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822827014471286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822827014471286 | validation: 0.7643098855565793]
	TIME [epoch: 0.716 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362620455096439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8362620455096439 | validation: 0.6863537360838845]
	TIME [epoch: 0.714 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631372293075328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8631372293075328 | validation: 0.9347956839759082]
	TIME [epoch: 0.712 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9654594361885279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9654594361885279 | validation: 0.7681989168334751]
	TIME [epoch: 0.712 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226543669035963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9226543669035963 | validation: 0.7231160380193569]
	TIME [epoch: 0.714 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.933885486517897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.933885486517897 | validation: 0.7580025614373764]
	TIME [epoch: 0.714 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442022814644605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8442022814644605 | validation: 0.6180624771278471]
	TIME [epoch: 0.712 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8307527355739979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8307527355739979 | validation: 0.7584426467907319]
	TIME [epoch: 0.712 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251326881989822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8251326881989822 | validation: 0.6191225950054817]
	TIME [epoch: 0.715 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448349005140281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448349005140281 | validation: 1.1244574389905773]
	TIME [epoch: 0.715 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9648068696085065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9648068696085065 | validation: 0.6025964032990261]
	TIME [epoch: 0.715 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01712032578122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01712032578122 | validation: 0.9657032830488739]
	TIME [epoch: 0.716 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032807454439812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032807454439812 | validation: 0.6213492501287396]
	TIME [epoch: 0.716 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110723496211537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110723496211537 | validation: 0.6577990678978289]
	TIME [epoch: 0.715 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8563092685263933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8563092685263933 | validation: 0.8800184349722612]
	TIME [epoch: 0.713 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412791812625863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9412791812625863 | validation: 0.8274817084151156]
	TIME [epoch: 0.714 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419231753846182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9419231753846182 | validation: 0.6395406971749824]
	TIME [epoch: 0.719 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424739519214609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9424739519214609 | validation: 0.6416979362488167]
	TIME [epoch: 0.717 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810261146650687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810261146650687 | validation: 0.7498294669387318]
	TIME [epoch: 0.716 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205847711677565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8205847711677565 | validation: 0.5856735755519209]
	TIME [epoch: 0.717 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738334872648722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738334872648722 | validation: 0.7931034669404012]
	TIME [epoch: 0.715 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453596342685319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453596342685319 | validation: 0.5761123862813172]
	TIME [epoch: 0.715 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415547137591677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415547137591677 | validation: 0.7998579728918481]
	TIME [epoch: 0.714 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212091257202642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8212091257202642 | validation: 0.6159923357222749]
	TIME [epoch: 0.717 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8351853300281357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8351853300281357 | validation: 1.0376210281776206]
	TIME [epoch: 0.717 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9376651769230187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9376651769230187 | validation: 0.7777993217322288]
	TIME [epoch: 0.716 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9814836351109159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9814836351109159 | validation: 0.9731488535247759]
	TIME [epoch: 0.716 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0343100112666985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0343100112666985 | validation: 0.7378917114405592]
	TIME [epoch: 0.72 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269696511466704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269696511466704 | validation: 0.5919152778739333]
	TIME [epoch: 0.717 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759805190240093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759805190240093 | validation: 0.9006270227334204]
	TIME [epoch: 0.716 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747392341843417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747392341843417 | validation: 0.6404248721841707]
	TIME [epoch: 0.716 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134760840411076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134760840411076 | validation: 0.5683638838938492]
	TIME [epoch: 0.716 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262224215389609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262224215389609 | validation: 0.8231812427908516]
	TIME [epoch: 0.716 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562176931538775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8562176931538775 | validation: 0.5638140560159988]
	TIME [epoch: 0.716 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876004455820366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876004455820366 | validation: 0.7742072615696662]
	TIME [epoch: 0.716 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353369536903634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353369536903634 | validation: 0.638390424276972]
	TIME [epoch: 0.717 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202127357098148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202127357098148 | validation: 0.7122985389813914]
	TIME [epoch: 0.716 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327321564404433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327321564404433 | validation: 0.9686942595412558]
	TIME [epoch: 0.714 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705989057918997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9705989057918997 | validation: 0.7426291125778105]
	TIME [epoch: 0.717 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9106901042610167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9106901042610167 | validation: 0.9092383348047737]
	TIME [epoch: 0.716 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095471472039856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9095471472039856 | validation: 0.6161964085408453]
	TIME [epoch: 0.715 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954925695254041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954925695254041 | validation: 0.5932490249249623]
	TIME [epoch: 0.716 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929227086890788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7929227086890788 | validation: 0.7730855925924977]
	TIME [epoch: 0.717 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102310337670631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102310337670631 | validation: 0.5847764647078705]
	TIME [epoch: 0.719 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289134702096143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289134702096143 | validation: 0.8815798728271868]
	TIME [epoch: 0.716 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487201586826849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8487201586826849 | validation: 0.5568836005854206]
	TIME [epoch: 0.716 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.847446000587925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.847446000587925 | validation: 0.7926674705514444]
	TIME [epoch: 0.717 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473317405314953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473317405314953 | validation: 0.5990076343946845]
	TIME [epoch: 0.716 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891972958470281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891972958470281 | validation: 0.7660231908994218]
	TIME [epoch: 0.714 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029602751196443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9029602751196443 | validation: 1.0790822234672148]
	TIME [epoch: 0.717 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030297602301144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030297602301144 | validation: 0.641718241103407]
	TIME [epoch: 0.717 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030544294586157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8030544294586157 | validation: 0.6130787188267413]
	TIME [epoch: 0.714 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7750925727148459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7750925727148459 | validation: 0.6811687089460379]
	TIME [epoch: 0.716 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881622891673372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881622891673372 | validation: 0.6536846206326082]
	TIME [epoch: 0.716 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966212739505211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966212739505211 | validation: 0.7948937717045726]
	TIME [epoch: 0.717 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592921919324121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592921919324121 | validation: 0.7623442443819458]
	TIME [epoch: 0.717 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366154911609303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9366154911609303 | validation: 0.9901758231864055]
	TIME [epoch: 0.716 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0071067077357192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0071067077357192 | validation: 0.6410018983388626]
	TIME [epoch: 0.716 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813243503882578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7813243503882578 | validation: 0.6115613785232666]
	TIME [epoch: 0.718 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705414442488214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705414442488214 | validation: 0.6483940270752017]
	TIME [epoch: 0.715 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044751932711108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044751932711108 | validation: 0.7167489229119252]
	TIME [epoch: 0.718 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334455709338967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334455709338967 | validation: 0.698901179165421]
	TIME [epoch: 0.721 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958407917024168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958407917024168 | validation: 0.7007809029064042]
	TIME [epoch: 0.719 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331816215037873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8331816215037873 | validation: 0.7420702019133221]
	TIME [epoch: 0.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241293813522259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241293813522259 | validation: 0.653194886466715]
	TIME [epoch: 0.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204050988191339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204050988191339 | validation: 0.913073802146247]
	TIME [epoch: 0.719 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580515794433066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580515794433066 | validation: 0.6037252103524587]
	TIME [epoch: 0.721 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262374545043761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262374545043761 | validation: 0.7768118111784524]
	TIME [epoch: 0.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064292785001151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8064292785001151 | validation: 0.6019265334246797]
	TIME [epoch: 0.719 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926760230869442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7926760230869442 | validation: 0.7150259733653522]
	TIME [epoch: 0.719 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966342039256341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966342039256341 | validation: 0.7045501538598629]
	TIME [epoch: 0.719 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84344636771098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84344636771098 | validation: 0.7799853743698137]
	TIME [epoch: 0.719 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0327753877279227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0327753877279227 | validation: 0.714709855222305]
	TIME [epoch: 0.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112429918493501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112429918493501 | validation: 0.6931005381653074]
	TIME [epoch: 0.721 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870041892795133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7870041892795133 | validation: 0.5934660577121375]
	TIME [epoch: 0.719 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738690713101588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738690713101588 | validation: 0.7597239954175925]
	TIME [epoch: 0.717 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898528656351285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898528656351285 | validation: 0.6231376058533766]
	TIME [epoch: 0.721 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240998370876672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240998370876672 | validation: 0.9546159686817153]
	TIME [epoch: 0.722 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871251110856819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871251110856819 | validation: 0.6136622625597407]
	TIME [epoch: 0.72 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001434584788557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8001434584788557 | validation: 0.6971373340281405]
	TIME [epoch: 0.719 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8141150069202872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8141150069202872 | validation: 0.7580966295750308]
	TIME [epoch: 0.719 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031583852367558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9031583852367558 | validation: 0.6778816773065961]
	TIME [epoch: 0.72 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9352788939566969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9352788939566969 | validation: 0.6620556201973307]
	TIME [epoch: 0.719 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765823497538213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765823497538213 | validation: 0.7496576473535361]
	TIME [epoch: 0.717 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855330907744253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855330907744253 | validation: 0.5500182559500115]
	TIME [epoch: 0.72 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020381032726596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8020381032726596 | validation: 0.7941554972265258]
	TIME [epoch: 0.719 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982865321638286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982865321638286 | validation: 0.5838015169850068]
	TIME [epoch: 0.718 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742854082832559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742854082832559 | validation: 0.6846890836259915]
	TIME [epoch: 184 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625003495423783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7625003495423783 | validation: 0.6840360235778983]
	TIME [epoch: 1.45 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8226665063657725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8226665063657725 | validation: 1.0310056483919177]
	TIME [epoch: 1.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038682076500335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.038682076500335 | validation: 0.7223154890748128]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767946975017438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767946975017438 | validation: 0.6703005730062493]
	TIME [epoch: 1.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021814862910783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021814862910783 | validation: 0.6356793899015839]
	TIME [epoch: 1.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543140901237467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543140901237467 | validation: 0.6369107967188132]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517433776500011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517433776500011 | validation: 0.6168701648416158]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507544177665482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507544177665482 | validation: 0.6351817978033314]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531137942690874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531137942690874 | validation: 0.6412760760347531]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796037204854367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.796037204854367 | validation: 0.7727153484854371]
	TIME [epoch: 1.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133660446787145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133660446787145 | validation: 1.1229939275190144]
	TIME [epoch: 1.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0656332023465576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0656332023465576 | validation: 0.5812004673539989]
	TIME [epoch: 1.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545012052665733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545012052665733 | validation: 0.6348275071203285]
	TIME [epoch: 1.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418107275470982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7418107275470982 | validation: 0.6583605353380855]
	TIME [epoch: 1.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627218299793594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7627218299793594 | validation: 0.6413218193311397]
	TIME [epoch: 1.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813733776271721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7813733776271721 | validation: 0.8325402122398526]
	TIME [epoch: 1.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390709698540302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8390709698540302 | validation: 0.6558306680181434]
	TIME [epoch: 1.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168278692325722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168278692325722 | validation: 0.7609976139042101]
	TIME [epoch: 1.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103977359147875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103977359147875 | validation: 0.641585143457864]
	TIME [epoch: 1.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775706984414137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775706984414137 | validation: 0.6247491468913321]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800739986239761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800739986239761 | validation: 0.6625480597002731]
	TIME [epoch: 1.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749820490495748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7749820490495748 | validation: 0.6448199861080302]
	TIME [epoch: 1.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876562538593246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876562538593246 | validation: 0.6694240267178259]
	TIME [epoch: 1.4 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999938504930766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999938504930766 | validation: 0.8006648926698553]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427494835161826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427494835161826 | validation: 0.6752589919346024]
	TIME [epoch: 1.4 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151934882107169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151934882107169 | validation: 0.8443496825929628]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061469637083897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061469637083897 | validation: 0.5879933109232179]
	TIME [epoch: 1.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476907432102319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476907432102319 | validation: 0.6576481304443063]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216098068680785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216098068680785 | validation: 0.5776542807054924]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196562219093514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196562219093514 | validation: 0.6896491838278056]
	TIME [epoch: 1.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484128766980372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484128766980372 | validation: 0.7160174038367125]
	TIME [epoch: 1.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489890804700576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8489890804700576 | validation: 0.810015030967329]
	TIME [epoch: 1.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9586069493165552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9586069493165552 | validation: 0.6149863266540798]
	TIME [epoch: 1.4 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542840786561609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542840786561609 | validation: 0.6483959894720206]
	TIME [epoch: 1.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321945994486845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7321945994486845 | validation: 0.5713232564278536]
	TIME [epoch: 1.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170098194935562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7170098194935562 | validation: 0.6519514605268206]
	TIME [epoch: 1.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187976212282519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7187976212282519 | validation: 0.6769779339231583]
	TIME [epoch: 1.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652773610561203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652773610561203 | validation: 0.7296553363663314]
	TIME [epoch: 1.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629824759337094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8629824759337094 | validation: 0.9795648180623815]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268345330185139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9268345330185139 | validation: 0.5495453442635346]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320704728991839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7320704728991839 | validation: 0.5657864582503942]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715054314571544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715054314571544 | validation: 0.6798479951642489]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750252949289024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750252949289024 | validation: 0.6311812293181205]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719646917715244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719646917715244 | validation: 0.7905387054618362]
	TIME [epoch: 1.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812466238728351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812466238728351 | validation: 0.634006666334323]
	TIME [epoch: 1.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183078344357411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183078344357411 | validation: 0.6634269618186119]
	TIME [epoch: 1.39 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710643583613715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710643583613715 | validation: 0.5728206685341016]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119534939412856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119534939412856 | validation: 0.5743648513828888]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956325428009604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956325428009604 | validation: 0.6017053805900673]
	TIME [epoch: 1.39 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6857616932609885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6857616932609885 | validation: 0.5663690879036378]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879913537483913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879913537483913 | validation: 0.689060945221582]
	TIME [epoch: 1.39 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079005094601553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079005094601553 | validation: 0.5645489057854404]
	TIME [epoch: 1.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281623299063176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281623299063176 | validation: 0.8787276117420966]
	TIME [epoch: 1.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131646148701646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8131646148701646 | validation: 0.700222976007014]
	TIME [epoch: 1.4 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556594837144124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556594837144124 | validation: 0.6646523472125461]
	TIME [epoch: 1.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046349603399547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9046349603399547 | validation: 0.6252369172545582]
	TIME [epoch: 1.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254174230783729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254174230783729 | validation: 0.5443170710769033]
	TIME [epoch: 1.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975032295725712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6975032295725712 | validation: 0.6995294541682167]
	TIME [epoch: 1.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367279842862703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7367279842862703 | validation: 0.6911632881772818]
	TIME [epoch: 1.4 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8258284770431104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8258284770431104 | validation: 0.7086882112871171]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211820982120065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211820982120065 | validation: 0.5494675842539424]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085121313104304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085121313104304 | validation: 0.5831671498486676]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117364439635779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117364439635779 | validation: 0.6016685114999236]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293382193234084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7293382193234084 | validation: 0.6395347326242575]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7408145901886838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7408145901886838 | validation: 0.753251620944714]
	TIME [epoch: 1.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742838783773107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742838783773107 | validation: 0.6105698209003022]
	TIME [epoch: 1.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7733187516007827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7733187516007827 | validation: 0.7082391641828203]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412378040807741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412378040807741 | validation: 0.5280721090725689]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945788771623282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945788771623282 | validation: 0.6362502474883203]
	TIME [epoch: 1.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840828220097805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6840828220097805 | validation: 0.5774240716775265]
	TIME [epoch: 1.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790667069088852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790667069088852 | validation: 0.5765675626016441]
	TIME [epoch: 1.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669622688359467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669622688359467 | validation: 0.5903056379088298]
	TIME [epoch: 1.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758722401337635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6758722401337635 | validation: 0.6553882882492745]
	TIME [epoch: 1.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324582567238034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324582567238034 | validation: 0.7101608422817265]
	TIME [epoch: 1.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8600173264266832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8600173264266832 | validation: 0.769664452632865]
	TIME [epoch: 1.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180069404897292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8180069404897292 | validation: 0.5131373488441812]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792962250618636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792962250618636 | validation: 0.59005795236136]
	TIME [epoch: 1.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758491821900989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6758491821900989 | validation: 0.6235807724344884]
	TIME [epoch: 1.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687017544065425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687017544065425 | validation: 0.6019821286785912]
	TIME [epoch: 1.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524169379769302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524169379769302 | validation: 0.7118318432217029]
	TIME [epoch: 1.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660902558851106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660902558851106 | validation: 0.5305553400337715]
	TIME [epoch: 1.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968591500399341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968591500399341 | validation: 0.649647740228011]
	TIME [epoch: 1.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850890430065321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6850890430065321 | validation: 0.5754806716977416]
	TIME [epoch: 1.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6907816998149469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6907816998149469 | validation: 0.5993457886320837]
	TIME [epoch: 1.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718738321324011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718738321324011 | validation: 0.6212896953106446]
	TIME [epoch: 1.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709082989582463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.709082989582463 | validation: 0.6649232801143786]
	TIME [epoch: 1.4 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705001854725432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.705001854725432 | validation: 0.5523336967223095]
	TIME [epoch: 1.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087517910282444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087517910282444 | validation: 0.6692761493346645]
	TIME [epoch: 1.4 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921472652466178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6921472652466178 | validation: 0.520539065107908]
	TIME [epoch: 1.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579397979368106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579397979368106 | validation: 0.6270694581089146]
	TIME [epoch: 1.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624674945192989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6624674945192989 | validation: 0.5285849109716415]
	TIME [epoch: 1.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675645952343178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.675645952343178 | validation: 0.6036086728093564]
	TIME [epoch: 1.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69339245712696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69339245712696 | validation: 0.5870992370156785]
	TIME [epoch: 1.41 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917365073417261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917365073417261 | validation: 0.6812970328568722]
	TIME [epoch: 1.41 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186070238522442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186070238522442 | validation: 0.5563610650798952]
	TIME [epoch: 1.41 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365905559886896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365905559886896 | validation: 0.7273470703952106]
	TIME [epoch: 1.41 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268389721912766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268389721912766 | validation: 0.49603960138287484]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6511224104703149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6511224104703149 | validation: 0.6201679350730455]
	TIME [epoch: 1.41 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435247764775504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6435247764775504 | validation: 0.5403722918219118]
	TIME [epoch: 1.41 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293769928391372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293769928391372 | validation: 0.5360848840842979]
	TIME [epoch: 1.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514248935621363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6514248935621363 | validation: 0.5780782548618463]
	TIME [epoch: 1.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679515082928137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679515082928137 | validation: 0.5786079351640389]
	TIME [epoch: 1.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955648717885805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955648717885805 | validation: 0.6546036453747884]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952814216715811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6952814216715811 | validation: 0.5303298126086219]
	TIME [epoch: 1.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858503248017866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6858503248017866 | validation: 0.5914767422199043]
	TIME [epoch: 1.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407156445950486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6407156445950486 | validation: 0.5036169490881724]
	TIME [epoch: 1.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112150428107905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112150428107905 | validation: 0.5244867098351634]
	TIME [epoch: 1.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143439079421095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6143439079421095 | validation: 0.49663568366825867]
	TIME [epoch: 1.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178067953394525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6178067953394525 | validation: 0.5462685926543557]
	TIME [epoch: 1.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627105225830489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.627105225830489 | validation: 0.5659919143563935]
	TIME [epoch: 1.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639552024512665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639552024512665 | validation: 0.5388521292231625]
	TIME [epoch: 1.4 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237628944837488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7237628944837488 | validation: 0.6908969324600713]
	TIME [epoch: 1.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684641065952056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684641065952056 | validation: 0.5293502584217993]
	TIME [epoch: 1.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287242609968968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6287242609968968 | validation: 0.45911030712991646]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652710633416667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652710633416667 | validation: 0.6595210079367236]
	TIME [epoch: 1.41 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639326374451291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639326374451291 | validation: 0.4980634524672654]
	TIME [epoch: 1.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969059095482167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5969059095482167 | validation: 0.4762218518786524]
	TIME [epoch: 1.41 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5999254057375071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5999254057375071 | validation: 0.4805762930725228]
	TIME [epoch: 1.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933731571049172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933731571049172 | validation: 0.59050200793257]
	TIME [epoch: 1.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615633722217233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.615633722217233 | validation: 0.48183378640829]
	TIME [epoch: 1.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647459313905847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647459313905847 | validation: 0.5328976966352243]
	TIME [epoch: 1.4 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5924264065231557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5924264065231557 | validation: 0.5162063300812998]
	TIME [epoch: 1.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855195061878978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5855195061878978 | validation: 0.39854784566557205]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5831046038829932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5831046038829932 | validation: 0.5947942213339669]
	TIME [epoch: 1.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871346214436658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5871346214436658 | validation: 0.4558289708762024]
	TIME [epoch: 1.41 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6040049815293239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6040049815293239 | validation: 0.5248813627006909]
	TIME [epoch: 1.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205355374596327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205355374596327 | validation: 0.5354347751816313]
	TIME [epoch: 1.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285935095421562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6285935095421562 | validation: 0.4851725241029101]
	TIME [epoch: 1.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5754836483699415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5754836483699415 | validation: 0.4385422533023564]
	TIME [epoch: 1.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555927177275004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5555927177275004 | validation: 0.5254414489257645]
	TIME [epoch: 1.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442082816455914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5442082816455914 | validation: 0.41463930909315017]
	TIME [epoch: 1.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5910992329230785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5910992329230785 | validation: 0.5012045906717636]
	TIME [epoch: 1.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5505271409165838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5505271409165838 | validation: 0.45269550751792836]
	TIME [epoch: 1.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.565540710103723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.565540710103723 | validation: 0.5182061437944567]
	TIME [epoch: 1.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.583262969468382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.583262969468382 | validation: 0.5099169772095702]
	TIME [epoch: 1.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218100281916747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218100281916747 | validation: 0.4124140471753024]
	TIME [epoch: 1.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139405739955258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139405739955258 | validation: 0.5911570372480366]
	TIME [epoch: 1.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5648229026542501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5648229026542501 | validation: 0.45462048891727913]
	TIME [epoch: 1.41 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964066391877926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964066391877926 | validation: 0.5126202104587896]
	TIME [epoch: 1.41 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147398035079743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147398035079743 | validation: 0.5466984038036563]
	TIME [epoch: 1.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440070787384234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5440070787384234 | validation: 0.47776564181290304]
	TIME [epoch: 1.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837020968263807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837020968263807 | validation: 0.510595532247596]
	TIME [epoch: 1.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5590058210171956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590058210171956 | validation: 0.46988948180825035]
	TIME [epoch: 1.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5126429399446265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5126429399446265 | validation: 0.41272532324687683]
	TIME [epoch: 1.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4998818972806498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4998818972806498 | validation: 0.4634571469758753]
	TIME [epoch: 1.4 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49284052301868136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49284052301868136 | validation: 0.44015380486578093]
	TIME [epoch: 1.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49576055343455494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49576055343455494 | validation: 0.4469271215858253]
	TIME [epoch: 1.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279459207697272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5279459207697272 | validation: 0.6093594924823604]
	TIME [epoch: 1.4 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.57677012807263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.57677012807263 | validation: 0.4259365910211697]
	TIME [epoch: 1.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103654864517418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7103654864517418 | validation: 0.5777843088493793]
	TIME [epoch: 1.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6091374737158713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6091374737158713 | validation: 0.5056703852034926]
	TIME [epoch: 1.4 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248273215979866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5248273215979866 | validation: 0.43671953268864283]
	TIME [epoch: 1.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813111448647386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4813111448647386 | validation: 0.4644736309505631]
	TIME [epoch: 1.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44621773655605446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44621773655605446 | validation: 0.3960890105935828]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4384797183555091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4384797183555091 | validation: 0.43035407924407454]
	TIME [epoch: 1.41 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43848994892280374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43848994892280374 | validation: 0.41799273795669356]
	TIME [epoch: 1.41 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47583938116969443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47583938116969443 | validation: 0.511873647879065]
	TIME [epoch: 1.41 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219132065957217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6219132065957217 | validation: 0.48234607809805263]
	TIME [epoch: 1.41 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46566414968263764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46566414968263764 | validation: 0.36278464017981943]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47195002542129394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47195002542129394 | validation: 0.7165185298013097]
	TIME [epoch: 1.41 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5576278867737524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5576278867737524 | validation: 0.43268961862087885]
	TIME [epoch: 1.41 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812262892904011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812262892904011 | validation: 0.44823626841537423]
	TIME [epoch: 1.41 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506015955770147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5506015955770147 | validation: 0.521697907294437]
	TIME [epoch: 1.41 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4808627419206524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4808627419206524 | validation: 0.4369359808016398]
	TIME [epoch: 1.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45568878863941065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45568878863941065 | validation: 0.44658368555053507]
	TIME [epoch: 1.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383702551552465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4383702551552465 | validation: 0.4595612006620342]
	TIME [epoch: 1.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4229473850478401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4229473850478401 | validation: 0.3821665449899741]
	TIME [epoch: 1.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41940601543718997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41940601543718997 | validation: 0.45340612377242134]
	TIME [epoch: 1.41 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39878299480324425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39878299480324425 | validation: 0.4504167549837459]
	TIME [epoch: 1.41 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40431841604891555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40431841604891555 | validation: 0.45785068603959406]
	TIME [epoch: 1.41 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5776119694057315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5776119694057315 | validation: 0.5166235656760686]
	TIME [epoch: 1.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40809064363852915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40809064363852915 | validation: 0.34496232850361735]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4275856412121556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4275856412121556 | validation: 0.6812713035134271]
	TIME [epoch: 1.4 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5563929160373051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5563929160373051 | validation: 0.5228027862736029]
	TIME [epoch: 1.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712421739435583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712421739435583 | validation: 0.5328012656414596]
	TIME [epoch: 1.41 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658172226335485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5658172226335485 | validation: 0.4775461664874618]
	TIME [epoch: 1.41 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4816446511492089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4816446511492089 | validation: 0.4826178003659921]
	TIME [epoch: 1.41 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41927708354243776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41927708354243776 | validation: 0.4206741869301784]
	TIME [epoch: 1.41 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3785610201724343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3785610201724343 | validation: 0.41148144718342916]
	TIME [epoch: 1.41 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36690477520923026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36690477520923026 | validation: 0.42079332112054324]
	TIME [epoch: 1.41 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35394386787136867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35394386787136867 | validation: 0.4026946392922754]
	TIME [epoch: 1.41 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415584109618659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3415584109618659 | validation: 0.3957383576586275]
	TIME [epoch: 1.41 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38084983557650914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38084983557650914 | validation: 0.5349569927644463]
	TIME [epoch: 1.41 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5285872742148604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5285872742148604 | validation: 0.518876153249377]
	TIME [epoch: 1.41 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222749034485955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222749034485955 | validation: 0.645927201680872]
	TIME [epoch: 1.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5809165201932518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5809165201932518 | validation: 0.8128894913789595]
	TIME [epoch: 1.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660337833235542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660337833235542 | validation: 0.32219933827839187]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4705356299399236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4705356299399236 | validation: 0.46470686087865154]
	TIME [epoch: 1.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608899445964327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4608899445964327 | validation: 0.5393137127829825]
	TIME [epoch: 1.39 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3846380610333401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3846380610333401 | validation: 0.4291453247845768]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3543686402460922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3543686402460922 | validation: 0.40198974851129776]
	TIME [epoch: 1.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534160818104978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3534160818104978 | validation: 0.42956911172125]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3305201843237289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3305201843237289 | validation: 0.3935997136313668]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30903488626537334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30903488626537334 | validation: 0.39790278596321615]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018035530473495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3018035530473495 | validation: 0.3300763590594608]
	TIME [epoch: 1.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935077778323097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2935077778323097 | validation: 0.5844181379132625]
	TIME [epoch: 1.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.455006908264706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.455006908264706 | validation: 0.6509433105892595]
	TIME [epoch: 1.41 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8751945910550347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8751945910550347 | validation: 0.823471986160487]
	TIME [epoch: 1.41 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381407504215238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381407504215238 | validation: 0.5567822855838207]
	TIME [epoch: 1.41 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723254105782476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723254105782476 | validation: 0.5107961725234896]
	TIME [epoch: 1.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709087322743109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4709087322743109 | validation: 0.4928558850471918]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3561293496216426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3561293496216426 | validation: 0.3792975492209173]
	TIME [epoch: 1.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32698508210883964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32698508210883964 | validation: 0.41657118880649247]
	TIME [epoch: 1.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187445517118463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3187445517118463 | validation: 0.3915577043704789]
	TIME [epoch: 1.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989887604946033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2989887604946033 | validation: 0.43667724727632024]
	TIME [epoch: 1.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193195940422918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3193195940422918 | validation: 0.39822861366551626]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447548684371541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5447548684371541 | validation: 0.5042012811799749]
	TIME [epoch: 1.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44415481933444173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44415481933444173 | validation: 0.38704516523030785]
	TIME [epoch: 1.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590194741673653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3590194741673653 | validation: 0.6756243639740738]
	TIME [epoch: 1.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5078114934178349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5078114934178349 | validation: 0.4097549771294432]
	TIME [epoch: 1.41 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4860238304975089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4860238304975089 | validation: 0.3982393457400073]
	TIME [epoch: 1.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2941194848156891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2941194848156891 | validation: 0.5003470500765802]
	TIME [epoch: 1.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38808431007564975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38808431007564975 | validation: 0.4046305760496841]
	TIME [epoch: 1.4 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39347384798184837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39347384798184837 | validation: 0.3650612831302249]
	TIME [epoch: 1.4 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31738670845410577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31738670845410577 | validation: 0.5037950199539447]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32619584788147876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32619584788147876 | validation: 0.35292694288377496]
	TIME [epoch: 1.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3090083778666314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3090083778666314 | validation: 0.3957902008244887]
	TIME [epoch: 1.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3455325361012471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3455325361012471 | validation: 0.4522188631639503]
	TIME [epoch: 1.41 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41825748817871344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41825748817871344 | validation: 0.4963930508321819]
	TIME [epoch: 1.41 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41113018432353415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41113018432353415 | validation: 0.34740745646491944]
	TIME [epoch: 1.4 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48358892008123394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48358892008123394 | validation: 0.36324275115702154]
	TIME [epoch: 1.41 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26138506180729243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26138506180729243 | validation: 0.5794768557258284]
	TIME [epoch: 1.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44320687408205756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44320687408205756 | validation: 0.3810273486238337]
	TIME [epoch: 1.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45524221402703957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45524221402703957 | validation: 0.34793376285193434]
	TIME [epoch: 1.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184413157255256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3184413157255256 | validation: 0.5130829546243378]
	TIME [epoch: 1.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35584257472832476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35584257472832476 | validation: 0.3976660792098652]
	TIME [epoch: 1.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197962907587016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3197962907587016 | validation: 0.35312330300544603]
	TIME [epoch: 1.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437086043452451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3437086043452451 | validation: 0.4051434381512593]
	TIME [epoch: 1.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2805879234934356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2805879234934356 | validation: 0.37960884741560597]
	TIME [epoch: 1.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803556951066698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2803556951066698 | validation: 0.37705594366258993]
	TIME [epoch: 1.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3150718508331847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3150718508331847 | validation: 0.3951834881968927]
	TIME [epoch: 1.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30600411340946526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30600411340946526 | validation: 0.3756142649148869]
	TIME [epoch: 1.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38527702707697303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38527702707697303 | validation: 0.45185965065818795]
	TIME [epoch: 1.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3159878558627112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3159878558627112 | validation: 0.32853605378886486]
	TIME [epoch: 1.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3787227668714243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3787227668714243 | validation: 0.4724223039339916]
	TIME [epoch: 1.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830881496720016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830881496720016 | validation: 0.2709294235318793]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22865217800325924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22865217800325924 | validation: 0.36248551658814976]
	TIME [epoch: 1.41 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21507220270343883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21507220270343883 | validation: 0.3183403176554691]
	TIME [epoch: 1.41 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23789378371900538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23789378371900538 | validation: 0.3642707162078138]
	TIME [epoch: 1.41 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4266803656422386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4266803656422386 | validation: 0.41462035310520506]
	TIME [epoch: 1.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35937378237830764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35937378237830764 | validation: 0.38250765633132505]
	TIME [epoch: 1.41 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39364969158227536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39364969158227536 | validation: 0.41794759405011406]
	TIME [epoch: 1.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26204646522120095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26204646522120095 | validation: 0.24550153607932024]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2838343556825907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2838343556825907 | validation: 0.442429901349376]
	TIME [epoch: 1.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25934276500180575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25934276500180575 | validation: 0.25939449635551676]
	TIME [epoch: 1.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502459141369275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502459141369275 | validation: 0.37813637149596935]
	TIME [epoch: 1.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26670121401058183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26670121401058183 | validation: 0.3309436798311284]
	TIME [epoch: 1.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3582711064462473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3582711064462473 | validation: 0.5674111589384245]
	TIME [epoch: 1.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49459187646089303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49459187646089303 | validation: 0.3061339806760943]
	TIME [epoch: 1.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20907894113235054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20907894113235054 | validation: 0.3088485588871834]
	TIME [epoch: 1.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21429323441678272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21429323441678272 | validation: 0.36343529044796374]
	TIME [epoch: 1.41 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22852551653484418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22852551653484418 | validation: 0.3238393884149187]
	TIME [epoch: 1.41 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2453361811714435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2453361811714435 | validation: 0.2811282728982893]
	TIME [epoch: 1.41 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20726895752507596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20726895752507596 | validation: 0.3564880515410104]
	TIME [epoch: 1.41 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24878270964939453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24878270964939453 | validation: 0.32922414084689333]
	TIME [epoch: 1.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37868593346376345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37868593346376345 | validation: 0.34615517004218904]
	TIME [epoch: 1.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3424204537107173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3424204537107173 | validation: 0.4549211884558407]
	TIME [epoch: 1.41 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109194589117226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3109194589117226 | validation: 0.2617179336201801]
	TIME [epoch: 1.41 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33615662367321414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33615662367321414 | validation: 0.518924733406224]
	TIME [epoch: 1.41 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823472908270696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823472908270696 | validation: 0.33668612014228017]
	TIME [epoch: 1.41 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4205847279734556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4205847279734556 | validation: 0.31361348080312657]
	TIME [epoch: 1.4 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3319923350679504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3319923350679504 | validation: 0.3438539493504662]
	TIME [epoch: 1.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20845498612839486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20845498612839486 | validation: 0.369632820426113]
	TIME [epoch: 1.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25351950196612616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25351950196612616 | validation: 0.30565682500388913]
	TIME [epoch: 1.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22105461027900902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22105461027900902 | validation: 0.3081203354983011]
	TIME [epoch: 1.41 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19939576024785105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19939576024785105 | validation: 0.239319900863954]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18845171320746779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18845171320746779 | validation: 0.4637590645055569]
	TIME [epoch: 1.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27259297854525466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27259297854525466 | validation: 0.24946780542077016]
	TIME [epoch: 1.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27091269094688697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27091269094688697 | validation: 0.3243752576838521]
	TIME [epoch: 1.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25682769819916057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25682769819916057 | validation: 0.32550897753077884]
	TIME [epoch: 1.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24462875637111603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24462875637111603 | validation: 0.31676620886573564]
	TIME [epoch: 1.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24120505993881783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24120505993881783 | validation: 0.24881584270390011]
	TIME [epoch: 1.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20993274081207516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20993274081207516 | validation: 0.35296659053877755]
	TIME [epoch: 1.41 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20562736808002993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20562736808002993 | validation: 0.22901095733610918]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2779223371658841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2779223371658841 | validation: 0.41226003585591986]
	TIME [epoch: 1.41 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618989684940288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2618989684940288 | validation: 0.22330297839425306]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18664886184653653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18664886184653653 | validation: 0.22043490734904939]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19023608339193615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19023608339193615 | validation: 0.3153722170661114]
	TIME [epoch: 1.41 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519204290563837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2519204290563837 | validation: 0.3758881336698574]
	TIME [epoch: 1.41 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3293833756192986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3293833756192986 | validation: 0.22926366251610386]
	TIME [epoch: 1.41 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3235014530207792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3235014530207792 | validation: 0.33548500386032437]
	TIME [epoch: 1.41 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19285864734394464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19285864734394464 | validation: 0.22554390658074738]
	TIME [epoch: 1.41 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15296561767486935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15296561767486935 | validation: 0.23951147416769125]
	TIME [epoch: 1.41 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14092114042612497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14092114042612497 | validation: 0.3227010278517956]
	TIME [epoch: 1.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18760146563971777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18760146563971777 | validation: 0.21576703091784852]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27605242102024746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27605242102024746 | validation: 0.6783254162652782]
	TIME [epoch: 1.4 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5968398770639282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5968398770639282 | validation: 0.3209265066902955]
	TIME [epoch: 1.4 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2631458882718757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2631458882718757 | validation: 0.2609960062821422]
	TIME [epoch: 1.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30030752865779675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30030752865779675 | validation: 0.33960904830103855]
	TIME [epoch: 1.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18292840574717364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18292840574717364 | validation: 0.24101189514111915]
	TIME [epoch: 1.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418854484986958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418854484986958 | validation: 0.20501485431801122]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15751985587061576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15751985587061576 | validation: 0.23502563825889228]
	TIME [epoch: 1.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13721399527315528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13721399527315528 | validation: 0.2107879075584064]
	TIME [epoch: 1.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18330437175270134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18330437175270134 | validation: 0.35962255588207653]
	TIME [epoch: 1.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37256363378120155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37256363378120155 | validation: 0.26144776352887267]
	TIME [epoch: 1.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2399359326574989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2399359326574989 | validation: 0.27509923644396184]
	TIME [epoch: 1.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037185682361018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2037185682361018 | validation: 0.3309237403205084]
	TIME [epoch: 1.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40319898229261797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40319898229261797 | validation: 0.2558892536583116]
	TIME [epoch: 1.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18838580097194843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18838580097194843 | validation: 0.3617329150249686]
	TIME [epoch: 1.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26614033553736066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26614033553736066 | validation: 0.23348571275812172]
	TIME [epoch: 190 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22627152666325237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22627152666325237 | validation: 0.20838645389188115]
	TIME [epoch: 2.77 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14034996120420742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14034996120420742 | validation: 0.24739081808191]
	TIME [epoch: 2.77 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321197623358241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1321197623358241 | validation: 0.19134105422802092]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13121591796561113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13121591796561113 | validation: 0.24413277923122909]
	TIME [epoch: 2.77 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728182358235552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728182358235552 | validation: 0.3551522243404412]
	TIME [epoch: 2.77 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23820281305718688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23820281305718688 | validation: 0.2945208607626012]
	TIME [epoch: 2.77 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21042382882112484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21042382882112484 | validation: 0.16843692331614257]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17727452020158285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17727452020158285 | validation: 0.22686134738210728]
	TIME [epoch: 2.77 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20006581730803696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20006581730803696 | validation: 0.6169547746774825]
	TIME [epoch: 2.77 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898979464284555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898979464284555 | validation: 0.2763079607032581]
	TIME [epoch: 2.77 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432208259790916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432208259790916 | validation: 0.2750807187018303]
	TIME [epoch: 2.77 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14938188691878854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14938188691878854 | validation: 0.22066775245351106]
	TIME [epoch: 2.77 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15626125695428075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15626125695428075 | validation: 0.21159944256961724]
	TIME [epoch: 2.77 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2400680078917627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2400680078917627 | validation: 0.2994448130860988]
	TIME [epoch: 2.77 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19516365168723807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19516365168723807 | validation: 0.2244370971437819]
	TIME [epoch: 2.77 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20480817314295954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20480817314295954 | validation: 0.24972426673865566]
	TIME [epoch: 2.77 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18622266811387872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18622266811387872 | validation: 0.22542441622374243]
	TIME [epoch: 2.77 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21690145447868125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21690145447868125 | validation: 0.1952284909535131]
	TIME [epoch: 2.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15140019750521064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15140019750521064 | validation: 0.17431718404646268]
	TIME [epoch: 2.77 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12172981320411215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12172981320411215 | validation: 0.22405215154434802]
	TIME [epoch: 2.77 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760249711234617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10760249711234617 | validation: 0.13954670524554222]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144865490659959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1144865490659959 | validation: 0.34974526642759485]
	TIME [epoch: 2.75 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18833244765245064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18833244765245064 | validation: 0.17621669520371852]
	TIME [epoch: 2.76 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2679216788689442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2679216788689442 | validation: 0.2956616009268899]
	TIME [epoch: 2.76 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23303240560460428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23303240560460428 | validation: 0.2872464486985969]
	TIME [epoch: 2.77 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21872055679712724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21872055679712724 | validation: 0.2776937156996031]
	TIME [epoch: 2.76 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21004201757783297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21004201757783297 | validation: 0.2000603334131927]
	TIME [epoch: 2.77 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18054014215774195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18054014215774195 | validation: 0.2596542088304485]
	TIME [epoch: 2.77 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155188890675226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155188890675226 | validation: 0.16471227187271198]
	TIME [epoch: 2.77 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15408757701055317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15408757701055317 | validation: 0.25308801416065857]
	TIME [epoch: 2.77 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14919552131551028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14919552131551028 | validation: 0.17167839228783008]
	TIME [epoch: 2.77 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714681637343206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1714681637343206 | validation: 0.23809200710051465]
	TIME [epoch: 2.77 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19973569482958464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19973569482958464 | validation: 0.21457367139126507]
	TIME [epoch: 2.77 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19431169606104512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19431169606104512 | validation: 0.20927273945566918]
	TIME [epoch: 2.77 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20302071497067345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20302071497067345 | validation: 0.4195444829897394]
	TIME [epoch: 2.77 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30840867445229797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30840867445229797 | validation: 0.23577006671303408]
	TIME [epoch: 2.77 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23535591053120655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23535591053120655 | validation: 0.2816394086589326]
	TIME [epoch: 2.78 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18296372426361587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18296372426361587 | validation: 0.17948181587144946]
	TIME [epoch: 2.78 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14127582480510045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14127582480510045 | validation: 0.3218891145974218]
	TIME [epoch: 2.77 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921369460038232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1921369460038232 | validation: 0.16208569357151914]
	TIME [epoch: 2.78 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1602763760214416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1602763760214416 | validation: 0.16010648488977008]
	TIME [epoch: 2.77 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11108229446722621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11108229446722621 | validation: 0.19971790839612358]
	TIME [epoch: 2.82 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11687519531090956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11687519531090956 | validation: 0.16010515139810894]
	TIME [epoch: 2.77 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10923418253160742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10923418253160742 | validation: 0.21565966627030467]
	TIME [epoch: 2.77 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094901535911963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12094901535911963 | validation: 0.1960155203958609]
	TIME [epoch: 2.77 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18213663975902858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18213663975902858 | validation: 0.29969982581332916]
	TIME [epoch: 2.78 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651661327685507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2651661327685507 | validation: 0.2720416527252548]
	TIME [epoch: 2.78 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961310131883087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961310131883087 | validation: 0.22411844852657686]
	TIME [epoch: 2.78 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15844554077363085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15844554077363085 | validation: 0.14602575961758996]
	TIME [epoch: 2.78 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851912169313652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10851912169313652 | validation: 0.18583675723518736]
	TIME [epoch: 2.78 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10504567054929527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10504567054929527 | validation: 0.1597119109005766]
	TIME [epoch: 2.77 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14148898509139507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14148898509139507 | validation: 0.19532807183672465]
	TIME [epoch: 2.77 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15204511637767013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15204511637767013 | validation: 0.25250845603444766]
	TIME [epoch: 2.77 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21755087632717307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21755087632717307 | validation: 0.1358640026979496]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18246036940662472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18246036940662472 | validation: 0.3259530394417909]
	TIME [epoch: 2.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23738211590768907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23738211590768907 | validation: 0.35107109668940106]
	TIME [epoch: 2.77 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37625594827816844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37625594827816844 | validation: 0.2534758365651195]
	TIME [epoch: 2.77 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615310092566034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615310092566034 | validation: 0.19530397454929527]
	TIME [epoch: 2.76 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13775567043205098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13775567043205098 | validation: 0.1576234290383599]
	TIME [epoch: 2.77 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767839918915677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09767839918915677 | validation: 0.12618322210384583]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720255485514395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07720255485514395 | validation: 0.1610914162193112]
	TIME [epoch: 2.78 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08370562184106199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08370562184106199 | validation: 0.14208558236636834]
	TIME [epoch: 2.77 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142867064512707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08142867064512707 | validation: 0.11512064037478598]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879355627768201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879355627768201 | validation: 0.16179412262491]
	TIME [epoch: 2.77 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210780798623347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11210780798623347 | validation: 0.2500185742401541]
	TIME [epoch: 2.78 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38526942197066333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38526942197066333 | validation: 0.5578106128002818]
	TIME [epoch: 2.78 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47000629890007717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47000629890007717 | validation: 0.1603513237625314]
	TIME [epoch: 2.78 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841848840758582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15841848840758582 | validation: 0.20757199324197254]
	TIME [epoch: 2.78 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14228940400808174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14228940400808174 | validation: 0.1576001673623083]
	TIME [epoch: 2.77 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12552879524658975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12552879524658975 | validation: 0.17546116556703084]
	TIME [epoch: 2.78 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17868058704038586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17868058704038586 | validation: 0.23030199524295192]
	TIME [epoch: 2.78 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23379539988529888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23379539988529888 | validation: 0.2449627670520938]
	TIME [epoch: 2.77 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21397660425577386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21397660425577386 | validation: 0.11154548094707689]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10398865559462309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10398865559462309 | validation: 0.1880826459543895]
	TIME [epoch: 2.76 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08899017050431052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08899017050431052 | validation: 0.12610190790643486]
	TIME [epoch: 2.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08305842119186094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08305842119186094 | validation: 0.11643046509568951]
	TIME [epoch: 2.76 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364876067522498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07364876067522498 | validation: 0.14751423785819873]
	TIME [epoch: 2.76 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499720754744722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10499720754744722 | validation: 0.14025769570733812]
	TIME [epoch: 2.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16554096329902104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16554096329902104 | validation: 0.3630599154159934]
	TIME [epoch: 2.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28882401789462087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28882401789462087 | validation: 0.2911712484270173]
	TIME [epoch: 2.76 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815958029046883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2815958029046883 | validation: 0.29824222303407144]
	TIME [epoch: 2.76 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208071893737101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2208071893737101 | validation: 0.13448211496671578]
	TIME [epoch: 2.77 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11336621003874196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11336621003874196 | validation: 0.20692770442699893]
	TIME [epoch: 2.76 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10748389722308002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10748389722308002 | validation: 0.13591885520357147]
	TIME [epoch: 2.78 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08965592163378279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08965592163378279 | validation: 0.1269388993072246]
	TIME [epoch: 2.77 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092225122245424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08092225122245424 | validation: 0.1363357186571683]
	TIME [epoch: 2.77 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10384484782361936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10384484782361936 | validation: 0.1566250897973619]
	TIME [epoch: 2.77 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16468334950436855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16468334950436855 | validation: 0.3691721395599707]
	TIME [epoch: 2.78 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43758666278674113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43758666278674113 | validation: 0.10546275906680894]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374899955437496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1374899955437496 | validation: 0.1992078480400058]
	TIME [epoch: 2.76 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26542739038059593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26542739038059593 | validation: 0.19905748078503302]
	TIME [epoch: 2.77 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428940318332674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428940318332674 | validation: 0.296179269771524]
	TIME [epoch: 2.78 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1569684473180379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1569684473180379 | validation: 0.15207625163943939]
	TIME [epoch: 2.78 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09032442548755233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09032442548755233 | validation: 0.12668437937399424]
	TIME [epoch: 2.78 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0914816454954884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0914816454954884 | validation: 0.22714111890271688]
	TIME [epoch: 2.77 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15556695772903767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15556695772903767 | validation: 0.1950185046421268]
	TIME [epoch: 2.77 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24600949754656445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24600949754656445 | validation: 0.4428343553358099]
	TIME [epoch: 2.77 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32382600030330255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32382600030330255 | validation: 0.18983466813708616]
	TIME [epoch: 2.78 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13069929736717917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13069929736717917 | validation: 0.14020299689397778]
	TIME [epoch: 2.78 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735861108962721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735861108962721 | validation: 0.15183149169657023]
	TIME [epoch: 2.77 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1185161478925276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1185161478925276 | validation: 0.15927741091029574]
	TIME [epoch: 2.77 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14811399948095003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14811399948095003 | validation: 0.23431724160020773]
	TIME [epoch: 2.77 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19290101841407187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19290101841407187 | validation: 0.12638379352654808]
	TIME [epoch: 2.77 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12160425604047052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12160425604047052 | validation: 0.17861054490409495]
	TIME [epoch: 2.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955233766322258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0955233766322258 | validation: 0.12066081276011396]
	TIME [epoch: 2.77 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1174376799728984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1174376799728984 | validation: 0.24046462519912098]
	TIME [epoch: 2.76 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15663034946991547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15663034946991547 | validation: 0.23641209742459904]
	TIME [epoch: 2.76 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20734855213315542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20734855213315542 | validation: 0.30105206332257983]
	TIME [epoch: 2.77 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2291501102703268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2291501102703268 | validation: 0.126834905926213]
	TIME [epoch: 2.77 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13706780984627462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13706780984627462 | validation: 0.16619700198512227]
	TIME [epoch: 2.77 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09612115370885028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09612115370885028 | validation: 0.1344005814914301]
	TIME [epoch: 2.76 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090004807616358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.090004807616358 | validation: 0.1395937462232674]
	TIME [epoch: 2.77 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785530205719653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09785530205719653 | validation: 0.15202835387323016]
	TIME [epoch: 2.76 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11685458540711675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11685458540711675 | validation: 0.20461913916855942]
	TIME [epoch: 2.76 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923625806823803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1923625806823803 | validation: 0.16391550135594246]
	TIME [epoch: 2.77 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22533425847426763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22533425847426763 | validation: 0.17590306961560997]
	TIME [epoch: 2.76 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545946097411278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10545946097411278 | validation: 0.10652332386884572]
	TIME [epoch: 2.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288842084275614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07288842084275614 | validation: 0.10349536267993016]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06751414200616955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06751414200616955 | validation: 0.15686269609739462]
	TIME [epoch: 2.78 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11451262626774801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11451262626774801 | validation: 0.32026572072067006]
	TIME [epoch: 2.77 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22411217288540194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22411217288540194 | validation: 0.21794356571425244]
	TIME [epoch: 2.78 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30279344898387656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30279344898387656 | validation: 0.31433498807646054]
	TIME [epoch: 2.77 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18589570192588725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18589570192588725 | validation: 0.08448838386812528]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638461253533503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08638461253533503 | validation: 0.09359491665116312]
	TIME [epoch: 2.78 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05600150823619001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05600150823619001 | validation: 0.12363169363874552]
	TIME [epoch: 2.77 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662851308210703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662851308210703 | validation: 0.09598171212847117]
	TIME [epoch: 2.78 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09043714074146576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09043714074146576 | validation: 0.1443965388442581]
	TIME [epoch: 2.77 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903916696405595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11903916696405595 | validation: 0.14050136393119225]
	TIME [epoch: 2.77 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12997521156776906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12997521156776906 | validation: 0.18858691538642938]
	TIME [epoch: 2.78 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687373502361795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687373502361795 | validation: 0.29460091086154067]
	TIME [epoch: 2.78 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26380196042963944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26380196042963944 | validation: 0.2216634403161562]
	TIME [epoch: 2.76 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20836423196617745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20836423196617745 | validation: 0.223987051012418]
	TIME [epoch: 2.76 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17091905313061595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17091905313061595 | validation: 0.11133015248576027]
	TIME [epoch: 2.76 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151899644196335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09151899644196335 | validation: 0.1569776690232012]
	TIME [epoch: 2.76 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09239974433284678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09239974433284678 | validation: 0.16473416904507443]
	TIME [epoch: 2.76 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18848389440948254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18848389440948254 | validation: 0.28634258602342116]
	TIME [epoch: 2.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20712143665836835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20712143665836835 | validation: 0.1285372379359285]
	TIME [epoch: 2.76 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537442620138958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10537442620138958 | validation: 0.13569800788353936]
	TIME [epoch: 2.75 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555950789388221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08555950789388221 | validation: 0.10725961268685805]
	TIME [epoch: 2.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08317594368800492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08317594368800492 | validation: 0.13511755356962307]
	TIME [epoch: 2.76 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08931430805694966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08931430805694966 | validation: 0.11360287874672746]
	TIME [epoch: 2.76 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08647815500008303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08647815500008303 | validation: 0.16043888180090865]
	TIME [epoch: 2.75 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1109873763221842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1109873763221842 | validation: 0.12555347342920947]
	TIME [epoch: 2.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648751862601707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1648751862601707 | validation: 0.2557562555213863]
	TIME [epoch: 2.76 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18540899493362376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18540899493362376 | validation: 0.10329077190966154]
	TIME [epoch: 2.76 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11098212921571982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11098212921571982 | validation: 0.1632952068269385]
	TIME [epoch: 2.76 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671094007678329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08671094007678329 | validation: 0.11095922533622932]
	TIME [epoch: 2.77 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10752921702833718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10752921702833718 | validation: 0.19979590095501226]
	TIME [epoch: 2.76 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15086423833298437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15086423833298437 | validation: 0.2302699137575949]
	TIME [epoch: 2.76 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2147163152308495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2147163152308495 | validation: 0.20335535961296866]
	TIME [epoch: 2.75 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17992036538584016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17992036538584016 | validation: 0.12345290995023112]
	TIME [epoch: 2.77 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09560297251298895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09560297251298895 | validation: 0.10613617265339748]
	TIME [epoch: 2.76 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962922370381277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0962922370381277 | validation: 0.19541314718353764]
	TIME [epoch: 2.76 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13323474521267162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13323474521267162 | validation: 0.13164394818344882]
	TIME [epoch: 2.76 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688928833425356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688928833425356 | validation: 0.24073252195200592]
	TIME [epoch: 2.76 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17193901523494792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17193901523494792 | validation: 0.11590162965906786]
	TIME [epoch: 2.77 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945983114044556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09945983114044556 | validation: 0.11868611952551454]
	TIME [epoch: 2.76 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690010109071832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09690010109071832 | validation: 0.10683472637231586]
	TIME [epoch: 2.76 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067359279955536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09067359279955536 | validation: 0.18156690083021126]
	TIME [epoch: 2.76 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583444344145364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10583444344145364 | validation: 0.14498378173749624]
	TIME [epoch: 2.75 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16886447396879647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16886447396879647 | validation: 0.13305643357450792]
	TIME [epoch: 2.75 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871596191864042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0871596191864042 | validation: 0.1456828793679814]
	TIME [epoch: 2.76 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962533128944661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0962533128944661 | validation: 0.21955828416482312]
	TIME [epoch: 2.75 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22729886689833984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22729886689833984 | validation: 0.3490152384564355]
	TIME [epoch: 2.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2419759194246332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2419759194246332 | validation: 0.12084142034852503]
	TIME [epoch: 2.77 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285336277233579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08285336277233579 | validation: 0.11453825344696161]
	TIME [epoch: 2.78 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07648942345093278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07648942345093278 | validation: 0.13530767839580427]
	TIME [epoch: 2.77 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10916820983066301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10916820983066301 | validation: 0.1806689928868433]
	TIME [epoch: 2.78 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13728083370901126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13728083370901126 | validation: 0.12760184685128004]
	TIME [epoch: 2.78 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1613842736686086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1613842736686086 | validation: 0.2453681325971019]
	TIME [epoch: 2.77 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18776858001960656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18776858001960656 | validation: 0.1153836309814476]
	TIME [epoch: 2.77 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487285882187845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487285882187845 | validation: 0.1142191621811513]
	TIME [epoch: 2.77 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320555000077449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320555000077449 | validation: 0.0729499184945643]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06089389442075377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06089389442075377 | validation: 0.12741687180641104]
	TIME [epoch: 2.76 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08698018647514404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08698018647514404 | validation: 0.12811513876653688]
	TIME [epoch: 2.76 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12779789011641982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12779789011641982 | validation: 0.20173935448679492]
	TIME [epoch: 2.77 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17828912466791166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17828912466791166 | validation: 0.1388701496829591]
	TIME [epoch: 3.59 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12941000627426053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12941000627426053 | validation: 0.12453119309957544]
	TIME [epoch: 2.78 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07452705341044882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07452705341044882 | validation: 0.08607707740917912]
	TIME [epoch: 2.78 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05542625563902682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05542625563902682 | validation: 0.10255708504132113]
	TIME [epoch: 2.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08519739118968067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08519739118968067 | validation: 0.2540752513907201]
	TIME [epoch: 2.77 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17158985101621368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17158985101621368 | validation: 0.16103089421930145]
	TIME [epoch: 2.77 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16728285953110372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16728285953110372 | validation: 0.12564804253174852]
	TIME [epoch: 2.78 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349881537425162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07349881537425162 | validation: 0.116812550758688]
	TIME [epoch: 2.77 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897400137316907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06897400137316907 | validation: 0.19574944913335945]
	TIME [epoch: 2.77 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13228518587453647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13228518587453647 | validation: 0.21021248855628905]
	TIME [epoch: 2.78 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.229803329898876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.229803329898876 | validation: 0.2718759697956829]
	TIME [epoch: 2.78 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19986812956680378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19986812956680378 | validation: 0.12337366392888377]
	TIME [epoch: 2.78 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16338614784789585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16338614784789585 | validation: 0.17830648540874233]
	TIME [epoch: 2.78 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1244773041834426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1244773041834426 | validation: 0.10084149295851011]
	TIME [epoch: 2.78 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192610881491322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192610881491322 | validation: 0.08757414788850615]
	TIME [epoch: 2.77 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08776419151022216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08776419151022216 | validation: 0.1112024405055243]
	TIME [epoch: 2.77 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07886037660385531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07886037660385531 | validation: 0.10794885423961303]
	TIME [epoch: 2.77 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07938530599123858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07938530599123858 | validation: 0.10112872133209941]
	TIME [epoch: 2.78 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08148002612697969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08148002612697969 | validation: 0.11329334790491943]
	TIME [epoch: 2.77 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08384867295959592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08384867295959592 | validation: 0.10462616879712841]
	TIME [epoch: 2.77 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08099972419617556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08099972419617556 | validation: 0.11249865383356449]
	TIME [epoch: 2.77 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08756791134468118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08756791134468118 | validation: 0.12606268929447514]
	TIME [epoch: 2.78 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1842350125930915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1842350125930915 | validation: 0.35603554179393615]
	TIME [epoch: 2.78 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27193226571465473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27193226571465473 | validation: 0.216068363989522]
	TIME [epoch: 2.78 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19845556035096834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19845556035096834 | validation: 0.2496744874025315]
	TIME [epoch: 2.78 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2075361198947978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2075361198947978 | validation: 0.07702425315931707]
	TIME [epoch: 2.77 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07018743222946557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07018743222946557 | validation: 0.0992154324509443]
	TIME [epoch: 2.77 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05284676245731422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05284676245731422 | validation: 0.08573331732322118]
	TIME [epoch: 2.78 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690176013767075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06690176013767075 | validation: 0.09568925467014269]
	TIME [epoch: 2.78 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837378145321851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07837378145321851 | validation: 0.12437686745922082]
	TIME [epoch: 2.77 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08267478148229096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08267478148229096 | validation: 0.14345276172792001]
	TIME [epoch: 2.78 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018403014804676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1018403014804676 | validation: 0.12418997375938182]
	TIME [epoch: 2.78 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417253064133097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13417253064133097 | validation: 0.2947275677225433]
	TIME [epoch: 2.78 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23336754269942042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23336754269942042 | validation: 0.1331884430348123]
	TIME [epoch: 2.78 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15621389372465916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15621389372465916 | validation: 0.21522937160112443]
	TIME [epoch: 2.78 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13763567086668094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13763567086668094 | validation: 0.11916779385052288]
	TIME [epoch: 2.77 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11047979422727294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11047979422727294 | validation: 0.09771109197367234]
	TIME [epoch: 2.77 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060708319743831826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060708319743831826 | validation: 0.061891488565717384]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0448732689956476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0448732689956476 | validation: 0.07627692842714907]
	TIME [epoch: 2.78 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041312093748256555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041312093748256555 | validation: 0.0706004891726959]
	TIME [epoch: 2.77 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05327080838983228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05327080838983228 | validation: 0.09306324124789761]
	TIME [epoch: 2.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09691957522937707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09691957522937707 | validation: 0.19707253383163464]
	TIME [epoch: 2.77 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17538852836922714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17538852836922714 | validation: 0.13429038366375887]
	TIME [epoch: 2.78 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400598899435576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1400598899435576 | validation: 0.20451982705440433]
	TIME [epoch: 2.77 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20844120892454776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20844120892454776 | validation: 0.09468996077959371]
	TIME [epoch: 2.78 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994826259220386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11994826259220386 | validation: 0.17737016906420552]
	TIME [epoch: 2.77 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14250799404494405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14250799404494405 | validation: 0.15605447491536456]
	TIME [epoch: 2.77 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11260209590122365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11260209590122365 | validation: 0.12389697117103836]
	TIME [epoch: 2.76 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904865792636684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07904865792636684 | validation: 0.12354512149418127]
	TIME [epoch: 2.77 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695186862133017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0695186862133017 | validation: 0.18619747769839706]
	TIME [epoch: 2.77 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12398247003193216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12398247003193216 | validation: 0.1843194518242795]
	TIME [epoch: 2.77 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2300214307738517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2300214307738517 | validation: 0.2520441196323316]
	TIME [epoch: 2.76 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14418782312385942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14418782312385942 | validation: 0.07545704495834037]
	TIME [epoch: 2.77 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534291021215424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534291021215424 | validation: 0.12152885541919932]
	TIME [epoch: 2.78 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666067741528473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08666067741528473 | validation: 0.12049045951526426]
	TIME [epoch: 2.77 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927934736939206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0927934736939206 | validation: 0.11426275542336001]
	TIME [epoch: 2.77 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490406118454562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07490406118454562 | validation: 0.07432329724437405]
	TIME [epoch: 2.78 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157201397916489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07157201397916489 | validation: 0.17723551112493363]
	TIME [epoch: 2.76 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11677621493448706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11677621493448706 | validation: 0.15242333098495986]
	TIME [epoch: 2.77 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1913213718494227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1913213718494227 | validation: 0.2555165641479671]
	TIME [epoch: 2.78 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19556044218547633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19556044218547633 | validation: 0.09926570274473738]
	TIME [epoch: 2.77 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545733607826515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08545733607826515 | validation: 0.07198071257121479]
	TIME [epoch: 2.77 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046999845717594545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046999845717594545 | validation: 0.06914138510877271]
	TIME [epoch: 2.78 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03904868839385349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03904868839385349 | validation: 0.0801727063042035]
	TIME [epoch: 2.78 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05353949732749661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05353949732749661 | validation: 0.14510068670927043]
	TIME [epoch: 2.78 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11991122765361806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11991122765361806 | validation: 0.22167108524205284]
	TIME [epoch: 2.78 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21638727463809326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21638727463809326 | validation: 0.1608299619177551]
	TIME [epoch: 2.78 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10971765443741291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10971765443741291 | validation: 0.05396178304366589]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668166383345947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03668166383345947 | validation: 0.04959454946240236]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031179594800221765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031179594800221765 | validation: 0.07776522203371478]
	TIME [epoch: 2.78 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05439777412903679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05439777412903679 | validation: 0.1266562357324494]
	TIME [epoch: 2.78 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17142715270691508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17142715270691508 | validation: 0.46282063732628764]
	TIME [epoch: 2.77 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528350607314685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3528350607314685 | validation: 0.12548580315765387]
	TIME [epoch: 2.77 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489074597972736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11489074597972736 | validation: 0.21139100818035417]
	TIME [epoch: 2.78 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1663492727508706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1663492727508706 | validation: 0.11149715095810847]
	TIME [epoch: 2.78 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161784313622459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10161784313622459 | validation: 0.12252846136658323]
	TIME [epoch: 2.78 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07582280790144105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07582280790144105 | validation: 0.09001177801505396]
	TIME [epoch: 2.78 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000510616365645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09000510616365645 | validation: 0.13716077649694075]
	TIME [epoch: 2.77 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13810677595252757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13810677595252757 | validation: 0.1241216862645135]
	TIME [epoch: 2.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09963247734967859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09963247734967859 | validation: 0.11615036297526478]
	TIME [epoch: 2.77 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226473583153797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07226473583153797 | validation: 0.06674229469436681]
	TIME [epoch: 2.77 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694154815243986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0694154815243986 | validation: 0.11291180304746505]
	TIME [epoch: 2.77 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07813725829829307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07813725829829307 | validation: 0.09611945459779452]
	TIME [epoch: 2.77 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10354317363132313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10354317363132313 | validation: 0.19422785484702731]
	TIME [epoch: 2.77 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440585167205569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1440585167205569 | validation: 0.14082057773585901]
	TIME [epoch: 2.77 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14989289120495666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14989289120495666 | validation: 0.18846075678891647]
	TIME [epoch: 2.78 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445193244551805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1445193244551805 | validation: 0.10488358782541125]
	TIME [epoch: 2.77 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10391738944648975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10391738944648975 | validation: 0.10066711242031437]
	TIME [epoch: 2.77 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08346401504769498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08346401504769498 | validation: 0.11549697670088982]
	TIME [epoch: 2.77 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646568345021347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08646568345021347 | validation: 0.09619716909097203]
	TIME [epoch: 2.77 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09602559388896305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09602559388896305 | validation: 0.14683952750891557]
	TIME [epoch: 2.77 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10543782818807305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10543782818807305 | validation: 0.09034532784460193]
	TIME [epoch: 2.78 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838174762237646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09838174762237646 | validation: 0.13514438784333013]
	TIME [epoch: 2.78 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09286509703042603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09286509703042603 | validation: 0.07453027861358244]
	TIME [epoch: 2.77 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09459677504851793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09459677504851793 | validation: 0.19293550425040543]
	TIME [epoch: 2.78 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11194454699596254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11194454699596254 | validation: 0.10685862938454378]
	TIME [epoch: 2.78 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10822921342146277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10822921342146277 | validation: 0.12645285260466718]
	TIME [epoch: 2.78 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09442230437580368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09442230437580368 | validation: 0.11837223316654218]
	TIME [epoch: 2.78 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436946482339171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08436946482339171 | validation: 0.08374374343194325]
	TIME [epoch: 2.78 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069942512855093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07069942512855093 | validation: 0.10156172309643252]
	TIME [epoch: 2.77 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08233217969544027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08233217969544027 | validation: 0.11951014004168554]
	TIME [epoch: 2.77 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11414797101598129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11414797101598129 | validation: 0.20985254825780952]
	TIME [epoch: 2.77 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2079051619342054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2079051619342054 | validation: 0.08933693154929062]
	TIME [epoch: 2.77 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08352555229946225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08352555229946225 | validation: 0.15728789547572317]
	TIME [epoch: 2.77 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365886380673942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10365886380673942 | validation: 0.060970215946287855]
	TIME [epoch: 2.76 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10871812792878074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10871812792878074 | validation: 0.10581404168737028]
	TIME [epoch: 2.77 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316219468639097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07316219468639097 | validation: 0.12348351850396946]
	TIME [epoch: 2.77 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09385108585527309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09385108585527309 | validation: 0.17655524229834596]
	TIME [epoch: 2.77 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12234483544860249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12234483544860249 | validation: 0.11022894580524212]
	TIME [epoch: 2.77 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12332822099529071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12332822099529071 | validation: 0.19091238553067197]
	TIME [epoch: 2.77 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10988342605636361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10988342605636361 | validation: 0.0824728424459567]
	TIME [epoch: 2.76 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343980176792578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07343980176792578 | validation: 0.08722455164415997]
	TIME [epoch: 2.76 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08086016013212906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08086016013212906 | validation: 0.18059860991966234]
	TIME [epoch: 2.77 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14586710525336277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14586710525336277 | validation: 0.1160953533915386]
	TIME [epoch: 2.77 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12480542302528379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12480542302528379 | validation: 0.16514463153570677]
	TIME [epoch: 2.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431863739760832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12431863739760832 | validation: 0.0974958109107889]
	TIME [epoch: 2.77 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09844548337924412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09844548337924412 | validation: 0.11731690379756243]
	TIME [epoch: 2.77 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840855935295515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07840855935295515 | validation: 0.07620764052644496]
	TIME [epoch: 2.77 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688273905812429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0688273905812429 | validation: 0.11318233366392914]
	TIME [epoch: 2.77 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965491241813314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06965491241813314 | validation: 0.08528772945099145]
	TIME [epoch: 2.78 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08561369034150097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08561369034150097 | validation: 0.19300551967582513]
	TIME [epoch: 2.77 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14636049177815252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14636049177815252 | validation: 0.1212379590193361]
	TIME [epoch: 2.77 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1628771346117973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1628771346117973 | validation: 0.07635833981748087]
	TIME [epoch: 2.76 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250196594996782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05250196594996782 | validation: 0.10388509415246679]
	TIME [epoch: 2.78 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636827935401721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0636827935401721 | validation: 0.11267951647736214]
	TIME [epoch: 2.76 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898540698157936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898540698157936 | validation: 0.1156283045224209]
	TIME [epoch: 2.76 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10866399620730252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10866399620730252 | validation: 0.18796411096906412]
	TIME [epoch: 2.77 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14134633273656672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14134633273656672 | validation: 0.10300110876732088]
	TIME [epoch: 2.78 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401271063865239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401271063865239 | validation: 0.17231920920818206]
	TIME [epoch: 2.77 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837578636211422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11837578636211422 | validation: 0.06251484908143688]
	TIME [epoch: 2.77 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050098586983274594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050098586983274594 | validation: 0.0682772311631336]
	TIME [epoch: 2.78 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04002855390707574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04002855390707574 | validation: 0.0894525021815204]
	TIME [epoch: 2.77 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525376126119951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0525376126119951 | validation: 0.12925715435542307]
	TIME [epoch: 2.77 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09695861847981804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09695861847981804 | validation: 0.15267015242092122]
	TIME [epoch: 2.77 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16403028356531038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16403028356531038 | validation: 0.14752965169613352]
	TIME [epoch: 2.77 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11254502259134883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11254502259134883 | validation: 0.08152623558263271]
	TIME [epoch: 2.77 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048754537325091295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048754537325091295 | validation: 0.050036302372399304]
	TIME [epoch: 2.77 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04536090994823266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04536090994823266 | validation: 0.1142734104471253]
	TIME [epoch: 2.78 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893018368443576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0893018368443576 | validation: 0.15528038469418234]
	TIME [epoch: 2.78 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17419236973449806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17419236973449806 | validation: 0.30139388239106035]
	TIME [epoch: 2.78 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26445550902478265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26445550902478265 | validation: 0.0773659651773133]
	TIME [epoch: 2.78 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305386991549664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07305386991549664 | validation: 0.10562333296467946]
	TIME [epoch: 2.77 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907775606538329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06907775606538329 | validation: 0.13236958570333432]
	TIME [epoch: 2.77 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11005671892390058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11005671892390058 | validation: 0.09799737507038032]
	TIME [epoch: 2.77 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837012318494181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07837012318494181 | validation: 0.05798409187857098]
	TIME [epoch: 2.78 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782833575497538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03782833575497538 | validation: 0.05494904523317125]
	TIME [epoch: 2.77 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031838265257241476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031838265257241476 | validation: 0.05205976939924653]
	TIME [epoch: 2.77 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04528231577850125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04528231577850125 | validation: 0.11637051873379872]
	TIME [epoch: 2.77 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07643060843612341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07643060843612341 | validation: 0.09447580764757124]
	TIME [epoch: 2.78 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1125861184524511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1125861184524511 | validation: 0.1740425030151839]
	TIME [epoch: 2.77 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14528829096136447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14528829096136447 | validation: 0.13248086526438843]
	TIME [epoch: 2.78 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070336411533311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070336411533311 | validation: 0.1781437265007141]
	TIME [epoch: 2.77 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140321843081987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.140321843081987 | validation: 0.11580487157392616]
	TIME [epoch: 2.77 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13858052603006957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13858052603006957 | validation: 0.09686884367355769]
	TIME [epoch: 2.77 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061480560003554466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061480560003554466 | validation: 0.06173159926698956]
	TIME [epoch: 2.77 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579684308745128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04579684308745128 | validation: 0.12653628499502365]
	TIME [epoch: 2.78 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09332662457077388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09332662457077388 | validation: 0.16812528812279096]
	TIME [epoch: 2.77 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19843160680397076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19843160680397076 | validation: 0.16030331052127478]
	TIME [epoch: 2.76 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10860519172892094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10860519172892094 | validation: 0.06196445197306388]
	TIME [epoch: 2.77 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047673689618949404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047673689618949404 | validation: 0.08102551449855797]
	TIME [epoch: 2.77 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06970651926156402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06970651926156402 | validation: 0.11131533182344269]
	TIME [epoch: 2.78 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886829287554734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09886829287554734 | validation: 0.18315555088245317]
	TIME [epoch: 2.77 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12579943471710542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12579943471710542 | validation: 0.09886538050986135]
	TIME [epoch: 2.77 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12751933236733376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12751933236733376 | validation: 0.1465861914134051]
	TIME [epoch: 2.76 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12461561424993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12461561424993 | validation: 0.061826069909094644]
	TIME [epoch: 2.76 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559181404776206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04559181404776206 | validation: 0.07321841017603124]
	TIME [epoch: 2.77 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039756561641351114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039756561641351114 | validation: 0.059718216559254106]
	TIME [epoch: 2.77 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04119308996219569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04119308996219569 | validation: 0.07711030465364166]
	TIME [epoch: 2.77 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554950120479009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0554950120479009 | validation: 0.07945890729693383]
	TIME [epoch: 2.77 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.087377836432424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.087377836432424 | validation: 0.16493951608986138]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_203033/states/model_phi1_4a_v_mmd2_847.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2119.524 seconds.
