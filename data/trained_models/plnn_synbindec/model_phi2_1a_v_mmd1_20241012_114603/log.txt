Args:
Namespace(name='model_phi2_1a_v_mmd1', outdir='out/model_training/model_phi2_1a_v_mmd1', training_data='data/training_data/basic/data_phi2_1a/training', validation_data='data/training_data/basic/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 668025723

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3368535936004333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3368535936004333 | validation: 2.7305305849063677]
	TIME [epoch: 108 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4575987397838386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4575987397838386 | validation: 2.296142266406423]
	TIME [epoch: 13.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0127808224961186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0127808224961186 | validation: 2.302532622628018]
	TIME [epoch: 13.2 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9643579135225036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9643579135225036 | validation: 1.6810539690275106]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5957925507229485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5957925507229485 | validation: 1.2004059514938232]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0228159758388014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0228159758388014 | validation: 0.8036528715225593]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8397124588344347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8397124588344347 | validation: 0.7104919803107399]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6480116102147628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6480116102147628 | validation: 0.47737469148802175]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5207450365490514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5207450365490514 | validation: 0.6015985752595553]
	TIME [epoch: 13.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4355397104634802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4355397104634802 | validation: 0.3129875683055434]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44329849451087755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44329849451087755 | validation: 0.268604110946279]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4984702694423909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4984702694423909 | validation: 0.4126548634465686]
	TIME [epoch: 13.2 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3699575314781588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3699575314781588 | validation: 0.25330375801489136]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39184049840480295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39184049840480295 | validation: 0.2763657295669736]
	TIME [epoch: 13.2 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40597713285410253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40597713285410253 | validation: 0.29436649849858904]
	TIME [epoch: 13.2 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3943258569206566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3943258569206566 | validation: 0.2474394158185143]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37932025620681886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37932025620681886 | validation: 0.31784721603043276]
	TIME [epoch: 13.2 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3647879514609884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3647879514609884 | validation: 0.20489713326185488]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4432646976569501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4432646976569501 | validation: 0.20643626934834883]
	TIME [epoch: 13.2 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3711927144797002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3711927144797002 | validation: 0.29645006669090834]
	TIME [epoch: 13.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3331002137822632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3331002137822632 | validation: 0.2414787927736729]
	TIME [epoch: 13.2 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4226425998465131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4226425998465131 | validation: 0.24929689771356442]
	TIME [epoch: 13.2 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201552475781819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3201552475781819 | validation: 0.2741230716631432]
	TIME [epoch: 13.2 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32485597086600126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32485597086600126 | validation: 0.3237830977956173]
	TIME [epoch: 13.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43821844177982805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43821844177982805 | validation: 0.31826758018795404]
	TIME [epoch: 13.2 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.358703262298194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.358703262298194 | validation: 0.27745601294854316]
	TIME [epoch: 13.2 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34845713094965136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34845713094965136 | validation: 0.2594161104203625]
	TIME [epoch: 13.2 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3419264133388735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3419264133388735 | validation: 0.26579452489170097]
	TIME [epoch: 13.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34715053169870547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34715053169870547 | validation: 0.2732648189924072]
	TIME [epoch: 13.2 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3443981021714819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3443981021714819 | validation: 0.20715746837546611]
	TIME [epoch: 13.2 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3363231752598393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3363231752598393 | validation: 0.2578651307651594]
	TIME [epoch: 13.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3456527413975447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3456527413975447 | validation: 0.196126452119327]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31490319686424184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31490319686424184 | validation: 0.25815122612044733]
	TIME [epoch: 13.2 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3482703145719762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482703145719762 | validation: 0.2246317181489284]
	TIME [epoch: 13.2 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34417089854237787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34417089854237787 | validation: 0.2532955796414843]
	TIME [epoch: 13.3 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32694065919662996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32694065919662996 | validation: 0.20804269396835198]
	TIME [epoch: 13.2 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29851682509697064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29851682509697064 | validation: 0.21636884556919378]
	TIME [epoch: 13.2 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36917604856547687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36917604856547687 | validation: 0.1939433234156885]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298968238279689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.298968238279689 | validation: 0.21328503602996934]
	TIME [epoch: 13.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31637008636838226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31637008636838226 | validation: 0.2649695470698567]
	TIME [epoch: 13.2 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3487550590950053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3487550590950053 | validation: 0.2093977384350202]
	TIME [epoch: 13.2 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2861207990201586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2861207990201586 | validation: 0.18283937393842367]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30775725063118137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30775725063118137 | validation: 0.37999106806771177]
	TIME [epoch: 13.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.355447919618605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.355447919618605 | validation: 0.1819280765694219]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958103181435687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2958103181435687 | validation: 0.20578755836761442]
	TIME [epoch: 13.2 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3274733128670589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3274733128670589 | validation: 0.18328514203625823]
	TIME [epoch: 13.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30102482874385533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30102482874385533 | validation: 0.17564133264034704]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3108970546935801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3108970546935801 | validation: 0.2936265606474745]
	TIME [epoch: 13.2 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33199279445439595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33199279445439595 | validation: 0.1962087033790746]
	TIME [epoch: 13.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29007472286717484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29007472286717484 | validation: 0.1664903911968591]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31177756290658565		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.31177756290658565 | validation: 0.32802656719661194]
	TIME [epoch: 13.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3230623103910814		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.3230623103910814 | validation: 0.18444801183548876]
	TIME [epoch: 13.2 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2845700336964485		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.2845700336964485 | validation: 0.24859531647829597]
	TIME [epoch: 13.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3183773788067359		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.3183773788067359 | validation: 0.21373947449024316]
	TIME [epoch: 13.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29017441667756944		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.29017441667756944 | validation: 0.18232197188828478]
	TIME [epoch: 13.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301118283471497		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.301118283471497 | validation: 0.2744890752358845]
	TIME [epoch: 13.2 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024110738807374		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.3024110738807374 | validation: 0.1861948529818825]
	TIME [epoch: 13.2 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29015414934683403		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.29015414934683403 | validation: 0.1677188233606835]
	TIME [epoch: 13.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30389980610357864		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.30389980610357864 | validation: 0.19825398773413244]
	TIME [epoch: 13.2 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919160574951871		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.2919160574951871 | validation: 0.19726842905030628]
	TIME [epoch: 13.2 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28846764814330916		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.28846764814330916 | validation: 0.1738845139420232]
	TIME [epoch: 13.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3116213742832432		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.3116213742832432 | validation: 0.23812457959835465]
	TIME [epoch: 13.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2924371219071335		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.2924371219071335 | validation: 0.1659961088970029]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754737389621924		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.2754737389621924 | validation: 0.1930710890364987]
	TIME [epoch: 13.2 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2965685118554605		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.2965685118554605 | validation: 0.24275039085707548]
	TIME [epoch: 13.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29604513981936453		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.29604513981936453 | validation: 0.1804851701467467]
	TIME [epoch: 13.2 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2742213709812187		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.2742213709812187 | validation: 0.16347358315267574]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752097234160494		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.2752097234160494 | validation: 0.15224058516643238]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27293488635590324		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.27293488635590324 | validation: 0.34395399715203223]
	TIME [epoch: 13.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.377313008478959		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.377313008478959 | validation: 0.17430730755399745]
	TIME [epoch: 13.2 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27563987417441715		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.27563987417441715 | validation: 0.15568951395863545]
	TIME [epoch: 13.2 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26938576021303184		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.26938576021303184 | validation: 0.15840921028898208]
	TIME [epoch: 13.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750576229007447		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.2750576229007447 | validation: 0.15702140936967238]
	TIME [epoch: 13.2 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29253463404822133		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.29253463404822133 | validation: 0.20319957666482868]
	TIME [epoch: 13.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2902153479150076		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.2902153479150076 | validation: 0.15605202169950894]
	TIME [epoch: 13.2 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26815187870266644		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.26815187870266644 | validation: 0.16765522942212963]
	TIME [epoch: 13.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875193230669051		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.2875193230669051 | validation: 0.1753444568346841]
	TIME [epoch: 13.2 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804695402647954		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.2804695402647954 | validation: 0.1722099247789561]
	TIME [epoch: 13.2 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27659909826181495		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.27659909826181495 | validation: 0.16744096445062367]
	TIME [epoch: 13.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27930337497219093		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.27930337497219093 | validation: 0.17739922353556473]
	TIME [epoch: 13.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27757186147010415		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.27757186147010415 | validation: 0.15282581889139424]
	TIME [epoch: 13.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26541522600427747		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.26541522600427747 | validation: 0.16570942615931877]
	TIME [epoch: 13.2 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30456537569325837		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.30456537569325837 | validation: 0.16420099248062114]
	TIME [epoch: 13.2 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27796449753809394		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.27796449753809394 | validation: 0.14904522891077537]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26550731795954485		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.26550731795954485 | validation: 0.15312804061239782]
	TIME [epoch: 13.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.282597996324413		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.282597996324413 | validation: 0.17972748442037492]
	TIME [epoch: 13.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803527270246465		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.2803527270246465 | validation: 0.16582810718839158]
	TIME [epoch: 13.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26694344349434584		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.26694344349434584 | validation: 0.1462521361481678]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26852718429567		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.26852718429567 | validation: 0.18355332915689923]
	TIME [epoch: 13.2 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2995602363184948		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.2995602363184948 | validation: 0.14777676242891044]
	TIME [epoch: 13.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26256076917245724		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.26256076917245724 | validation: 0.1473095317994963]
	TIME [epoch: 13.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2624232469647047		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.2624232469647047 | validation: 0.20806986647818349]
	TIME [epoch: 13.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2896400624853595		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.2896400624853595 | validation: 0.15193069739644124]
	TIME [epoch: 13.2 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633060395674939		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.2633060395674939 | validation: 0.16841937785610295]
	TIME [epoch: 13.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2697045905223838		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.2697045905223838 | validation: 0.16710491418219525]
	TIME [epoch: 13.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28218986905020105		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.28218986905020105 | validation: 0.1497224384905375]
	TIME [epoch: 13.2 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2655071927387148		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.2655071927387148 | validation: 0.15503588664148335]
	TIME [epoch: 13.2 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668316790348173		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.2668316790348173 | validation: 0.18338831182869897]
	TIME [epoch: 13.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.276558122702567		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.276558122702567 | validation: 0.14667730122914888]
	TIME [epoch: 13.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25871793174532126		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.25871793174532126 | validation: 0.15660126589258316]
	TIME [epoch: 13.2 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27623396641112435		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.27623396641112435 | validation: 0.1648980672740975]
	TIME [epoch: 13.2 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27513279872761254		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.27513279872761254 | validation: 0.14784363011742166]
	TIME [epoch: 13.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2638914577187571		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.2638914577187571 | validation: 0.14390700100674225]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26571269666415226		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.26571269666415226 | validation: 0.20308148151742209]
	TIME [epoch: 13.2 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29241245876074384		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.29241245876074384 | validation: 0.1625147144522324]
	TIME [epoch: 13.2 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26740482498274504		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.26740482498274504 | validation: 0.14303368910819939]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25890790613698855		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.25890790613698855 | validation: 0.15094153949648959]
	TIME [epoch: 13.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2606373232920886		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.2606373232920886 | validation: 0.15369132684263548]
	TIME [epoch: 13.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873411255152183		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.2873411255152183 | validation: 0.14960150678177597]
	TIME [epoch: 13.2 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2642995925621332		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2642995925621332 | validation: 0.14413299846389874]
	TIME [epoch: 13.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2621162348558916		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.2621162348558916 | validation: 0.1412954182876384]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26453311557184567		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.26453311557184567 | validation: 0.18499226181407913]
	TIME [epoch: 13.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27477355765579914		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.27477355765579914 | validation: 0.14670633093633037]
	TIME [epoch: 13.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628488143815496		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.2628488143815496 | validation: 0.14672869534967312]
	TIME [epoch: 13.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27194020443253836		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.27194020443253836 | validation: 0.16559856597713515]
	TIME [epoch: 13.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2705596204797909		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2705596204797909 | validation: 0.14225650126181238]
	TIME [epoch: 13.2 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577414862449463		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.2577414862449463 | validation: 0.14554726362547676]
	TIME [epoch: 13.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649354647847588		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.2649354647847588 | validation: 0.17655112244974114]
	TIME [epoch: 13.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737722044922812		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.2737722044922812 | validation: 0.14573452692594516]
	TIME [epoch: 13.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654823652681624		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.2654823652681624 | validation: 0.1485859596165361]
	TIME [epoch: 13.2 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609865972454762		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2609865972454762 | validation: 0.1474229273704229]
	TIME [epoch: 13.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2679791947197533		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2679791947197533 | validation: 0.16189228493032065]
	TIME [epoch: 13.2 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26786142680627933		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.26786142680627933 | validation: 0.1447026124590601]
	TIME [epoch: 13.2 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25796216214167517		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.25796216214167517 | validation: 0.1547465432080939]
	TIME [epoch: 13.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2722892625719631		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2722892625719631 | validation: 0.1522141297192229]
	TIME [epoch: 13.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2589953257904804		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2589953257904804 | validation: 0.14724936896916294]
	TIME [epoch: 13.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684257990351575		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2684257990351575 | validation: 0.1510913250065555]
	TIME [epoch: 13.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2660725626796575		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2660725626796575 | validation: 0.1399624866998459]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2574952238673949		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2574952238673949 | validation: 0.15073693488085269]
	TIME [epoch: 13.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746084545858998		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2746084545858998 | validation: 0.1501011333491037]
	TIME [epoch: 13.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576502401265957		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.2576502401265957 | validation: 0.14582731509889388]
	TIME [epoch: 13.2 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2666299388943501		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.2666299388943501 | validation: 0.14361064211383073]
	TIME [epoch: 13.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629675453571333		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.2629675453571333 | validation: 0.14712622433954847]
	TIME [epoch: 13.2 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618809683186069		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.2618809683186069 | validation: 0.14889308018356648]
	TIME [epoch: 13.2 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26418760090827054		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.26418760090827054 | validation: 0.14557346804309007]
	TIME [epoch: 13.2 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26202887932205776		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.26202887932205776 | validation: 0.16684718032084167]
	TIME [epoch: 13.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26260631766565895		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.26260631766565895 | validation: 0.14223614513275]
	TIME [epoch: 13.2 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26139951750407797		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.26139951750407797 | validation: 0.14952010318335238]
	TIME [epoch: 13.2 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582190861647992		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2582190861647992 | validation: 0.1459382995847598]
	TIME [epoch: 13.2 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2749720933934927		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2749720933934927 | validation: 0.15482223675071055]
	TIME [epoch: 13.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601883692863051		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.2601883692863051 | validation: 0.14036591631904738]
	TIME [epoch: 13.2 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25606526991557843		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.25606526991557843 | validation: 0.15085727990917813]
	TIME [epoch: 13.2 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667050179934048		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.2667050179934048 | validation: 0.156914137110773]
	TIME [epoch: 13.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26246939281069437		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.26246939281069437 | validation: 0.14542048376381903]
	TIME [epoch: 13.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26023371762295533		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.26023371762295533 | validation: 0.1567004060177657]
	TIME [epoch: 13.2 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26327802151085333		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.26327802151085333 | validation: 0.15784949158784342]
	TIME [epoch: 13.2 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26493834204789224		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.26493834204789224 | validation: 0.1399092699020068]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565453261283646		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.2565453261283646 | validation: 0.14204212296054608]
	TIME [epoch: 13.2 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25795470187273983		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.25795470187273983 | validation: 0.14418876060561395]
	TIME [epoch: 13.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27050250822408883		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.27050250822408883 | validation: 0.1443863358217624]
	TIME [epoch: 13.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.256587923571169		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.256587923571169 | validation: 0.1405405094363028]
	TIME [epoch: 13.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.251958995698596		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.251958995698596 | validation: 0.15003615050058727]
	TIME [epoch: 13.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27281314405511636		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.27281314405511636 | validation: 0.14918009665708587]
	TIME [epoch: 13.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26223721430547126		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.26223721430547126 | validation: 0.14784342885232662]
	TIME [epoch: 13.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2550870187677505		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.2550870187677505 | validation: 0.1347250706701389]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2533858146411062		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.2533858146411062 | validation: 0.14385981690768457]
	TIME [epoch: 13.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2670839519365329		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.2670839519365329 | validation: 0.14415237745910667]
	TIME [epoch: 13.2 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633158959582194		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.2633158959582194 | validation: 0.14460340918491482]
	TIME [epoch: 13.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25457509390532573		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.25457509390532573 | validation: 0.1425872764339866]
	TIME [epoch: 13.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26176704802501516		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.26176704802501516 | validation: 0.147602097569132]
	TIME [epoch: 13.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24913370432747686		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.24913370432747686 | validation: 0.1186980787912763]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22441933889916166		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.22441933889916166 | validation: 0.09615845751212845]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1642051905609629		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.1642051905609629 | validation: 0.12426560251184074]
	TIME [epoch: 13.2 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15525112484012565		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.15525112484012565 | validation: 0.10157422743296722]
	TIME [epoch: 13.2 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14607154015910617		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.14607154015910617 | validation: 0.11215813047447723]
	TIME [epoch: 13.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12110589812997502		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.12110589812997502 | validation: 0.1632458971444335]
	TIME [epoch: 13.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10880714553575858		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.10880714553575858 | validation: 0.0689502083441344]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1229407891273604		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.1229407891273604 | validation: 0.053007296738377924]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14310797982340637		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.14310797982340637 | validation: 0.22200525562763893]
	TIME [epoch: 13.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12293571014376457		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.12293571014376457 | validation: 0.19276272436306735]
	TIME [epoch: 13.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1192213781890158		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.1192213781890158 | validation: 0.07376902110614468]
	TIME [epoch: 13.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13816822689836475		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.13816822689836475 | validation: 0.05370075565585927]
	TIME [epoch: 13.2 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08422097251949631		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.08422097251949631 | validation: 0.08340656218603312]
	TIME [epoch: 13.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07127613042367406		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.07127613042367406 | validation: 0.15978518050439294]
	TIME [epoch: 13.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11887623000919802		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.11887623000919802 | validation: 0.04489090785452239]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09631169836110577		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.09631169836110577 | validation: 0.0572995374167628]
	TIME [epoch: 13.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07855806820519994		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.07855806820519994 | validation: 0.0641629286300881]
	TIME [epoch: 13.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07674666174183176		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.07674666174183176 | validation: 0.11132972368950271]
	TIME [epoch: 13.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10136482973594013		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.10136482973594013 | validation: 0.04782347026423797]
	TIME [epoch: 13.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050668967340019466		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.050668967340019466 | validation: 0.04140946553765877]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13220078184918527		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.13220078184918527 | validation: 0.22521175890537115]
	TIME [epoch: 13.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17527494175625974		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.17527494175625974 | validation: 0.1409727267947497]
	TIME [epoch: 13.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1757482136628281		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.1757482136628281 | validation: 0.05352108073726372]
	TIME [epoch: 13.2 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07446545067809945		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.07446545067809945 | validation: 0.061252416109630545]
	TIME [epoch: 13.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0758712717328568		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.0758712717328568 | validation: 0.039811533976345174]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08378488021081268		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.08378488021081268 | validation: 0.07252852855743953]
	TIME [epoch: 13.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10116110328567977		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.10116110328567977 | validation: 0.04268995380440359]
	TIME [epoch: 13.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06025402257935908		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.06025402257935908 | validation: 0.04071094250545772]
	TIME [epoch: 13.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0793300239332462		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.0793300239332462 | validation: 0.048033734608419296]
	TIME [epoch: 13.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05513384179052186		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.05513384179052186 | validation: 0.09454472844731654]
	TIME [epoch: 13.2 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09000833978923495		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.09000833978923495 | validation: 0.09645866464218471]
	TIME [epoch: 13.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14202866238573109		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.14202866238573109 | validation: 0.11283076440281203]
	TIME [epoch: 13.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1005621519148294		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.1005621519148294 | validation: 0.04153639251045335]
	TIME [epoch: 13.2 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051013256648072665		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.051013256648072665 | validation: 0.058349514266450214]
	TIME [epoch: 13.2 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07398922960678836		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.07398922960678836 | validation: 0.08145374182456316]
	TIME [epoch: 13.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06793654823522918		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.06793654823522918 | validation: 0.08599265374432752]
	TIME [epoch: 13.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05199128098156303		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.05199128098156303 | validation: 0.03615929354601361]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07719387727037383		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.07719387727037383 | validation: 0.09190759731758968]
	TIME [epoch: 13.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08313605112318732		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.08313605112318732 | validation: 0.24322843200431996]
	TIME [epoch: 13.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11805725928237674		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.11805725928237674 | validation: 0.060522457872486446]
	TIME [epoch: 13.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0600604213551023		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.0600604213551023 | validation: 0.04781099352638555]
	TIME [epoch: 119 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0491096628370217		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.0491096628370217 | validation: 0.07271706996929445]
	TIME [epoch: 25.5 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09123693430645653		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.09123693430645653 | validation: 0.036053998991979856]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03816918659824168		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.03816918659824168 | validation: 0.06414766281447584]
	TIME [epoch: 25.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05459802145347473		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.05459802145347473 | validation: 0.04702672332060297]
	TIME [epoch: 25.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04044944610637154		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.04044944610637154 | validation: 0.23612000142623424]
	TIME [epoch: 25.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1572121064629343		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.1572121064629343 | validation: 0.16982940483354292]
	TIME [epoch: 25.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09717825307939922		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.09717825307939922 | validation: 0.06360698913391911]
	TIME [epoch: 25.5 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11427925819856782		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.11427925819856782 | validation: 0.03365589154879068]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04995293482726442		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.04995293482726442 | validation: 0.029081105092568872]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05266275733628382		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.05266275733628382 | validation: 0.0387140696483162]
	TIME [epoch: 25.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04061647252481473		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.04061647252481473 | validation: 0.07973827693506377]
	TIME [epoch: 25.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06216785286126858		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.06216785286126858 | validation: 0.06466338128380603]
	TIME [epoch: 25.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09914524287439955		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.09914524287439955 | validation: 0.17839343188007561]
	TIME [epoch: 25.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10662737207547582		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.10662737207547582 | validation: 0.10331726315251533]
	TIME [epoch: 25.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13194070534214278		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.13194070534214278 | validation: 0.08127421408275354]
	TIME [epoch: 25.5 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05793566784763189		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.05793566784763189 | validation: 0.049196307053354295]
	TIME [epoch: 25.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04529106574136522		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.04529106574136522 | validation: 0.027263184877221874]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03669312863981427		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.03669312863981427 | validation: 0.022070200873563185]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06777441815334867		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.06777441815334867 | validation: 0.039145857150548496]
	TIME [epoch: 25.5 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07939356318929866		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.07939356318929866 | validation: 0.1039798324121531]
	TIME [epoch: 25.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06793458421161223		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.06793458421161223 | validation: 0.02878796047606554]
	TIME [epoch: 25.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0496322820557975		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.0496322820557975 | validation: 0.03499491465532309]
	TIME [epoch: 25.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04706851650851057		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.04706851650851057 | validation: 0.07754673219685368]
	TIME [epoch: 25.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054506180003926014		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.054506180003926014 | validation: 0.023842835745685468]
	TIME [epoch: 25.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0554251658009533		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.0554251658009533 | validation: 0.05033213707885752]
	TIME [epoch: 25.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037586656597481706		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.037586656597481706 | validation: 0.048001578530684604]
	TIME [epoch: 25.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057471640146384256		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.057471640146384256 | validation: 0.02418497029846632]
	TIME [epoch: 25.5 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0456498914217583		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.0456498914217583 | validation: 0.039934476870791394]
	TIME [epoch: 25.4 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06499995526945353		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.06499995526945353 | validation: 0.16695653391628484]
	TIME [epoch: 25.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09059394935963977		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.09059394935963977 | validation: 0.09492213855268825]
	TIME [epoch: 25.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15298703922280565		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.15298703922280565 | validation: 0.05615249830152522]
	TIME [epoch: 25.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04526817344421991		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.04526817344421991 | validation: 0.039787353408568545]
	TIME [epoch: 25.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03651966388363172		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.03651966388363172 | validation: 0.03182447089105031]
	TIME [epoch: 25.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08160602310588992		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.08160602310588992 | validation: 0.047972343531928884]
	TIME [epoch: 25.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047619157935185605		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.047619157935185605 | validation: 0.023818585367949292]
	TIME [epoch: 25.5 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028276449567192885		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.028276449567192885 | validation: 0.0209719904596476]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03630869053391561		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.03630869053391561 | validation: 0.04918689039694625]
	TIME [epoch: 25.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12906036403312515		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.12906036403312515 | validation: 0.1108920821111081]
	TIME [epoch: 25.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09465003202953629		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.09465003202953629 | validation: 0.023651125912834726]
	TIME [epoch: 25.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02965658585938137		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.02965658585938137 | validation: 0.03042766452980133]
	TIME [epoch: 25.5 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0510126279655526		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.0510126279655526 | validation: 0.02836572104264211]
	TIME [epoch: 25.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030281707402121577		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.030281707402121577 | validation: 0.01916075046417763]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021939842967945156		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.021939842967945156 | validation: 0.01887674522602141]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02978184158383995		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.02978184158383995 | validation: 0.029002451251610488]
	TIME [epoch: 25.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07530269235190519		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.07530269235190519 | validation: 0.021891053850574178]
	TIME [epoch: 25.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027989230823233208		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.027989230823233208 | validation: 0.026487958800230672]
	TIME [epoch: 25.5 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04137862750843171		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.04137862750843171 | validation: 0.02070040611407268]
	TIME [epoch: 25.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08998175297252912		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.08998175297252912 | validation: 0.09246987846344987]
	TIME [epoch: 25.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07414877496063084		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.07414877496063084 | validation: 0.02005328680526218]
	TIME [epoch: 25.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04900171968890853		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.04900171968890853 | validation: 0.031812014757168695]
	TIME [epoch: 25.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03626708754766167		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.03626708754766167 | validation: 0.02769568707093409]
	TIME [epoch: 25.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664409630174745		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.0664409630174745 | validation: 0.04176403087541283]
	TIME [epoch: 25.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049150272212527615		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.049150272212527615 | validation: 0.01965928988477511]
	TIME [epoch: 25.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02475238865920521		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.02475238865920521 | validation: 0.03843022639152749]
	TIME [epoch: 25.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0302417670310638		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.0302417670310638 | validation: 0.02723712598871713]
	TIME [epoch: 25.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061082010923683164		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.061082010923683164 | validation: 0.024405837919473657]
	TIME [epoch: 25.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13661850783971763		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.13661850783971763 | validation: 0.03073676560366062]
	TIME [epoch: 25.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05057230521657642		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.05057230521657642 | validation: 0.02838764672864363]
	TIME [epoch: 25.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036168316860609104		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.036168316860609104 | validation: 0.019242163572583963]
	TIME [epoch: 25.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034682247874930766		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.034682247874930766 | validation: 0.021518925577261386]
	TIME [epoch: 25.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023247196817605308		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.023247196817605308 | validation: 0.021970481574093564]
	TIME [epoch: 25.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042412175648193065		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.042412175648193065 | validation: 0.027997365349490315]
	TIME [epoch: 25.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022572212711568423		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.022572212711568423 | validation: 0.015316931528468015]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05570620215854638		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.05570620215854638 | validation: 0.02224490044907653]
	TIME [epoch: 25.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09093300784161971		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.09093300784161971 | validation: 0.039083768031812315]
	TIME [epoch: 25.5 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042888035639706196		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.042888035639706196 | validation: 0.017281807126326133]
	TIME [epoch: 25.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031136619184020664		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.031136619184020664 | validation: 0.022568202683251867]
	TIME [epoch: 25.5 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03530759047792419		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.03530759047792419 | validation: 0.030881298333363776]
	TIME [epoch: 25.4 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02429556660271735		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.02429556660271735 | validation: 0.026774975289454557]
	TIME [epoch: 25.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04500914250247849		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.04500914250247849 | validation: 0.021309264603989506]
	TIME [epoch: 25.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023895734654627205		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.023895734654627205 | validation: 0.014682122517522121]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04203253762755528		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.04203253762755528 | validation: 0.028056967146683535]
	TIME [epoch: 25.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0198918285297426		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.0198918285297426 | validation: 0.01576572786934425]
	TIME [epoch: 25.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036081141255188895		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.036081141255188895 | validation: 0.0709344641639694]
	TIME [epoch: 25.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046939110551359		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.046939110551359 | validation: 0.036681031744709755]
	TIME [epoch: 25.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038440896353965325		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.038440896353965325 | validation: 0.027653575828197153]
	TIME [epoch: 25.4 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029830847892003187		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.029830847892003187 | validation: 0.030227979737674136]
	TIME [epoch: 25.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02270250350478358		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.02270250350478358 | validation: 0.08948958427164122]
	TIME [epoch: 25.5 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03836940420559515		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.03836940420559515 | validation: 0.014845743177646378]
	TIME [epoch: 25.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01926816777031063		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.01926816777031063 | validation: 0.0716484037041188]
	TIME [epoch: 25.5 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08636843690569224		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.08636843690569224 | validation: 0.022353536072464975]
	TIME [epoch: 25.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026316945320200704		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.026316945320200704 | validation: 0.01223050405016426]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016754164707939043		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.016754164707939043 | validation: 0.013707493608036522]
	TIME [epoch: 25.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02036573704601109		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.02036573704601109 | validation: 0.01501820247878912]
	TIME [epoch: 25.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05728249263734117		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.05728249263734117 | validation: 0.05104072802369565]
	TIME [epoch: 25.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03485544102497907		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.03485544102497907 | validation: 0.018420250955379674]
	TIME [epoch: 25.5 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056558694380405984		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.056558694380405984 | validation: 0.021403257697906096]
	TIME [epoch: 25.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031681896782653		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.031681896782653 | validation: 0.014619942429024403]
	TIME [epoch: 25.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016945117915795137		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.016945117915795137 | validation: 0.0163229232671821]
	TIME [epoch: 25.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04853270539796785		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.04853270539796785 | validation: 0.02138907601083843]
	TIME [epoch: 25.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030860433275291598		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.030860433275291598 | validation: 0.020810952933582885]
	TIME [epoch: 25.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028374810355084483		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.028374810355084483 | validation: 0.017064753190703053]
	TIME [epoch: 25.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02326527835945541		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.02326527835945541 | validation: 0.04166630326700455]
	TIME [epoch: 25.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02582029065557837		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.02582029065557837 | validation: 0.014177880242168837]
	TIME [epoch: 25.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02050487572037811		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.02050487572037811 | validation: 0.03862721478217705]
	TIME [epoch: 25.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03554075476994798		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.03554075476994798 | validation: 0.0209888112859727]
	TIME [epoch: 25.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0345527062747152		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.0345527062747152 | validation: 0.018361513460737167]
	TIME [epoch: 25.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06120526051925879		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.06120526051925879 | validation: 0.04975242667960783]
	TIME [epoch: 25.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03647356515112822		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.03647356515112822 | validation: 0.02035575117571113]
	TIME [epoch: 25.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018529225829388214		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.018529225829388214 | validation: 0.013452460259818558]
	TIME [epoch: 25.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018138470868969025		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.018138470868969025 | validation: 0.05177289549559182]
	TIME [epoch: 25.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03607580932676745		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.03607580932676745 | validation: 0.0206062865595539]
	TIME [epoch: 25.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02974180799958317		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.02974180799958317 | validation: 0.05849653934720433]
	TIME [epoch: 25.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03687860236803156		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.03687860236803156 | validation: 0.012962025901840589]
	TIME [epoch: 25.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016065205501521698		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.016065205501521698 | validation: 0.04551240587821115]
	TIME [epoch: 25.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030160532032292655		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.030160532032292655 | validation: 0.037095123429269405]
	TIME [epoch: 25.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029762405114337753		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.029762405114337753 | validation: 0.04500101304940868]
	TIME [epoch: 25.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562143477072443		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03562143477072443 | validation: 0.013073564409241218]
	TIME [epoch: 25.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016066288329267278		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.016066288329267278 | validation: 0.01713993900238782]
	TIME [epoch: 25.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051413938299313816		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.051413938299313816 | validation: 0.09805396777869824]
	TIME [epoch: 25.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04310339459047748		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.04310339459047748 | validation: 0.016542635335644075]
	TIME [epoch: 25.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015945467968214673		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.015945467968214673 | validation: 0.010674956946579928]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014048236356485597		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.014048236356485597 | validation: 0.014724757820017819]
	TIME [epoch: 25.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03320237154806091		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.03320237154806091 | validation: 0.025436833489129538]
	TIME [epoch: 25.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023366802608837303		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.023366802608837303 | validation: 0.011699250103396688]
	TIME [epoch: 25.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031016079632092623		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.031016079632092623 | validation: 0.018505244400023733]
	TIME [epoch: 25.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021103610433804217		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.021103610433804217 | validation: 0.01993529615794156]
	TIME [epoch: 25.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03080604983071719		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.03080604983071719 | validation: 0.029260928971712022]
	TIME [epoch: 25.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020241337507538783		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.020241337507538783 | validation: 0.01330018026961715]
	TIME [epoch: 25.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012911775803483287		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.012911775803483287 | validation: 0.015982410387875897]
	TIME [epoch: 25.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07318211582131093		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07318211582131093 | validation: 0.01721903410094937]
	TIME [epoch: 25.4 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031371283885957876		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.031371283885957876 | validation: 0.01714485333827959]
	TIME [epoch: 25.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017395818949219797		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.017395818949219797 | validation: 0.012414856433964432]
	TIME [epoch: 25.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011488949086355189		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.011488949086355189 | validation: 0.0104541467810079]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02780439270122821		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.02780439270122821 | validation: 0.01761902569078508]
	TIME [epoch: 25.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01647675104007984		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.01647675104007984 | validation: 0.01392541399483361]
	TIME [epoch: 25.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04391811288778097		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.04391811288778097 | validation: 0.0628359191687716]
	TIME [epoch: 25.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04723583116318951		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.04723583116318951 | validation: 0.010970203410742514]
	TIME [epoch: 25.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015476900551357184		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.015476900551357184 | validation: 0.012853610250772286]
	TIME [epoch: 25.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014942476415387847		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.014942476415387847 | validation: 0.010810217366587122]
	TIME [epoch: 25.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011406259542152818		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.011406259542152818 | validation: 0.010028567180356765]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023309572219938894		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.023309572219938894 | validation: 0.09754257927282578]
	TIME [epoch: 25.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05217894897621568		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.05217894897621568 | validation: 0.0121263788811735]
	TIME [epoch: 25.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016428583645198114		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.016428583645198114 | validation: 0.011681506878875076]
	TIME [epoch: 25.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01400395754913856		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.01400395754913856 | validation: 0.019060491913122743]
	TIME [epoch: 25.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01772393777601541		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.01772393777601541 | validation: 0.01865015665148366]
	TIME [epoch: 25.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0517990921600995		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.0517990921600995 | validation: 0.01659273716459082]
	TIME [epoch: 25.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0336605051714083		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.0336605051714083 | validation: 0.013596985627403888]
	TIME [epoch: 25.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014631506115109652		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.014631506115109652 | validation: 0.012778929209258664]
	TIME [epoch: 25.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024545581422592038		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.024545581422592038 | validation: 0.016979599779478112]
	TIME [epoch: 25.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012013176713996928		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.012013176713996928 | validation: 0.01209623616237827]
	TIME [epoch: 25.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027934901202691428		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.027934901202691428 | validation: 0.03958659579751367]
	TIME [epoch: 25.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0457263220384205		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.0457263220384205 | validation: 0.02200571447448535]
	TIME [epoch: 25.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013550552994516938		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.013550552994516938 | validation: 0.011958751961194768]
	TIME [epoch: 25.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01085690258807911		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.01085690258807911 | validation: 0.010183016432603418]
	TIME [epoch: 25.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014270859144634636		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.014270859144634636 | validation: 0.023978655366504103]
	TIME [epoch: 25.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053426987578482715		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.053426987578482715 | validation: 0.026064937367807982]
	TIME [epoch: 25.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023843012764286346		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.023843012764286346 | validation: 0.01185651219814025]
	TIME [epoch: 25.4 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012372224079610672		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.012372224079610672 | validation: 0.010737354203687877]
	TIME [epoch: 25.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010922805016082787		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.010922805016082787 | validation: 0.016110182364492066]
	TIME [epoch: 25.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01143483513845659		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.01143483513845659 | validation: 0.007932666644980733]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009704219324758775		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.009704219324758775 | validation: 0.014984762056966616]
	TIME [epoch: 25.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03153756160986705		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.03153756160986705 | validation: 0.04493737095092319]
	TIME [epoch: 25.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06241362210284493		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.06241362210284493 | validation: 0.03957263858954089]
	TIME [epoch: 25.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03474614659275343		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.03474614659275343 | validation: 0.013606572547838397]
	TIME [epoch: 25.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01481291941093158		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.01481291941093158 | validation: 0.01165004743689831]
	TIME [epoch: 25.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01030879564986368		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.01030879564986368 | validation: 0.009948611270855362]
	TIME [epoch: 25.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010432131616484323		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.010432131616484323 | validation: 0.00943592912039217]
	TIME [epoch: 25.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011708412509762674		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.011708412509762674 | validation: 0.0469219138640904]
	TIME [epoch: 25.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044309565215682314		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.044309565215682314 | validation: 0.015309758047680855]
	TIME [epoch: 25.4 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012927938807963389		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.012927938807963389 | validation: 0.010338662935819533]
	TIME [epoch: 25.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012630990429231402		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.012630990429231402 | validation: 0.011737566610027235]
	TIME [epoch: 25.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011647374319668748		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.011647374319668748 | validation: 0.013877748307389564]
	TIME [epoch: 25.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029406094811029918		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.029406094811029918 | validation: 0.013038597637576584]
	TIME [epoch: 25.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014115421922462124		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.014115421922462124 | validation: 0.010515091026294086]
	TIME [epoch: 25.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010053593862346404		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.010053593862346404 | validation: 0.016140745996473995]
	TIME [epoch: 25.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01953364128161992		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.01953364128161992 | validation: 0.013675059800593622]
	TIME [epoch: 25.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03981468347875154		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.03981468347875154 | validation: 0.035250619994021196]
	TIME [epoch: 25.4 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041089765120856124		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.041089765120856124 | validation: 0.030281703375404832]
	TIME [epoch: 25.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017608782922022066		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.017608782922022066 | validation: 0.010081999484102804]
	TIME [epoch: 25.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014283869717668396		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.014283869717668396 | validation: 0.01773843553901089]
	TIME [epoch: 25.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017310853640715055		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.017310853640715055 | validation: 0.012395364043199469]
	TIME [epoch: 25.4 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04610350084749834		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.04610350084749834 | validation: 0.2040829844185101]
	TIME [epoch: 25.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10394648579581164		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.10394648579581164 | validation: 0.024934228081046655]
	TIME [epoch: 25.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024616942956272553		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.024616942956272553 | validation: 0.019603260387105772]
	TIME [epoch: 25.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015046307847294538		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.015046307847294538 | validation: 0.010196620690493036]
	TIME [epoch: 25.4 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10961374592093073		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.10961374592093073 | validation: 0.1631233962723136]
	TIME [epoch: 25.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09182847637770619		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.09182847637770619 | validation: 0.033996397515646246]
	TIME [epoch: 25.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145610173398579		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.03145610173398579 | validation: 0.0164856534788913]
	TIME [epoch: 25.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018560807686230054		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.018560807686230054 | validation: 0.011724004812256328]
	TIME [epoch: 25.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014364726406762063		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.014364726406762063 | validation: 0.01143299246488112]
	TIME [epoch: 25.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011737043200955574		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.011737043200955574 | validation: 0.010005392991985832]
	TIME [epoch: 25.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011461608990607913		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.011461608990607913 | validation: 0.011713233520602946]
	TIME [epoch: 25.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014900859351364346		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.014900859351364346 | validation: 0.012482539697820667]
	TIME [epoch: 25.4 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07107397473151769		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.07107397473151769 | validation: 0.03669304202292491]
	TIME [epoch: 25.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030829843714912205		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.030829843714912205 | validation: 0.010942373558207399]
	TIME [epoch: 25.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009693390646607628		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.009693390646607628 | validation: 0.008278855731432016]
	TIME [epoch: 25.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012041345328957943		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.012041345328957943 | validation: 0.009435520043333808]
	TIME [epoch: 25.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01176076924292796		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.01176076924292796 | validation: 0.02754532100111361]
	TIME [epoch: 25.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0551905801002723		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.0551905801002723 | validation: 0.05334897439402181]
	TIME [epoch: 25.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03603385327231211		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.03603385327231211 | validation: 0.011227508853658787]
	TIME [epoch: 25.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010064934640362984		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.010064934640362984 | validation: 0.009092346241108459]
	TIME [epoch: 25.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00963560248081651		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.00963560248081651 | validation: 0.011482910231803506]
	TIME [epoch: 25.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009604501041748717		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.009604501041748717 | validation: 0.00952488071181717]
	TIME [epoch: 25.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01039074452132112		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.01039074452132112 | validation: 0.03301648178841078]
	TIME [epoch: 25.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0208216641411706		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.0208216641411706 | validation: 0.00810483605576652]
	TIME [epoch: 25.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00829063499208282		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.00829063499208282 | validation: 0.008290747765641532]
	TIME [epoch: 25.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007542211597134385		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.007542211597134385 | validation: 0.012736232194943818]
	TIME [epoch: 25.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02547799316818432		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.02547799316818432 | validation: 0.008393295804341066]
	TIME [epoch: 25.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012818021744574054		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.012818021744574054 | validation: 0.007679543219620369]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009228614989773933		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.009228614989773933 | validation: 0.010192698215170107]
	TIME [epoch: 25.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012220879417698284		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.012220879417698284 | validation: 0.013412007786451716]
	TIME [epoch: 25.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029967626144958258		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.029967626144958258 | validation: 0.01172089534218426]
	TIME [epoch: 25.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01500263127246363		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.01500263127246363 | validation: 0.013674075945240782]
	TIME [epoch: 25.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011112456320661561		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.011112456320661561 | validation: 0.012798728114393202]
	TIME [epoch: 25.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008512223437987843		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.008512223437987843 | validation: 0.00760298836789142]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00832504631768577		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.00832504631768577 | validation: 0.010111886516354111]
	TIME [epoch: 25.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013204564293740423		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.013204564293740423 | validation: 0.010717387075075236]
	TIME [epoch: 25.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0183698373754014		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.0183698373754014 | validation: 0.011527681605583227]
	TIME [epoch: 25.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0336647193159872		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.0336647193159872 | validation: 0.013987350198455196]
	TIME [epoch: 25.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0192068524947943		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.0192068524947943 | validation: 0.009686960569726781]
	TIME [epoch: 25.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009350771774049707		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.009350771774049707 | validation: 0.009746744677599032]
	TIME [epoch: 25.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007669405210231319		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.007669405210231319 | validation: 0.008274650098126658]
	TIME [epoch: 25.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008486773122028345		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.008486773122028345 | validation: 0.007267601439291543]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014421283245576573		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.014421283245576573 | validation: 0.018216557347905588]
	TIME [epoch: 25.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011746484125004598		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.011746484125004598 | validation: 0.16697872227595645]
	TIME [epoch: 25.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09140456619323134		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.09140456619323134 | validation: 0.019863033350895114]
	TIME [epoch: 25.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01926990131909329		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.01926990131909329 | validation: 0.010403240987365149]
	TIME [epoch: 25.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01022500270550828		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.01022500270550828 | validation: 0.007727681397754962]
	TIME [epoch: 25.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008049035871795125		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.008049035871795125 | validation: 0.009900843477032352]
	TIME [epoch: 25.4 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009136359795284444		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.009136359795284444 | validation: 0.016323944179168548]
	TIME [epoch: 25.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021688005708408686		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.021688005708408686 | validation: 0.010029082001346931]
	TIME [epoch: 25.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011628133759389212		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.011628133759389212 | validation: 0.00874463528168984]
	TIME [epoch: 25.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014811061089293545		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.014811061089293545 | validation: 0.015289957653288382]
	TIME [epoch: 25.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012033528358527732		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.012033528358527732 | validation: 0.012624983721780596]
	TIME [epoch: 25.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00962469792381317		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.00962469792381317 | validation: 0.00725691357772866]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016396502910969744		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.016396502910969744 | validation: 0.016313948139035778]
	TIME [epoch: 25.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028621557113397422		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.028621557113397422 | validation: 0.015528141341420101]
	TIME [epoch: 25.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03357865573608747		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.03357865573608747 | validation: 0.016104774427247323]
	TIME [epoch: 25.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011918508222913748		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.011918508222913748 | validation: 0.009033515170772003]
	TIME [epoch: 25.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006681317995893827		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.006681317995893827 | validation: 0.0071268899225524675]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00999975442752164		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.00999975442752164 | validation: 0.007712476689595713]
	TIME [epoch: 25.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008008127169215604		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.008008127169215604 | validation: 0.00814137225319464]
	TIME [epoch: 25.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006742076245143122		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.006742076245143122 | validation: 0.009813760073557401]
	TIME [epoch: 25.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024225427968410942		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.024225427968410942 | validation: 0.010907013225566208]
	TIME [epoch: 25.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00845425914167752		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.00845425914167752 | validation: 0.042910003688968815]
	TIME [epoch: 25.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029947914704110862		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.029947914704110862 | validation: 0.008286148326069456]
	TIME [epoch: 25.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015304614678551733		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.015304614678551733 | validation: 0.015996100244995894]
	TIME [epoch: 25.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009992846654955603		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.009992846654955603 | validation: 0.007276856750591532]
	TIME [epoch: 25.5 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006009435899941867		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.006009435899941867 | validation: 0.008067261762240787]
	TIME [epoch: 25.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076964195175980915		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.0076964195175980915 | validation: 0.009506302565600361]
	TIME [epoch: 25.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009098801373940354		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.009098801373940354 | validation: 0.01303943010713127]
	TIME [epoch: 25.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010063120771486578		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.010063120771486578 | validation: 0.005921806635127627]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006497135491084928		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.006497135491084928 | validation: 0.008439495898378184]
	TIME [epoch: 25.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04887387903137248		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04887387903137248 | validation: 0.040091956842239165]
	TIME [epoch: 25.5 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01957829746771443		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.01957829746771443 | validation: 0.007350727989547664]
	TIME [epoch: 25.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011487265333690868		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.011487265333690868 | validation: 0.010851634529197646]
	TIME [epoch: 25.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011138604648930283		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.011138604648930283 | validation: 0.009781507443170388]
	TIME [epoch: 25.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022758635448771374		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.022758635448771374 | validation: 0.012282738681075222]
	TIME [epoch: 25.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010699084478566203		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.010699084478566203 | validation: 0.006638925488238943]
	TIME [epoch: 25.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017260437303156115		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.017260437303156115 | validation: 0.007382092929776495]
	TIME [epoch: 25.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073707475427210335		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.0073707475427210335 | validation: 0.007120593863963078]
	TIME [epoch: 25.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006040539799395375		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.006040539799395375 | validation: 0.006111380422581792]
	TIME [epoch: 25.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015650976723557405		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.015650976723557405 | validation: 0.011513606384458735]
	TIME [epoch: 25.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007751727082572686		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.007751727082572686 | validation: 0.007748520845961488]
	TIME [epoch: 25.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006388294909357576		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.006388294909357576 | validation: 0.007328633445584961]
	TIME [epoch: 25.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009935306035430346		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.009935306035430346 | validation: 0.014731056606969432]
	TIME [epoch: 25.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008373554221671228		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.008373554221671228 | validation: 0.0071689535607145595]
	TIME [epoch: 25.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01209319127259916		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.01209319127259916 | validation: 0.008851259991139223]
	TIME [epoch: 25.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012254125436488813		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.012254125436488813 | validation: 0.015050791007056197]
	TIME [epoch: 25.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009818329761479493		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.009818329761479493 | validation: 0.004915406852665713]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005058310058479309		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.005058310058479309 | validation: 0.006128973296959923]
	TIME [epoch: 25.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071712295346084256		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.0071712295346084256 | validation: 0.009763108947122196]
	TIME [epoch: 25.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077220235838565245		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.0077220235838565245 | validation: 0.007109942796580662]
	TIME [epoch: 25.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00635031213438821		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.00635031213438821 | validation: 0.006595111419432847]
	TIME [epoch: 25.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01399927855655099		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.01399927855655099 | validation: 0.005843078930130031]
	TIME [epoch: 25.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006042921937452263		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.006042921937452263 | validation: 0.006837313932259153]
	TIME [epoch: 25.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005828291151854934		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.005828291151854934 | validation: 0.005627938270687641]
	TIME [epoch: 25.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009449956451818012		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.009449956451818012 | validation: 0.006578534972229957]
	TIME [epoch: 25.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048732477120953565		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.0048732477120953565 | validation: 0.012190509825452197]
	TIME [epoch: 25.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010460254282226739		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.010460254282226739 | validation: 0.005000993757061978]
	TIME [epoch: 25.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025010056113303977		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.025010056113303977 | validation: 0.009455581626082388]
	TIME [epoch: 25.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010547223487315915		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.010547223487315915 | validation: 0.006696286850262028]
	TIME [epoch: 25.4 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008092751660435797		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.008092751660435797 | validation: 0.005980027964979548]
	TIME [epoch: 25.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005016423942413358		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.005016423942413358 | validation: 0.004822141486692974]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004273890919384298		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.004273890919384298 | validation: 0.004902344899136821]
	TIME [epoch: 25.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007583753505260963		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.007583753505260963 | validation: 0.009298968774386636]
	TIME [epoch: 25.5 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02880281172978974		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.02880281172978974 | validation: 0.018425164780288772]
	TIME [epoch: 25.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009323906579604153		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.009323906579604153 | validation: 0.005899221470983585]
	TIME [epoch: 25.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059479897150131866		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.0059479897150131866 | validation: 0.0034886189990281508]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034321471017835586		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.0034321471017835586 | validation: 0.00621533354264033]
	TIME [epoch: 25.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005960025475739697		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.005960025475739697 | validation: 0.011092934402116734]
	TIME [epoch: 25.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048590902127081895		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0048590902127081895 | validation: 0.0040976425089522175]
	TIME [epoch: 25.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004261232829884082		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.004261232829884082 | validation: 0.016159232694483695]
	TIME [epoch: 25.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01993209377947009		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.01993209377947009 | validation: 0.0069458721700180505]
	TIME [epoch: 25.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004991629367551796		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.004991629367551796 | validation: 0.030761900838811508]
	TIME [epoch: 25.4 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02026110297647756		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.02026110297647756 | validation: 0.008929383453149396]
	TIME [epoch: 25.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005502958897894111		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.005502958897894111 | validation: 0.003905679036517912]
	TIME [epoch: 25.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002782122064639257		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.002782122064639257 | validation: 0.0048251740096274896]
	TIME [epoch: 25.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047926846092862015		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.0047926846092862015 | validation: 0.006099892608226799]
	TIME [epoch: 25.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004533951090268154		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.004533951090268154 | validation: 0.0056097427411420965]
	TIME [epoch: 25.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004580270426735148		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.004580270426735148 | validation: 0.004352856297427231]
	TIME [epoch: 25.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006638581379204636		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.006638581379204636 | validation: 0.018627901143733444]
	TIME [epoch: 25.5 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013585416583826872		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.013585416583826872 | validation: 0.0042790519601021626]
	TIME [epoch: 25.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004253591524189386		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.004253591524189386 | validation: 0.008054704216855122]
	TIME [epoch: 25.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029738734239278084		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.0029738734239278084 | validation: 0.007937704052493545]
	TIME [epoch: 25.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006742268155285726		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.006742268155285726 | validation: 0.004744254575009035]
	TIME [epoch: 25.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007229204025516123		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.007229204025516123 | validation: 0.0051044680622784125]
	TIME [epoch: 25.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007294580190230857		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.007294580190230857 | validation: 0.0037837740430935875]
	TIME [epoch: 25.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005770120004372282		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.005770120004372282 | validation: 0.004438250301498716]
	TIME [epoch: 143 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016635721771345438		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.016635721771345438 | validation: 0.007525048363441573]
	TIME [epoch: 50.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076953422771681235		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.0076953422771681235 | validation: 0.004453555864313001]
	TIME [epoch: 50.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006285329131216092		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.006285329131216092 | validation: 0.003910606285023474]
	TIME [epoch: 50.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008676469122431271		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.008676469122431271 | validation: 0.005548471213409907]
	TIME [epoch: 50.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004832284264430283		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.004832284264430283 | validation: 0.003978881200056527]
	TIME [epoch: 50.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087902994913087		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.0087902994913087 | validation: 0.00811107511261967]
	TIME [epoch: 50.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004387545834405606		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.004387545834405606 | validation: 0.0034739660956693277]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025117020697911302		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.025117020697911302 | validation: 0.005389672811161757]
	TIME [epoch: 50.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047131184120556245		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.0047131184120556245 | validation: 0.0017978273849047085]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01942631129468123		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.01942631129468123 | validation: 0.009402165974035622]
	TIME [epoch: 50.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005251396919126194		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.005251396919126194 | validation: 0.0037939336524241956]
	TIME [epoch: 50.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021955862590191102		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.0021955862590191102 | validation: 0.0018053326344929462]
	TIME [epoch: 50.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023203429273776378		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.0023203429273776378 | validation: 0.0032156813350465024]
	TIME [epoch: 50.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035678762617993216		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.0035678762617993216 | validation: 0.002917087646344717]
	TIME [epoch: 50.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002560056328180287		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.002560056328180287 | validation: 0.00551802847979248]
	TIME [epoch: 50.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033476486196473243		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.0033476486196473243 | validation: 0.002020997353409602]
	TIME [epoch: 50.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024256658330558374		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.0024256658330558374 | validation: 0.0028091460364476108]
	TIME [epoch: 50.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010399945376015029		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.010399945376015029 | validation: 0.01615478436521176]
	TIME [epoch: 50.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008224493183837684		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.008224493183837684 | validation: 0.0035517157054878725]
	TIME [epoch: 50.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004046748434971531		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.004046748434971531 | validation: 0.0062994531273497505]
	TIME [epoch: 50.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004073463810328348		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.004073463810328348 | validation: 0.018177867671216964]
	TIME [epoch: 50.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01147180488958624		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.01147180488958624 | validation: 0.006933574570989039]
	TIME [epoch: 50.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035853478030578073		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.0035853478030578073 | validation: 0.006398093064996018]
	TIME [epoch: 50.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003395755681638916		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.003395755681638916 | validation: 0.003906335101647883]
	TIME [epoch: 50.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00844502891008706		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.00844502891008706 | validation: 0.009291910918028378]
	TIME [epoch: 50.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011730456225214632		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.011730456225214632 | validation: 0.007845658417161509]
	TIME [epoch: 50.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049425189095467495		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.0049425189095467495 | validation: 0.003018079577752451]
	TIME [epoch: 50.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037379654911059938		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.0037379654911059938 | validation: 0.007179098114003261]
	TIME [epoch: 50.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004698973305874454		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.004698973305874454 | validation: 0.003269239751404243]
	TIME [epoch: 50.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028908696781110668		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.0028908696781110668 | validation: 0.00755792720708803]
	TIME [epoch: 50.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005632474994707679		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.005632474994707679 | validation: 0.005436343767810466]
	TIME [epoch: 50.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002963609754999536		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.002963609754999536 | validation: 0.0019963151743590903]
	TIME [epoch: 50.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032227452058501776		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.0032227452058501776 | validation: 0.004622984285777282]
	TIME [epoch: 50.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004421053954755463		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.004421053954755463 | validation: 0.0027921992939559975]
	TIME [epoch: 50.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008934151460899278		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.008934151460899278 | validation: 0.007048789042988554]
	TIME [epoch: 50.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005418165563292396		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.005418165563292396 | validation: 0.002151465063956649]
	TIME [epoch: 50.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027776581510826027		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.0027776581510826027 | validation: 0.002734904920730005]
	TIME [epoch: 50.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002614075854622288		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.002614075854622288 | validation: 0.004552684212887273]
	TIME [epoch: 50.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004857492756196434		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.004857492756196434 | validation: 0.005092554827081117]
	TIME [epoch: 50.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00854219512177513		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.00854219512177513 | validation: 0.005542511893829247]
	TIME [epoch: 50.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003206322531138108		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.003206322531138108 | validation: 0.0016318018298198155]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006152741687895909		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.006152741687895909 | validation: 0.005298682329642546]
	TIME [epoch: 50.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006051745197677385		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.006051745197677385 | validation: 0.0051270289323321755]
	TIME [epoch: 50.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023055604256343523		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.0023055604256343523 | validation: 0.0016995967479937148]
	TIME [epoch: 50.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021445454651923593		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.0021445454651923593 | validation: 0.00916779820517083]
	TIME [epoch: 50.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005699151325788503		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.005699151325788503 | validation: 0.0029185881073663213]
	TIME [epoch: 50.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002715640720960888		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.002715640720960888 | validation: 0.0020532208358869423]
	TIME [epoch: 50.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002182307150290649		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.002182307150290649 | validation: 0.003504916088527693]
	TIME [epoch: 50.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014021366394860433		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.014021366394860433 | validation: 0.005686804430546363]
	TIME [epoch: 50.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030512832794739528		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.0030512832794739528 | validation: 0.0016572310975061393]
	TIME [epoch: 50.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021222279430057416		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.0021222279430057416 | validation: 0.0031106118143827723]
	TIME [epoch: 50.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002114662308521162		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.002114662308521162 | validation: 0.0025839463574239417]
	TIME [epoch: 50.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002411305947130325		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.002411305947130325 | validation: 0.0016875609872772377]
	TIME [epoch: 50.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002529721989831591		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.002529721989831591 | validation: 0.0024851953575114345]
	TIME [epoch: 50.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013300570282196972		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.013300570282196972 | validation: 0.005941001326143764]
	TIME [epoch: 50.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034922979143676316		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.0034922979143676316 | validation: 0.00372772389340666]
	TIME [epoch: 50.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003051214996185583		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.003051214996185583 | validation: 0.001565460735842952]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005211127926729493		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.005211127926729493 | validation: 0.005136031283145798]
	TIME [epoch: 50.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004479601536050834		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.004479601536050834 | validation: 0.0027829904553085907]
	TIME [epoch: 50.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004306276579419113		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.004306276579419113 | validation: 0.002082602469626684]
	TIME [epoch: 50.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017357100835290653		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0017357100835290653 | validation: 0.0021898677304532516]
	TIME [epoch: 50.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013145147689465307		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.013145147689465307 | validation: 0.004248176649922015]
	TIME [epoch: 50.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026861457093201205		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.0026861457093201205 | validation: 0.0018932073280940281]
	TIME [epoch: 50.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001415034493133756		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.001415034493133756 | validation: 0.0040577275328899645]
	TIME [epoch: 50.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018630489159121692		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.0018630489159121692 | validation: 0.0014108116975248435]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004855547685095248		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.004855547685095248 | validation: 0.0014783536437377811]
	TIME [epoch: 50.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028570898749160118		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.0028570898749160118 | validation: 0.0014700559206740657]
	TIME [epoch: 50.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004501657099075954		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.004501657099075954 | validation: 0.002853702782199162]
	TIME [epoch: 50.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019047264858202934		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.019047264858202934 | validation: 0.0039072246564866246]
	TIME [epoch: 50.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003112994730818796		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.003112994730818796 | validation: 0.0025309447519143748]
	TIME [epoch: 50.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001940601425798369		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.001940601425798369 | validation: 0.0027933549706187465]
	TIME [epoch: 50.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001529487425700558		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.001529487425700558 | validation: 0.002106007738638986]
	TIME [epoch: 50.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024442824411595233		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.0024442824411595233 | validation: 0.00118108912178233]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005941238226038165		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.005941238226038165 | validation: 0.03571257623763857]
	TIME [epoch: 50.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09897061048285558		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.09897061048285558 | validation: 0.007189929129933199]
	TIME [epoch: 50.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009821300531368758		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.009821300531368758 | validation: 0.003882660273358543]
	TIME [epoch: 50.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030486935507773125		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.0030486935507773125 | validation: 0.0027243431761914826]
	TIME [epoch: 50.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012015720437338966		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.012015720437338966 | validation: 0.0250757084006064]
	TIME [epoch: 50.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010605463591626063		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.010605463591626063 | validation: 0.0015475645022686257]
	TIME [epoch: 50.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020918604821532007		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.0020918604821532007 | validation: 0.0010581423859747884]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025036482221242567		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.0025036482221242567 | validation: 0.002029651633602591]
	TIME [epoch: 50.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017416548325556854		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.0017416548325556854 | validation: 0.002148457222544082]
	TIME [epoch: 50.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024627146775021806		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.0024627146775021806 | validation: 0.006783524354925356]
	TIME [epoch: 50.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003902778192103715		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.003902778192103715 | validation: 0.0018078558521160263]
	TIME [epoch: 50.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035074078740928633		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.0035074078740928633 | validation: 0.002536046673414909]
	TIME [epoch: 50.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003055372981850963		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.003055372981850963 | validation: 0.001651337480209485]
	TIME [epoch: 50.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01410035499060302		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.01410035499060302 | validation: 0.00359255001988057]
	TIME [epoch: 50.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005989899865171822		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.005989899865171822 | validation: 0.001609740847787319]
	TIME [epoch: 50.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002291761048745401		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.002291761048745401 | validation: 0.0012560506740932537]
	TIME [epoch: 50.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019303973513024346		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.0019303973513024346 | validation: 0.00175467404276383]
	TIME [epoch: 50.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017642762395218639		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.0017642762395218639 | validation: 0.0019552039643871743]
	TIME [epoch: 50.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004071730902883678		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.004071730902883678 | validation: 0.005276355266963252]
	TIME [epoch: 50.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00896854923910822		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.00896854923910822 | validation: 0.0026452964779974213]
	TIME [epoch: 50.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012872161589697765		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.0012872161589697765 | validation: 0.0016236314030089911]
	TIME [epoch: 50.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023123700018678577		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.0023123700018678577 | validation: 0.0027165332299456902]
	TIME [epoch: 50.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020592815708509113		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.0020592815708509113 | validation: 0.0009864367129047648]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00137745479089757		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.00137745479089757 | validation: 0.002654521732823549]
	TIME [epoch: 50.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00205504881409218		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.00205504881409218 | validation: 0.002369970141908784]
	TIME [epoch: 50.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026375670288015674		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.0026375670288015674 | validation: 0.0020827236380235883]
	TIME [epoch: 50.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006683268356829821		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.006683268356829821 | validation: 0.006465193952845335]
	TIME [epoch: 50.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049206903924469145		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0049206903924469145 | validation: 0.003449960026405992]
	TIME [epoch: 50.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004827243208814198		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.004827243208814198 | validation: 0.005542649077917497]
	TIME [epoch: 50.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009961599287555307		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.009961599287555307 | validation: 0.007948864457573434]
	TIME [epoch: 50.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004291273111035648		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.004291273111035648 | validation: 0.0016300529683993057]
	TIME [epoch: 50.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001572844425358951		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.001572844425358951 | validation: 0.000794351693202343]
	TIME [epoch: 50.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011525364619980997		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0011525364619980997 | validation: 0.002493812796732342]
	TIME [epoch: 50.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001569268486797114		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.001569268486797114 | validation: 0.00275544661981335]
	TIME [epoch: 50.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001738067604433199		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.001738067604433199 | validation: 0.0020752377459439596]
	TIME [epoch: 50.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037500997030446674		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0037500997030446674 | validation: 0.004414658398714017]
	TIME [epoch: 50.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002475452501127064		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.002475452501127064 | validation: 0.0015215329007720088]
	TIME [epoch: 50.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027908696956363507		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.0027908696956363507 | validation: 0.0033518010567962497]
	TIME [epoch: 50.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012137446506136454		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.012137446506136454 | validation: 0.024999751088084693]
	TIME [epoch: 50.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015779774155369537		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.015779774155369537 | validation: 0.0031253229990275463]
	TIME [epoch: 50.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023506667513696518		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.0023506667513696518 | validation: 0.0025353030740757603]
	TIME [epoch: 50.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001608727212723429		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.001608727212723429 | validation: 0.0015376091499356018]
	TIME [epoch: 50.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011051609739156675		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.0011051609739156675 | validation: 0.002582334658892214]
	TIME [epoch: 50.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006269172116024692		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.006269172116024692 | validation: 0.0029195642003870767]
	TIME [epoch: 50.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023332639510970305		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.0023332639510970305 | validation: 0.003302692773471555]
	TIME [epoch: 50.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009195254778920288		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.009195254778920288 | validation: 0.033700189658559594]
	TIME [epoch: 50.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022821958845311498		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.022821958845311498 | validation: 0.011758399208569425]
	TIME [epoch: 50.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00796905960283867		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.00796905960283867 | validation: 0.006325306271890358]
	TIME [epoch: 50.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00286758815448545		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.00286758815448545 | validation: 0.0029974639514923265]
	TIME [epoch: 50.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002335585544469842		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.002335585544469842 | validation: 0.001877281329828034]
	TIME [epoch: 50.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013489100316314648		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.0013489100316314648 | validation: 0.0015217217010797311]
	TIME [epoch: 50.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014950922904434316		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.0014950922904434316 | validation: 0.0006799873914100764]
	TIME [epoch: 50.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011040254034358368		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.0011040254034358368 | validation: 0.028365684901715267]
	TIME [epoch: 50.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03720652369379218		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.03720652369379218 | validation: 0.009612963528000576]
	TIME [epoch: 50.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008874247222369017		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.008874247222369017 | validation: 0.003989841237476087]
	TIME [epoch: 50.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004837322845951492		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.004837322845951492 | validation: 0.0018417001608613793]
	TIME [epoch: 50.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014338071105606531		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.014338071105606531 | validation: 0.0020575513109036075]
	TIME [epoch: 50.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004138453157915454		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.004138453157915454 | validation: 0.0027379967307907893]
	TIME [epoch: 50.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002744745988246321		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.002744745988246321 | validation: 0.0011338063267312905]
	TIME [epoch: 50.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00216038734743856		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.00216038734743856 | validation: 0.0026803120463607544]
	TIME [epoch: 50.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015620258504232447		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.0015620258504232447 | validation: 0.0018166282423655805]
	TIME [epoch: 50.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016413658873843536		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.0016413658873843536 | validation: 0.002338835119343171]
	TIME [epoch: 50.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002674454958445398		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.002674454958445398 | validation: 0.0020306212947607915]
	TIME [epoch: 50.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018134584011976758		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0018134584011976758 | validation: 0.002100379649132766]
	TIME [epoch: 50.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002422165024800518		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.002422165024800518 | validation: 0.00248663188214408]
	TIME [epoch: 50.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017775332226534237		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.0017775332226534237 | validation: 0.0013691352227052183]
	TIME [epoch: 50.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019586519410477086		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.0019586519410477086 | validation: 0.0019219196997375732]
	TIME [epoch: 50.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001255278643771275		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.001255278643771275 | validation: 0.0009611369425287539]
	TIME [epoch: 50.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017163007198949215		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.0017163007198949215 | validation: 0.0016713595375296806]
	TIME [epoch: 50.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029380924520739024		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.0029380924520739024 | validation: 0.0028199597442376888]
	TIME [epoch: 50.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002671262811880766		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.002671262811880766 | validation: 0.003906727629432752]
	TIME [epoch: 50.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002283739537795612		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.002283739537795612 | validation: 0.0026047324309086796]
	TIME [epoch: 50.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002907593921536454		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.002907593921536454 | validation: 0.0033197198715597467]
	TIME [epoch: 50.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019967043377162735		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0019967043377162735 | validation: 0.0022739069672624256]
	TIME [epoch: 50.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004352042249550549		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.004352042249550549 | validation: 0.004669084660251869]
	TIME [epoch: 50.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029425898285769356		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.0029425898285769356 | validation: 0.0036404129726271195]
	TIME [epoch: 50.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023237890820709066		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.0023237890820709066 | validation: 0.002390960094268581]
	TIME [epoch: 50.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004659225065683332		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.004659225065683332 | validation: 0.0024375939015140167]
	TIME [epoch: 50.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01062352939247784		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.01062352939247784 | validation: 0.003094185901972496]
	TIME [epoch: 50.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032832150227114195		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.0032832150227114195 | validation: 0.0016717507267230043]
	TIME [epoch: 50.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012570007370475346		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.0012570007370475346 | validation: 0.0021282158813257343]
	TIME [epoch: 50.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00433401676119881		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.00433401676119881 | validation: 0.0017749002944912537]
	TIME [epoch: 50.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00920058771471352		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.00920058771471352 | validation: 0.016511308257249335]
	TIME [epoch: 50.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008033937874648213		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.008033937874648213 | validation: 0.006134280023123114]
	TIME [epoch: 50.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00465364640961766		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.00465364640961766 | validation: 0.0027138764561189265]
	TIME [epoch: 50.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020483361468679		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.0020483361468679 | validation: 0.0012288459794008158]
	TIME [epoch: 50.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029912774211642203		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0029912774211642203 | validation: 0.0026168101064535866]
	TIME [epoch: 50.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017713970873524713		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0017713970873524713 | validation: 0.0025147123701835684]
	TIME [epoch: 50.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027076752320003552		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0027076752320003552 | validation: 0.001402267916657113]
	TIME [epoch: 50.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029193844263340402		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.0029193844263340402 | validation: 0.0015131830482724485]
	TIME [epoch: 50.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010914767875330811		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.0010914767875330811 | validation: 0.0018116940204736603]
	TIME [epoch: 50.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001255859330779751		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.001255859330779751 | validation: 0.0018302460992998216]
	TIME [epoch: 50.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001345048204191615		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.001345048204191615 | validation: 0.00255899502075893]
	TIME [epoch: 50.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003223144449692223		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.003223144449692223 | validation: 0.0018942857834552546]
	TIME [epoch: 50.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07359204106761219		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.07359204106761219 | validation: 0.03956368328628507]
	TIME [epoch: 50.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026199895400067962		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.026199895400067962 | validation: 0.0053329091389769726]
	TIME [epoch: 50.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003765531252087088		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.003765531252087088 | validation: 0.0023934973755622853]
	TIME [epoch: 50.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003438828275544299		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.003438828275544299 | validation: 0.0010515725851758925]
	TIME [epoch: 50.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022045327223591807		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.0022045327223591807 | validation: 0.0014344456883885648]
	TIME [epoch: 50.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025326924959858414		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0025326924959858414 | validation: 0.0010809849354550772]
	TIME [epoch: 50.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002595235210894016		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.002595235210894016 | validation: 0.0019193812660874263]
	TIME [epoch: 50.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003124404330183139		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.003124404330183139 | validation: 0.0014166201307677334]
	TIME [epoch: 50.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014671788090949347		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0014671788090949347 | validation: 0.0013161804195958352]
	TIME [epoch: 50.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010305478186041189		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.0010305478186041189 | validation: 0.0011434316166584707]
	TIME [epoch: 50.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001491389615801968		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.001491389615801968 | validation: 0.0015853787775372843]
	TIME [epoch: 50.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010664446134311919		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.0010664446134311919 | validation: 0.005617109716941196]
	TIME [epoch: 50.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005145931413402971		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.005145931413402971 | validation: 0.0018226723068475376]
	TIME [epoch: 50.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021904835670928002		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.0021904835670928002 | validation: 0.0011402124806365256]
	TIME [epoch: 50.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018264363201397457		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.0018264363201397457 | validation: 0.0013533551199024804]
	TIME [epoch: 50.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009274763638282666		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0009274763638282666 | validation: 0.0015570931762969576]
	TIME [epoch: 50.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015428564636077759		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.0015428564636077759 | validation: 0.00399139934146782]
	TIME [epoch: 50.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016042649515186538		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.0016042649515186538 | validation: 0.001488859194309752]
	TIME [epoch: 50.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016434765649877754		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.0016434765649877754 | validation: 0.001844164368929496]
	TIME [epoch: 50.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008154596924292718		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.0008154596924292718 | validation: 0.0009366632824217777]
	TIME [epoch: 50.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001111028770268299		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.001111028770268299 | validation: 0.015089663747940286]
	TIME [epoch: 50.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007962225653778257		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.007962225653778257 | validation: 0.0015534558083827475]
	TIME [epoch: 50.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013232292663300095		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0013232292663300095 | validation: 0.0016056525299304814]
	TIME [epoch: 50.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019270067121439451		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.0019270067121439451 | validation: 0.0007348432374568623]
	TIME [epoch: 50.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009956276509870807		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.0009956276509870807 | validation: 0.00114650185280518]
	TIME [epoch: 50.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006641393986429949		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.0006641393986429949 | validation: 0.0015537606085888318]
	TIME [epoch: 50.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016295675349307702		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.0016295675349307702 | validation: 0.012721074435166738]
	TIME [epoch: 50.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005964818052027814		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.005964818052027814 | validation: 0.0009643807238132527]
	TIME [epoch: 50.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014777626484986879		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.014777626484986879 | validation: 0.00320043290585104]
	TIME [epoch: 50.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004090102548102824		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004090102548102824 | validation: 0.002231532034730078]
	TIME [epoch: 50.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013098412590914555		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.0013098412590914555 | validation: 0.0008462810007339417]
	TIME [epoch: 50.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007190553750237441		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.0007190553750237441 | validation: 0.0009023696430321344]
	TIME [epoch: 50.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014206554998301257		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.014206554998301257 | validation: 0.021662804638016415]
	TIME [epoch: 50.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011451378313037545		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.011451378313037545 | validation: 0.0016343691874179714]
	TIME [epoch: 50.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016228412058126912		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.0016228412058126912 | validation: 0.0016050784691662324]
	TIME [epoch: 50.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001293191147963665		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.001293191147963665 | validation: 0.0015785976196638542]
	TIME [epoch: 50.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010434283102014727		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.0010434283102014727 | validation: 0.0014491224074328953]
	TIME [epoch: 50.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008417362629921288		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0008417362629921288 | validation: 0.0017363713150821266]
	TIME [epoch: 50.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059189516130129915		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.0059189516130129915 | validation: 0.00704480962388596]
	TIME [epoch: 50.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003912257380176553		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.003912257380176553 | validation: 0.0019427564194080965]
	TIME [epoch: 50.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002255999158468523		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.002255999158468523 | validation: 0.0012525670605863129]
	TIME [epoch: 50.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00212503817064678		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.00212503817064678 | validation: 0.001340782531119217]
	TIME [epoch: 50.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000958527262936525		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.000958527262936525 | validation: 0.002228457884868658]
	TIME [epoch: 50.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015456277105638668		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0015456277105638668 | validation: 0.0007514770620690672]
	TIME [epoch: 50.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001334478204667373		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.001334478204667373 | validation: 0.003673135564812859]
	TIME [epoch: 50.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002246095994623762		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.002246095994623762 | validation: 0.00034997219223452045]
	TIME [epoch: 50.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013439385557691866		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.0013439385557691866 | validation: 0.0013114707887314542]
	TIME [epoch: 50.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016311594817762611		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.0016311594817762611 | validation: 0.0013132036541641883]
	TIME [epoch: 50.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029620776989502926		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0029620776989502926 | validation: 0.0018533860969214369]
	TIME [epoch: 50.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007911200409602603		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0007911200409602603 | validation: 0.0010328166333980976]
	TIME [epoch: 50.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006368417270338218		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0006368417270338218 | validation: 0.0014189816489073818]
	TIME [epoch: 50.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015501913963423097		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.0015501913963423097 | validation: 0.0020479862994678315]
	TIME [epoch: 50.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011027391847084846		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0011027391847084846 | validation: 0.002124952729872432]
	TIME [epoch: 50.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014834593911785914		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.0014834593911785914 | validation: 0.002979807681117038]
	TIME [epoch: 50.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002595653656516801		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.002595653656516801 | validation: 0.0046160867036257325]
	TIME [epoch: 50.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002662297954150924		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.002662297954150924 | validation: 0.001629880660380968]
	TIME [epoch: 50.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002212471578781188		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.002212471578781188 | validation: 0.003695932429490417]
	TIME [epoch: 50.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036062165385810064		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.0036062165385810064 | validation: 0.0005030603291408706]
	TIME [epoch: 50.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009881722128967957		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0009881722128967957 | validation: 0.0013337906625103101]
	TIME [epoch: 50.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001511964703971479		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.001511964703971479 | validation: 0.0022066582603330323]
	TIME [epoch: 50.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002240023046052695		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.002240023046052695 | validation: 0.0006775418401777805]
	TIME [epoch: 50.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012029852947299865		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0012029852947299865 | validation: 0.0025988390020468583]
	TIME [epoch: 50.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012014115528808157		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.0012014115528808157 | validation: 0.0024008540341131476]
	TIME [epoch: 50.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009121944257679324		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.009121944257679324 | validation: 0.00923794537426783]
	TIME [epoch: 50.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011054321574762877		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.011054321574762877 | validation: 0.010620471335459177]
	TIME [epoch: 50.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010233442031483585		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.010233442031483585 | validation: 0.0032282949319145833]
	TIME [epoch: 50.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014122863433968086		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.0014122863433968086 | validation: 0.001450417206469416]
	TIME [epoch: 50.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001135000967272297		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.001135000967272297 | validation: 0.001283936000209938]
	TIME [epoch: 50.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004419875504552491		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0004419875504552491 | validation: 0.0010452729503517696]
	TIME [epoch: 50.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010924140619018728		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0010924140619018728 | validation: 0.01642118536048169]
	TIME [epoch: 50.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03139375333131674		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.03139375333131674 | validation: 0.021712956568268417]
	TIME [epoch: 50.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014315848721298987		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.014315848721298987 | validation: 0.00451590406376017]
	TIME [epoch: 50.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004234116677734654		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.004234116677734654 | validation: 0.0070564653117424145]
	TIME [epoch: 50.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005970263599323641		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.005970263599323641 | validation: 0.0037444975592467317]
	TIME [epoch: 50.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033567656859996562		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0033567656859996562 | validation: 0.003257610585065264]
	TIME [epoch: 50.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007718068753752138		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.0007718068753752138 | validation: 0.000997611305187541]
	TIME [epoch: 50.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009716625776078345		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.0009716625776078345 | validation: 0.0009000442602842993]
	TIME [epoch: 50.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007413082723267204		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.0007413082723267204 | validation: 0.000790588213781347]
	TIME [epoch: 50.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036253522464399897		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.00036253522464399897 | validation: 0.0013364299658339919]
	TIME [epoch: 50.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005178849506682575		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.0005178849506682575 | validation: 0.0010579313795111651]
	TIME [epoch: 50.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002786612156376207		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.002786612156376207 | validation: 0.003973616031483582]
	TIME [epoch: 50.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002380968068289629		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.002380968068289629 | validation: 0.007396367513273635]
	TIME [epoch: 50.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002966421151670666		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.002966421151670666 | validation: 0.005787748358768237]
	TIME [epoch: 50.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006796300732481119		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.006796300732481119 | validation: 0.0020445513905823884]
	TIME [epoch: 50.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015439450212853386		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0015439450212853386 | validation: 0.0014036653195911054]
	TIME [epoch: 50.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000676503224799498		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.000676503224799498 | validation: 0.010434464602573667]
	TIME [epoch: 50.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012704217072541215		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.012704217072541215 | validation: 0.0010574017235686402]
	TIME [epoch: 50.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009910697763382706		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0009910697763382706 | validation: 0.0015550514107328848]
	TIME [epoch: 50.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008126545188028661		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0008126545188028661 | validation: 0.0014581596253906998]
	TIME [epoch: 50.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017023165133004717		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0017023165133004717 | validation: 0.001708302426130824]
	TIME [epoch: 50.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013960165138219758		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0013960165138219758 | validation: 0.001043320400815638]
	TIME [epoch: 50.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335578753509139e-05		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 3.335578753509139e-05 | validation: 0.00038738305030703657]
	TIME [epoch: 50.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007150203867334494		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0007150203867334494 | validation: 0.0012657163717099992]
	TIME [epoch: 50.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002428764911298125		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0002428764911298125 | validation: 0.0020719894436221037]
	TIME [epoch: 50.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005036211963648838		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0005036211963648838 | validation: 0.0005188168441576999]
	TIME [epoch: 50.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006342232932155781		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.006342232932155781 | validation: 0.006317305370526705]
	TIME [epoch: 50.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016062721578426692		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.0016062721578426692 | validation: 0.0004311526837424249]
	TIME [epoch: 50.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041299676583993823		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.00041299676583993823 | validation: 0.0006507447693135706]
	TIME [epoch: 50.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001215034311754917		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.001215034311754917 | validation: 0.0019316955790439577]
	TIME [epoch: 50.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036434836305178596		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.0036434836305178596 | validation: 0.005494701533934167]
	TIME [epoch: 50.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002629253525135951		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.002629253525135951 | validation: 0.0011341416833592084]
	TIME [epoch: 50.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009177601560520812		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0009177601560520812 | validation: 0.0016422054004749877]
	TIME [epoch: 50.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001292759677070174		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.001292759677070174 | validation: 0.00041297784375948776]
	TIME [epoch: 50.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012204690407926757		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.0012204690407926757 | validation: 0.0005544197005453664]
	TIME [epoch: 50.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005234882832334174		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.0005234882832334174 | validation: 0.0016273823810477503]
	TIME [epoch: 50.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046735114722937633		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.00046735114722937633 | validation: 0.0013861913430541204]
	TIME [epoch: 50.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00667654004234883		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.00667654004234883 | validation: 0.0007664976153885044]
	TIME [epoch: 50.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011955694764797685		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0011955694764797685 | validation: 0.0014384839438177357]
	TIME [epoch: 50.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007179097513006172		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0007179097513006172 | validation: 0.001399783248791774]
	TIME [epoch: 50.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015232311263256627		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0015232311263256627 | validation: 0.0007514978477763995]
	TIME [epoch: 50.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001362248099410231		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.001362248099410231 | validation: 0.00113932247073836]
	TIME [epoch: 50.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011859878111560556		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.0011859878111560556 | validation: 0.0009067431828271042]
	TIME [epoch: 50.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002759415133015534		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.002759415133015534 | validation: 0.0038249219950966026]
	TIME [epoch: 50.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019073461561409629		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0019073461561409629 | validation: 0.001673399170627684]
	TIME [epoch: 50.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005141037356953289		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0005141037356953289 | validation: 0.0010485796072040522]
	TIME [epoch: 50.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037825731027422304		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.00037825731027422304 | validation: 0.0004429551046656255]
	TIME [epoch: 50.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005800980466242247		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.0005800980466242247 | validation: 0.0015982330174759826]
	TIME [epoch: 50.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000680263502520224		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.000680263502520224 | validation: 0.001288507285806919]
	TIME [epoch: 50.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020964223772991304		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.0020964223772991304 | validation: 0.002205368877144413]
	TIME [epoch: 50.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013900574550620413		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.0013900574550620413 | validation: 0.0005535566970750807]
	TIME [epoch: 50.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007834993309148247		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.007834993309148247 | validation: 0.0042131297842599585]
	TIME [epoch: 50.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019698319147875435		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.0019698319147875435 | validation: 0.0017791154026329688]
	TIME [epoch: 50.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00281836725078572		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.00281836725078572 | validation: 0.0009896427798670967]
	TIME [epoch: 50.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008089046424838933		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0008089046424838933 | validation: 0.0008500647126430527]
	TIME [epoch: 50.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007024450700917685		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.0007024450700917685 | validation: 0.0013287280367729154]
	TIME [epoch: 50.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011537423420501978		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.0011537423420501978 | validation: 0.00045618472017006794]
	TIME [epoch: 50.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008102697705853259		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.0008102697705853259 | validation: 0.0007630567605334791]
	TIME [epoch: 50.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005379177094185521		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0005379177094185521 | validation: 0.0006911902110919375]
	TIME [epoch: 50.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006839387554011116		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.0006839387554011116 | validation: 0.00062680980525497]
	TIME [epoch: 50.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006938794045018298		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0006938794045018298 | validation: 0.0013794314914854148]
	TIME [epoch: 50.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005127957036989649		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0005127957036989649 | validation: 0.005210302256837903]
	TIME [epoch: 50.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043794355288554725		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0043794355288554725 | validation: 0.004290409260114394]
	TIME [epoch: 50.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002384423088120311		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.002384423088120311 | validation: 0.000562741090750333]
	TIME [epoch: 50.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033780050259618936		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.0033780050259618936 | validation: 0.0015340482602733707]
	TIME [epoch: 50.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001239911412179227		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.001239911412179227 | validation: 0.004221139166282806]
	TIME [epoch: 50.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005456758296082968		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.005456758296082968 | validation: 0.0008906508702912177]
	TIME [epoch: 50.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016659950225503754		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0016659950225503754 | validation: 0.0025466631983650403]
	TIME [epoch: 50.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012538474819090754		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.0012538474819090754 | validation: 0.0011362467206409732]
	TIME [epoch: 50.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012690354874689475		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.0012690354874689475 | validation: 0.0019160065765347323]
	TIME [epoch: 50.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013301906111362134		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0013301906111362134 | validation: 0.0007395537628907403]
	TIME [epoch: 50.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005355114521410005		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0005355114521410005 | validation: 0.0006494365544575991]
	TIME [epoch: 50.5 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022778909727450064		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0022778909727450064 | validation: 0.0035505597764926853]
	TIME [epoch: 50.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012486060918567386		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0012486060918567386 | validation: -0.0002400392937317774]
	TIME [epoch: 50.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022740813339712762		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.00022740813339712762 | validation: 0.0011704609151930688]
	TIME [epoch: 50.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010738919721926129		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0010738919721926129 | validation: 0.0029001187648402334]
	TIME [epoch: 50.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014776485313969395		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.0014776485313969395 | validation: 0.002094605383928201]
	TIME [epoch: 50.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003096725709766009		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.003096725709766009 | validation: 0.002984509323126874]
	TIME [epoch: 50.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005573906426995802		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.005573906426995802 | validation: 0.0019043267617667367]
	TIME [epoch: 50.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033821271167443676		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.0033821271167443676 | validation: 0.01022720697511574]
	TIME [epoch: 50.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004072117083457925		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.004072117083457925 | validation: 0.00020156194220761445]
	TIME [epoch: 50.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001160409904880202		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.001160409904880202 | validation: 0.0007481624472775228]
	TIME [epoch: 50.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006148825132983964		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.0006148825132983964 | validation: 0.0005995707088744418]
	TIME [epoch: 50.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006945672764725745		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.0006945672764725745 | validation: -0.0002630293878805277]
	TIME [epoch: 50.7 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114603/states/model_phi2_1a_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01005239444809638		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.01005239444809638 | validation: 0.0010049546077314777]
	TIME [epoch: 50.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003109967462072924		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.003109967462072924 | validation: 0.003218226870909153]
	TIME [epoch: 50.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016807221594328891		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.0016807221594328891 | validation: 0.0006812575787523887]
	TIME [epoch: 50.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048377117928829767		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.00048377117928829767 | validation: 0.0020855437732592765]
	TIME [epoch: 50.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032240087957309814		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.00032240087957309814 | validation: 0.0007686862605611089]
	TIME [epoch: 50.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005825902907492617		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0005825902907492617 | validation: 0.00046005005051823433]
	TIME [epoch: 50.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005158101164083275		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.0005158101164083275 | validation: 0.0018434750218741997]
	TIME [epoch: 50.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040784448078536495		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.00040784448078536495 | validation: 0.00023827640713844825]
	TIME [epoch: 50.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009640666411305805		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.0009640666411305805 | validation: 0.009265889177594334]
	TIME [epoch: 50.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005740760271070467		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.005740760271070467 | validation: 0.0010852192001270687]
	TIME [epoch: 50.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007513913330662331		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0007513913330662331 | validation: 0.0009667888707979783]
	TIME [epoch: 50.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008093831619944694		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.0008093831619944694 | validation: 0.0012103247461873945]
	TIME [epoch: 50.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007125873766834063		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.0007125873766834063 | validation: 0.000708351372281896]
	TIME [epoch: 50.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005170701234391282		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0005170701234391282 | validation: 0.0024302017116295707]
	TIME [epoch: 50.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007966103304962897		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0007966103304962897 | validation: 0.0009990054069301175]
	TIME [epoch: 50.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002704391122386094		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.002704391122386094 | validation: 0.0038137881309786154]
	TIME [epoch: 50.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009083423591562561		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.009083423591562561 | validation: 0.010395543411984488]
	TIME [epoch: 50.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007528684002556954		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.007528684002556954 | validation: 0.00035983878634519064]
	TIME [epoch: 50.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004924552289158821		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0004924552289158821 | validation: 0.0015124678153971903]
	TIME [epoch: 50.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036251808210075766		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.00036251808210075766 | validation: 0.0007105791691580006]
	TIME [epoch: 50.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00026941202604410016		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: -0.00026941202604410016 | validation: 0.00414268448795788]
	TIME [epoch: 50.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015994347224143552		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.0015994347224143552 | validation: 0.0007356113049838341]
	TIME [epoch: 50.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000734538102195127		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.000734538102195127 | validation: 0.0013050987084776793]
	TIME [epoch: 50.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004466107700163564		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0004466107700163564 | validation: 0.0007748703778478912]
	TIME [epoch: 50.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030353835303111845		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.00030353835303111845 | validation: 0.000527457013041178]
	TIME [epoch: 50.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005596656211259987		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0005596656211259987 | validation: 0.0008725822173550899]
	TIME [epoch: 50.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009320963819270912		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0009320963819270912 | validation: 0.0009409592952918212]
	TIME [epoch: 50.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002299038622149765		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.002299038622149765 | validation: 0.003681274192364071]
	TIME [epoch: 50.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018135952312325159		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0018135952312325159 | validation: 0.0016185929690728479]
	TIME [epoch: 50.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001623397601856127		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0001623397601856127 | validation: 0.0006281192608301191]
	TIME [epoch: 50.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: -1.2467228008267595e-05		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: -1.2467228008267595e-05 | validation: 0.00034357144710852336]
	TIME [epoch: 50.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005345822886403469		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.0005345822886403469 | validation: 0.0007739760843784276]
	TIME [epoch: 50.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015350907991514156		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.00015350907991514156 | validation: 0.0001680987015587849]
	TIME [epoch: 50.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023686230634317295		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.00023686230634317295 | validation: 0.0005173507811332798]
	TIME [epoch: 50.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001227184278905027		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0001227184278905027 | validation: 0.0014072521676777843]
	TIME [epoch: 50.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022158804964800426		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.00022158804964800426 | validation: 0.0007429692417684915]
	TIME [epoch: 50.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004584869553537987		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0004584869553537987 | validation: 0.0009954039211278278]
	TIME [epoch: 50.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011464754609578787		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.00011464754609578787 | validation: 0.0004738711289956177]
	TIME [epoch: 50.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002595777258631282		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.0002595777258631282 | validation: 0.0006902768014684977]
	TIME [epoch: 50.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032391046974104625		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.00032391046974104625 | validation: 0.001530554489975588]
	TIME [epoch: 50.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012300148665481084		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0012300148665481084 | validation: 0.001432035149882355]
	TIME [epoch: 50.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005314704671637347		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0005314704671637347 | validation: 0.0009966742221307046]
	TIME [epoch: 50.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014517939282764425		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0014517939282764425 | validation: 0.005168741031338431]
	TIME [epoch: 50.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016810986667441193		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.0016810986667441193 | validation: 0.0010251603529974314]
	TIME [epoch: 50.5 sec]
EPOCH 866/2000:
	Training over batches...
