Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/basic/data_phi1_3c/training', validation_data='data/training_data/basic/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 534454983

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.943002634378304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.943002634378304 | validation: 7.003614439003034]
	TIME [epoch: 260 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.864181131892881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.864181131892881 | validation: 6.817253050321954]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.674755145387743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.674755145387743 | validation: 5.622755826447508]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.694428056994059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.694428056994059 | validation: 4.451653855160207]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.49184823879564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.49184823879564 | validation: 6.963170807256072]
	TIME [epoch: 2.76 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.929286857264756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.929286857264756 | validation: 6.978271575148598]
	TIME [epoch: 2.77 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.915663360620447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.915663360620447 | validation: 6.581999022984252]
	TIME [epoch: 2.76 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.590147193049302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.590147193049302 | validation: 5.669084716531606]
	TIME [epoch: 2.75 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.722091263885204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.722091263885204 | validation: 4.42603597298137]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.428571100360544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.428571100360544 | validation: 4.432915354798377]
	TIME [epoch: 2.76 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.540422536999886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.540422536999886 | validation: 4.265471504546054]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.363951360683072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.363951360683072 | validation: 3.8455483074363688]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8319063058608207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8319063058608207 | validation: 3.660792332382961]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.859300780057244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859300780057244 | validation: 3.536610701949252]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7328179949205174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7328179949205174 | validation: 3.6206296440898513]
	TIME [epoch: 2.76 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69341941831258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69341941831258 | validation: 3.532547116373679]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.644934215631191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.644934215631191 | validation: 3.4253891152548706]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.628221825218808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.628221825218808 | validation: 3.4473040650781033]
	TIME [epoch: 2.76 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5859574138969936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5859574138969936 | validation: 3.383478318016237]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.558726966389236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.558726966389236 | validation: 3.3514739528879347]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5263938280759066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5263938280759066 | validation: 3.299647868828585]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.500355701352514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.500355701352514 | validation: 3.2998251772020826]
	TIME [epoch: 2.76 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.485445570574257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485445570574257 | validation: 3.272410628828447]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4920989651793577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4920989651793577 | validation: 3.4338916030373414]
	TIME [epoch: 2.75 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5718584401656437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5718584401656437 | validation: 3.282493444677534]
	TIME [epoch: 2.75 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5284603272559956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5284603272559956 | validation: 3.325082992056915]
	TIME [epoch: 2.75 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.518220599095093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.518220599095093 | validation: 3.2676974489844968]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4368796870628717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4368796870628717 | validation: 3.1526635863449615]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3989165894894047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3989165894894047 | validation: 3.1795391340751005]
	TIME [epoch: 2.75 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.35740847406984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.35740847406984 | validation: 3.125033426043503]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.325455253248856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.325455253248856 | validation: 3.111682959026287]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.315521286093946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.315521286093946 | validation: 3.1102029885264337]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.304171651455352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304171651455352 | validation: 3.0779652905326293]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2977478901605024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2977478901605024 | validation: 3.1268363303502174]
	TIME [epoch: 2.75 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.318084238661656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.318084238661656 | validation: 3.128617856485821]
	TIME [epoch: 2.74 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.354428958954137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.354428958954137 | validation: 3.1638275766398323]
	TIME [epoch: 2.75 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3762653005968035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3762653005968035 | validation: 3.262237205515902]
	TIME [epoch: 2.75 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.402198322148272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.402198322148272 | validation: 3.0477969905033935]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.285472875545679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.285472875545679 | validation: 3.0540871628389348]
	TIME [epoch: 2.75 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.230088334075412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.230088334075412 | validation: 3.011855913162578]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2111025303013454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2111025303013454 | validation: 2.98271442183756]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.202805801028452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202805801028452 | validation: 3.014231517289609]
	TIME [epoch: 2.75 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.189650248948435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.189650248948435 | validation: 2.969836517022192]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.183970329129206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.183970329129206 | validation: 2.994484294983839]
	TIME [epoch: 2.75 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1803115435188265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1803115435188265 | validation: 2.9706391632426734]
	TIME [epoch: 2.75 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.198498300832749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.198498300832749 | validation: 3.121337673648284]
	TIME [epoch: 2.75 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.28703553300444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.28703553300444 | validation: 2.9647329182156685]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.18327033411934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.18327033411934 | validation: 2.9583143436851427]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1511572235314915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1511572235314915 | validation: 2.9149583300002067]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.116154109641842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.116154109641842 | validation: 2.917441064412782]
	TIME [epoch: 2.75 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0974130949690735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0974130949690735 | validation: 2.8975545151783018]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.08841817955194		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.08841817955194 | validation: 2.8810794835840716]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.081335073495465		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.081335073495465 | validation: 2.870404543065515]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0681878186999016		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.0681878186999016 | validation: 2.886594397556403]
	TIME [epoch: 2.75 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.068880518754103		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.068880518754103 | validation: 2.891426143846017]
	TIME [epoch: 2.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0940078896120635		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.0940078896120635 | validation: 2.938754371879387]
	TIME [epoch: 2.75 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.112894138737213		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.112894138737213 | validation: 2.875619565783868]
	TIME [epoch: 2.75 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.07099547530132		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.07099547530132 | validation: 2.8673044889178962]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0593774408752017		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0593774408752017 | validation: 2.949974488892721]
	TIME [epoch: 2.76 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0939052882906073		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.0939052882906073 | validation: 2.828941342602734]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0398871318690452		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.0398871318690452 | validation: 2.8473821943651596]
	TIME [epoch: 2.75 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0181276999359143		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.0181276999359143 | validation: 2.8208039398671456]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9977139353672486		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.9977139353672486 | validation: 2.81201032754326]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9808150795435053		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.9808150795435053 | validation: 2.7881725450852883]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.968136410185073		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.968136410185073 | validation: 2.790988318341132]
	TIME [epoch: 2.75 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9695275453954766		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.9695275453954766 | validation: 2.775703934821033]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.959646354595415		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.959646354595415 | validation: 2.7814985433035497]
	TIME [epoch: 2.75 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.955108128587177		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.955108128587177 | validation: 2.7996241137528832]
	TIME [epoch: 2.75 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9833994348561137		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.9833994348561137 | validation: 2.8742669180739773]
	TIME [epoch: 2.76 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0586767843750686		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.0586767843750686 | validation: 2.7752978968551947]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9595115913418613		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.9595115913418613 | validation: 2.7366871369026304]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927866015919211		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.927866015919211 | validation: 2.7382505830283974]
	TIME [epoch: 2.76 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.915767339877568		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.915767339877568 | validation: 2.727130546322929]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.910159204939247		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.910159204939247 | validation: 2.7382404917175087]
	TIME [epoch: 2.76 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.915752240266669		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.915752240266669 | validation: 2.73346817516778]
	TIME [epoch: 2.76 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9143305227486587		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.9143305227486587 | validation: 2.787338381021195]
	TIME [epoch: 2.75 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9341591706267627		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.9341591706267627 | validation: 2.7405307491035047]
	TIME [epoch: 2.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9262843734307085		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.9262843734307085 | validation: 2.7317674567215207]
	TIME [epoch: 2.75 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902979114799056		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.902979114799056 | validation: 2.7069816839152914]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8838093189880456		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.8838093189880456 | validation: 2.699354870496903]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.871142674321701		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.871142674321701 | validation: 2.686648154363671]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.864464970304781		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.864464970304781 | validation: 2.6766717304718135]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8605258015120576		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.8605258015120576 | validation: 2.695117268417082]
	TIME [epoch: 2.76 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8556991257401045		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.8556991257401045 | validation: 2.6856376160558]
	TIME [epoch: 2.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.859672534289844		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.859672534289844 | validation: 2.8252419071122437]
	TIME [epoch: 2.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9800748083332396		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.9800748083332396 | validation: 2.837235799028629]
	TIME [epoch: 2.76 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.003235012914312		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.003235012914312 | validation: 2.67743183082759]
	TIME [epoch: 2.76 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8376812435607266		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.8376812435607266 | validation: 2.705677895368087]
	TIME [epoch: 2.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.870937875476492		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.870937875476492 | validation: 2.6695921589061737]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8272262061071194		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.8272262061071194 | validation: 2.6535217139980136]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8286770059009116		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.8286770059009116 | validation: 2.655951776282769]
	TIME [epoch: 2.76 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8213092092715217		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.8213092092715217 | validation: 2.6590734185672797]
	TIME [epoch: 2.76 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8092778400678218		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.8092778400678218 | validation: 2.6495419897177026]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8090307220260264		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.8090307220260264 | validation: 2.649862729180782]
	TIME [epoch: 2.76 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.803941942311771		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.803941942311771 | validation: 2.6530080043140636]
	TIME [epoch: 2.76 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8015195392897887		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.8015195392897887 | validation: 2.6312973371831974]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.787884325550836		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.787884325550836 | validation: 2.6376571608641584]
	TIME [epoch: 2.76 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.786971394762896		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.786971394762896 | validation: 2.640413170969185]
	TIME [epoch: 2.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7748590056571323		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.7748590056571323 | validation: 2.692641910352872]
	TIME [epoch: 2.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8745283282285476		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.8745283282285476 | validation: 3.107814685046012]
	TIME [epoch: 2.76 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.273457763648677		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 3.273457763648677 | validation: 2.7455530313267147]
	TIME [epoch: 2.76 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8731068721257205		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.8731068721257205 | validation: 2.723566530789144]
	TIME [epoch: 2.76 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8678466625810937		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.8678466625810937 | validation: 2.6261197647749657]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7774622081218916		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.7774622081218916 | validation: 2.6423742868322613]
	TIME [epoch: 2.75 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7914564157801722		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.7914564157801722 | validation: 2.6236256473793897]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.771533755397616		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.771533755397616 | validation: 2.6213383111935133]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7741313729528554		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.7741313729528554 | validation: 2.5981610877788714]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.759888212315508		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.759888212315508 | validation: 2.6115292072011003]
	TIME [epoch: 2.75 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7585332584924505		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.7585332584924505 | validation: 2.6070040556015672]
	TIME [epoch: 2.75 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7527027946494367		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.7527027946494367 | validation: 2.6041997355650377]
	TIME [epoch: 2.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7491447739742383		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.7491447739742383 | validation: 2.5987987169576807]
	TIME [epoch: 2.75 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.739804898700413		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.739804898700413 | validation: 2.595179662287406]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.743440012488961		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.743440012488961 | validation: 2.588721691869296]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7298058181882396		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.7298058181882396 | validation: 2.56808964439035]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.725108974854104		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.725108974854104 | validation: 2.587535511122772]
	TIME [epoch: 2.75 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.718969369801074		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.718969369801074 | validation: 2.558974454004186]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.714819829682241		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.714819829682241 | validation: 2.5800133840914197]
	TIME [epoch: 2.76 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7217899985047134		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.7217899985047134 | validation: 2.5354510195646647]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7085069827983217		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.7085069827983217 | validation: 2.434252449924049]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.542273491189187		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.542273491189187 | validation: 1.8351695674597264]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9360261678447057		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.9360261678447057 | validation: 1.364678530368459]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5157555345302496		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5157555345302496 | validation: 2.838159377173149]
	TIME [epoch: 2.76 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0950556458213043		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 3.0950556458213043 | validation: 1.7517540419437663]
	TIME [epoch: 2.76 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8904178738625337		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.8904178738625337 | validation: 1.206111756286779]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2570149464805036		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2570149464805036 | validation: 1.2473110908185914]
	TIME [epoch: 2.76 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3932201039088954		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3932201039088954 | validation: 1.0987083972133842]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1458064604739338		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1458064604739338 | validation: 1.0296005611833328]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053235087797595		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.053235087797595 | validation: 0.9890173848948383]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01110037228689		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.01110037228689 | validation: 0.9942035968691502]
	TIME [epoch: 2.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0231484783290967		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0231484783290967 | validation: 0.9281945655750832]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465873775322191		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9465873775322191 | validation: 0.9347447819097234]
	TIME [epoch: 2.75 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366399413831714		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9366399413831714 | validation: 0.9577411474620541]
	TIME [epoch: 2.75 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705517570631769		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9705517570631769 | validation: 0.9249606109388729]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159083811240578		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9159083811240578 | validation: 0.8786847268962621]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999258783388079		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8999258783388079 | validation: 0.865626951776052]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708196160556295		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8708196160556295 | validation: 0.8467468105459658]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589602343658505		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8589602343658505 | validation: 0.8362286358802831]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576860281510864		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8576860281510864 | validation: 0.8276821389071727]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459236491799668		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8459236491799668 | validation: 0.8274343704362178]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296877689764038		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8296877689764038 | validation: 0.8288704117399889]
	TIME [epoch: 2.75 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.836714045176439		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.836714045176439 | validation: 0.8325971582372976]
	TIME [epoch: 2.77 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210619480599016		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8210619480599016 | validation: 0.8060333800398642]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812366659045899		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.812366659045899 | validation: 0.8293076039190072]
	TIME [epoch: 2.76 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197634947852822		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8197634947852822 | validation: 0.8152388544999142]
	TIME [epoch: 2.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177750950883408		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8177750950883408 | validation: 0.7925643907782569]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900504173777495		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7900504173777495 | validation: 0.7925438080286146]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953009251972347		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7953009251972347 | validation: 0.7790867564948665]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699006298994163		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7699006298994163 | validation: 0.7827942686802637]
	TIME [epoch: 2.76 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684509024405209		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7684509024405209 | validation: 0.7759093346879372]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791214672743854		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7791214672743854 | validation: 0.8016340378777613]
	TIME [epoch: 2.77 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766186880393994		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7766186880393994 | validation: 0.769121551707332]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710868683008937		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7710868683008937 | validation: 0.7612170280036327]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495847019910366		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7495847019910366 | validation: 0.7514851108819077]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385412130756045		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7385412130756045 | validation: 0.7645208251318323]
	TIME [epoch: 2.76 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499532629892818		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7499532629892818 | validation: 0.7938235038844381]
	TIME [epoch: 2.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653832060238169		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7653832060238169 | validation: 0.7646497952690695]
	TIME [epoch: 2.76 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267611029250369		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7267611029250369 | validation: 0.7418229188387193]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726015035801381		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.726015035801381 | validation: 0.747723685860783]
	TIME [epoch: 2.75 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174081254952992		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7174081254952992 | validation: 0.7492276917094225]
	TIME [epoch: 2.76 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383929993792276		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7383929993792276 | validation: 0.8059345966604581]
	TIME [epoch: 2.75 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047857628122196		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8047857628122196 | validation: 0.7803444071733855]
	TIME [epoch: 2.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765919423723177		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.765919423723177 | validation: 0.7294819242391966]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356390224237913		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7356390224237913 | validation: 0.7557784547907134]
	TIME [epoch: 2.76 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399904698907781		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7399904698907781 | validation: 0.7327979526411577]
	TIME [epoch: 2.75 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045829286324065		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7045829286324065 | validation: 0.7475531364165416]
	TIME [epoch: 2.75 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024226562590422		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7024226562590422 | validation: 0.7401547294991757]
	TIME [epoch: 2.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985447338726746		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6985447338726746 | validation: 0.7389107522593942]
	TIME [epoch: 2.77 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020577485407605		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7020577485407605 | validation: 0.7305166664853934]
	TIME [epoch: 2.77 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977207663270792		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6977207663270792 | validation: 0.7394592450831077]
	TIME [epoch: 2.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011478138227186		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7011478138227186 | validation: 0.7237102194610838]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028520506417525		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7028520506417525 | validation: 0.741957604103253]
	TIME [epoch: 2.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999094000275236		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.6999094000275236 | validation: 0.7172021871240755]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6950308756891076		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6950308756891076 | validation: 0.7326218737116835]
	TIME [epoch: 2.76 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942039143281972		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6942039143281972 | validation: 0.7102655603764042]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684641270534737		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.684641270534737 | validation: 0.7131242408046488]
	TIME [epoch: 2.75 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761104698627557		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6761104698627557 | validation: 0.7366233261552289]
	TIME [epoch: 2.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7195490055226884		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7195490055226884 | validation: 0.7937348764088652]
	TIME [epoch: 2.75 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898964395085983		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7898964395085983 | validation: 0.6981438788285165]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884796682474652		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6884796682474652 | validation: 0.7630173869359622]
	TIME [epoch: 2.76 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492517671564903		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7492517671564903 | validation: 0.7641024852895706]
	TIME [epoch: 2.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737730959296195		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.737730959296195 | validation: 0.7217309845171962]
	TIME [epoch: 2.76 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020442842840718		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7020442842840718 | validation: 0.7360452027827118]
	TIME [epoch: 2.76 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6938738039347919		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6938738039347919 | validation: 0.695701230083826]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777034194189098		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6777034194189098 | validation: 0.7209853218058481]
	TIME [epoch: 2.75 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817842101291948		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6817842101291948 | validation: 0.7072076342023137]
	TIME [epoch: 2.75 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707187977132102		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6707187977132102 | validation: 0.7009176331098081]
	TIME [epoch: 2.76 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704238427768582		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6704238427768582 | validation: 0.7177657598227536]
	TIME [epoch: 2.76 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737277195614447		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6737277195614447 | validation: 0.7270432767996237]
	TIME [epoch: 2.75 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7241604416387243		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7241604416387243 | validation: 0.7426541790252479]
	TIME [epoch: 2.76 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297524463656491		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7297524463656491 | validation: 0.7119236999594042]
	TIME [epoch: 2.76 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805374166391636		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.6805374166391636 | validation: 0.7180687283219432]
	TIME [epoch: 2.76 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989099906477432		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6989099906477432 | validation: 0.7186520111256738]
	TIME [epoch: 2.76 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984015918196892		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6984015918196892 | validation: 0.7082591154467326]
	TIME [epoch: 2.76 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820800307494939		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6820800307494939 | validation: 0.7246611057255858]
	TIME [epoch: 2.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865709746639136		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6865709746639136 | validation: 0.7120985264017238]
	TIME [epoch: 2.76 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687860544955586		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.687860544955586 | validation: 0.695559086578903]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673420705494388		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6673420705494388 | validation: 0.7058329102347406]
	TIME [epoch: 2.75 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738714319375106		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6738714319375106 | validation: 0.6869087443997866]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630313311802335		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.6630313311802335 | validation: 0.6899634341850007]
	TIME [epoch: 2.76 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656177358721729		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6656177358721729 | validation: 0.6882182702129157]
	TIME [epoch: 2.77 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596662966480329		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6596662966480329 | validation: 0.7019042042211086]
	TIME [epoch: 271 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592474118327941		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6592474118327941 | validation: 0.694230546479456]
	TIME [epoch: 5.94 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623911928749806		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6623911928749806 | validation: 0.7216278331705465]
	TIME [epoch: 5.94 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454046568220984		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7454046568220984 | validation: 0.7508454098079227]
	TIME [epoch: 5.94 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022492728853499		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8022492728853499 | validation: 0.6903713909974581]
	TIME [epoch: 5.95 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818605819739946		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6818605819739946 | validation: 0.7167376133376226]
	TIME [epoch: 5.95 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906813108192702		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6906813108192702 | validation: 0.6964249409816052]
	TIME [epoch: 5.94 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624734599301112		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6624734599301112 | validation: 0.6985487554574707]
	TIME [epoch: 5.94 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672563563292572		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6672563563292572 | validation: 0.68845535916443]
	TIME [epoch: 5.94 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564604886949058		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6564604886949058 | validation: 0.7015366843145923]
	TIME [epoch: 5.93 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575819146661492		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6575819146661492 | validation: 0.6956339223566195]
	TIME [epoch: 5.94 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599213763923614		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6599213763923614 | validation: 0.685401392181446]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495348746367735		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6495348746367735 | validation: 0.6949085122370038]
	TIME [epoch: 5.92 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576139282900104		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6576139282900104 | validation: 0.6900874393287874]
	TIME [epoch: 5.91 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600293084555249		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6600293084555249 | validation: 0.7029420463870673]
	TIME [epoch: 5.91 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737552847835281		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6737552847835281 | validation: 0.7179129100585325]
	TIME [epoch: 5.92 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965117540079558		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6965117540079558 | validation: 0.6884380688091275]
	TIME [epoch: 5.91 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825596458070884		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6825596458070884 | validation: 0.6815787970030697]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579496384430743		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6579496384430743 | validation: 0.7074756474431734]
	TIME [epoch: 5.91 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649451283185006		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6649451283185006 | validation: 0.6856041430893653]
	TIME [epoch: 5.91 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6558052068526652		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6558052068526652 | validation: 0.6979491631816128]
	TIME [epoch: 5.92 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605011163868337		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6605011163868337 | validation: 0.6894936146656268]
	TIME [epoch: 5.92 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544836209635609		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6544836209635609 | validation: 0.6829087556299399]
	TIME [epoch: 5.91 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601440709480868		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6601440709480868 | validation: 0.68364971032927]
	TIME [epoch: 5.91 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569650482831645		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6569650482831645 | validation: 0.7100544343054208]
	TIME [epoch: 5.91 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005263911463047		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7005263911463047 | validation: 0.6768403605788942]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676149463810157		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.676149463810157 | validation: 0.686038874278223]
	TIME [epoch: 5.9 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591789327188553		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6591789327188553 | validation: 0.6923704597793736]
	TIME [epoch: 5.91 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528545913057522		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6528545913057522 | validation: 0.7007900903401698]
	TIME [epoch: 5.91 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728044291123157		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6728044291123157 | validation: 0.6961130173593799]
	TIME [epoch: 5.9 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660782006774373		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6660782006774373 | validation: 0.6837208284404396]
	TIME [epoch: 5.92 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553454523281381		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6553454523281381 | validation: 0.6820799329148187]
	TIME [epoch: 5.92 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944349149297071		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6944349149297071 | validation: 0.7214842829061479]
	TIME [epoch: 5.92 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69701675548207		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.69701675548207 | validation: 0.6862998155172924]
	TIME [epoch: 5.91 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654873292864958		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.654873292864958 | validation: 0.7186318460634279]
	TIME [epoch: 5.91 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992818426655873		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6992818426655873 | validation: 0.7167604735544214]
	TIME [epoch: 5.91 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987350918160127		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6987350918160127 | validation: 0.7041156028578752]
	TIME [epoch: 5.92 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687684520663654		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6687684520663654 | validation: 0.6911136569030436]
	TIME [epoch: 5.9 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832683158225052		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6832683158225052 | validation: 0.6871234869354776]
	TIME [epoch: 5.91 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535734785930913		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6535734785930913 | validation: 0.6913472691695788]
	TIME [epoch: 5.9 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536775025761479		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6536775025761479 | validation: 0.6995693140019658]
	TIME [epoch: 5.91 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505308507859864		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6505308507859864 | validation: 0.6937676369620503]
	TIME [epoch: 5.92 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471282411433438		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6471282411433438 | validation: 0.6878537900475417]
	TIME [epoch: 5.91 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506777267009144		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6506777267009144 | validation: 0.6923429128069472]
	TIME [epoch: 5.91 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524579338335331		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6524579338335331 | validation: 0.6849163840649798]
	TIME [epoch: 5.91 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686740062926555		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6686740062926555 | validation: 0.6851495984052873]
	TIME [epoch: 5.9 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6460477049603109		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6460477049603109 | validation: 0.6696252585252092]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441995324679709		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6441995324679709 | validation: 0.6952490712721782]
	TIME [epoch: 5.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461011165254203		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6461011165254203 | validation: 0.6906297832899668]
	TIME [epoch: 5.92 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744540732097991		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6744540732097991 | validation: 0.6961192080299883]
	TIME [epoch: 5.9 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965218424351557		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6965218424351557 | validation: 0.6818587448159781]
	TIME [epoch: 5.91 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568912083689947		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6568912083689947 | validation: 0.6921269666539844]
	TIME [epoch: 5.91 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642722916519427		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6642722916519427 | validation: 0.6869642825058978]
	TIME [epoch: 5.92 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551084075297823		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6551084075297823 | validation: 0.6919337934923231]
	TIME [epoch: 5.91 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646491109921835		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.646491109921835 | validation: 0.704100483073489]
	TIME [epoch: 5.91 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803254724104949		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6803254724104949 | validation: 0.6726282767372113]
	TIME [epoch: 5.91 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7000237569272179		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7000237569272179 | validation: 0.6821909211042313]
	TIME [epoch: 5.93 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66447097670483		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.66447097670483 | validation: 0.7087457822340325]
	TIME [epoch: 5.9 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6852253566827587		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6852253566827587 | validation: 0.6767340379014684]
	TIME [epoch: 5.92 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530048906798018		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6530048906798018 | validation: 0.6882025865127388]
	TIME [epoch: 5.91 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647379494209509		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6647379494209509 | validation: 0.6803090957724939]
	TIME [epoch: 5.98 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6511399984747577		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6511399984747577 | validation: 0.681334148203233]
	TIME [epoch: 5.9 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399809894026056		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6399809894026056 | validation: 0.685285396044129]
	TIME [epoch: 5.92 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643071912660968		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.643071912660968 | validation: 0.6792315326345758]
	TIME [epoch: 5.92 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646414039616861		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.646414039616861 | validation: 0.6856843572111235]
	TIME [epoch: 5.93 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675406000153288		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6675406000153288 | validation: 0.6948952035027176]
	TIME [epoch: 5.91 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607144131452752		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6607144131452752 | validation: 0.6793545636789821]
	TIME [epoch: 5.92 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541227120093034		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6541227120093034 | validation: 0.6859063952200467]
	TIME [epoch: 5.9 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508203861179779		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6508203861179779 | validation: 0.6854443645266844]
	TIME [epoch: 5.92 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516871786414666		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6516871786414666 | validation: 0.6839997276549259]
	TIME [epoch: 5.91 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646919808137196		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.646919808137196 | validation: 0.6837107662804085]
	TIME [epoch: 5.91 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640029247218939		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.640029247218939 | validation: 0.6911439640450822]
	TIME [epoch: 5.92 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648299838002691		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.6648299838002691 | validation: 0.683997152234954]
	TIME [epoch: 5.91 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430810779913796		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6430810779913796 | validation: 0.6705894673192039]
	TIME [epoch: 5.91 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444319143113779		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6444319143113779 | validation: 0.6915458935688901]
	TIME [epoch: 5.91 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659671352429927		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.659671352429927 | validation: 0.671275151922026]
	TIME [epoch: 5.92 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510686672575323		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6510686672575323 | validation: 0.6927492412609138]
	TIME [epoch: 5.91 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659974639489809		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.659974639489809 | validation: 0.6839584601003125]
	TIME [epoch: 5.92 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524637794853021		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6524637794853021 | validation: 0.6781966242122935]
	TIME [epoch: 5.92 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492816285167445		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6492816285167445 | validation: 0.6844032519842655]
	TIME [epoch: 5.9 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425413303132534		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6425413303132534 | validation: 0.6710540105572863]
	TIME [epoch: 5.91 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644200038615667		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.644200038615667 | validation: 0.6990542152705311]
	TIME [epoch: 5.91 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6846113830725093		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6846113830725093 | validation: 0.6812302855790611]
	TIME [epoch: 5.91 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563611875215384		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6563611875215384 | validation: 0.683022301680345]
	TIME [epoch: 5.91 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397082331584142		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6397082331584142 | validation: 0.6851279346036181]
	TIME [epoch: 5.91 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654944478072303		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.654944478072303 | validation: 0.6868696941000514]
	TIME [epoch: 5.91 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473160985960235		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6473160985960235 | validation: 0.6805293737769289]
	TIME [epoch: 5.91 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421910788511014		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6421910788511014 | validation: 0.6783322709770074]
	TIME [epoch: 5.91 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436746484373667		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6436746484373667 | validation: 0.6829674937856329]
	TIME [epoch: 5.91 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561262953727036		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6561262953727036 | validation: 0.6829696597005184]
	TIME [epoch: 5.92 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645712423199667		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.645712423199667 | validation: 0.6857832138454389]
	TIME [epoch: 5.92 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509425508466575		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6509425508466575 | validation: 0.6845317363664686]
	TIME [epoch: 5.92 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650255455503316		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.650255455503316 | validation: 0.685851750295586]
	TIME [epoch: 5.9 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459961907626215		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6459961907626215 | validation: 0.6903948777931858]
	TIME [epoch: 5.91 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448851915927947		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6448851915927947 | validation: 0.6798117388097263]
	TIME [epoch: 5.92 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437023194332138		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6437023194332138 | validation: 0.6968679510717953]
	TIME [epoch: 5.91 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585127351374324		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6585127351374324 | validation: 0.6874172309885435]
	TIME [epoch: 5.92 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608987812278266		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6608987812278266 | validation: 0.6753412369871472]
	TIME [epoch: 5.91 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425086677267128		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.6425086677267128 | validation: 0.6784026226929465]
	TIME [epoch: 5.93 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6756476793280157		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6756476793280157 | validation: 0.689060480884895]
	TIME [epoch: 5.91 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650150996329095		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.650150996329095 | validation: 0.6909231069779922]
	TIME [epoch: 5.93 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687317966897957		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6687317966897957 | validation: 0.6759532579798604]
	TIME [epoch: 5.94 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496066019132806		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6496066019132806 | validation: 0.6753067039556186]
	TIME [epoch: 5.95 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492776646885531		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6492776646885531 | validation: 0.6655378896627697]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435117007432072		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6435117007432072 | validation: 0.6821430360429087]
	TIME [epoch: 5.94 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554111424043688		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6554111424043688 | validation: 0.6756169893144416]
	TIME [epoch: 5.94 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441127299720216		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6441127299720216 | validation: 0.6804468067129159]
	TIME [epoch: 5.94 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387455301646595		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6387455301646595 | validation: 0.6809066231325887]
	TIME [epoch: 5.91 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638722179924743		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.638722179924743 | validation: 0.6754714188264783]
	TIME [epoch: 5.91 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338457078515664		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6338457078515664 | validation: 0.6727898231599962]
	TIME [epoch: 5.91 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437665010681011		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6437665010681011 | validation: 0.6794533399294578]
	TIME [epoch: 5.91 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774579049258782		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6774579049258782 | validation: 0.6955173399495163]
	TIME [epoch: 5.92 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610475937021602		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6610475937021602 | validation: 0.6781297992029102]
	TIME [epoch: 5.92 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358125044128372		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6358125044128372 | validation: 0.684121417569675]
	TIME [epoch: 5.91 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408744467265476		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6408744467265476 | validation: 0.6815595848107243]
	TIME [epoch: 5.92 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634363536965409		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.634363536965409 | validation: 0.6808128850229229]
	TIME [epoch: 5.92 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397079129934817		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6397079129934817 | validation: 0.6728723808560265]
	TIME [epoch: 5.92 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416176887820411		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6416176887820411 | validation: 0.6808485173238762]
	TIME [epoch: 5.91 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487264202937759		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6487264202937759 | validation: 0.6650401207536016]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642457499674435		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.642457499674435 | validation: 0.6789075989657135]
	TIME [epoch: 5.93 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400220021013298		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6400220021013298 | validation: 0.6644322389727135]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354868948312767		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6354868948312767 | validation: 0.6984449201092415]
	TIME [epoch: 5.92 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352808493761356		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6352808493761356 | validation: 0.6884299523954176]
	TIME [epoch: 5.92 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518515933205475		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6518515933205475 | validation: 0.6806714865628115]
	TIME [epoch: 5.92 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390426179604792		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6390426179604792 | validation: 0.6649728333867687]
	TIME [epoch: 5.92 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298162673803533		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6298162673803533 | validation: 0.6811656865966383]
	TIME [epoch: 5.94 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354220752591955		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6354220752591955 | validation: 0.6624560351261026]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635366451689473		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.635366451689473 | validation: 0.6679291521471225]
	TIME [epoch: 5.93 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559078802334779		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6559078802334779 | validation: 0.6866714103978566]
	TIME [epoch: 5.93 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638817826951079		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.638817826951079 | validation: 0.6627584878138102]
	TIME [epoch: 5.93 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370033807847841		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6370033807847841 | validation: 0.6840857759547117]
	TIME [epoch: 5.94 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441739940147936		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6441739940147936 | validation: 0.674764724832098]
	TIME [epoch: 5.95 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487289611524303		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6487289611524303 | validation: 0.6966115439123199]
	TIME [epoch: 5.94 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385131930531562		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6385131930531562 | validation: 0.67551756826779]
	TIME [epoch: 5.94 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629205057860245		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.629205057860245 | validation: 0.6626587325373463]
	TIME [epoch: 5.94 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300069913551164		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6300069913551164 | validation: 0.6689953742945095]
	TIME [epoch: 5.93 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333639153400605		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6333639153400605 | validation: 0.685269738167494]
	TIME [epoch: 5.94 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311365554961145		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6311365554961145 | validation: 0.6731747452046153]
	TIME [epoch: 5.94 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503933972529506		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6503933972529506 | validation: 0.6794632704693075]
	TIME [epoch: 5.93 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483490611244791		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6483490611244791 | validation: 0.6685179905982862]
	TIME [epoch: 5.93 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367603575438868		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6367603575438868 | validation: 0.6632822176738525]
	TIME [epoch: 5.94 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294780573591507		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6294780573591507 | validation: 0.6702753978170874]
	TIME [epoch: 5.95 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6302873931182391		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6302873931182391 | validation: 0.671117991134411]
	TIME [epoch: 5.95 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307390754821506		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6307390754821506 | validation: 0.6794811561094436]
	TIME [epoch: 5.94 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311460638467488		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6311460638467488 | validation: 0.6686833382983415]
	TIME [epoch: 5.94 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388401809531412		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6388401809531412 | validation: 0.6714999194322775]
	TIME [epoch: 5.94 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629250749780272		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.629250749780272 | validation: 0.6648291641881624]
	TIME [epoch: 5.94 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307763895174646		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6307763895174646 | validation: 0.6752047219001839]
	TIME [epoch: 5.94 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277759194988665		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6277759194988665 | validation: 0.6712092318730107]
	TIME [epoch: 5.93 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409429733472592		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6409429733472592 | validation: 0.68752275841358]
	TIME [epoch: 5.94 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699683326398749		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6699683326398749 | validation: 0.7005799198591719]
	TIME [epoch: 5.94 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725762407678957		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6725762407678957 | validation: 0.6867479541847681]
	TIME [epoch: 5.94 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396074794721267		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.6396074794721267 | validation: 0.6811429805835374]
	TIME [epoch: 5.93 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284788279510855		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6284788279510855 | validation: 0.6665540555228207]
	TIME [epoch: 5.92 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339351456191072		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6339351456191072 | validation: 0.6778298637231498]
	TIME [epoch: 5.93 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253975859960272		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6253975859960272 | validation: 0.6586840831105851]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625306144227832		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.625306144227832 | validation: 0.6560713838387834]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259099248391895		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6259099248391895 | validation: 0.6786747907341613]
	TIME [epoch: 5.93 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174080888032165		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6174080888032165 | validation: 0.6648601228110691]
	TIME [epoch: 5.93 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177601981809078		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6177601981809078 | validation: 0.6574116377649789]
	TIME [epoch: 5.93 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6172175675304713		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6172175675304713 | validation: 0.685750714841232]
	TIME [epoch: 5.93 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646674276530347		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.646674276530347 | validation: 0.6775062572081986]
	TIME [epoch: 5.94 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515425285512271		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6515425285512271 | validation: 0.6571351911786867]
	TIME [epoch: 5.98 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157723287540712		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6157723287540712 | validation: 0.6594077794155476]
	TIME [epoch: 5.92 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243941827701008		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.6243941827701008 | validation: 0.6601871202350829]
	TIME [epoch: 5.93 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6175071352526089		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6175071352526089 | validation: 0.6640446814236741]
	TIME [epoch: 5.93 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298826382365427		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.6298826382365427 | validation: 0.6697283835646348]
	TIME [epoch: 5.93 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208583744567643		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6208583744567643 | validation: 0.6848559316360348]
	TIME [epoch: 5.92 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352179918232124		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6352179918232124 | validation: 0.6832124382660687]
	TIME [epoch: 5.93 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443084259821191		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6443084259821191 | validation: 0.6709014192446587]
	TIME [epoch: 5.93 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6145628532771533		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6145628532771533 | validation: 0.6555545496224524]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201619764761476		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.6201619764761476 | validation: 0.6540468821812153]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6086355742321555		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6086355742321555 | validation: 0.6618168401952734]
	TIME [epoch: 5.93 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028781834194571		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6028781834194571 | validation: 0.631896470707198]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6070411867987427		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.6070411867987427 | validation: 0.6614548981398448]
	TIME [epoch: 5.93 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600407382590482		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.600407382590482 | validation: 0.6426262511943389]
	TIME [epoch: 5.92 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605295678060972		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.605295678060972 | validation: 0.7084561479332039]
	TIME [epoch: 5.93 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6216796918453071		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.6216796918453071 | validation: 0.6464320041395045]
	TIME [epoch: 5.93 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230631041073668		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.6230631041073668 | validation: 0.6769048979487895]
	TIME [epoch: 5.93 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961262241497829		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.5961262241497829 | validation: 0.6283101959693347]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727150254122336		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5727150254122336 | validation: 0.6366542195919345]
	TIME [epoch: 5.92 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5616942197822966		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5616942197822966 | validation: 0.6467249058031307]
	TIME [epoch: 5.93 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.554702760845999		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.554702760845999 | validation: 0.6190504491579377]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558120930871352		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.558120930871352 | validation: 0.7371086124129592]
	TIME [epoch: 5.93 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503354063620225		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6503354063620225 | validation: 0.637342820139634]
	TIME [epoch: 5.92 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574730206812813		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.6574730206812813 | validation: 0.6778260119368927]
	TIME [epoch: 5.93 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.611317558079399		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.611317558079399 | validation: 0.6180514734658864]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544666121952041		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.5544666121952041 | validation: 0.6047666443595636]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5745531735871859		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5745531735871859 | validation: 0.6111701190575529]
	TIME [epoch: 5.91 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5429273146653733		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5429273146653733 | validation: 0.6151670391942633]
	TIME [epoch: 5.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420411461961694		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5420411461961694 | validation: 0.5866789181128964]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323667943065391		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5323667943065391 | validation: 0.6074500736658872]
	TIME [epoch: 5.91 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5228501543887406		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5228501543887406 | validation: 0.5799139131412332]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5220411865172594		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5220411865172594 | validation: 0.5907536904366888]
	TIME [epoch: 5.91 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5093398361906993		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5093398361906993 | validation: 0.6004810530596814]
	TIME [epoch: 5.9 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5104083839236384		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.5104083839236384 | validation: 0.6034575756941741]
	TIME [epoch: 5.91 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025240537577508		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.6025240537577508 | validation: 0.8195934013656218]
	TIME [epoch: 5.92 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046335915307559		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.8046335915307559 | validation: 0.6058227731749484]
	TIME [epoch: 5.91 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487653038608192		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5487653038608192 | validation: 0.6004161558494483]
	TIME [epoch: 5.91 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59078452506717		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.59078452506717 | validation: 0.5551655393868028]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186692397239508		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.5186692397239508 | validation: 0.6012023271804575]
	TIME [epoch: 5.94 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5315066988947799		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5315066988947799 | validation: 0.5469408038581943]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49911312658661283		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.49911312658661283 | validation: 0.5445807227680677]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4976764218460474		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.4976764218460474 | validation: 0.570816349055005]
	TIME [epoch: 5.91 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4924736517340364		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.4924736517340364 | validation: 0.536512178936625]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48503811617343195		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.48503811617343195 | validation: 0.548434184331103]
	TIME [epoch: 5.89 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4767547685615766		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4767547685615766 | validation: 0.5423281210926517]
	TIME [epoch: 5.91 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4754864778685736		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.4754864778685736 | validation: 0.5455246003580573]
	TIME [epoch: 5.91 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47661036988769007		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.47661036988769007 | validation: 0.5300270800959185]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4734235816850418		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4734235816850418 | validation: 0.5426202046973145]
	TIME [epoch: 5.93 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4791303824377295		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.4791303824377295 | validation: 0.5965426370061099]
	TIME [epoch: 5.93 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825914244630951		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5825914244630951 | validation: 0.7088329709365888]
	TIME [epoch: 5.94 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376665397733134		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7376665397733134 | validation: 0.5753769133522901]
	TIME [epoch: 5.94 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439677420119771		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5439677420119771 | validation: 0.5389806560572327]
	TIME [epoch: 5.94 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424412096979077		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5424412096979077 | validation: 0.5090884907358308]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49710856656182406		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.49710856656182406 | validation: 0.5399378032508259]
	TIME [epoch: 5.94 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.502221397703619		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.502221397703619 | validation: 0.5181157708616929]
	TIME [epoch: 5.93 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46311482517713515		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.46311482517713515 | validation: 0.5199289208407641]
	TIME [epoch: 5.94 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46595980996063247		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.46595980996063247 | validation: 0.5455823095603227]
	TIME [epoch: 5.94 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47303185922226465		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.47303185922226465 | validation: 0.5327085222153172]
	TIME [epoch: 5.95 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47652576788602885		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.47652576788602885 | validation: 0.5572126660991511]
	TIME [epoch: 5.94 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5051528418471136		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5051528418471136 | validation: 0.5085623612856304]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47155865182108786		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.47155865182108786 | validation: 0.5202979042439856]
	TIME [epoch: 5.94 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45972856282433766		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.45972856282433766 | validation: 0.49052335217143334]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45108836937550595		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.45108836937550595 | validation: 0.49679802223457403]
	TIME [epoch: 5.94 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4443956029922607		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.4443956029922607 | validation: 0.5027439840002219]
	TIME [epoch: 5.93 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46098718736925487		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.46098718736925487 | validation: 0.5825236706348539]
	TIME [epoch: 5.94 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5611922360957893		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.5611922360957893 | validation: 0.4883413452105302]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45927820992393154		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.45927820992393154 | validation: 0.48344191040333495]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4415981580154961		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.4415981580154961 | validation: 0.4821206085522409]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4356441236963028		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.4356441236963028 | validation: 0.4866692887163607]
	TIME [epoch: 5.91 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4325166198352622		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.4325166198352622 | validation: 0.5015444317270262]
	TIME [epoch: 5.92 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45218195433931185		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.45218195433931185 | validation: 0.5324460676577365]
	TIME [epoch: 5.94 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4939746726662345		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.4939746726662345 | validation: 0.6196177282406545]
	TIME [epoch: 5.93 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428045804515031		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.6428045804515031 | validation: 0.4999811279960324]
	TIME [epoch: 5.94 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45929499818563385		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.45929499818563385 | validation: 0.521997295050399]
	TIME [epoch: 5.94 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5044951310782558		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5044951310782558 | validation: 0.49814634873720137]
	TIME [epoch: 5.94 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46108009138804046		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.46108009138804046 | validation: 0.4991025636031681]
	TIME [epoch: 5.94 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43387222809121534		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.43387222809121534 | validation: 0.4750405648377779]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44147862990298264		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.44147862990298264 | validation: 0.49364934545239364]
	TIME [epoch: 5.91 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43029988123890706		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.43029988123890706 | validation: 0.4699078560174086]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4142898220960077		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.4142898220960077 | validation: 0.47408181805378036]
	TIME [epoch: 5.89 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40954666255775657		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.40954666255775657 | validation: 0.48922266732204434]
	TIME [epoch: 5.9 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42596315367099047		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.42596315367099047 | validation: 0.4790230193058908]
	TIME [epoch: 5.9 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43747065619794007		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.43747065619794007 | validation: 0.538664976245377]
	TIME [epoch: 5.9 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.51524112952202		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.51524112952202 | validation: 0.4609002341077821]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42183091969564857		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.42183091969564857 | validation: 0.48033612600144643]
	TIME [epoch: 5.92 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43460461597015865		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.43460461597015865 | validation: 0.5328341684147172]
	TIME [epoch: 5.92 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4792473867866237		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.4792473867866237 | validation: 0.45968219290808165]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41140900624669147		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.41140900624669147 | validation: 0.457976089258455]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4023334970107408		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.4023334970107408 | validation: 0.45788506067778817]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3999270911786067		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3999270911786067 | validation: 0.4619899358554023]
	TIME [epoch: 5.92 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029865756790219		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.4029865756790219 | validation: 0.4957771148811496]
	TIME [epoch: 5.92 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44077986323574		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.44077986323574 | validation: 0.48639118034116074]
	TIME [epoch: 5.92 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44007638352179823		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.44007638352179823 | validation: 0.5063520512870424]
	TIME [epoch: 5.92 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49882087630865424		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.49882087630865424 | validation: 0.45525864536379146]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4164604283661354		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.4164604283661354 | validation: 0.45923239332231003]
	TIME [epoch: 5.92 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920203681445404		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.3920203681445404 | validation: 0.4706424642936078]
	TIME [epoch: 5.92 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4100522396596775		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4100522396596775 | validation: 0.4877283984657925]
	TIME [epoch: 5.92 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41917024545079173		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.41917024545079173 | validation: 0.524533162827355]
	TIME [epoch: 5.94 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46610017797165265		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.46610017797165265 | validation: 0.44735994840785254]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4011880418646423		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.4011880418646423 | validation: 0.44242968842135]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853447193629785		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.3853447193629785 | validation: 0.47215297932244643]
	TIME [epoch: 5.94 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136402735162535		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.4136402735162535 | validation: 0.48076318362263176]
	TIME [epoch: 5.94 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258579428062997		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.4258579428062997 | validation: 0.5304232992463026]
	TIME [epoch: 5.93 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48173866882802424		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.48173866882802424 | validation: 0.4455939799299886]
	TIME [epoch: 5.94 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3964805557566976		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.3964805557566976 | validation: 0.4759053969614658]
	TIME [epoch: 5.94 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4295922483066019		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4295922483066019 | validation: 0.465325281337093]
	TIME [epoch: 5.94 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399183820653055		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.4399183820653055 | validation: 0.4424606961194604]
	TIME [epoch: 5.93 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735203580990992		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.3735203580990992 | validation: 0.43014241100904976]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37844320427028805		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.37844320427028805 | validation: 0.45682353063778564]
	TIME [epoch: 5.94 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.390730494841956		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.390730494841956 | validation: 0.4507900018790727]
	TIME [epoch: 5.94 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38143960024598444		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.38143960024598444 | validation: 0.4508347747589815]
	TIME [epoch: 5.95 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3946200980793871		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.3946200980793871 | validation: 0.43799601153596407]
	TIME [epoch: 5.94 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37407398097469446		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.37407398097469446 | validation: 0.42643245701266075]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3694552178908708		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.3694552178908708 | validation: 0.4326010862470795]
	TIME [epoch: 5.94 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36743369595222647		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.36743369595222647 | validation: 0.4428496216139415]
	TIME [epoch: 5.94 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950162579676196		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.3950162579676196 | validation: 0.43626760286783084]
	TIME [epoch: 5.94 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38354901903437205		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.38354901903437205 | validation: 0.43968957507300477]
	TIME [epoch: 5.95 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3902693319521755		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.3902693319521755 | validation: 0.41855965222825753]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3591322075617142		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.3591322075617142 | validation: 0.4076344048517725]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35011274257190894		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.35011274257190894 | validation: 0.4165072887884289]
	TIME [epoch: 5.94 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3434293252409961		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.3434293252409961 | validation: 0.40810242099986316]
	TIME [epoch: 5.95 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457412309207263		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.3457412309207263 | validation: 0.4188383337598356]
	TIME [epoch: 5.94 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35357199539193146		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.35357199539193146 | validation: 0.4349500422520892]
	TIME [epoch: 5.95 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4030560444499268		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4030560444499268 | validation: 0.46102232736995374]
	TIME [epoch: 5.95 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39156359880822555		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.39156359880822555 | validation: 0.44876717949030365]
	TIME [epoch: 5.94 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40917107158707955		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.40917107158707955 | validation: 0.37975076157843407]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34385188672110795		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.34385188672110795 | validation: 0.43042960865054314]
	TIME [epoch: 5.94 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37357148116320005		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.37357148116320005 | validation: 0.45652794831316934]
	TIME [epoch: 5.96 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41358339027179225		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.41358339027179225 | validation: 0.39748738168333353]
	TIME [epoch: 5.94 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33529839332061345		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.33529839332061345 | validation: 0.41311310136035895]
	TIME [epoch: 5.94 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3298523792076466		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.3298523792076466 | validation: 0.4220769364802506]
	TIME [epoch: 5.94 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3608021315415943		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.3608021315415943 | validation: 0.44092650970310654]
	TIME [epoch: 5.94 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36828894610744245		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.36828894610744245 | validation: 0.42296970092896063]
	TIME [epoch: 5.94 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3868555266464162		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.3868555266464162 | validation: 0.3904706013203718]
	TIME [epoch: 5.94 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3291685168942861		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.3291685168942861 | validation: 0.39373466250153927]
	TIME [epoch: 5.94 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33093214445929703		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.33093214445929703 | validation: 0.39017485368558114]
	TIME [epoch: 5.94 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534963353680963		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.3534963353680963 | validation: 0.39469139446701784]
	TIME [epoch: 5.94 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3302328250772365		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.3302328250772365 | validation: 0.3932849848287935]
	TIME [epoch: 5.93 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477925575502777		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.3477925575502777 | validation: 0.3828625380453358]
	TIME [epoch: 277 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3232575325842532		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.3232575325842532 | validation: 0.3720728697352764]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038347931999476		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.3038347931999476 | validation: 0.3855164307577328]
	TIME [epoch: 12.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31125230225742295		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.31125230225742295 | validation: 0.3940791664358495]
	TIME [epoch: 12.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3373389852801889		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.3373389852801889 | validation: 0.4491893959685598]
	TIME [epoch: 12.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3627262605669756		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.3627262605669756 | validation: 0.431304329611638]
	TIME [epoch: 12.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41225718997001454		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.41225718997001454 | validation: 0.3814421487865668]
	TIME [epoch: 12.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31756699381373793		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.31756699381373793 | validation: 0.4004047305253005]
	TIME [epoch: 12.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34373267462513857		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.34373267462513857 | validation: 0.37744003951666233]
	TIME [epoch: 12.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3417807251543934		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.3417807251543934 | validation: 0.37930035609651264]
	TIME [epoch: 12.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2930597610905284		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.2930597610905284 | validation: 0.3635771432824302]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29742415922681065		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.29742415922681065 | validation: 0.3381961559554141]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2974110875229992		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.2974110875229992 | validation: 0.37831899614299386]
	TIME [epoch: 12.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3068166521903726		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.3068166521903726 | validation: 0.40456275012008636]
	TIME [epoch: 12.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3573879092783493		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.3573879092783493 | validation: 0.36680862114393553]
	TIME [epoch: 12.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2946729052523488		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.2946729052523488 | validation: 0.3280972348218951]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2850395223067422		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.2850395223067422 | validation: 0.3452826446746132]
	TIME [epoch: 12.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28562374367623333		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.28562374367623333 | validation: 0.3537057648605764]
	TIME [epoch: 12.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2850487306757749		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.2850487306757749 | validation: 0.3502922750064927]
	TIME [epoch: 12.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740453055755478		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.2740453055755478 | validation: 0.358696201441475]
	TIME [epoch: 12.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2796412299608079		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.2796412299608079 | validation: 0.34494631065113296]
	TIME [epoch: 12.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2767182624224209		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.2767182624224209 | validation: 0.4221344179307336]
	TIME [epoch: 12.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3396982810185497		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.3396982810185497 | validation: 0.4463006815253078]
	TIME [epoch: 12.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5064757710469318		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.5064757710469318 | validation: 0.3915103951987172]
	TIME [epoch: 12.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31584640661549684		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.31584640661549684 | validation: 0.4624207355460863]
	TIME [epoch: 12.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3884849221511128		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.3884849221511128 | validation: 0.34276088338004873]
	TIME [epoch: 12.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162128100586454		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.3162128100586454 | validation: 0.3295496635379683]
	TIME [epoch: 12.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28752729006256245		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.28752729006256245 | validation: 0.38563681337722344]
	TIME [epoch: 12.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038084349825807		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.3038084349825807 | validation: 0.3400364560022587]
	TIME [epoch: 12.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2772697603881784		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.2772697603881784 | validation: 0.3338224094503541]
	TIME [epoch: 12.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2595603340027248		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.2595603340027248 | validation: 0.34299730574256276]
	TIME [epoch: 12.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618574143561107		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.2618574143561107 | validation: 0.3247350729483186]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2608426132181319		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.2608426132181319 | validation: 0.3411717665168005]
	TIME [epoch: 12.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26207764295998304		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.26207764295998304 | validation: 0.3358935219076995]
	TIME [epoch: 12.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2793985228041044		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.2793985228041044 | validation: 0.3765495069418374]
	TIME [epoch: 12.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27520291595161905		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.27520291595161905 | validation: 0.3320871123032387]
	TIME [epoch: 12.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2980593289250858		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.2980593289250858 | validation: 0.32557901191272287]
	TIME [epoch: 12.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526860541584372		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.2526860541584372 | validation: 0.32077364881818665]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24254367709039332		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.24254367709039332 | validation: 0.32443913125045376]
	TIME [epoch: 12.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24580885798009838		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.24580885798009838 | validation: 0.3401857408498623]
	TIME [epoch: 12.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24253079665811889		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.24253079665811889 | validation: 0.31083616445759144]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26606340598251715		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.26606340598251715 | validation: 0.4003843369529885]
	TIME [epoch: 12.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145058230035174		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.3145058230035174 | validation: 0.3512795432959777]
	TIME [epoch: 12.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37484596973574424		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.37484596973574424 | validation: 0.3121740852575216]
	TIME [epoch: 12.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26260250385160405		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.26260250385160405 | validation: 0.38771032489455476]
	TIME [epoch: 12.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318837899514046		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.318837899514046 | validation: 0.31131461845521624]
	TIME [epoch: 12.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29156182422438665		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.29156182422438665 | validation: 0.3099080467452118]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24186139269050763		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.24186139269050763 | validation: 0.367292976642553]
	TIME [epoch: 12.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27751608629509344		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.27751608629509344 | validation: 0.31478651649370254]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28202402198997356		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.28202402198997356 | validation: 0.30289290490356785]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22986757067504793		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.22986757067504793 | validation: 0.34393098818868356]
	TIME [epoch: 12.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25039150592678155		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.25039150592678155 | validation: 0.3167902385489305]
	TIME [epoch: 12.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26866535196062935		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.26866535196062935 | validation: 0.3066725588362486]
	TIME [epoch: 12.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22884103019456078		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.22884103019456078 | validation: 0.30349298383903056]
	TIME [epoch: 12.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22616491960203924		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.22616491960203924 | validation: 0.2994406277483701]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23628351157926056		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.23628351157926056 | validation: 0.3542852721266376]
	TIME [epoch: 12.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2491378167635872		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.2491378167635872 | validation: 0.3142341195398066]
	TIME [epoch: 12.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28589343267547085		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.28589343267547085 | validation: 0.29745861004240004]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22034331099137455		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.22034331099137455 | validation: 0.30044858509313493]
	TIME [epoch: 12.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22362631759326726		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.22362631759326726 | validation: 0.2884603348504233]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23192651882294357		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.23192651882294357 | validation: 0.3572658376144839]
	TIME [epoch: 12.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24357703394357103		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.24357703394357103 | validation: 0.3032931965196054]
	TIME [epoch: 12.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2631065780970011		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.2631065780970011 | validation: 0.3237439086508709]
	TIME [epoch: 12.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22557759768872884		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.22557759768872884 | validation: 0.28396579769116387]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121018856844394		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.2121018856844394 | validation: 0.28353108942764277]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21124998479308374		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.21124998479308374 | validation: 0.2973794185056849]
	TIME [epoch: 12.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20719644035006896		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.20719644035006896 | validation: 0.2880045363374276]
	TIME [epoch: 12.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20397185238073673		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.20397185238073673 | validation: 0.27955852699721423]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037717306846647		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.2037717306846647 | validation: 0.2799492470329416]
	TIME [epoch: 12.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20541650166949363		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.20541650166949363 | validation: 0.28757292574294196]
	TIME [epoch: 12.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19932564159123567		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.19932564159123567 | validation: 0.27881486354548485]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19463752234370968		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.19463752234370968 | validation: 0.2817987022060116]
	TIME [epoch: 12.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20033945661294322		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.20033945661294322 | validation: 0.2799379591637187]
	TIME [epoch: 12.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20108876814824456		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.20108876814824456 | validation: 0.28768197285826375]
	TIME [epoch: 12.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2396761457770776		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.2396761457770776 | validation: 0.5115172581137614]
	TIME [epoch: 12.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4291033646439668		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.4291033646439668 | validation: 0.2913961161209471]
	TIME [epoch: 12.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33114724468975		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.33114724468975 | validation: 0.2991405172062556]
	TIME [epoch: 12.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25304827981965644		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.25304827981965644 | validation: 0.33531403518689074]
	TIME [epoch: 12.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23906041681664314		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.23906041681664314 | validation: 0.2686354336402828]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1986528332399408		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1986528332399408 | validation: 0.26378005700271695]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20639984986891985		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.20639984986891985 | validation: 0.29527861353754487]
	TIME [epoch: 12.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19645836688698182		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.19645836688698182 | validation: 0.28129750421087324]
	TIME [epoch: 12.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1909312404761912		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1909312404761912 | validation: 0.273739613561051]
	TIME [epoch: 12.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887361305705482		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1887361305705482 | validation: 0.2758194939914647]
	TIME [epoch: 12.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19365591143435368		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.19365591143435368 | validation: 0.2834664931244334]
	TIME [epoch: 12.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887100962081244		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1887100962081244 | validation: 0.26920849268740893]
	TIME [epoch: 12.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20110286670806532		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.20110286670806532 | validation: 0.29836424101703596]
	TIME [epoch: 12.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20268979619795002		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.20268979619795002 | validation: 0.26431191712059526]
	TIME [epoch: 12.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20731914629756099		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.20731914629756099 | validation: 0.2784977886515008]
	TIME [epoch: 12.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18609825476812425		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.18609825476812425 | validation: 0.2590095452932801]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19326973390818825		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.19326973390818825 | validation: 0.2687716306411422]
	TIME [epoch: 12.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18541105731136573		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.18541105731136573 | validation: 0.25512698048413024]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19613363794239266		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.19613363794239266 | validation: 0.3036019541716119]
	TIME [epoch: 12.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2003065835503841		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.2003065835503841 | validation: 0.263297255011254]
	TIME [epoch: 12.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23143744414486966		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.23143744414486966 | validation: 0.2712826266227618]
	TIME [epoch: 12.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18749014613696183		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.18749014613696183 | validation: 0.24687936202722083]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17460138879196863		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.17460138879196863 | validation: 0.2774804810241035]
	TIME [epoch: 12.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17608552137919062		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.17608552137919062 | validation: 0.25783187651408773]
	TIME [epoch: 12.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18045147456828795		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.18045147456828795 | validation: 0.28545173974301424]
	TIME [epoch: 12.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1890036616542031		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1890036616542031 | validation: 0.2694023356561307]
	TIME [epoch: 12.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253659212729101		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.253659212729101 | validation: 0.2744145633322478]
	TIME [epoch: 12.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19270664419926553		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.19270664419926553 | validation: 0.2339861615867298]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17683345273155063		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.17683345273155063 | validation: 0.24884428197888037]
	TIME [epoch: 12.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749152033664218		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1749152033664218 | validation: 0.2603860962152906]
	TIME [epoch: 12.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17771764197012974		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.17771764197012974 | validation: 0.3096562195507384]
	TIME [epoch: 12.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20143247521537055		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.20143247521537055 | validation: 0.261636653082917]
	TIME [epoch: 12.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787198933611594		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.2787198933611594 | validation: 0.25280874090369443]
	TIME [epoch: 12.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17525895833264923		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.17525895833264923 | validation: 0.2769663323857874]
	TIME [epoch: 12.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984237760351695		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.1984237760351695 | validation: 0.24665229548559925]
	TIME [epoch: 12.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22875676299300324		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.22875676299300324 | validation: 0.24390898172853637]
	TIME [epoch: 12.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16605944572784778		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.16605944572784778 | validation: 0.2599233155088462]
	TIME [epoch: 12.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17861581505883575		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.17861581505883575 | validation: 0.23893471674847266]
	TIME [epoch: 12.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21295282849789762		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.21295282849789762 | validation: 0.2610558800184777]
	TIME [epoch: 12.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781297311684922		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1781297311684922 | validation: 0.23711868114833787]
	TIME [epoch: 12.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1760570490557813		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.1760570490557813 | validation: 0.2529657827092128]
	TIME [epoch: 12.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16956002151744032		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.16956002151744032 | validation: 0.24098354032717842]
	TIME [epoch: 12.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16684352268072425		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.16684352268072425 | validation: 0.2649620438244524]
	TIME [epoch: 12.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734319279160685		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1734319279160685 | validation: 0.24123144050267453]
	TIME [epoch: 12.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20585193971777174		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.20585193971777174 | validation: 0.28382105716098555]
	TIME [epoch: 12.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18479510738528526		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.18479510738528526 | validation: 0.23864742584849374]
	TIME [epoch: 12.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18415650001944803		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.18415650001944803 | validation: 0.25879630896462014]
	TIME [epoch: 12.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1647640170606834		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1647640170606834 | validation: 0.2352442592036983]
	TIME [epoch: 12.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16402115191998576		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.16402115191998576 | validation: 0.24438765730665937]
	TIME [epoch: 12.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16130688063245302		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.16130688063245302 | validation: 0.2227984909950215]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16598293147682355		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.16598293147682355 | validation: 0.2682756841591103]
	TIME [epoch: 12.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16495241436157634		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.16495241436157634 | validation: 0.2195522879931905]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20222601506032406		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.20222601506032406 | validation: 0.29013972971307195]
	TIME [epoch: 12.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20092721078436399		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.20092721078436399 | validation: 0.24731179479206392]
	TIME [epoch: 12.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22423616404375044		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.22423616404375044 | validation: 0.24951979331021512]
	TIME [epoch: 12.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646471345652447		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.1646471345652447 | validation: 0.26769836278665116]
	TIME [epoch: 12.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18548403432310004		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.18548403432310004 | validation: 0.22941688685135286]
	TIME [epoch: 12.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21580454461426363		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.21580454461426363 | validation: 0.27718193594340823]
	TIME [epoch: 12.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16686926512218433		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.16686926512218433 | validation: 0.23724462488705794]
	TIME [epoch: 12.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15800219123632608		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.15800219123632608 | validation: 0.22413414385073557]
	TIME [epoch: 12.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16471600118063276		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.16471600118063276 | validation: 0.2518498187323834]
	TIME [epoch: 12.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1705200389394823		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.1705200389394823 | validation: 0.2228550955682995]
	TIME [epoch: 12.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176600236652522		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.176600236652522 | validation: 0.24608531470250564]
	TIME [epoch: 12.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16269338327395477		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.16269338327395477 | validation: 0.2368655091399542]
	TIME [epoch: 12.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16129882530867273		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.16129882530867273 | validation: 0.2347196737691368]
	TIME [epoch: 12.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1596842968129898		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.1596842968129898 | validation: 0.21544212369144822]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17455084794545614		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.17455084794545614 | validation: 0.2452443256296483]
	TIME [epoch: 12.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850093255566648		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.16850093255566648 | validation: 0.22162675305908888]
	TIME [epoch: 12.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17883780630326926		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.17883780630326926 | validation: 0.24315458107952959]
	TIME [epoch: 12.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16674607227479862		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.16674607227479862 | validation: 0.2155439963698562]
	TIME [epoch: 12.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16544404661028672		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.16544404661028672 | validation: 0.2335449741198656]
	TIME [epoch: 12.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15658214617068575		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.15658214617068575 | validation: 0.20512133634093313]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15806272886469117		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.15806272886469117 | validation: 0.2537673985019216]
	TIME [epoch: 12.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17073886720984313		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.17073886720984313 | validation: 0.23660809329202595]
	TIME [epoch: 12.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028374207158258		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.2028374207158258 | validation: 0.24467795850853957]
	TIME [epoch: 12.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15311284433716707		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.15311284433716707 | validation: 0.20079069102999988]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15048587857401705		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.15048587857401705 | validation: 0.21610120384514972]
	TIME [epoch: 12.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15474978021475252		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.15474978021475252 | validation: 0.265166563713167]
	TIME [epoch: 12.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16870399782592002		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.16870399782592002 | validation: 0.23130772276427095]
	TIME [epoch: 12.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19263623360179175		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.19263623360179175 | validation: 0.21301760043934467]
	TIME [epoch: 12.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14722747293113733		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.14722747293113733 | validation: 0.22304965223074655]
	TIME [epoch: 12.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15201221575069204		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.15201221575069204 | validation: 0.21636106554982604]
	TIME [epoch: 12.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15351894655587686		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.15351894655587686 | validation: 0.2449932763917882]
	TIME [epoch: 12.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16677595519965366		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.16677595519965366 | validation: 0.2449769543460338]
	TIME [epoch: 12.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1959238553237729		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.1959238553237729 | validation: 0.23059683972575085]
	TIME [epoch: 12.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14677817090662365		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.14677817090662365 | validation: 0.2185205202303025]
	TIME [epoch: 12.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14979160764079585		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.14979160764079585 | validation: 0.22580816107394164]
	TIME [epoch: 12.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15287270349334967		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.15287270349334967 | validation: 0.2396678791993665]
	TIME [epoch: 12.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17085518217849713		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.17085518217849713 | validation: 0.2187771510078855]
	TIME [epoch: 12.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20536226776960553		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.20536226776960553 | validation: 0.2143246468517954]
	TIME [epoch: 12.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14833774084917983		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.14833774084917983 | validation: 0.22656077571746744]
	TIME [epoch: 12.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445031580407255		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1445031580407255 | validation: 0.20384175179184158]
	TIME [epoch: 12.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15590986486710795		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.15590986486710795 | validation: 0.2388536626659309]
	TIME [epoch: 12.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15749341937738076		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.15749341937738076 | validation: 0.21733346049126823]
	TIME [epoch: 12.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16688952096797566		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.16688952096797566 | validation: 0.23760257811245178]
	TIME [epoch: 12.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1656448005772395		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1656448005772395 | validation: 0.19929684730393687]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17756275555774068		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.17756275555774068 | validation: 0.22541260472069702]
	TIME [epoch: 12.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14523345882666833		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.14523345882666833 | validation: 0.21250809001312582]
	TIME [epoch: 12.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369107298142992		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.1369107298142992 | validation: 0.20746438772480122]
	TIME [epoch: 12.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14462671739477956		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.14462671739477956 | validation: 0.23451821423956265]
	TIME [epoch: 12.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16098961714956006		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.16098961714956006 | validation: 0.23074387217139336]
	TIME [epoch: 12.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18464734722075424		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.18464734722075424 | validation: 0.21677619582434718]
	TIME [epoch: 12.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14949863656782655		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.14949863656782655 | validation: 0.21053570665019278]
	TIME [epoch: 12.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430258994934894		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.1430258994934894 | validation: 0.22226892903986833]
	TIME [epoch: 12.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13936173205081434		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.13936173205081434 | validation: 0.21228527889846271]
	TIME [epoch: 12.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13751345075920857		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.13751345075920857 | validation: 0.19867190900676734]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14301182216099234		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.14301182216099234 | validation: 0.22903101206214949]
	TIME [epoch: 12.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15788603834101453		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.15788603834101453 | validation: 0.19767385813626173]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19841267622497263		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.19841267622497263 | validation: 0.21511673931880615]
	TIME [epoch: 12.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14604623496986385		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.14604623496986385 | validation: 0.22245905854191392]
	TIME [epoch: 12.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14807679678307734		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.14807679678307734 | validation: 0.22274178200021655]
	TIME [epoch: 12.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14035821537067653		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.14035821537067653 | validation: 0.21915163543215327]
	TIME [epoch: 12.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16776857234754494		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.16776857234754494 | validation: 0.22870285142559876]
	TIME [epoch: 12.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1586632333632194		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.1586632333632194 | validation: 0.19392609530294366]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17231004499441502		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.17231004499441502 | validation: 0.20822215668353997]
	TIME [epoch: 12.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13967055696975458		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.13967055696975458 | validation: 0.1969824241850885]
	TIME [epoch: 12.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13765871390716425		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.13765871390716425 | validation: 0.18963674181305776]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13517040132356192		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.13517040132356192 | validation: 0.2200116192315277]
	TIME [epoch: 12.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13907850821804071		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.13907850821804071 | validation: 0.19591236513324348]
	TIME [epoch: 12.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594252254229806		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.1594252254229806 | validation: 0.2336211331031556]
	TIME [epoch: 12.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605789682803346		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.1605789682803346 | validation: 0.22212733070527496]
	TIME [epoch: 12.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18077704109728887		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.18077704109728887 | validation: 0.20646572021285456]
	TIME [epoch: 12.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13879960311013104		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.13879960311013104 | validation: 0.2051960306824419]
	TIME [epoch: 12.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375536947442011		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1375536947442011 | validation: 0.21910920138730502]
	TIME [epoch: 12.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1437921114284295		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1437921114284295 | validation: 0.2349242474036336]
	TIME [epoch: 12.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15506999271271946		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.15506999271271946 | validation: 0.20019718134168893]
	TIME [epoch: 12.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19040668624494025		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.19040668624494025 | validation: 0.21314450930703802]
	TIME [epoch: 12.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13389734979477919		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.13389734979477919 | validation: 0.21141736462816774]
	TIME [epoch: 12.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13969816475865227		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.13969816475865227 | validation: 0.19197562097780857]
	TIME [epoch: 12.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500895014206353		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.1500895014206353 | validation: 0.21504854599156564]
	TIME [epoch: 12.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14689879315811438		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.14689879315811438 | validation: 0.20450368626161436]
	TIME [epoch: 12.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725126501486342		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.15725126501486342 | validation: 0.21337331578818405]
	TIME [epoch: 12.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13543312962188478		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.13543312962188478 | validation: 0.19779844431453766]
	TIME [epoch: 12.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064426532444626		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.13064426532444626 | validation: 0.19723369214500328]
	TIME [epoch: 12.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13386981372593673		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.13386981372593673 | validation: 0.19761757032828453]
	TIME [epoch: 12.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13286311409674498		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.13286311409674498 | validation: 0.20566031746232094]
	TIME [epoch: 12.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381199198533149		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.1381199198533149 | validation: 0.1901353945389357]
	TIME [epoch: 12.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13951201465004417		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.13951201465004417 | validation: 0.21900157611525245]
	TIME [epoch: 12.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17378446109108048		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.17378446109108048 | validation: 0.22501739496210882]
	TIME [epoch: 12.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14798601414396942		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.14798601414396942 | validation: 0.16269264419201046]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15900175083391074		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.15900175083391074 | validation: 0.21661078916518509]
	TIME [epoch: 12.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13621218277376915		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.13621218277376915 | validation: 0.2180680919027833]
	TIME [epoch: 12.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14049205710245738		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.14049205710245738 | validation: 0.1965091061858032]
	TIME [epoch: 12.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13059380595516545		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.13059380595516545 | validation: 0.18427866104179882]
	TIME [epoch: 12.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463546062039363		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.1463546062039363 | validation: 0.2137847154744651]
	TIME [epoch: 12.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15078458645025497		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.15078458645025497 | validation: 0.19380220372672574]
	TIME [epoch: 12.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1643301475948557		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.1643301475948557 | validation: 0.19172307417458925]
	TIME [epoch: 12.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13280441436330723		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.13280441436330723 | validation: 0.20165580034773434]
	TIME [epoch: 12.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270725484931784		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.1270725484931784 | validation: 0.2014708565301545]
	TIME [epoch: 12.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13201329965191377		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.13201329965191377 | validation: 0.1977750865325848]
	TIME [epoch: 12.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12655539028428187		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.12655539028428187 | validation: 0.1892642931866918]
	TIME [epoch: 12.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13880042405707232		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.13880042405707232 | validation: 0.2123536133882532]
	TIME [epoch: 12.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14849051602477276		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.14849051602477276 | validation: 0.18393106596969322]
	TIME [epoch: 12.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16608283473083138		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.16608283473083138 | validation: 0.19576234891474165]
	TIME [epoch: 12.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12952233496899462		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.12952233496899462 | validation: 0.1884359947339389]
	TIME [epoch: 12.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13079456847287246		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.13079456847287246 | validation: 0.1930121016689228]
	TIME [epoch: 12.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377509749239279		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.1377509749239279 | validation: 0.2094627917961184]
	TIME [epoch: 12.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444048680542107		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.1444048680542107 | validation: 0.17964532146167814]
	TIME [epoch: 12.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15909494459385698		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.15909494459385698 | validation: 0.1955251159804337]
	TIME [epoch: 12.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13252499117837352		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.13252499117837352 | validation: 0.1989974568308166]
	TIME [epoch: 12.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12476398458458697		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.12476398458458697 | validation: 0.17836500177263348]
	TIME [epoch: 12.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12645673880523833		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.12645673880523833 | validation: 0.1950228348313524]
	TIME [epoch: 12.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12382786908012883		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.12382786908012883 | validation: 0.18667152225359235]
	TIME [epoch: 12.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12452839773406801		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.12452839773406801 | validation: 0.18529552715214692]
	TIME [epoch: 12.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878401524819616		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.12878401524819616 | validation: 0.2017266570037335]
	TIME [epoch: 12.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14561834818033836		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.14561834818033836 | validation: 0.211930768848415]
	TIME [epoch: 12.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18812175246987828		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.18812175246987828 | validation: 0.1854829110914606]
	TIME [epoch: 12.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12353923098227478		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.12353923098227478 | validation: 0.19737713121397943]
	TIME [epoch: 12.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13995105265569335		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.13995105265569335 | validation: 0.19348816010174763]
	TIME [epoch: 12.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16929766538611418		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.16929766538611418 | validation: 0.20084458594393995]
	TIME [epoch: 12.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12724430009009097		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.12724430009009097 | validation: 0.20119699325844742]
	TIME [epoch: 12.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14358927556487927		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.14358927556487927 | validation: 0.18285957629576444]
	TIME [epoch: 12.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16299760951493167		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.16299760951493167 | validation: 0.21726869077611488]
	TIME [epoch: 12.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13581241449423032		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.13581241449423032 | validation: 0.17724344750885085]
	TIME [epoch: 12.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1192622459428108		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.1192622459428108 | validation: 0.16448705186237966]
	TIME [epoch: 12.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12419960223901357		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.12419960223901357 | validation: 0.21332031454373346]
	TIME [epoch: 12.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13265855202710974		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.13265855202710974 | validation: 0.18496267727799848]
	TIME [epoch: 12.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13670497290266523		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.13670497290266523 | validation: 0.18674219863132702]
	TIME [epoch: 12.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13039504141131983		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.13039504141131983 | validation: 0.18095156367823684]
	TIME [epoch: 12.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327172248367806		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.12327172248367806 | validation: 0.1907761544831598]
	TIME [epoch: 12.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12382731100601958		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.12382731100601958 | validation: 0.1639046316516911]
	TIME [epoch: 12.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13384606961401344		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.13384606961401344 | validation: 0.2095426319051896]
	TIME [epoch: 12.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13824186335295252		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.13824186335295252 | validation: 0.19485643721874674]
	TIME [epoch: 12.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646175849372797		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.1646175849372797 | validation: 0.19783619694755528]
	TIME [epoch: 12.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12768048677374896		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.12768048677374896 | validation: 0.1798835371545174]
	TIME [epoch: 12.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315410475720532		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.1315410475720532 | validation: 0.18087580315694607]
	TIME [epoch: 12.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316693920942069		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.1316693920942069 | validation: 0.20076751589103659]
	TIME [epoch: 12.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.134397856421609		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.134397856421609 | validation: 0.18085325566718904]
	TIME [epoch: 12.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14111118525201247		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.14111118525201247 | validation: 0.1824816646980236]
	TIME [epoch: 12.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12251512572308355		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.12251512572308355 | validation: 0.17815256477473085]
	TIME [epoch: 12.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12502286902369567		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.12502286902369567 | validation: 0.18363956956344696]
	TIME [epoch: 12.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11988057302275276		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.11988057302275276 | validation: 0.17350662103184877]
	TIME [epoch: 12.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12383559925063053		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.12383559925063053 | validation: 0.19481131900248433]
	TIME [epoch: 12.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13173757286601626		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.13173757286601626 | validation: 0.18575963660457306]
	TIME [epoch: 12.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14682372230084376		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.14682372230084376 | validation: 0.1916665598854892]
	TIME [epoch: 12.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13382503721861802		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.13382503721861802 | validation: 0.19794938180213256]
	TIME [epoch: 12.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14034659868507413		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.14034659868507413 | validation: 0.19878066258079233]
	TIME [epoch: 12.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12710212637182156		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.12710212637182156 | validation: 0.16339332150537875]
	TIME [epoch: 12.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12134528942839928		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.12134528942839928 | validation: 0.15665848547949004]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11870640575959975		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.11870640575959975 | validation: 0.18919925309756194]
	TIME [epoch: 12.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12072802110215332		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.12072802110215332 | validation: 0.16111116208825482]
	TIME [epoch: 12.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12093023242956996		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.12093023242956996 | validation: 0.15766117123525106]
	TIME [epoch: 12.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13463170600937768		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.13463170600937768 | validation: 0.1978106725988188]
	TIME [epoch: 12.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13977128996149138		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.13977128996149138 | validation: 0.187703901771872]
	TIME [epoch: 12.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15815471635788486		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.15815471635788486 | validation: 0.17047979966689095]
	TIME [epoch: 12.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12234566083589918		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.12234566083589918 | validation: 0.18579788859078916]
	TIME [epoch: 12.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12586362949767416		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.12586362949767416 | validation: 0.17603900511223933]
	TIME [epoch: 12.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14177902356307345		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.14177902356307345 | validation: 0.19660780545649953]
	TIME [epoch: 12.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12422157137582523		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.12422157137582523 | validation: 0.17026478679404525]
	TIME [epoch: 12.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12202021041805339		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.12202021041805339 | validation: 0.17322783786279067]
	TIME [epoch: 12.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522971224938454		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.11522971224938454 | validation: 0.18793774468680866]
	TIME [epoch: 12.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11832276179040516		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.11832276179040516 | validation: 0.16271808418733524]
	TIME [epoch: 12.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994174008977834		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.11994174008977834 | validation: 0.17612190878063494]
	TIME [epoch: 12.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11884622963571044		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.11884622963571044 | validation: 0.18336187589750486]
	TIME [epoch: 12.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11682183994650121		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.11682183994650121 | validation: 0.1735548038458445]
	TIME [epoch: 12.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1155299453628516		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.1155299453628516 | validation: 0.17391482871167496]
	TIME [epoch: 12.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12188231625383908		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.12188231625383908 | validation: 0.19810464229155045]
	TIME [epoch: 12.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12494423512922989		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.12494423512922989 | validation: 0.2360876209766219]
	TIME [epoch: 12.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16269047312372378		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.16269047312372378 | validation: 0.1830170659969085]
	TIME [epoch: 12.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18554811890352574		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.18554811890352574 | validation: 0.16301965150686393]
	TIME [epoch: 12.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13279601195608895		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.13279601195608895 | validation: 0.22282310989121062]
	TIME [epoch: 12.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16430567605955626		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.16430567605955626 | validation: 0.18546586347230082]
	TIME [epoch: 12.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339492600771774		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.1339492600771774 | validation: 0.16347181576653044]
	TIME [epoch: 12.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12137568015748106		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.12137568015748106 | validation: 0.18942493766432816]
	TIME [epoch: 12.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1237084934238373		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.1237084934238373 | validation: 0.1628935376739066]
	TIME [epoch: 12.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11673132802666392		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.11673132802666392 | validation: 0.15438259981807106]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11525215714392012		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.11525215714392012 | validation: 0.18275281550400457]
	TIME [epoch: 12.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1184661227734663		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.1184661227734663 | validation: 0.17396039798652857]
	TIME [epoch: 12.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11447697120493816		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.11447697120493816 | validation: 0.16834668534774222]
	TIME [epoch: 12.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12008267560813153		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.12008267560813153 | validation: 0.1653896337544674]
	TIME [epoch: 12.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11685514042776224		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.11685514042776224 | validation: 0.1809111697729444]
	TIME [epoch: 12.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11753635165625635		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.11753635165625635 | validation: 0.15375244940268087]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362426320122669		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.12362426320122669 | validation: 0.17488135069309352]
	TIME [epoch: 12.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11711862970195902		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.11711862970195902 | validation: 0.16457582337044524]
	TIME [epoch: 12.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11700817401648017		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.11700817401648017 | validation: 0.17721289403870602]
	TIME [epoch: 12.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12340142784322432		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.12340142784322432 | validation: 0.17509017805878935]
	TIME [epoch: 12.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13448269622630796		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.13448269622630796 | validation: 0.17171899734274879]
	TIME [epoch: 12.5 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12091114036704464		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.12091114036704464 | validation: 0.1592003008010682]
	TIME [epoch: 12.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1115353958446174		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.1115353958446174 | validation: 0.16404194458355878]
	TIME [epoch: 12.5 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11498971531916104		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.11498971531916104 | validation: 0.17885319240269798]
	TIME [epoch: 12.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11160349449798719		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.11160349449798719 | validation: 0.15942891576298623]
	TIME [epoch: 12.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310340339076799		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.11310340339076799 | validation: 0.1600716702023135]
	TIME [epoch: 12.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10972170543506195		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.10972170543506195 | validation: 0.1820377351018204]
	TIME [epoch: 12.5 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11335430968304624		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.11335430968304624 | validation: 0.17193618052732726]
	TIME [epoch: 12.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11617127168853657		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.11617127168853657 | validation: 0.1811678462263662]
	TIME [epoch: 12.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261775996346641		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.1261775996346641 | validation: 0.19320631426692333]
	TIME [epoch: 12.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17044993424291663		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.17044993424291663 | validation: 0.16648455383985783]
	TIME [epoch: 12.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11413269855254257		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.11413269855254257 | validation: 0.18611854883559495]
	TIME [epoch: 12.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12909554194684938		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.12909554194684938 | validation: 0.17866671593925]
	TIME [epoch: 12.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13520978194895125		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.13520978194895125 | validation: 0.16929957381198021]
	TIME [epoch: 12.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11090346404473218		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.11090346404473218 | validation: 0.17497647413071432]
	TIME [epoch: 12.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663880942131737		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.11663880942131737 | validation: 0.15465751387830648]
	TIME [epoch: 12.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12093312655741226		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.12093312655741226 | validation: 0.18030884976949968]
	TIME [epoch: 12.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11799799786421676		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.11799799786421676 | validation: 0.15803662801452084]
	TIME [epoch: 12.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1124050105548947		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.1124050105548947 | validation: 0.146042087682518]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11152372988990272		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.11152372988990272 | validation: 0.17186733341003332]
	TIME [epoch: 12.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11458620803788282		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.11458620803788282 | validation: 0.15858659207039327]
	TIME [epoch: 12.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1193583195963282		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.1193583195963282 | validation: 0.1625912133199277]
	TIME [epoch: 12.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12585892135467228		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.12585892135467228 | validation: 0.1951621197769851]
	TIME [epoch: 12.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299353777894059		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.12299353777894059 | validation: 0.16672467653847023]
	TIME [epoch: 12.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287777868995876		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.1287777868995876 | validation: 0.17732447946721833]
	TIME [epoch: 12.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1106028924919525		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.1106028924919525 | validation: 0.16521707191351515]
	TIME [epoch: 12.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1170719651187679		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.1170719651187679 | validation: 0.1636697893930762]
	TIME [epoch: 12.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12558884255430167		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.12558884255430167 | validation: 0.1681091723058663]
	TIME [epoch: 12.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116012810068677		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.1116012810068677 | validation: 0.16687751243036772]
	TIME [epoch: 12.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11142267042366846		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.11142267042366846 | validation: 0.18325002986671005]
	TIME [epoch: 12.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913302123408397		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10913302123408397 | validation: 0.15060132969061812]
	TIME [epoch: 12.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10909384149815538		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.10909384149815538 | validation: 0.15058754770355043]
	TIME [epoch: 12.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081262253421305		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.11081262253421305 | validation: 0.16838768892433567]
	TIME [epoch: 12.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10882023341595297		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.10882023341595297 | validation: 0.1535077833606196]
	TIME [epoch: 12.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1210837235975653		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.1210837235975653 | validation: 0.19864729656574803]
	TIME [epoch: 12.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797272463776938		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.13797272463776938 | validation: 0.17721577206125755]
	TIME [epoch: 12.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440885676526192		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.1440885676526192 | validation: 0.1430333168541334]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11108727703886491		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.11108727703886491 | validation: 0.18538496506888327]
	TIME [epoch: 12.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431355936387732		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.12431355936387732 | validation: 0.1726391124932056]
	TIME [epoch: 12.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292499529810573		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.1292499529810573 | validation: 0.16699472276747285]
	TIME [epoch: 12.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11089664962779829		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.11089664962779829 | validation: 0.18345123557870976]
	TIME [epoch: 12.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13274036918182266		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.13274036918182266 | validation: 0.1726054870093072]
	TIME [epoch: 12.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316634892938569		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.1316634892938569 | validation: 0.1728836038466843]
	TIME [epoch: 12.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10919631605719005		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.10919631605719005 | validation: 0.16389896159801276]
	TIME [epoch: 12.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12022105189114082		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.12022105189114082 | validation: 0.15881193203301205]
	TIME [epoch: 12.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12075167235249566		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.12075167235249566 | validation: 0.1488816848924522]
	TIME [epoch: 12.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136064022876514		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.11136064022876514 | validation: 0.18371526760485785]
	TIME [epoch: 12.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10636061156924317		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.10636061156924317 | validation: 0.1403321926591725]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1102435207731714		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.1102435207731714 | validation: 0.16529995122040608]
	TIME [epoch: 12.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10872399513204198		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.10872399513204198 | validation: 0.16312272565643818]
	TIME [epoch: 12.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11159527676352916		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.11159527676352916 | validation: 0.17367977662877054]
	TIME [epoch: 12.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147176291094442		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.1147176291094442 | validation: 0.15094553678168865]
	TIME [epoch: 12.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11326364828809553		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.11326364828809553 | validation: 0.17605881023097436]
	TIME [epoch: 12.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123085295449293		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.11123085295449293 | validation: 0.15795511057011793]
	TIME [epoch: 12.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11449919262718046		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.11449919262718046 | validation: 0.15153249972857818]
	TIME [epoch: 12.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11375274707539591		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.11375274707539591 | validation: 0.14534465450906675]
	TIME [epoch: 12.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10935864959884324		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.10935864959884324 | validation: 0.1582483871623797]
	TIME [epoch: 12.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11033371525188333		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.11033371525188333 | validation: 0.14992419397818568]
	TIME [epoch: 12.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581516300951453		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.11581516300951453 | validation: 0.1791165501711536]
	TIME [epoch: 12.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12291627162545067		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.12291627162545067 | validation: 0.1562218259670059]
	TIME [epoch: 12.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12182505256978324		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.12182505256978324 | validation: 0.15503942272514457]
	TIME [epoch: 12.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10889322110454973		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.10889322110454973 | validation: 0.16592197862141855]
	TIME [epoch: 12.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10889902296069867		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.10889902296069867 | validation: 0.1527031617404583]
	TIME [epoch: 12.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10712445560811626		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.10712445560811626 | validation: 0.14728955597704282]
	TIME [epoch: 12.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10576832514623241		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.10576832514623241 | validation: 0.14951363545781882]
	TIME [epoch: 12.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047001627445113		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.1047001627445113 | validation: 0.19002395948410689]
	TIME [epoch: 12.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994647749970273		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.11994647749970273 | validation: 0.16594451789635012]
	TIME [epoch: 12.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12707098251503002		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.12707098251503002 | validation: 0.1439156196961339]
	TIME [epoch: 12.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10565064941088703		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.10565064941088703 | validation: 0.15387719663943003]
	TIME [epoch: 12.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10888180085003858		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.10888180085003858 | validation: 0.14836960684703407]
	TIME [epoch: 12.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932004487644256		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.10932004487644256 | validation: 0.17067744836106422]
	TIME [epoch: 12.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11014727404921304		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.11014727404921304 | validation: 0.15798407652177776]
	TIME [epoch: 12.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824481105178504		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.10824481105178504 | validation: 0.14527767333195665]
	TIME [epoch: 12.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11256628250186862		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.11256628250186862 | validation: 0.15912810001909405]
	TIME [epoch: 12.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12072126509492716		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.12072126509492716 | validation: 0.15562016226968148]
	TIME [epoch: 12.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10741533070995385		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.10741533070995385 | validation: 0.15056684126934813]
	TIME [epoch: 12.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10856445371872343		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.10856445371872343 | validation: 0.14745605313753155]
	TIME [epoch: 12.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1061928465844699		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.1061928465844699 | validation: 0.15724618522709555]
	TIME [epoch: 12.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10312871422136922		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.10312871422136922 | validation: 0.1652018026157796]
	TIME [epoch: 12.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11190801977569889		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.11190801977569889 | validation: 0.1595076919039161]
	TIME [epoch: 12.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12039513622899886		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.12039513622899886 | validation: 0.17319717633257736]
	TIME [epoch: 12.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11550934234285323		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.11550934234285323 | validation: 0.14626623676610873]
	TIME [epoch: 12.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11668318670345862		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.11668318670345862 | validation: 0.14885388931307464]
	TIME [epoch: 12.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10398347286771359		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.10398347286771359 | validation: 0.1633222644082467]
	TIME [epoch: 12.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086542859127875		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.1086542859127875 | validation: 0.17660784657258616]
	TIME [epoch: 12.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12357485781545269		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.12357485781545269 | validation: 0.1652882472522754]
	TIME [epoch: 12.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465027182144752		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.10465027182144752 | validation: 0.14888697412073834]
	TIME [epoch: 12.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10382449313979471		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.10382449313979471 | validation: 0.1658511865931519]
	TIME [epoch: 12.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10295960615288027		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.10295960615288027 | validation: 0.1565297691773201]
	TIME [epoch: 12.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10580970830298736		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.10580970830298736 | validation: 0.14450733337202012]
	TIME [epoch: 12.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10656275051573307		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.10656275051573307 | validation: 0.16845734839550686]
	TIME [epoch: 12.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110571829881119		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.1110571829881119 | validation: 0.154380904449407]
	TIME [epoch: 12.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167127813677251		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.11167127813677251 | validation: 0.1572996426688958]
	TIME [epoch: 12.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10611582246605608		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.10611582246605608 | validation: 0.15409708202420577]
	TIME [epoch: 12.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251037394115831		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.11251037394115831 | validation: 0.14572097011435656]
	TIME [epoch: 12.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10544624409293675		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.10544624409293675 | validation: 0.1635101541490344]
	TIME [epoch: 12.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039043100597203		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.1039043100597203 | validation: 0.14911367671971407]
	TIME [epoch: 12.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10881435273825166		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.10881435273825166 | validation: 0.17518839349793966]
	TIME [epoch: 12.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10712031321399074		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.10712031321399074 | validation: 0.16083338880744116]
	TIME [epoch: 12.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12153041012649136		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.12153041012649136 | validation: 0.1380790194311186]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10617277541350806		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.10617277541350806 | validation: 0.15108353637491564]
	TIME [epoch: 12.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10439437406151324		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.10439437406151324 | validation: 0.152207924591527]
	TIME [epoch: 12.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012031149073697		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.1012031149073697 | validation: 0.15121499236827543]
	TIME [epoch: 12.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470586997612015		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.10470586997612015 | validation: 0.14903632062666552]
	TIME [epoch: 12.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040617961681599		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.1040617961681599 | validation: 0.15226375586449326]
	TIME [epoch: 12.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10683312206726565		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.10683312206726565 | validation: 0.1645801786746326]
	TIME [epoch: 12.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12962872605234083		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.12962872605234083 | validation: 0.18593250133181116]
	TIME [epoch: 12.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11195302006185769		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.11195302006185769 | validation: 0.13725233196047418]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10212867718750471		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.10212867718750471 | validation: 0.15153763768183504]
	TIME [epoch: 12.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10598930405654958		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.10598930405654958 | validation: 0.1657809369160462]
	TIME [epoch: 12.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10231382365851756		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.10231382365851756 | validation: 0.1488940988202564]
	TIME [epoch: 12.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452470370858973		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.10452470370858973 | validation: 0.15269132426673443]
	TIME [epoch: 12.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187335231945528		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.10187335231945528 | validation: 0.15391971827626516]
	TIME [epoch: 12.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11025887231721453		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.11025887231721453 | validation: 0.16162734721121735]
	TIME [epoch: 12.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10573274613108048		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.10573274613108048 | validation: 0.15053523664499782]
	TIME [epoch: 12.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10656084090800054		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.10656084090800054 | validation: 0.15687253536695106]
	TIME [epoch: 12.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10528276524723978		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.10528276524723978 | validation: 0.13949624254712598]
	TIME [epoch: 12.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1033711140535422		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.1033711140535422 | validation: 0.19240804658375188]
	TIME [epoch: 12.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10357658258132292		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.10357658258132292 | validation: 0.16850597769540293]
	TIME [epoch: 12.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10665718194903129		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.10665718194903129 | validation: 0.14064954322901818]
	TIME [epoch: 12.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545214981690901		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.10545214981690901 | validation: 0.15145140269092075]
	TIME [epoch: 12.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10426138325175761		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.10426138325175761 | validation: 0.15869860205216144]
	TIME [epoch: 12.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10203744564835504		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.10203744564835504 | validation: 0.15008554698682336]
	TIME [epoch: 12.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356690166699974		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.10356690166699974 | validation: 0.1431990765686618]
	TIME [epoch: 12.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10628324852909816		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.10628324852909816 | validation: 0.15901437924028214]
	TIME [epoch: 12.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10573295778863435		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.10573295778863435 | validation: 0.1599335682988322]
	TIME [epoch: 12.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583713298122982		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.10583713298122982 | validation: 0.15622045079663774]
	TIME [epoch: 12.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1023005609477361		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.1023005609477361 | validation: 0.13470792382745733]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_938.pth
	Model improved!!!
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10448620274763772		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.10448620274763772 | validation: 0.15440923631388004]
	TIME [epoch: 12.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10130607108455873		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.10130607108455873 | validation: 0.1426363018295871]
	TIME [epoch: 12.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997766698194048		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.0997766698194048 | validation: 0.15511829754068518]
	TIME [epoch: 12.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10471538953148166		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.10471538953148166 | validation: 0.14826853745572377]
	TIME [epoch: 12.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132471011496644		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.1132471011496644 | validation: 0.1491647683494945]
	TIME [epoch: 12.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735342079132846		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.09735342079132846 | validation: 0.15647645832538412]
	TIME [epoch: 12.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.098554014155539		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.098554014155539 | validation: 0.14354063460056135]
	TIME [epoch: 12.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10005567622525188		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.10005567622525188 | validation: 0.1625514247528248]
	TIME [epoch: 12.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551416924002822		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.10551416924002822 | validation: 0.16563215586750274]
	TIME [epoch: 12.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10384277926876086		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.10384277926876086 | validation: 0.1547254974061094]
	TIME [epoch: 12.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09963011589129575		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.09963011589129575 | validation: 0.1390943381516398]
	TIME [epoch: 12.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09713322080781123		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.09713322080781123 | validation: 0.15653564101397036]
	TIME [epoch: 12.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09909398546471218		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.09909398546471218 | validation: 0.1627820976942748]
	TIME [epoch: 12.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09932921975616321		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.09932921975616321 | validation: 0.14352236801111593]
	TIME [epoch: 12.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031208347412403		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.10031208347412403 | validation: 0.14055230828902468]
	TIME [epoch: 12.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09940521453044149		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.09940521453044149 | validation: 0.13998455760498918]
	TIME [epoch: 12.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09942567954286506		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.09942567954286506 | validation: 0.1575307038762354]
	TIME [epoch: 12.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10859489620912545		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.10859489620912545 | validation: 0.15063108035616302]
	TIME [epoch: 12.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328310981976768		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.1328310981976768 | validation: 0.1410941060632628]
	TIME [epoch: 12.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10814857793659834		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.10814857793659834 | validation: 0.15574042256508352]
	TIME [epoch: 12.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396175981733052		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.10396175981733052 | validation: 0.17412510263605901]
	TIME [epoch: 12.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10858172785255392		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.10858172785255392 | validation: 0.17067819633375203]
	TIME [epoch: 12.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998978397448691		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0998978397448691 | validation: 0.15415021894727207]
	TIME [epoch: 12.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10276040242156037		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.10276040242156037 | validation: 0.13678493215200863]
	TIME [epoch: 12.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746717202864378		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.10746717202864378 | validation: 0.1504007599699614]
	TIME [epoch: 12.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976945661995845		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.09976945661995845 | validation: 0.14949578820028275]
	TIME [epoch: 12.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10215949862382928		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.10215949862382928 | validation: 0.13921949502954442]
	TIME [epoch: 12.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10107662625029246		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.10107662625029246 | validation: 0.14187959559754246]
	TIME [epoch: 12.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09757658539445153		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.09757658539445153 | validation: 0.149945087246635]
	TIME [epoch: 12.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09862811395237137		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.09862811395237137 | validation: 0.14657612426544717]
	TIME [epoch: 12.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10648524326152735		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.10648524326152735 | validation: 0.13150102993214893]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10240720245844827		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.10240720245844827 | validation: 0.1639158628177481]
	TIME [epoch: 12.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838439789747071		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.09838439789747071 | validation: 0.15549198486896448]
	TIME [epoch: 12.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09620502991385052		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.09620502991385052 | validation: 0.149027852382949]
	TIME [epoch: 12.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740367407923273		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.09740367407923273 | validation: 0.13933760971438675]
	TIME [epoch: 12.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600870209873685		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.09600870209873685 | validation: 0.13643149449429978]
	TIME [epoch: 12.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707497470295605		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.09707497470295605 | validation: 0.1297884968739328]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09588381787794599		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.09588381787794599 | validation: 0.13614365121714295]
	TIME [epoch: 12.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09672462675004383		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.09672462675004383 | validation: 0.15880393767151255]
	TIME [epoch: 12.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556001097473693		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.09556001097473693 | validation: 0.13749102540653815]
	TIME [epoch: 12.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960975193031755		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0960975193031755 | validation: 0.14409744128010893]
	TIME [epoch: 12.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101106040222019		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.101106040222019 | validation: 0.13341105954554067]
	TIME [epoch: 12.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10270075380952598		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.10270075380952598 | validation: 0.15953949884878327]
	TIME [epoch: 12.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11329712822089938		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.11329712822089938 | validation: 0.1553008907666794]
	TIME [epoch: 12.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10076571619916495		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.10076571619916495 | validation: 0.1482101375685314]
	TIME [epoch: 12.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942531498928658		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.0942531498928658 | validation: 0.15019707242317498]
	TIME [epoch: 12.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707038738501288		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.09707038738501288 | validation: 0.15819893113655895]
	TIME [epoch: 12.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09592960805967966		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.09592960805967966 | validation: 0.14470322220696397]
	TIME [epoch: 12.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09496507410155997		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.09496507410155997 | validation: 0.13252718816363981]
	TIME [epoch: 12.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09384209809297957		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.09384209809297957 | validation: 0.13955377522570728]
	TIME [epoch: 12.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09716781755468484		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.09716781755468484 | validation: 0.17242130532508326]
	TIME [epoch: 12.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1121887002603766		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.1121887002603766 | validation: 0.13675020007872712]
	TIME [epoch: 12.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985511544824653		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.10985511544824653 | validation: 0.14493643804886083]
	TIME [epoch: 12.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175491895202904		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.10175491895202904 | validation: 0.15823910332072655]
	TIME [epoch: 12.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09629864424106245		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.09629864424106245 | validation: 0.14390771301807637]
	TIME [epoch: 12.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788431445720192		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.09788431445720192 | validation: 0.14308359020970618]
	TIME [epoch: 12.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0944630805994805		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.0944630805994805 | validation: 0.15330999415400792]
	TIME [epoch: 12.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735878761825628		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.09735878761825628 | validation: 0.16386228560084037]
	TIME [epoch: 12.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09832790422802633		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.09832790422802633 | validation: 0.1434766984343972]
	TIME [epoch: 12.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097649922411046		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.097649922411046 | validation: 0.13893650973308677]
	TIME [epoch: 12.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09554240867508998		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.09554240867508998 | validation: 0.14887022098594827]
	TIME [epoch: 12.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09930282625574942		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.09930282625574942 | validation: 0.1628735573926866]
	TIME [epoch: 12.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09614268842332704		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.09614268842332704 | validation: 0.14339373967190924]
	TIME [epoch: 294 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779885583825929		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.09779885583825929 | validation: 0.14572442283292486]
	TIME [epoch: 25.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922072323844507		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.09922072323844507 | validation: 0.15159964160858658]
	TIME [epoch: 25.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10042441008793632		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.10042441008793632 | validation: 0.15541331642288575]
	TIME [epoch: 25.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0946331583091687		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.0946331583091687 | validation: 0.13864703798613573]
	TIME [epoch: 25.8 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618067966845224		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.09618067966845224 | validation: 0.14073048349283565]
	TIME [epoch: 25.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10019480609620374		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.10019480609620374 | validation: 0.13746285734696706]
	TIME [epoch: 25.8 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09890059859579096		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.09890059859579096 | validation: 0.14403496941288838]
	TIME [epoch: 25.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158679586812856		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.10158679586812856 | validation: 0.13258479986284094]
	TIME [epoch: 25.8 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961722145734557		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.0961722145734557 | validation: 0.1390866664860955]
	TIME [epoch: 25.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932538558444594		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0932538558444594 | validation: 0.15407041985288827]
	TIME [epoch: 25.8 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09454269135666937		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.09454269135666937 | validation: 0.1447597595686305]
	TIME [epoch: 25.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386241264665147		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.09386241264665147 | validation: 0.1275319661773958]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1013.pth
	Model improved!!!
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10071406051574044		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.10071406051574044 | validation: 0.1417505327687209]
	TIME [epoch: 25.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09157623433227711		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.09157623433227711 | validation: 0.1574482386874769]
	TIME [epoch: 25.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183489450108964		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.10183489450108964 | validation: 0.13917892428638884]
	TIME [epoch: 25.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10574003462874078		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.10574003462874078 | validation: 0.12962185335588516]
	TIME [epoch: 25.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09889528535869195		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.09889528535869195 | validation: 0.1535009010258806]
	TIME [epoch: 25.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09733330204250949		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.09733330204250949 | validation: 0.14292543195074983]
	TIME [epoch: 25.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10215916814022452		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.10215916814022452 | validation: 0.14571657840291424]
	TIME [epoch: 25.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09429105449103663		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.09429105449103663 | validation: 0.15259398288700377]
	TIME [epoch: 25.8 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09761088140699264		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.09761088140699264 | validation: 0.13347541867007448]
	TIME [epoch: 25.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0953427235920102		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.0953427235920102 | validation: 0.15364448107966916]
	TIME [epoch: 25.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572906090063961		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.09572906090063961 | validation: 0.1349568027040382]
	TIME [epoch: 25.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928467426204233		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.0928467426204233 | validation: 0.13656679379098188]
	TIME [epoch: 25.8 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09689212117187462		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.09689212117187462 | validation: 0.14610146119680353]
	TIME [epoch: 25.8 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10117190489268543		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.10117190489268543 | validation: 0.14600499851264345]
	TIME [epoch: 25.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09265082721498008		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.09265082721498008 | validation: 0.12811084787179902]
	TIME [epoch: 25.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09628748516999668		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.09628748516999668 | validation: 0.15020527282193885]
	TIME [epoch: 25.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330031860379035		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.10330031860379035 | validation: 0.1566292691715424]
	TIME [epoch: 25.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032799816451885		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.11032799816451885 | validation: 0.13527280622740956]
	TIME [epoch: 25.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09514464357670872		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.09514464357670872 | validation: 0.14313063767828466]
	TIME [epoch: 25.8 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756069251344204		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.09756069251344204 | validation: 0.25016813998704274]
	TIME [epoch: 25.8 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17904604055629528		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.17904604055629528 | validation: 0.1258277775015484]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1119704007079465		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.1119704007079465 | validation: 0.13733953102403867]
	TIME [epoch: 25.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10577580292838866		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.10577580292838866 | validation: 0.15306166552554884]
	TIME [epoch: 25.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542277758861463		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.09542277758861463 | validation: 0.16424634007076447]
	TIME [epoch: 25.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09696630285105372		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.09696630285105372 | validation: 0.1522382905307434]
	TIME [epoch: 25.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09382160912210634		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.09382160912210634 | validation: 0.13120857681238488]
	TIME [epoch: 25.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09467475126399691		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.09467475126399691 | validation: 0.13677411158501404]
	TIME [epoch: 25.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093887008555174		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.093887008555174 | validation: 0.13712536592015206]
	TIME [epoch: 25.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09323815401983668		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.09323815401983668 | validation: 0.13586729223870325]
	TIME [epoch: 25.8 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203354524656344		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.09203354524656344 | validation: 0.13136499142149885]
	TIME [epoch: 25.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09415018740891352		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.09415018740891352 | validation: 0.13275387963288807]
	TIME [epoch: 25.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422297145182086		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.09422297145182086 | validation: 0.12366490846474139]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1045.pth
	Model improved!!!
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402861800053654		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.09402861800053654 | validation: 0.13816508260094437]
	TIME [epoch: 25.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09320490076440713		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.09320490076440713 | validation: 0.15169354899235055]
	TIME [epoch: 25.8 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327531250390766		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.09327531250390766 | validation: 0.13633017915597073]
	TIME [epoch: 25.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09324311797436924		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.09324311797436924 | validation: 0.13515737906795774]
	TIME [epoch: 25.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129294114398005		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.09129294114398005 | validation: 0.14703373595440997]
	TIME [epoch: 25.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09322915760847543		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.09322915760847543 | validation: 0.12137289189731332]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09080448344479328		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.09080448344479328 | validation: 0.12811342869896666]
	TIME [epoch: 25.8 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09100761630892121		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.09100761630892121 | validation: 0.15151206184988056]
	TIME [epoch: 25.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09844909758200299		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.09844909758200299 | validation: 0.13563605163827847]
	TIME [epoch: 25.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09284473373055807		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.09284473373055807 | validation: 0.12704529892227825]
	TIME [epoch: 25.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577984425810393		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.09577984425810393 | validation: 0.13806472573142978]
	TIME [epoch: 25.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09532265176588427		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.09532265176588427 | validation: 0.15107111681664792]
	TIME [epoch: 25.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811580513696043		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.09811580513696043 | validation: 0.15127076519565485]
	TIME [epoch: 25.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803227405864093		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.09803227405864093 | validation: 0.13705486940625294]
	TIME [epoch: 25.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09442127218580341		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.09442127218580341 | validation: 0.13041611306381615]
	TIME [epoch: 25.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282321297860893		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.09282321297860893 | validation: 0.13544876697285213]
	TIME [epoch: 25.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530563039815611		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.09530563039815611 | validation: 0.15174358769440438]
	TIME [epoch: 25.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204068479301918		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.09204068479301918 | validation: 0.14416566537022374]
	TIME [epoch: 25.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09575320050263879		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.09575320050263879 | validation: 0.13644157936163331]
	TIME [epoch: 25.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09459399844412054		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.09459399844412054 | validation: 0.14200657693105248]
	TIME [epoch: 25.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09424036670837467		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.09424036670837467 | validation: 0.14162392185003514]
	TIME [epoch: 25.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08850177537203915		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.08850177537203915 | validation: 0.13112662708154355]
	TIME [epoch: 25.8 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669366384070666		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.09669366384070666 | validation: 0.13930103398932214]
	TIME [epoch: 25.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09149662955387171		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.09149662955387171 | validation: 0.14481036025874583]
	TIME [epoch: 25.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09230014997850906		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.09230014997850906 | validation: 0.13405912777117296]
	TIME [epoch: 25.8 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09622420510728126		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.09622420510728126 | validation: 0.1478599974947099]
	TIME [epoch: 25.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09427231275108701		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.09427231275108701 | validation: 0.14333024056640434]
	TIME [epoch: 25.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09277117383340226		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.09277117383340226 | validation: 0.13506206870520046]
	TIME [epoch: 25.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09094892249679809		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.09094892249679809 | validation: 0.12963388854282856]
	TIME [epoch: 25.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122802537114201		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.09122802537114201 | validation: 0.13135500804358133]
	TIME [epoch: 25.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09385655921288709		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.09385655921288709 | validation: 0.1417535957222805]
	TIME [epoch: 25.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660429557535874		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.08660429557535874 | validation: 0.1324829945961313]
	TIME [epoch: 25.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08951965135932662		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.08951965135932662 | validation: 0.13269005973029713]
	TIME [epoch: 25.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405605304511178		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.09405605304511178 | validation: 0.14040348552795864]
	TIME [epoch: 25.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08999043285060758		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.08999043285060758 | validation: 0.1278011046561906]
	TIME [epoch: 25.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09533361276553663		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.09533361276553663 | validation: 0.12689789221577988]
	TIME [epoch: 25.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0920266310861763		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.0920266310861763 | validation: 0.15034132763604935]
	TIME [epoch: 25.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09799213292014597		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.09799213292014597 | validation: 0.13346331859247781]
	TIME [epoch: 25.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326579548728146		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.09326579548728146 | validation: 0.12936908347209838]
	TIME [epoch: 25.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921773676717115		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.0921773676717115 | validation: 0.1549117688624409]
	TIME [epoch: 25.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930031623514741		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.0930031623514741 | validation: 0.14954506958143104]
	TIME [epoch: 25.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09346656565804512		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.09346656565804512 | validation: 0.13857652508878707]
	TIME [epoch: 25.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09775749130188491		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.09775749130188491 | validation: 0.1412379812682357]
	TIME [epoch: 25.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08904441018389367		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.08904441018389367 | validation: 0.13155163150892626]
	TIME [epoch: 25.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056819197583331		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.09056819197583331 | validation: 0.1313993532149734]
	TIME [epoch: 25.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08941703025820698		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.08941703025820698 | validation: 0.13072706786663368]
	TIME [epoch: 25.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09008633076569514		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.09008633076569514 | validation: 0.13787968726425573]
	TIME [epoch: 25.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919707093144124		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.0919707093144124 | validation: 0.13766420593237202]
	TIME [epoch: 25.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09352414701395137		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.09352414701395137 | validation: 0.12693981956882086]
	TIME [epoch: 25.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886978203569505		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.0886978203569505 | validation: 0.1271528313733105]
	TIME [epoch: 25.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917050931621932		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.0917050931621932 | validation: 0.14311644425646922]
	TIME [epoch: 25.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09092735965166349		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.09092735965166349 | validation: 0.12307319881036155]
	TIME [epoch: 25.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09419595026719221		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.09419595026719221 | validation: 0.14310990094997322]
	TIME [epoch: 25.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08869845484662041		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.08869845484662041 | validation: 0.1348752643431737]
	TIME [epoch: 25.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587137556720297		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.09587137556720297 | validation: 0.14536899406685527]
	TIME [epoch: 25.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734700233183642		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.09734700233183642 | validation: 0.13649898401988092]
	TIME [epoch: 25.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142163570060667		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.09142163570060667 | validation: 0.1397335996008545]
	TIME [epoch: 25.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885559342893303		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.0885559342893303 | validation: 0.13456019430777386]
	TIME [epoch: 25.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08782183372567627		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.08782183372567627 | validation: 0.13081452576378003]
	TIME [epoch: 25.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923467489825605		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.08923467489825605 | validation: 0.14473157697210787]
	TIME [epoch: 25.8 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09096034541343297		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.09096034541343297 | validation: 0.13354030992315]
	TIME [epoch: 25.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990068606769736		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.08990068606769736 | validation: 0.12437979986982972]
	TIME [epoch: 25.8 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09499492163212986		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.09499492163212986 | validation: 0.13963985174431423]
	TIME [epoch: 25.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09322335937525063		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.09322335937525063 | validation: 0.15456923220837204]
	TIME [epoch: 25.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788881871744005		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.09788881871744005 | validation: 0.13199390276286233]
	TIME [epoch: 25.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08970717728617818		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.08970717728617818 | validation: 0.1337409477637718]
	TIME [epoch: 25.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09083565650475599		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.09083565650475599 | validation: 0.13267500242676608]
	TIME [epoch: 25.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09015037581960213		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.09015037581960213 | validation: 0.136464612676961]
	TIME [epoch: 25.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365468570323937		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.09365468570323937 | validation: 0.14271706690736882]
	TIME [epoch: 25.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936170987758317		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.08936170987758317 | validation: 0.13689674845912317]
	TIME [epoch: 25.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952700511429182		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.08952700511429182 | validation: 0.13223633655821806]
	TIME [epoch: 25.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08972532209889572		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.08972532209889572 | validation: 0.14964488125862707]
	TIME [epoch: 25.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779664138974276		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.08779664138974276 | validation: 0.1266744206407877]
	TIME [epoch: 25.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790786401912516		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.08790786401912516 | validation: 0.13082380636793486]
	TIME [epoch: 25.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912973609295082		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.0912973609295082 | validation: 0.13637543917722955]
	TIME [epoch: 25.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09487320083775912		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.09487320083775912 | validation: 0.1510653725942783]
	TIME [epoch: 25.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582792075101567		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.08582792075101567 | validation: 0.1338639151578038]
	TIME [epoch: 25.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09051677888919267		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.09051677888919267 | validation: 0.1345918617935805]
	TIME [epoch: 25.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909006483387922		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.0909006483387922 | validation: 0.14607742189086578]
	TIME [epoch: 25.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667590303293885		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.09667590303293885 | validation: 0.14536652824819749]
	TIME [epoch: 25.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08998672132820629		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.08998672132820629 | validation: 0.12889488993864795]
	TIME [epoch: 25.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08797936540718851		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.08797936540718851 | validation: 0.12316762035351236]
	TIME [epoch: 25.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08983252224258102		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.08983252224258102 | validation: 0.14313864463221804]
	TIME [epoch: 25.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08973777747027457		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.08973777747027457 | validation: 0.1503036977559531]
	TIME [epoch: 25.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08591068425338812		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.08591068425338812 | validation: 0.1392464804772919]
	TIME [epoch: 25.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778234522025107		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.08778234522025107 | validation: 0.13760251454589437]
	TIME [epoch: 25.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09145928094289829		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.09145928094289829 | validation: 0.15314676473408154]
	TIME [epoch: 25.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08629504175815995		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.08629504175815995 | validation: 0.14217171196577927]
	TIME [epoch: 25.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338117078302492		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.09338117078302492 | validation: 0.1331576691586636]
	TIME [epoch: 25.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08837869960703049		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.08837869960703049 | validation: 0.138038497162764]
	TIME [epoch: 25.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878309995192991		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.0878309995192991 | validation: 0.1370002309478376]
	TIME [epoch: 25.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08664879637353057		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.08664879637353057 | validation: 0.1436741629276097]
	TIME [epoch: 25.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886429049805688		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.0886429049805688 | validation: 0.13398496887731676]
	TIME [epoch: 25.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09019428330392891		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.09019428330392891 | validation: 0.16102086028310297]
	TIME [epoch: 25.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596939908470098		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.09596939908470098 | validation: 0.12965074500388832]
	TIME [epoch: 25.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288803278112756		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.09288803278112756 | validation: 0.13222884931585618]
	TIME [epoch: 25.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165075406508315		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.09165075406508315 | validation: 0.1330749382137803]
	TIME [epoch: 25.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902495825071079		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.0902495825071079 | validation: 0.15022813185344297]
	TIME [epoch: 25.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08503891191529421		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.08503891191529421 | validation: 0.13108289721023697]
	TIME [epoch: 25.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518554079274758		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.08518554079274758 | validation: 0.1116395743728249]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1145.pth
	Model improved!!!
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08927818087719999		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.08927818087719999 | validation: 0.13156314102142866]
	TIME [epoch: 25.8 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0891760134123716		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.0891760134123716 | validation: 0.153268293953507]
	TIME [epoch: 25.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08519115339495048		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.08519115339495048 | validation: 0.1402010411913751]
	TIME [epoch: 25.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625582939114514		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.08625582939114514 | validation: 0.12887652510560446]
	TIME [epoch: 25.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08984685236922868		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.08984685236922868 | validation: 0.1317090115910641]
	TIME [epoch: 25.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08845967309357673		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.08845967309357673 | validation: 0.13216581362882424]
	TIME [epoch: 25.8 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686166055307341		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.08686166055307341 | validation: 0.14299511910627036]
	TIME [epoch: 25.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587303634240737		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.08587303634240737 | validation: 0.1237366340541374]
	TIME [epoch: 25.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08810328706212114		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.08810328706212114 | validation: 0.12146937199860086]
	TIME [epoch: 25.7 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08689695745729452		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.08689695745729452 | validation: 0.12796165596951908]
	TIME [epoch: 25.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08836557760672903		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.08836557760672903 | validation: 0.13398793544150608]
	TIME [epoch: 25.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09047344641855105		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.09047344641855105 | validation: 0.13075234937831387]
	TIME [epoch: 25.8 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900918896233836		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.0900918896233836 | validation: 0.14455753995381254]
	TIME [epoch: 25.8 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0854419381438367		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.0854419381438367 | validation: 0.1370742665728601]
	TIME [epoch: 25.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09021351222634205		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.09021351222634205 | validation: 0.1349360509065773]
	TIME [epoch: 25.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467885756255969		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.08467885756255969 | validation: 0.14162849842253697]
	TIME [epoch: 25.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08682519122154651		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.08682519122154651 | validation: 0.13459991282410608]
	TIME [epoch: 25.7 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778500723930836		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.08778500723930836 | validation: 0.14845768924889738]
	TIME [epoch: 25.8 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799368151983725		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.08799368151983725 | validation: 0.14479209664372222]
	TIME [epoch: 25.7 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288539735087858		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.08288539735087858 | validation: 0.12672331297301379]
	TIME [epoch: 25.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08598828671433381		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.08598828671433381 | validation: 0.1452322549158986]
	TIME [epoch: 25.7 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625431879173284		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.08625431879173284 | validation: 0.11422829702377038]
	TIME [epoch: 25.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0834477401096841		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.0834477401096841 | validation: 0.15335717565812612]
	TIME [epoch: 25.8 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08805665202857878		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.08805665202857878 | validation: 0.1331372249449569]
	TIME [epoch: 25.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08781652733015399		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.08781652733015399 | validation: 0.14033936292165985]
	TIME [epoch: 25.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559820760255928		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.08559820760255928 | validation: 0.13196691927759724]
	TIME [epoch: 25.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303450673987562		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.09303450673987562 | validation: 0.11713575816996886]
	TIME [epoch: 25.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08922422799932676		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.08922422799932676 | validation: 0.11529809025660397]
	TIME [epoch: 25.8 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237435411529425		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.09237435411529425 | validation: 0.13860491787580353]
	TIME [epoch: 25.8 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802424717844069		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.08802424717844069 | validation: 0.1515855696661844]
	TIME [epoch: 25.8 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818612431973759		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.08818612431973759 | validation: 0.14703594296153696]
	TIME [epoch: 25.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08485050443377516		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.08485050443377516 | validation: 0.13338522079849127]
	TIME [epoch: 25.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08517278312247614		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.08517278312247614 | validation: 0.13859483636105777]
	TIME [epoch: 25.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586281948409788		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.08586281948409788 | validation: 0.13175963259039336]
	TIME [epoch: 25.8 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549841836605236		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.08549841836605236 | validation: 0.13321341788314028]
	TIME [epoch: 25.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779719053543986		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.08779719053543986 | validation: 0.13128209920496167]
	TIME [epoch: 25.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08752238148981271		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.08752238148981271 | validation: 0.14239136582689535]
	TIME [epoch: 25.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812451978930806		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.08812451978930806 | validation: 0.11975317264982294]
	TIME [epoch: 25.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08474188462645603		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.08474188462645603 | validation: 0.13744446801881502]
	TIME [epoch: 25.8 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838780338914565		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.0838780338914565 | validation: 0.13507025830056382]
	TIME [epoch: 25.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825749865511708		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.0825749865511708 | validation: 0.12491386792750668]
	TIME [epoch: 25.8 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0844442831460524		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.0844442831460524 | validation: 0.12525362093775472]
	TIME [epoch: 25.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08501372595915317		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.08501372595915317 | validation: 0.14107420223069522]
	TIME [epoch: 25.8 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996705902211401		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.08996705902211401 | validation: 0.1366136571861912]
	TIME [epoch: 25.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08662387298070945		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.08662387298070945 | validation: 0.12467412105256868]
	TIME [epoch: 25.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442555977573019		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.08442555977573019 | validation: 0.12454126025837442]
	TIME [epoch: 25.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436671731256787		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.08436671731256787 | validation: 0.11396078133485923]
	TIME [epoch: 25.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08722747944001785		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.08722747944001785 | validation: 0.1248908001051019]
	TIME [epoch: 25.8 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08764712134064177		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.08764712134064177 | validation: 0.1347030566896795]
	TIME [epoch: 25.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455325232059081		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.08455325232059081 | validation: 0.14222080423404537]
	TIME [epoch: 25.8 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08756016642285687		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.08756016642285687 | validation: 0.13431078866281196]
	TIME [epoch: 25.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08610600585245047		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.08610600585245047 | validation: 0.12071947184976024]
	TIME [epoch: 25.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08282087674002629		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.08282087674002629 | validation: 0.11805558173075004]
	TIME [epoch: 25.8 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08907861946042622		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.08907861946042622 | validation: 0.12209024931645646]
	TIME [epoch: 25.8 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08497265924325334		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.08497265924325334 | validation: 0.12323028234312719]
	TIME [epoch: 25.8 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.082931066235079		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.082931066235079 | validation: 0.11198291642839142]
	TIME [epoch: 25.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08416389508450106		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.08416389508450106 | validation: 0.12678484381770116]
	TIME [epoch: 25.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0855545217972265		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.0855545217972265 | validation: 0.14288585572714424]
	TIME [epoch: 25.8 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841420329403561		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.0841420329403561 | validation: 0.12605251831901776]
	TIME [epoch: 25.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08691422311455359		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.08691422311455359 | validation: 0.11467973895194256]
	TIME [epoch: 25.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08447840035922496		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.08447840035922496 | validation: 0.13320159302292375]
	TIME [epoch: 25.8 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08929012090538617		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.08929012090538617 | validation: 0.13676744894399293]
	TIME [epoch: 25.8 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243604077375262		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.08243604077375262 | validation: 0.12534847420549808]
	TIME [epoch: 25.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133079707460217		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.08133079707460217 | validation: 0.12757245944556464]
	TIME [epoch: 25.8 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735875696587979		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.08735875696587979 | validation: 0.132138540214449]
	TIME [epoch: 25.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08605190861522621		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.08605190861522621 | validation: 0.13178881534541317]
	TIME [epoch: 25.8 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08287879907193631		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.08287879907193631 | validation: 0.12976001683132574]
	TIME [epoch: 25.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166416249197138		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.08166416249197138 | validation: 0.12222349948116187]
	TIME [epoch: 25.8 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08167312682369515		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.08167312682369515 | validation: 0.13577697363221217]
	TIME [epoch: 25.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002887578280067		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.08002887578280067 | validation: 0.12349772505404553]
	TIME [epoch: 25.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08953501315128103		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.08953501315128103 | validation: 0.13906207586281058]
	TIME [epoch: 25.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504208664297659		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.08504208664297659 | validation: 0.1408028314074867]
	TIME [epoch: 25.8 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08316724036613532		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.08316724036613532 | validation: 0.12650628210856327]
	TIME [epoch: 25.7 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08263249602706893		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.08263249602706893 | validation: 0.12825751525809234]
	TIME [epoch: 25.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08676311856741585		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.08676311856741585 | validation: 0.12517579757291494]
	TIME [epoch: 25.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08859504746367126		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.08859504746367126 | validation: 0.14407961939353636]
	TIME [epoch: 25.8 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432716689027542		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.08432716689027542 | validation: 0.14046616867202277]
	TIME [epoch: 25.7 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958261367871995		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.07958261367871995 | validation: 0.11863863971657472]
	TIME [epoch: 25.8 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08476723539011662		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.08476723539011662 | validation: 0.13332507310887276]
	TIME [epoch: 25.8 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087467353901341		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.08087467353901341 | validation: 0.13204159762113177]
	TIME [epoch: 25.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08529178982818052		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.08529178982818052 | validation: 0.126654653369015]
	TIME [epoch: 25.8 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811452661452736		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.0811452661452736 | validation: 0.11953780752088355]
	TIME [epoch: 25.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0881159966179831		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0881159966179831 | validation: 0.12871198920186572]
	TIME [epoch: 25.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845282157044582		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.0845282157044582 | validation: 0.13191074448563653]
	TIME [epoch: 25.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08409518643608449		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.08409518643608449 | validation: 0.1316139533772003]
	TIME [epoch: 25.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08426560938472577		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.08426560938472577 | validation: 0.11841181639911853]
	TIME [epoch: 25.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08508887798036038		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.08508887798036038 | validation: 0.13534033569932213]
	TIME [epoch: 25.8 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466901398753945		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.08466901398753945 | validation: 0.1376899332416926]
	TIME [epoch: 25.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549907672163777		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.08549907672163777 | validation: 0.12020321587624888]
	TIME [epoch: 25.7 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142206279754383		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.08142206279754383 | validation: 0.13029144478321247]
	TIME [epoch: 25.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128463782133145		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.08128463782133145 | validation: 0.1327503640227085]
	TIME [epoch: 25.7 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08480397459223986		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.08480397459223986 | validation: 0.15005655517584626]
	TIME [epoch: 25.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017173342029466		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.08017173342029466 | validation: 0.12874573011918597]
	TIME [epoch: 25.7 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880999264776954		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.0880999264776954 | validation: 0.12542063095022007]
	TIME [epoch: 25.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08749008108914268		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.08749008108914268 | validation: 0.11671088491797198]
	TIME [epoch: 25.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837505137931366		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.07837505137931366 | validation: 0.12183404569032637]
	TIME [epoch: 25.8 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08656188565546877		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.08656188565546877 | validation: 0.1290454286895764]
	TIME [epoch: 25.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924097721766637		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.0924097721766637 | validation: 0.12460795096331911]
	TIME [epoch: 25.8 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08770116996011094		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.08770116996011094 | validation: 0.12270116750280201]
	TIME [epoch: 25.8 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663270318950726		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.08663270318950726 | validation: 0.14113962257136045]
	TIME [epoch: 25.8 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755978882452356		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.08755978882452356 | validation: 0.15003464237866165]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153156/states/model_phi1_3c_v_mmd1_1246.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 16145.921 seconds.
