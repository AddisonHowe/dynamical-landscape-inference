Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/basic/data_phi1_1a/training', validation_data='data/training_data/basic/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3257187163

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.1023722745505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1023722745505 | validation: 6.344025416343118]
	TIME [epoch: 400 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.639241159656317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.639241159656317 | validation: 6.0998144306606665]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.347578816445439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.347578816445439 | validation: 6.041076310021825]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2325098554204725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2325098554204725 | validation: 6.139920457362365]
	TIME [epoch: 6.11 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.165632448184724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.165632448184724 | validation: 5.91135764731375]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013483346830801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.013483346830801 | validation: 5.8343538056788455]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.019407559559031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.019407559559031 | validation: 5.914084941405212]
	TIME [epoch: 6.11 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90173933971224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.90173933971224 | validation: 5.731817048865464]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.804472133774915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.804472133774915 | validation: 5.667611063372378]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.791562285921815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.791562285921815 | validation: 5.645920090828646]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.726111939605973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.726111939605973 | validation: 5.50282742541059]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.645745004738487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.645745004738487 | validation: 5.429071846148844]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.530883381040232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.530883381040232 | validation: 5.371141013786598]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.395867217073427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.395867217073427 | validation: 4.990191744879818]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.89854085507099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.89854085507099 | validation: 3.512565076265373]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.228237955124073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.228237955124073 | validation: 3.269443953945575]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5844633014446132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5844633014446132 | validation: 2.7363927905800063]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3190182313977665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3190182313977665 | validation: 2.5752157448745763]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.203696511201156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.203696511201156 | validation: 2.5205255869846206]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1750491803782133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1750491803782133 | validation: 2.3771629985673703]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.170030531869879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.170030531869879 | validation: 2.5905480918017356]
	TIME [epoch: 6.11 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.995396704984077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.995396704984077 | validation: 2.2826338993822897]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.077351416910519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.077351416910519 | validation: 2.275385742960803]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.812440780879384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.812440780879384 | validation: 1.8332549909899973]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7108921938888682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7108921938888682 | validation: 1.7176796161422274]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4541210484324574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4541210484324574 | validation: 1.359051976719562]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4726679183674478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4726679183674478 | validation: 1.3125990696896772]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3347463823846082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3347463823846082 | validation: 1.2200702767608727]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1630123179224232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1630123179224232 | validation: 1.3050159303977091]
	TIME [epoch: 6.12 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2739797782288504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2739797782288504 | validation: 1.0742105569730922]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0954501937148553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0954501937148553 | validation: 1.1336380109868842]
	TIME [epoch: 6.11 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.088663346247545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.088663346247545 | validation: 0.9730855380360183]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9146803585472342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9146803585472342 | validation: 1.230169105932188]
	TIME [epoch: 6.11 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0479297461473382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0479297461473382 | validation: 1.1812385157708682]
	TIME [epoch: 6.11 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9223210528799626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9223210528799626 | validation: 0.9757374046112602]
	TIME [epoch: 6.1 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.018637014432729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.018637014432729 | validation: 1.267555405237365]
	TIME [epoch: 6.11 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0657685796919751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0657685796919751 | validation: 1.0654918280962553]
	TIME [epoch: 6.58 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.031772084248117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.031772084248117 | validation: 0.8121325204502527]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7751029050010412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751029050010412 | validation: 0.8931497717019188]
	TIME [epoch: 6.12 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7246606830575278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246606830575278 | validation: 1.1027876607064928]
	TIME [epoch: 6.12 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7364613424937744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364613424937744 | validation: 0.8500318143629186]
	TIME [epoch: 6.11 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.739116156821279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739116156821279 | validation: 1.1337790761928486]
	TIME [epoch: 6.11 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0777184375402418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0777184375402418 | validation: 0.7677508250782142]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7692048429337631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7692048429337631 | validation: 0.6161806969267178]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5949891963449598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5949891963449598 | validation: 0.5237346500237239]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6615764296416277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6615764296416277 | validation: 0.8960981631383622]
	TIME [epoch: 6.12 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6896222456366047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896222456366047 | validation: 0.5878007102162516]
	TIME [epoch: 6.11 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.783997485358071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.783997485358071 | validation: 0.7220812746691803]
	TIME [epoch: 6.13 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6837314737282129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837314737282129 | validation: 0.4718773805970223]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48791657325900273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48791657325900273 | validation: 0.8040530530171797]
	TIME [epoch: 6.13 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5851654848485377		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.5851654848485377 | validation: 0.4506685683977103]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5815708945375131		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.5815708945375131 | validation: 0.4195019691868512]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6133711752209876		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6133711752209876 | validation: 0.5272886348113892]
	TIME [epoch: 6.11 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47042374830443245		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.47042374830443245 | validation: 0.4995440871773805]
	TIME [epoch: 6.13 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5258482494512824		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.5258482494512824 | validation: 0.43095236854880636]
	TIME [epoch: 6.13 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5663309255531929		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.5663309255531929 | validation: 0.41539116803409687]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4602645980615146		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.4602645980615146 | validation: 0.6089198928410713]
	TIME [epoch: 6.13 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47130708123528536		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.47130708123528536 | validation: 0.34000148218670345]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4962318836120485		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.4962318836120485 | validation: 0.37975186894246427]
	TIME [epoch: 6.13 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3941567927914664		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.3941567927914664 | validation: 0.4810401572524473]
	TIME [epoch: 6.12 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47023242928187825		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.47023242928187825 | validation: 0.45707937175458807]
	TIME [epoch: 6.12 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38430274634641165		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.38430274634641165 | validation: 0.5690866274244532]
	TIME [epoch: 6.12 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4281099243894919		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.4281099243894919 | validation: 0.36724285473816215]
	TIME [epoch: 6.13 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4308994356123685		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.4308994356123685 | validation: 0.4105203548478955]
	TIME [epoch: 6.12 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3329352401847004		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.3329352401847004 | validation: 0.4461886696480655]
	TIME [epoch: 6.11 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4235239375490678		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.4235239375490678 | validation: 0.34722150802186624]
	TIME [epoch: 6.12 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3795849931691878		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.3795849931691878 | validation: 0.26547503701892294]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43455366658471356		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.43455366658471356 | validation: 0.3830378970362514]
	TIME [epoch: 6.12 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33773129690096143		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.33773129690096143 | validation: 0.2652826789014632]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33008857794780044		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.33008857794780044 | validation: 0.486872507143318]
	TIME [epoch: 6.13 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4014782905349393		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.4014782905349393 | validation: 0.2835223835991176]
	TIME [epoch: 6.12 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26709399999766964		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.26709399999766964 | validation: 0.301894446472766]
	TIME [epoch: 6.12 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33411456697378944		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.33411456697378944 | validation: 0.25004645160082606]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3837893120104276		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.3837893120104276 | validation: 0.2534274015126121]
	TIME [epoch: 6.13 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3089262759668192		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.3089262759668192 | validation: 0.30462084955084234]
	TIME [epoch: 6.12 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2812796296049503		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.2812796296049503 | validation: 0.3498528495568557]
	TIME [epoch: 6.13 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31625817956597496		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.31625817956597496 | validation: 0.36940044558301066]
	TIME [epoch: 6.13 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298546480716529		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.298546480716529 | validation: 0.2458874490900359]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31079336873575164		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.31079336873575164 | validation: 0.4248757401942687]
	TIME [epoch: 6.13 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31493603990146		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.31493603990146 | validation: 0.41880920626481566]
	TIME [epoch: 6.12 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3485699091731441		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.3485699091731441 | validation: 0.3184180540702354]
	TIME [epoch: 6.12 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25718205803268135		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.25718205803268135 | validation: 0.2202945474056116]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29180714703734445		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.29180714703734445 | validation: 0.22728320331680993]
	TIME [epoch: 6.12 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21421265632767034		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.21421265632767034 | validation: 0.44452919783989453]
	TIME [epoch: 6.11 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3568574064815606		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.3568574064815606 | validation: 0.3102112868584526]
	TIME [epoch: 6.11 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26636685573358776		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.26636685573358776 | validation: 0.2564467608754849]
	TIME [epoch: 6.12 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2544163661956659		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.2544163661956659 | validation: 0.3798881020693332]
	TIME [epoch: 6.11 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3848569140497721		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.3848569140497721 | validation: 0.4298794450872456]
	TIME [epoch: 6.12 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820686734913258		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.2820686734913258 | validation: 0.2412438470958289]
	TIME [epoch: 6.12 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25256798349433873		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.25256798349433873 | validation: 0.21722042307851303]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27646173740116553		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.27646173740116553 | validation: 0.28201203037080874]
	TIME [epoch: 6.12 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3328529755582773		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.3328529755582773 | validation: 0.42649980278882693]
	TIME [epoch: 6.11 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36661915233634446		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.36661915233634446 | validation: 0.3314742560122721]
	TIME [epoch: 6.12 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26780071137359635		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.26780071137359635 | validation: 0.2203874907238279]
	TIME [epoch: 6.13 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23873121652150742		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.23873121652150742 | validation: 0.20562187717107872]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21647059270450897		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.21647059270450897 | validation: 0.3064635140629443]
	TIME [epoch: 6.13 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28814982415588175		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.28814982415588175 | validation: 0.36923980354660474]
	TIME [epoch: 6.13 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.263265144700152		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.263265144700152 | validation: 0.21628213612808672]
	TIME [epoch: 6.12 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24786191248203776		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.24786191248203776 | validation: 0.2802465945761107]
	TIME [epoch: 6.12 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35020646187149745		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.35020646187149745 | validation: 0.2127362331812058]
	TIME [epoch: 6.13 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23663015898434858		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.23663015898434858 | validation: 0.20019695692739997]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2352662993940795		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.2352662993940795 | validation: 0.28599790114004436]
	TIME [epoch: 6.12 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22152265263415827		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.22152265263415827 | validation: 0.18782339313679206]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23951795746879187		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.23951795746879187 | validation: 0.16990372197285344]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2562402142880955		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2562402142880955 | validation: 0.19481139005309717]
	TIME [epoch: 6.11 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22014744398718702		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.22014744398718702 | validation: 0.31411874087952685]
	TIME [epoch: 6.12 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3125975676322327		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.3125975676322327 | validation: 0.3606935786329537]
	TIME [epoch: 6.1 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105799871965443		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3105799871965443 | validation: 0.24556196644382838]
	TIME [epoch: 6.11 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20915927617469085		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.20915927617469085 | validation: 0.22281508470244166]
	TIME [epoch: 6.12 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19700631420522902		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.19700631420522902 | validation: 0.2845534394932739]
	TIME [epoch: 6.11 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28697889654102227		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.28697889654102227 | validation: 0.27452734084723845]
	TIME [epoch: 6.1 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26184117844268795		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.26184117844268795 | validation: 0.18385333430983083]
	TIME [epoch: 6.11 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2032610399542533		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.2032610399542533 | validation: 0.23040741038434231]
	TIME [epoch: 6.1 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2390909092571491		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.2390909092571491 | validation: 0.3689939993370168]
	TIME [epoch: 6.11 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33506017555892054		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.33506017555892054 | validation: 0.20788971897486008]
	TIME [epoch: 6.12 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2296071875755027		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2296071875755027 | validation: 0.2189919945868508]
	TIME [epoch: 6.12 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1996167806604638		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.1996167806604638 | validation: 0.208870101508236]
	TIME [epoch: 6.11 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22517698599901348		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.22517698599901348 | validation: 0.342801155637971]
	TIME [epoch: 6.12 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22907994105428633		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.22907994105428633 | validation: 0.16381808070666942]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21525287564368534		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.21525287564368534 | validation: 0.2670839573374657]
	TIME [epoch: 6.14 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605860137357455		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2605860137357455 | validation: 0.18912648195118842]
	TIME [epoch: 6.15 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2333646382786297		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2333646382786297 | validation: 0.19128290792101907]
	TIME [epoch: 6.16 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20324468829367895		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.20324468829367895 | validation: 0.2930526590697899]
	TIME [epoch: 6.15 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23547241343851127		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.23547241343851127 | validation: 0.2200251611010281]
	TIME [epoch: 6.16 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19976671471712082		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.19976671471712082 | validation: 0.21386341797541564]
	TIME [epoch: 6.15 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21244930831695066		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.21244930831695066 | validation: 0.20301484124935987]
	TIME [epoch: 6.16 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540636236709586		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2540636236709586 | validation: 0.29358662232251126]
	TIME [epoch: 6.16 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2305446948737502		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2305446948737502 | validation: 0.2741982471366966]
	TIME [epoch: 6.15 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20678805348578413		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.20678805348578413 | validation: 0.17727088470406588]
	TIME [epoch: 6.14 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1919679143778391		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.1919679143778391 | validation: 0.30146536961543313]
	TIME [epoch: 6.14 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1907834475047649		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.1907834475047649 | validation: 0.15123442842854404]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20828371128315776		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.20828371128315776 | validation: 0.24257402863935656]
	TIME [epoch: 6.11 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2016384218543722		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.2016384218543722 | validation: 0.2912008498450417]
	TIME [epoch: 6.11 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.203029872731238		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.203029872731238 | validation: 0.1679938112901575]
	TIME [epoch: 6.11 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19459159155369743		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.19459159155369743 | validation: 0.24915990053306858]
	TIME [epoch: 6.1 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668703566743869		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2668703566743869 | validation: 0.19893118672244897]
	TIME [epoch: 6.11 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1904072352541032		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.1904072352541032 | validation: 0.14156313010621502]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14606281749318234		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.14606281749318234 | validation: 0.22152424483537025]
	TIME [epoch: 6.13 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2191350029760959		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2191350029760959 | validation: 0.15441849042904873]
	TIME [epoch: 6.13 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18821301887986228		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.18821301887986228 | validation: 0.1722528288336055]
	TIME [epoch: 6.13 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18073694244795607		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.18073694244795607 | validation: 0.27699257593298954]
	TIME [epoch: 6.13 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21234524724054973		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.21234524724054973 | validation: 0.12985448482265596]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14525441151820845		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.14525441151820845 | validation: 0.16251472654445126]
	TIME [epoch: 6.12 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19213914793655834		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.19213914793655834 | validation: 0.3010116261985614]
	TIME [epoch: 6.11 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17043654771026404		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.17043654771026404 | validation: 0.1540588528262504]
	TIME [epoch: 6.11 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.162295689024871		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.162295689024871 | validation: 0.12883665710068737]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1586937904044911		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.1586937904044911 | validation: 0.13647484832464582]
	TIME [epoch: 6.12 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19630240771570973		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.19630240771570973 | validation: 0.16991924633936611]
	TIME [epoch: 6.12 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15903722021206895		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.15903722021206895 | validation: 0.16783223648741438]
	TIME [epoch: 6.12 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14220992875158842		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.14220992875158842 | validation: 0.16281690178756425]
	TIME [epoch: 6.13 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17391803413339746		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.17391803413339746 | validation: 0.2519067173436317]
	TIME [epoch: 6.13 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16363687629187512		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.16363687629187512 | validation: 0.1314012667946915]
	TIME [epoch: 6.12 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18105807799000756		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.18105807799000756 | validation: 0.12621068735068203]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1534337885593515		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.1534337885593515 | validation: 0.2109595241940206]
	TIME [epoch: 6.13 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1605674199812038		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.1605674199812038 | validation: 0.17884806742120088]
	TIME [epoch: 6.13 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1850316529241821		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1850316529241821 | validation: 0.13278297907020947]
	TIME [epoch: 6.13 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16874536612698565		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.16874536612698565 | validation: 0.14033506968587373]
	TIME [epoch: 6.12 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11996582504025759		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.11996582504025759 | validation: 0.09024149057972063]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16468063592114535		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.16468063592114535 | validation: 0.15558820535979284]
	TIME [epoch: 6.11 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14620285630856103		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.14620285630856103 | validation: 0.11906862056963559]
	TIME [epoch: 6.1 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14412424251426506		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.14412424251426506 | validation: 0.09784612056538183]
	TIME [epoch: 6.11 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1137073396183873		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.1137073396183873 | validation: 0.14385510590662126]
	TIME [epoch: 6.11 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13872800854673398		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.13872800854673398 | validation: 0.0924869054343766]
	TIME [epoch: 6.11 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13831115091361199		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.13831115091361199 | validation: 0.17643231713435234]
	TIME [epoch: 6.11 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1659031437409858		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.1659031437409858 | validation: 0.1087585234273148]
	TIME [epoch: 6.11 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11973322828472695		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.11973322828472695 | validation: 0.09049690414974385]
	TIME [epoch: 6.11 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13095021772301108		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.13095021772301108 | validation: 0.1318594918019933]
	TIME [epoch: 6.11 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14658310047828113		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.14658310047828113 | validation: 0.13822626699521295]
	TIME [epoch: 6.11 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12464461908101754		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.12464461908101754 | validation: 0.07748322689341011]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12933636593494008		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.12933636593494008 | validation: 0.099581908007564]
	TIME [epoch: 6.12 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11101317868334461		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.11101317868334461 | validation: 0.07847092537023155]
	TIME [epoch: 6.12 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12484470919959761		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.12484470919959761 | validation: 0.1250115115003549]
	TIME [epoch: 6.12 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13956749837772187		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.13956749837772187 | validation: 0.07907971498029986]
	TIME [epoch: 6.11 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10520294591439894		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.10520294591439894 | validation: 0.10891124744281344]
	TIME [epoch: 6.12 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0953284868069476		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.0953284868069476 | validation: 0.07367202268255482]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09227503222963605		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.09227503222963605 | validation: 0.10083343087655998]
	TIME [epoch: 6.11 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11106508715885872		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.11106508715885872 | validation: 0.09099840791362936]
	TIME [epoch: 6.11 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12160593377267885		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.12160593377267885 | validation: 0.0709832052817694]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07115671347133838		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.07115671347133838 | validation: 0.09691381722801184]
	TIME [epoch: 6.11 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08926079505450434		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.08926079505450434 | validation: 0.10388880064528343]
	TIME [epoch: 6.11 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13745158471083574		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.13745158471083574 | validation: 0.0973519497171499]
	TIME [epoch: 6.1 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08228293391033933		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.08228293391033933 | validation: 0.09040453495617354]
	TIME [epoch: 6.12 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0922439896543511		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.0922439896543511 | validation: 0.12417853755712566]
	TIME [epoch: 6.1 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12002109257126807		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.12002109257126807 | validation: 0.1348705655291787]
	TIME [epoch: 6.11 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10449646782968088		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.10449646782968088 | validation: 0.08297317845079413]
	TIME [epoch: 6.1 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1020853619877587		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.1020853619877587 | validation: 0.07048163326065612]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06815412689809777		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.06815412689809777 | validation: 0.11167967354375023]
	TIME [epoch: 6.11 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1324467832789374		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.1324467832789374 | validation: 0.07166685142687505]
	TIME [epoch: 6.12 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08661746276217724		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.08661746276217724 | validation: 0.09283904447470066]
	TIME [epoch: 6.12 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07887552458510953		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.07887552458510953 | validation: 0.06785394716655418]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11694640425708762		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.11694640425708762 | validation: 0.14426288563414597]
	TIME [epoch: 6.12 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0907740647116401		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.0907740647116401 | validation: 0.07770421010518877]
	TIME [epoch: 6.11 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08219683889096563		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.08219683889096563 | validation: 0.09337617447431949]
	TIME [epoch: 6.11 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08973049539272382		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.08973049539272382 | validation: 0.06686894771032804]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08317418960247715		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.08317418960247715 | validation: 0.1029350324866044]
	TIME [epoch: 6.12 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08532647092670735		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.08532647092670735 | validation: 0.09373454731562453]
	TIME [epoch: 6.1 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07053227922315086		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.07053227922315086 | validation: 0.08021948740457926]
	TIME [epoch: 6.11 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06906282610537105		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.06906282610537105 | validation: 0.0799111853357444]
	TIME [epoch: 6.11 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09350498002977953		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.09350498002977953 | validation: 0.11650986049163582]
	TIME [epoch: 6.13 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07777863768391755		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.07777863768391755 | validation: 0.07321091750932757]
	TIME [epoch: 6.13 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07271431769226705		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.07271431769226705 | validation: 0.17953416903884817]
	TIME [epoch: 421 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10455586344959585		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.10455586344959585 | validation: 0.05080762236717781]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06747816517413621		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.06747816517413621 | validation: 0.11826974205609661]
	TIME [epoch: 12.1 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08549145185119839		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.08549145185119839 | validation: 0.08020264832666552]
	TIME [epoch: 12.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09219126131074204		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.09219126131074204 | validation: 0.07112632611392537]
	TIME [epoch: 12.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06508994240139011		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.06508994240139011 | validation: 0.0953122058619156]
	TIME [epoch: 12.1 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07988809737538677		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.07988809737538677 | validation: 0.06628795812338052]
	TIME [epoch: 12.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06864117393663563		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.06864117393663563 | validation: 0.09994903202176525]
	TIME [epoch: 12.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07868184367789352		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.07868184367789352 | validation: 0.06646049764767971]
	TIME [epoch: 12.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14991327196094842		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.14991327196094842 | validation: 0.12478961989993213]
	TIME [epoch: 12.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07959433695282034		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.07959433695282034 | validation: 0.08242124806868945]
	TIME [epoch: 12.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054703607562573545		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.054703607562573545 | validation: 0.09211626476142126]
	TIME [epoch: 12.1 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08319175018338247		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.08319175018338247 | validation: 0.09071740815089761]
	TIME [epoch: 12.1 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10038387313437079		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.10038387313437079 | validation: 0.03970084065943143]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05540448094973015		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.05540448094973015 | validation: 0.0936932004746566]
	TIME [epoch: 12.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07597479245344445		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.07597479245344445 | validation: 0.07164041101302865]
	TIME [epoch: 12.1 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06125580350067773		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.06125580350067773 | validation: 0.05438608602091073]
	TIME [epoch: 12.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07643078342439293		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.07643078342439293 | validation: 0.0553162184116021]
	TIME [epoch: 12.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061051053878784636		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.061051053878784636 | validation: 0.056333793080258786]
	TIME [epoch: 12.1 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047279303552588095		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.047279303552588095 | validation: 0.03817235454920872]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053564964501232194		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.053564964501232194 | validation: 0.07799224129818744]
	TIME [epoch: 12.1 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0927230823544197		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.0927230823544197 | validation: 0.043057694138989036]
	TIME [epoch: 12.1 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04416984553966505		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.04416984553966505 | validation: 0.05132380150120942]
	TIME [epoch: 12.1 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08083149925291526		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.08083149925291526 | validation: 0.06504445089576946]
	TIME [epoch: 12.1 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06999881078405697		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.06999881078405697 | validation: 0.06119396591899676]
	TIME [epoch: 12.1 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0523635935395319		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.0523635935395319 | validation: 0.028865552239744463]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05284589681557195		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.05284589681557195 | validation: 0.1053829891537969]
	TIME [epoch: 12.1 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07456217732706133		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.07456217732706133 | validation: 0.055282489780490766]
	TIME [epoch: 12.1 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06167032972612574		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.06167032972612574 | validation: 0.056681261256497695]
	TIME [epoch: 12.1 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07669545993219579		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.07669545993219579 | validation: 0.06686336067754459]
	TIME [epoch: 12.1 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868947826298214		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.05868947826298214 | validation: 0.038174445447868896]
	TIME [epoch: 12.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04569207143967857		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.04569207143967857 | validation: 0.05099116038444325]
	TIME [epoch: 12.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04684144041368173		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.04684144041368173 | validation: 0.047234303270879976]
	TIME [epoch: 12.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06366473481208354		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.06366473481208354 | validation: 0.06038107078515541]
	TIME [epoch: 12.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04307726759557784		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.04307726759557784 | validation: 0.045017893746288895]
	TIME [epoch: 12.1 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06295260831603797		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.06295260831603797 | validation: 0.07099453328794099]
	TIME [epoch: 12.1 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04539787555381262		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.04539787555381262 | validation: 0.04259739172246685]
	TIME [epoch: 12.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04164301117286667		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.04164301117286667 | validation: 0.061971971312077156]
	TIME [epoch: 12.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08687825164706219		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.08687825164706219 | validation: 0.07259180082715452]
	TIME [epoch: 12.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059415280524521906		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.059415280524521906 | validation: 0.07887248740474055]
	TIME [epoch: 12.1 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05485436654997877		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.05485436654997877 | validation: 0.05454392262286739]
	TIME [epoch: 12.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05023758487503943		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.05023758487503943 | validation: 0.0568495531191535]
	TIME [epoch: 12.1 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057503573988541064		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.057503573988541064 | validation: 0.033362620142077734]
	TIME [epoch: 12.1 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03443640267551622		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.03443640267551622 | validation: 0.04020041918777144]
	TIME [epoch: 12.1 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05889514228309007		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.05889514228309007 | validation: 0.08226958592914403]
	TIME [epoch: 12.1 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05154492662985655		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.05154492662985655 | validation: 0.020943591050447477]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041926935546571446		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.041926935546571446 | validation: 0.08213891513838933]
	TIME [epoch: 12.1 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08215785231737133		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.08215785231737133 | validation: 0.05670017520427533]
	TIME [epoch: 12.1 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03881363561870531		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.03881363561870531 | validation: 0.05241567229416032]
	TIME [epoch: 12.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056376536723835934		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.056376536723835934 | validation: 0.04919681269704819]
	TIME [epoch: 12.1 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04718584905617716		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.04718584905617716 | validation: 0.03580290121380308]
	TIME [epoch: 12.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03818909907029721		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.03818909907029721 | validation: 0.03432347702605179]
	TIME [epoch: 12.1 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06843813866243197		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.06843813866243197 | validation: 0.04602420701768157]
	TIME [epoch: 12.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04180742121121411		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.04180742121121411 | validation: 0.08648137361815256]
	TIME [epoch: 12.1 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047441686764547145		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.047441686764547145 | validation: 0.024803493198130116]
	TIME [epoch: 12.1 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05056936073886653		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.05056936073886653 | validation: 0.0594283246469996]
	TIME [epoch: 12.1 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05731589485400188		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.05731589485400188 | validation: 0.04542767489036796]
	TIME [epoch: 12.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037264733051751604		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.037264733051751604 | validation: 0.025978792602146222]
	TIME [epoch: 12.1 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03387158766917975		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.03387158766917975 | validation: 0.064602550910107]
	TIME [epoch: 12.1 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049185902739908964		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.049185902739908964 | validation: 0.047394275826419335]
	TIME [epoch: 12.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03760212070095745		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.03760212070095745 | validation: 0.03597191134464913]
	TIME [epoch: 12.1 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052440019797785335		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.052440019797785335 | validation: 0.0345705510314534]
	TIME [epoch: 12.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025975472320355637		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.025975472320355637 | validation: 0.03532014588069228]
	TIME [epoch: 12.1 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048176379305990925		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.048176379305990925 | validation: 0.025531285429571872]
	TIME [epoch: 12.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035138498635158326		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.035138498635158326 | validation: 0.06755133808989675]
	TIME [epoch: 12.1 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06573889045113186		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.06573889045113186 | validation: 0.036958733202175825]
	TIME [epoch: 12.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029921101211657312		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.029921101211657312 | validation: 0.03734467001241081]
	TIME [epoch: 12.1 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03846513618264201		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.03846513618264201 | validation: 0.04275686303624526]
	TIME [epoch: 12.1 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059676852857441565		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.059676852857441565 | validation: 0.04960190184000511]
	TIME [epoch: 12.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034957439628133226		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.034957439628133226 | validation: 0.02862885305211807]
	TIME [epoch: 12.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04295138406431628		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.04295138406431628 | validation: 0.04458775463244754]
	TIME [epoch: 12.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04084318180638713		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.04084318180638713 | validation: 0.026032883293574956]
	TIME [epoch: 12.1 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02413180911236888		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.02413180911236888 | validation: 0.022612324587604406]
	TIME [epoch: 12.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07091251816662754		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.07091251816662754 | validation: 0.04104657528690982]
	TIME [epoch: 12.1 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05196051073618563		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.05196051073618563 | validation: 0.06597254084200596]
	TIME [epoch: 12.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044673084031696945		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.044673084031696945 | validation: 0.024854950882292032]
	TIME [epoch: 12.1 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023299453397944057		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.023299453397944057 | validation: 0.023396843642129225]
	TIME [epoch: 12.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026537764175450902		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.026537764175450902 | validation: 0.030143787620361927]
	TIME [epoch: 12.1 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050322595044786055		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.050322595044786055 | validation: 0.04496225246368829]
	TIME [epoch: 12.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04878784513209274		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.04878784513209274 | validation: 0.07532465091739582]
	TIME [epoch: 12.1 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038963178956605816		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.038963178956605816 | validation: 0.02261295634598296]
	TIME [epoch: 12.1 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02646688814219837		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.02646688814219837 | validation: 0.03563783768248039]
	TIME [epoch: 12.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03330511596898394		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.03330511596898394 | validation: 0.030072548961554393]
	TIME [epoch: 12.1 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046156021237590654		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.046156021237590654 | validation: 0.018186675067239696]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027196991019190174		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.027196991019190174 | validation: 0.030346897729817086]
	TIME [epoch: 12.1 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02684567767681105		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.02684567767681105 | validation: 0.034580038086488546]
	TIME [epoch: 12.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03217342952747752		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.03217342952747752 | validation: 0.025504390467006546]
	TIME [epoch: 12.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03365915676099232		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.03365915676099232 | validation: 0.1776041914013371]
	TIME [epoch: 12.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08916502568396804		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.08916502568396804 | validation: 0.04748289914529204]
	TIME [epoch: 12.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0387144985890855		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.0387144985890855 | validation: 0.02360152316954691]
	TIME [epoch: 12.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020506965050249213		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.020506965050249213 | validation: 0.04299445856696733]
	TIME [epoch: 12.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03429160287519497		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.03429160287519497 | validation: 0.03302112349697815]
	TIME [epoch: 12.1 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038236061300649875		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.038236061300649875 | validation: 0.037278405640724835]
	TIME [epoch: 12.1 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030900855906532874		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.030900855906532874 | validation: 0.022709859553450833]
	TIME [epoch: 12.1 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0208462241936393		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.0208462241936393 | validation: 0.04116876088595227]
	TIME [epoch: 12.1 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049727440201054504		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.049727440201054504 | validation: 0.02527433794105039]
	TIME [epoch: 12.1 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030683934800363022		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.030683934800363022 | validation: 0.02756017219788958]
	TIME [epoch: 12.1 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026191802520304848		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.026191802520304848 | validation: 0.03785664042780047]
	TIME [epoch: 12.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03580423788680304		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.03580423788680304 | validation: 0.0479559696190424]
	TIME [epoch: 12.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028586815132204187		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.028586815132204187 | validation: 0.04732533755010441]
	TIME [epoch: 12.1 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04107623701759759		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.04107623701759759 | validation: 0.031972897063274296]
	TIME [epoch: 12.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030469177774551822		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.030469177774551822 | validation: 0.018164341378577815]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02959547258865966		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.02959547258865966 | validation: 0.01698853655977592]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02518186142228122		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.02518186142228122 | validation: 0.015436745514193596]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03624859700949698		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.03624859700949698 | validation: 0.04350458754615874]
	TIME [epoch: 12.1 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024707158063424235		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.024707158063424235 | validation: 0.02673433778628278]
	TIME [epoch: 12.1 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024733402592318165		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.024733402592318165 | validation: 0.036803696143374816]
	TIME [epoch: 12.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035932752401675955		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.035932752401675955 | validation: 0.05909937883258896]
	TIME [epoch: 12.1 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03968818212319733		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03968818212319733 | validation: 0.023050418464032402]
	TIME [epoch: 12.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02355299139219464		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.02355299139219464 | validation: 0.032334716175752475]
	TIME [epoch: 12.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03555271102631362		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.03555271102631362 | validation: 0.03602738169279816]
	TIME [epoch: 12.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03627613242365437		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.03627613242365437 | validation: 0.029672825885712177]
	TIME [epoch: 12.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03294110149775396		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.03294110149775396 | validation: 0.026307639226089065]
	TIME [epoch: 12.1 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02800204884474701		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.02800204884474701 | validation: 0.020954346716953627]
	TIME [epoch: 12.1 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033801780344568884		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.033801780344568884 | validation: 0.0239067330048364]
	TIME [epoch: 12.1 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0235378849910812		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.0235378849910812 | validation: 0.028301137472886057]
	TIME [epoch: 12.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03587158180449196		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.03587158180449196 | validation: 0.01817482254015872]
	TIME [epoch: 12.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02489332703584369		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.02489332703584369 | validation: 0.025668757805797048]
	TIME [epoch: 12.1 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023094237229967614		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.023094237229967614 | validation: 0.030848095309596257]
	TIME [epoch: 12.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03131891337862282		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.03131891337862282 | validation: 0.03164348522873216]
	TIME [epoch: 12.1 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02124887956140876		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.02124887956140876 | validation: 0.019230488437727383]
	TIME [epoch: 12.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028234236750197567		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.028234236750197567 | validation: 0.048980705280319926]
	TIME [epoch: 12.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03067199767454504		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.03067199767454504 | validation: 0.03395481380734253]
	TIME [epoch: 12.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02651740275737772		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.02651740275737772 | validation: 0.01706625421930586]
	TIME [epoch: 12.1 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024087476988071694		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.024087476988071694 | validation: 0.060405168797490756]
	TIME [epoch: 12.1 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037880064973394204		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.037880064973394204 | validation: 0.033223266297298965]
	TIME [epoch: 12.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02329871278665164		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.02329871278665164 | validation: 0.015411979963815317]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03833693294716463		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03833693294716463 | validation: 0.031030916997513827]
	TIME [epoch: 12.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021863072823151662		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.021863072823151662 | validation: 0.02028911478847136]
	TIME [epoch: 12.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017980486993439862		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.017980486993439862 | validation: 0.019249232235390757]
	TIME [epoch: 12.1 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027795620836732544		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.027795620836732544 | validation: 0.039547092827985436]
	TIME [epoch: 12.1 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04903826321273959		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.04903826321273959 | validation: 0.056720856817586617]
	TIME [epoch: 12.1 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03613829467250896		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.03613829467250896 | validation: 0.015582629447035392]
	TIME [epoch: 12.1 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031080965687270087		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.031080965687270087 | validation: 0.01955731036254279]
	TIME [epoch: 12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020398505537849516		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.020398505537849516 | validation: 0.047965117337292945]
	TIME [epoch: 12.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029895215512086792		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.029895215512086792 | validation: 0.019574754533468654]
	TIME [epoch: 12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021619666733127697		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.021619666733127697 | validation: 0.02083700936467372]
	TIME [epoch: 12.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025758784464104123		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.025758784464104123 | validation: 0.04055397595598521]
	TIME [epoch: 12.1 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0241524678289889		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.0241524678289889 | validation: 0.015542597165338984]
	TIME [epoch: 12.1 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024154455874728892		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.024154455874728892 | validation: 0.01626649091133379]
	TIME [epoch: 12.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02256393026318483		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.02256393026318483 | validation: 0.022956516338585177]
	TIME [epoch: 12.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01960861892821162		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.01960861892821162 | validation: 0.01469988428934047]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027793495067885796		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.027793495067885796 | validation: 0.03217007214900778]
	TIME [epoch: 12.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023891532168299475		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.023891532168299475 | validation: 0.016427863991607478]
	TIME [epoch: 12.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03448266715609107		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.03448266715609107 | validation: 0.02068179856973236]
	TIME [epoch: 12.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02964813781884352		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.02964813781884352 | validation: 0.016398008700724217]
	TIME [epoch: 12.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017223186482548777		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.017223186482548777 | validation: 0.018487351042880233]
	TIME [epoch: 12.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023761342757596827		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.023761342757596827 | validation: 0.01996102760547514]
	TIME [epoch: 12.1 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02467539103306282		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.02467539103306282 | validation: 0.03292452885451971]
	TIME [epoch: 12.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025623173329049		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.025623173329049 | validation: 0.02062639665746932]
	TIME [epoch: 12.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022341911203591934		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.022341911203591934 | validation: 0.021998752204109608]
	TIME [epoch: 12.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02068895426936711		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.02068895426936711 | validation: 0.018723307611183813]
	TIME [epoch: 12.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026438452099929163		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.026438452099929163 | validation: 0.021497595844586333]
	TIME [epoch: 12.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019697635954875725		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.019697635954875725 | validation: 0.019289782066054556]
	TIME [epoch: 12.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02285027026140497		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.02285027026140497 | validation: 0.0365371410521657]
	TIME [epoch: 12.1 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026996457519563748		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.026996457519563748 | validation: 0.025691683264646173]
	TIME [epoch: 12.1 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020870941242941754		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.020870941242941754 | validation: 0.017858421435648877]
	TIME [epoch: 12.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021271950927379164		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.021271950927379164 | validation: 0.10396150240506302]
	TIME [epoch: 12.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053702469243652046		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.053702469243652046 | validation: 0.016964683963110447]
	TIME [epoch: 12.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01827300341418801		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.01827300341418801 | validation: 0.030025870581210784]
	TIME [epoch: 12.1 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019291727404907432		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.019291727404907432 | validation: 0.016146178249897]
	TIME [epoch: 12.1 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025926431897683774		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.025926431897683774 | validation: 0.017254828121067822]
	TIME [epoch: 12.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017681209598378603		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.017681209598378603 | validation: 0.01607215574889609]
	TIME [epoch: 12.1 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021577142596612815		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.021577142596612815 | validation: 0.031834925701582764]
	TIME [epoch: 12.1 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019906287361397292		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.019906287361397292 | validation: 0.017898023948344187]
	TIME [epoch: 12.1 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01971887873913097		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.01971887873913097 | validation: 0.015709674687810746]
	TIME [epoch: 12.1 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027969879054125145		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.027969879054125145 | validation: 0.03143393468480638]
	TIME [epoch: 12.1 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042111770122357284		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.042111770122357284 | validation: 0.04610658097157255]
	TIME [epoch: 12.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03142180012692608		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.03142180012692608 | validation: 0.017782122372461497]
	TIME [epoch: 12.1 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01352050441738924		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.01352050441738924 | validation: 0.014177166080270094]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013839612095861088		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.013839612095861088 | validation: 0.014192266315845118]
	TIME [epoch: 12.1 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027696116288940392		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.027696116288940392 | validation: 0.01618792510830136]
	TIME [epoch: 12.1 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020213560631072262		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.020213560631072262 | validation: 0.024541584572030078]
	TIME [epoch: 12.1 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01649048142977124		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.01649048142977124 | validation: 0.08956477695835949]
	TIME [epoch: 12.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052451760468721284		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.052451760468721284 | validation: 0.022663295307248282]
	TIME [epoch: 12.1 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01453186135197007		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.01453186135197007 | validation: 0.011378845129583605]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0149738328141672		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.0149738328141672 | validation: 0.026675844897397105]
	TIME [epoch: 12.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020490065801206356		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.020490065801206356 | validation: 0.013048922005078813]
	TIME [epoch: 12.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015886627142103912		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.015886627142103912 | validation: 0.02109625268690311]
	TIME [epoch: 12.1 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028197470001657815		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.028197470001657815 | validation: 0.044141814126995645]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028902381139291007		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.028902381139291007 | validation: 0.019763061302787037]
	TIME [epoch: 12.1 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01690703326354782		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.01690703326354782 | validation: 0.015537940976500018]
	TIME [epoch: 12.1 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02152908374431116		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.02152908374431116 | validation: 0.021732619765255826]
	TIME [epoch: 12.1 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016577065994907766		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.016577065994907766 | validation: 0.01663939925021425]
	TIME [epoch: 12.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01919618500884922		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.01919618500884922 | validation: 0.018022745144575393]
	TIME [epoch: 12.1 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016354203818684916		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.016354203818684916 | validation: 0.016011010506557594]
	TIME [epoch: 12.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02140074440138591		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.02140074440138591 | validation: 0.019950429065639888]
	TIME [epoch: 12.1 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014735196418309143		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.014735196418309143 | validation: 0.03225056053966113]
	TIME [epoch: 12.1 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022518599218635565		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.022518599218635565 | validation: 0.02706084272311879]
	TIME [epoch: 12.1 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01934950964295287		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.01934950964295287 | validation: 0.01540489838019354]
	TIME [epoch: 12.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01490620830139451		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.01490620830139451 | validation: 0.01446767278938298]
	TIME [epoch: 12.1 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025167843855580255		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.025167843855580255 | validation: 0.024328607962794235]
	TIME [epoch: 12.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015246341783223665		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.015246341783223665 | validation: 0.018989407729861456]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019713592538616723		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.019713592538616723 | validation: 0.02211312804716848]
	TIME [epoch: 12.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018902282875221946		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.018902282875221946 | validation: 0.012576407072677612]
	TIME [epoch: 12.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01708824018809287		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.01708824018809287 | validation: 0.02057731566047784]
	TIME [epoch: 12.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01831535236553223		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.01831535236553223 | validation: 0.02977596905445047]
	TIME [epoch: 12.1 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01887209407180826		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.01887209407180826 | validation: 0.023135770681701973]
	TIME [epoch: 12.1 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01404119173758004		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.01404119173758004 | validation: 0.013897073048259108]
	TIME [epoch: 12.1 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01891255330168565		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.01891255330168565 | validation: 0.02187702015995213]
	TIME [epoch: 12.1 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015000499763537957		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.015000499763537957 | validation: 0.020730573505269252]
	TIME [epoch: 12.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023844600324889168		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.023844600324889168 | validation: 0.020873145932625667]
	TIME [epoch: 12.1 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014915468700278451		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.014915468700278451 | validation: 0.014136821592250734]
	TIME [epoch: 12.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016975552475711544		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.016975552475711544 | validation: 0.0201753935879629]
	TIME [epoch: 12.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014106695474186852		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.014106695474186852 | validation: 0.025366064449468498]
	TIME [epoch: 12.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020096822203528927		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.020096822203528927 | validation: 0.03565790861126715]
	TIME [epoch: 12.1 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024012988138698777		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.024012988138698777 | validation: 0.01802800691945843]
	TIME [epoch: 12.1 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014166139426639052		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.014166139426639052 | validation: 0.021958493723473152]
	TIME [epoch: 12.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01661607129897622		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.01661607129897622 | validation: 0.011266382083444874]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014388678954934894		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.014388678954934894 | validation: 0.015797171312084655]
	TIME [epoch: 12.1 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014932663472952865		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.014932663472952865 | validation: 0.01490434612637569]
	TIME [epoch: 12.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013718394703207886		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.013718394703207886 | validation: 0.01559560661690591]
	TIME [epoch: 12.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013300266133793447		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.013300266133793447 | validation: 0.01585460693813647]
	TIME [epoch: 12.1 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01654228545225951		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.01654228545225951 | validation: 0.03942674810179954]
	TIME [epoch: 12.1 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022317040157451947		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.022317040157451947 | validation: 0.01757658476341665]
	TIME [epoch: 12.1 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01535614013624476		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.01535614013624476 | validation: 0.016800466883552998]
	TIME [epoch: 12.1 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013088903900869359		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.013088903900869359 | validation: 0.01357663105223323]
	TIME [epoch: 12.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013770447783251719		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.013770447783251719 | validation: 0.024732215086887194]
	TIME [epoch: 12.1 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02551176691829686		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.02551176691829686 | validation: 0.014880689761551161]
	TIME [epoch: 12.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045269936419417234		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.045269936419417234 | validation: 0.052249193645162534]
	TIME [epoch: 12.1 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026473501091142382		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.026473501091142382 | validation: 0.015061059743764515]
	TIME [epoch: 12.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010839240687528513		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.010839240687528513 | validation: 0.013375632963001809]
	TIME [epoch: 12.1 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014077863727943727		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.014077863727943727 | validation: 0.014222309184368096]
	TIME [epoch: 12.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013078518080299155		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.013078518080299155 | validation: 0.015026596928025666]
	TIME [epoch: 12.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015824063260159456		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.015824063260159456 | validation: 0.021704547794661317]
	TIME [epoch: 12.1 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013659388801625686		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.013659388801625686 | validation: 0.01105125278428094]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012108291778616189		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.012108291778616189 | validation: 0.02179204308175818]
	TIME [epoch: 12.1 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020528737790384558		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.020528737790384558 | validation: 0.013965244342085952]
	TIME [epoch: 12.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01287052891878935		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01287052891878935 | validation: 0.019098217361604466]
	TIME [epoch: 12.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013737248300442503		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.013737248300442503 | validation: 0.012811676711604755]
	TIME [epoch: 12.1 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011331681053141803		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.011331681053141803 | validation: 0.015980089952476285]
	TIME [epoch: 12.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014639363766359652		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.014639363766359652 | validation: 0.01672821337695514]
	TIME [epoch: 12.1 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016249669478907783		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.016249669478907783 | validation: 0.04407397772438143]
	TIME [epoch: 12.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021820366229053065		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.021820366229053065 | validation: 0.011084992998032625]
	TIME [epoch: 12.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011436198766158262		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.011436198766158262 | validation: 0.01048216911402803]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013367277131987425		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.013367277131987425 | validation: 0.015970713966729805]
	TIME [epoch: 12.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011615256747750316		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.011615256747750316 | validation: 0.019847971017403786]
	TIME [epoch: 12.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01952960631982032		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.01952960631982032 | validation: 0.011523633582565512]
	TIME [epoch: 12.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010440582035536598		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.010440582035536598 | validation: 0.010076088953414865]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010880052031585121		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.010880052031585121 | validation: 0.013097195854133593]
	TIME [epoch: 12.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013993765314347427		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.013993765314347427 | validation: 0.01116564571141377]
	TIME [epoch: 12.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013916068536087862		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.013916068536087862 | validation: 0.019401235617318158]
	TIME [epoch: 12.1 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011510567984920752		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.011510567984920752 | validation: 0.010763030812967048]
	TIME [epoch: 12.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015077177938645788		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.015077177938645788 | validation: 0.02026531369520693]
	TIME [epoch: 12.1 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014796582654385316		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.014796582654385316 | validation: 0.013535935584740491]
	TIME [epoch: 12.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013063176502654384		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.013063176502654384 | validation: 0.011737099853300953]
	TIME [epoch: 12.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011067322245651375		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.011067322245651375 | validation: 0.01600485854014801]
	TIME [epoch: 12.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013046207962941625		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.013046207962941625 | validation: 0.01799085324315803]
	TIME [epoch: 12.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015664274108535638		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.015664274108535638 | validation: 0.03505441250907983]
	TIME [epoch: 12.1 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016349125774015316		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.016349125774015316 | validation: 0.010499419933655045]
	TIME [epoch: 12.1 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013984513892420777		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.013984513892420777 | validation: 0.015777836156043598]
	TIME [epoch: 12.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01218958224472668		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.01218958224472668 | validation: 0.011171003118691052]
	TIME [epoch: 12.1 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01025195510961212		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.01025195510961212 | validation: 0.012442684630241983]
	TIME [epoch: 12.1 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011004072470864237		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.011004072470864237 | validation: 0.013329275814536239]
	TIME [epoch: 12.1 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013095150573559693		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.013095150573559693 | validation: 0.0125254169967504]
	TIME [epoch: 12.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01073074458181582		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.01073074458181582 | validation: 0.009966116184376542]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015486451812885452		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.015486451812885452 | validation: 0.016622970598619034]
	TIME [epoch: 12.1 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010145767764241417		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.010145767764241417 | validation: 0.009173494497628455]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010696768632912933		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.010696768632912933 | validation: 0.010231236670805324]
	TIME [epoch: 12.1 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014169399592071016		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.014169399592071016 | validation: 0.021339234365049177]
	TIME [epoch: 12.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01257886195866769		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.01257886195866769 | validation: 0.012091987976938827]
	TIME [epoch: 12.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010114051131811148		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.010114051131811148 | validation: 0.009235476954587004]
	TIME [epoch: 12.1 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011197467042797515		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.011197467042797515 | validation: 0.00807138602858304]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011001081944361436		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.011001081944361436 | validation: 0.014484066712390772]
	TIME [epoch: 12.1 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011281483945876363		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.011281483945876363 | validation: 0.01290974252245991]
	TIME [epoch: 12.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013886957601487502		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.013886957601487502 | validation: 0.012654276312836997]
	TIME [epoch: 12.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014402210797355356		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.014402210797355356 | validation: 0.009670893853621836]
	TIME [epoch: 12.1 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009259192476138994		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.009259192476138994 | validation: 0.010779779036810539]
	TIME [epoch: 12.1 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010056554077667122		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.010056554077667122 | validation: 0.014353402076461415]
	TIME [epoch: 12.1 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011456646332468954		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.011456646332468954 | validation: 0.011905498181000264]
	TIME [epoch: 12.1 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023615760116364307		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.023615760116364307 | validation: 0.016826538794089456]
	TIME [epoch: 12.1 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012433712936987201		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.012433712936987201 | validation: 0.0076720466261264155]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008007331144567437		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.008007331144567437 | validation: 0.006839873789825832]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009149637759070881		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.009149637759070881 | validation: 0.009615799922995905]
	TIME [epoch: 12.1 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007491891908585561		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.007491891908585561 | validation: 0.006220493708425034]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010579843071460872		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.010579843071460872 | validation: 0.016117884532235945]
	TIME [epoch: 12.1 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011842790013280438		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.011842790013280438 | validation: 0.009416954807382966]
	TIME [epoch: 12.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01088150123151522		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.01088150123151522 | validation: 0.008542319645871193]
	TIME [epoch: 12.1 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008444245013447926		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.008444245013447926 | validation: 0.013508417082727597]
	TIME [epoch: 12.1 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01226697474443357		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.01226697474443357 | validation: 0.013368587589974477]
	TIME [epoch: 12.1 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010637774780148136		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.010637774780148136 | validation: 0.0071570552039829476]
	TIME [epoch: 12.1 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009022109040451353		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.009022109040451353 | validation: 0.01727309288511615]
	TIME [epoch: 12.1 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010926050576101298		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.010926050576101298 | validation: 0.009302769579841339]
	TIME [epoch: 12.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00973709224728175		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.00973709224728175 | validation: 0.01664807823298152]
	TIME [epoch: 12.1 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00970700973667881		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.00970700973667881 | validation: 0.006572066163867899]
	TIME [epoch: 12.1 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007455770494397808		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.007455770494397808 | validation: 0.015898584857690183]
	TIME [epoch: 12.1 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014161789139071398		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.014161789139071398 | validation: 0.008147837605792928]
	TIME [epoch: 12.1 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006664642492618486		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.006664642492618486 | validation: 0.0106718617069377]
	TIME [epoch: 12.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01072279921895375		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.01072279921895375 | validation: 0.009654612923894652]
	TIME [epoch: 12.1 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011370166960448132		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.011370166960448132 | validation: 0.013149116875253145]
	TIME [epoch: 12.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00928924951473107		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.00928924951473107 | validation: 0.007401209283939822]
	TIME [epoch: 12.1 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010575756569730262		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.010575756569730262 | validation: 0.016101403401693894]
	TIME [epoch: 12.1 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01424549023335193		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.01424549023335193 | validation: 0.010740659923461997]
	TIME [epoch: 12.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008819994065290252		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.008819994065290252 | validation: 0.014392892819692487]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00969826851014508		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.00969826851014508 | validation: 0.009507018460267776]
	TIME [epoch: 12.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007628697515934603		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.007628697515934603 | validation: 0.008039948261938296]
	TIME [epoch: 12.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009173178501044994		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.009173178501044994 | validation: 0.010519312147668745]
	TIME [epoch: 12.1 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010329699578053771		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.010329699578053771 | validation: 0.006915567813612307]
	TIME [epoch: 12.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007627777265420335		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.007627777265420335 | validation: 0.01322909458485057]
	TIME [epoch: 12.1 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010283753729396673		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.010283753729396673 | validation: 0.010678536589468452]
	TIME [epoch: 12.1 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00635878103744587		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.00635878103744587 | validation: 0.007215687805584118]
	TIME [epoch: 441 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008462230665687099		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.008462230665687099 | validation: 0.008126897738850522]
	TIME [epoch: 25.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01174271969629199		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.01174271969629199 | validation: 0.007352313908528679]
	TIME [epoch: 25.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007585815087348166		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.007585815087348166 | validation: 0.005298575264007076]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074613880694450255		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.0074613880694450255 | validation: 0.010688079975451945]
	TIME [epoch: 25.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00714300932810239		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.00714300932810239 | validation: 0.014592735623549907]
	TIME [epoch: 25.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012450669036093511		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.012450669036093511 | validation: 0.008875387571443051]
	TIME [epoch: 26 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008335683316554765		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.008335683316554765 | validation: 0.007396672516019611]
	TIME [epoch: 25.9 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012813192020595564		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.012813192020595564 | validation: 0.011618407820932743]
	TIME [epoch: 25.9 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008267771017843841		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.008267771017843841 | validation: 0.007183314073729514]
	TIME [epoch: 25.9 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071940276801615265		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.0071940276801615265 | validation: 0.00817819359416627]
	TIME [epoch: 25.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006289699570640269		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.006289699570640269 | validation: 0.007139563537474143]
	TIME [epoch: 25.9 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005833030399703116		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.005833030399703116 | validation: 0.013401726214244458]
	TIME [epoch: 25.9 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011173624947248308		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.011173624947248308 | validation: 0.006894481245302405]
	TIME [epoch: 25.9 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006443878120250668		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.006443878120250668 | validation: 0.00703899710971065]
	TIME [epoch: 25.9 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00686797530422973		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.00686797530422973 | validation: 0.011494213783041971]
	TIME [epoch: 25.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008518184444865316		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.008518184444865316 | validation: 0.004933294156747165]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007736009026887912		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.007736009026887912 | validation: 0.012298217223256896]
	TIME [epoch: 26 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009486430064264582		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.009486430064264582 | validation: 0.0078032906995097896]
	TIME [epoch: 25.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00636902237612108		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.00636902237612108 | validation: 0.008059719788752664]
	TIME [epoch: 25.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007010741928002974		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.007010741928002974 | validation: 0.009948908478701422]
	TIME [epoch: 25.9 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008206888627013912		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.008206888627013912 | validation: 0.008367555194348585]
	TIME [epoch: 25.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00642765416386505		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.00642765416386505 | validation: 0.004925730619543033]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006968030120403425		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.006968030120403425 | validation: 0.013623537972493601]
	TIME [epoch: 25.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008642367296578104		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.008642367296578104 | validation: 0.006728871542738629]
	TIME [epoch: 25.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005833879705765871		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.005833879705765871 | validation: 0.009962217942996095]
	TIME [epoch: 25.9 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007841312244452373		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.007841312244452373 | validation: 0.006147722209166934]
	TIME [epoch: 25.9 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005854303479035768		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.005854303479035768 | validation: 0.008933768806477277]
	TIME [epoch: 25.9 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008669108180895658		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.008669108180895658 | validation: 0.0060257822788998515]
	TIME [epoch: 25.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067775559613591045		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.0067775559613591045 | validation: 0.012175407513254426]
	TIME [epoch: 25.8 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00864826964036317		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.00864826964036317 | validation: 0.008481759933354488]
	TIME [epoch: 25.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011099203967418023		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.011099203967418023 | validation: 0.008215716279821001]
	TIME [epoch: 25.8 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007888985713420279		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.007888985713420279 | validation: 0.00524735520205267]
	TIME [epoch: 25.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005891354525766167		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.005891354525766167 | validation: 0.007122346410178511]
	TIME [epoch: 25.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005398952795452632		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.005398952795452632 | validation: 0.007756594374072693]
	TIME [epoch: 25.9 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006164209694337635		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.006164209694337635 | validation: 0.007395112824380667]
	TIME [epoch: 25.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008243119034002482		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.008243119034002482 | validation: 0.007162850958760187]
	TIME [epoch: 25.9 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005207722951112626		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.005207722951112626 | validation: 0.007141257933415055]
	TIME [epoch: 25.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005547679293658556		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.005547679293658556 | validation: 0.007951321370779942]
	TIME [epoch: 25.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010380340497480304		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.010380340497480304 | validation: 0.008836594064533252]
	TIME [epoch: 26 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008466536962075935		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.008466536962075935 | validation: 0.00777634661467572]
	TIME [epoch: 25.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005003003136348445		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.005003003136348445 | validation: 0.00751650180500095]
	TIME [epoch: 26 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052113309383264046		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.0052113309383264046 | validation: 0.00645099393935593]
	TIME [epoch: 25.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008523948552256806		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.008523948552256806 | validation: 0.0081443133129653]
	TIME [epoch: 25.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005293974563442691		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.005293974563442691 | validation: 0.0082292377874195]
	TIME [epoch: 25.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004850862175903343		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.004850862175903343 | validation: 0.005395953816675757]
	TIME [epoch: 25.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005157538229065343		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.005157538229065343 | validation: 0.011270752895189506]
	TIME [epoch: 25.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009459213888163173		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.009459213888163173 | validation: 0.006062497761422788]
	TIME [epoch: 25.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005148678866840067		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.005148678866840067 | validation: 0.004749379783879916]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005695151512737064		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.005695151512737064 | validation: 0.008438791647975596]
	TIME [epoch: 25.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00673802495831258		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.00673802495831258 | validation: 0.006180391557970253]
	TIME [epoch: 25.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051319572286852785		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.0051319572286852785 | validation: 0.005193174795548592]
	TIME [epoch: 25.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007158797206561922		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.007158797206561922 | validation: 0.006407937873418596]
	TIME [epoch: 25.8 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006805806715544904		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.006805806715544904 | validation: 0.005446416778064456]
	TIME [epoch: 25.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071421608695533744		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.0071421608695533744 | validation: 0.004527628928537291]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003780857824407043		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.003780857824407043 | validation: 0.005586108583445416]
	TIME [epoch: 25.9 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005228503515494335		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.005228503515494335 | validation: 0.01052343540245922]
	TIME [epoch: 25.8 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007066984651695342		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.007066984651695342 | validation: 0.004134915288628699]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00591029428554123		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.00591029428554123 | validation: 0.007615505084195246]
	TIME [epoch: 25.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005682768695528122		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.005682768695528122 | validation: 0.00900524144320935]
	TIME [epoch: 25.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007825148211493502		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.007825148211493502 | validation: 0.006305733373715161]
	TIME [epoch: 25.8 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004108679204510291		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.004108679204510291 | validation: 0.003859661915819104]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005572940600502019		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.005572940600502019 | validation: 0.008852645561270266]
	TIME [epoch: 25.9 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004909742774700606		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.004909742774700606 | validation: 0.004360796641164531]
	TIME [epoch: 25.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006948947447990466		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.006948947447990466 | validation: 0.00531193194070923]
	TIME [epoch: 25.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007179497033855153		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.007179497033855153 | validation: 0.0077091878061644765]
	TIME [epoch: 25.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004919438201073538		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.004919438201073538 | validation: 0.00476362872206549]
	TIME [epoch: 25.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003478273446682764		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.003478273446682764 | validation: 0.0038116841346683347]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036554705152223414		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.0036554705152223414 | validation: 0.008368258131460088]
	TIME [epoch: 25.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007435996546531154		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.007435996546531154 | validation: 0.00541029312079415]
	TIME [epoch: 25.9 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005559803428538589		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.005559803428538589 | validation: 0.006141315851388082]
	TIME [epoch: 26 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004578766699521561		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.004578766699521561 | validation: 0.0040311174761707095]
	TIME [epoch: 26 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072269830633165996		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.0072269830633165996 | validation: 0.006334253916135685]
	TIME [epoch: 26 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005504413951964594		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.005504413951964594 | validation: 0.0060094697195713885]
	TIME [epoch: 25.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009065871905610146		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.009065871905610146 | validation: 0.025084042126563164]
	TIME [epoch: 25.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010597674951554028		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.010597674951554028 | validation: 0.006652568900348614]
	TIME [epoch: 25.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00548024378217366		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.00548024378217366 | validation: 0.00664983545406009]
	TIME [epoch: 25.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602274590536791		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.004602274590536791 | validation: 0.0038426219620579837]
	TIME [epoch: 26 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052592641804671166		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.0052592641804671166 | validation: 0.010875421511839026]
	TIME [epoch: 25.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005142081564619524		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.005142081564619524 | validation: 0.005634758014095289]
	TIME [epoch: 25.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004001563079012718		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.004001563079012718 | validation: 0.004933579162782186]
	TIME [epoch: 25.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003747606258636216		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.003747606258636216 | validation: 0.005676539122320242]
	TIME [epoch: 25.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00627711950377255		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.00627711950377255 | validation: 0.008114768341915829]
	TIME [epoch: 25.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005560032057758967		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.005560032057758967 | validation: 0.004998735797571735]
	TIME [epoch: 25.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051955958535379225		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0051955958535379225 | validation: 0.0055853061173946564]
	TIME [epoch: 26 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004414657205842955		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.004414657205842955 | validation: 0.005292399579008532]
	TIME [epoch: 26 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003982409642954643		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.003982409642954643 | validation: 0.004989085864922299]
	TIME [epoch: 25.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059283054073959495		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.0059283054073959495 | validation: 0.00959835156243797]
	TIME [epoch: 25.9 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054891084814469235		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.0054891084814469235 | validation: 0.0037363181029931416]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033642233058255445		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.0033642233058255445 | validation: 0.005712955821404785]
	TIME [epoch: 25.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004722137200167146		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.004722137200167146 | validation: 0.002786853898624562]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055343507505660056		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.0055343507505660056 | validation: 0.004682452463654908]
	TIME [epoch: 25.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031280514482698064		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.0031280514482698064 | validation: 0.003910311999310679]
	TIME [epoch: 25.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005794198471392361		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.005794198471392361 | validation: 0.007386114753766277]
	TIME [epoch: 25.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005665737942873518		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.005665737942873518 | validation: 0.0043945136011131316]
	TIME [epoch: 25.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031628023499277987		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.0031628023499277987 | validation: 0.004190044513695489]
	TIME [epoch: 26 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008285921698040636		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.008285921698040636 | validation: 0.013881422475328153]
	TIME [epoch: 26 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007625178798062844		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.007625178798062844 | validation: 0.004612700563682564]
	TIME [epoch: 25.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038416406013409895		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.0038416406013409895 | validation: 0.004475996243761123]
	TIME [epoch: 25.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004063988423890098		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.004063988423890098 | validation: 0.005197115421243929]
	TIME [epoch: 25.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003941245107144561		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.003941245107144561 | validation: 0.004913774871293218]
	TIME [epoch: 25.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005591402390140343		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.005591402390140343 | validation: 0.002912636099589378]
	TIME [epoch: 25.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004728060187545824		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.004728060187545824 | validation: 0.004377995571400828]
	TIME [epoch: 25.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033595220257974356		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0033595220257974356 | validation: 0.004205385225519388]
	TIME [epoch: 25.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004721912185826539		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.004721912185826539 | validation: 0.007667570007789417]
	TIME [epoch: 25.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004791103208819271		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.004791103208819271 | validation: 0.006515666853173378]
	TIME [epoch: 25.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037968764441993407		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0037968764441993407 | validation: 0.003263920969402149]
	TIME [epoch: 25.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003136729646039798		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.003136729646039798 | validation: 0.004350655001083019]
	TIME [epoch: 25.8 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005781637793690667		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.005781637793690667 | validation: 0.0041023627708527905]
	TIME [epoch: 25.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039858244924974705		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0039858244924974705 | validation: 0.0032316016735629027]
	TIME [epoch: 25.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027705834040465255		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.0027705834040465255 | validation: 0.0036452901414292197]
	TIME [epoch: 25.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039461309391558745		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.0039461309391558745 | validation: 0.0069406634418982815]
	TIME [epoch: 25.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004653083742249428		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.004653083742249428 | validation: 0.005996960060041737]
	TIME [epoch: 25.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00389946428028204		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.00389946428028204 | validation: 0.007548617687928187]
	TIME [epoch: 25.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004089201248086842		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.004089201248086842 | validation: 0.0037071916803351626]
	TIME [epoch: 25.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00424857299569294		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.00424857299569294 | validation: 0.004546846450567343]
	TIME [epoch: 25.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030769381839246155		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.0030769381839246155 | validation: 0.004706374150487823]
	TIME [epoch: 25.8 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035702027551662643		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.0035702027551662643 | validation: 0.007020320886018535]
	TIME [epoch: 25.8 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00517280496516718		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.00517280496516718 | validation: 0.005500341571233183]
	TIME [epoch: 25.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034273856390834717		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.0034273856390834717 | validation: 0.005140553103965149]
	TIME [epoch: 25.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002890603725638242		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.002890603725638242 | validation: 0.006724957220452481]
	TIME [epoch: 25.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004260079251705733		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.004260079251705733 | validation: 0.0035091807972071755]
	TIME [epoch: 25.8 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003586708927626226		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.003586708927626226 | validation: 0.004533901605347555]
	TIME [epoch: 25.8 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004036462504166669		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.004036462504166669 | validation: 0.004518368379462755]
	TIME [epoch: 25.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00493086826054151		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.00493086826054151 | validation: 0.004203038133297108]
	TIME [epoch: 25.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027093115621862657		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.0027093115621862657 | validation: 0.0038620260304643864]
	TIME [epoch: 25.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038211036571018106		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.0038211036571018106 | validation: 0.0038480711730521674]
	TIME [epoch: 25.9 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027483872379081616		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.0027483872379081616 | validation: 0.0047781437843943215]
	TIME [epoch: 25.8 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00503803320856294		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.00503803320856294 | validation: 0.0044716392775206604]
	TIME [epoch: 25.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033913419865941943		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.0033913419865941943 | validation: 0.0036841284746913553]
	TIME [epoch: 25.8 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026181812376949005		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.0026181812376949005 | validation: 0.0031903566243246433]
	TIME [epoch: 25.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004480107913647346		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.004480107913647346 | validation: 0.006969200670464285]
	TIME [epoch: 25.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004518054941329493		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.004518054941329493 | validation: 0.003187150114558189]
	TIME [epoch: 25.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003080412717588938		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.003080412717588938 | validation: 0.004571210866999298]
	TIME [epoch: 25.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033043991218279084		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.0033043991218279084 | validation: 0.0039869146600700935]
	TIME [epoch: 25.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003369623313139697		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.003369623313139697 | validation: 0.0037067913403683334]
	TIME [epoch: 26 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002623054520881776		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.002623054520881776 | validation: 0.003980120314018976]
	TIME [epoch: 26 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051192285240746		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0051192285240746 | validation: 0.0048642792986966985]
	TIME [epoch: 25.9 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004241310361986772		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.004241310361986772 | validation: 0.0031882866038060193]
	TIME [epoch: 25.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024803050527426916		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.0024803050527426916 | validation: 0.002520997870765381]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002264071225098102		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.002264071225098102 | validation: 0.0037161434423527285]
	TIME [epoch: 25.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044547687038587035		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.0044547687038587035 | validation: 0.00460111468504605]
	TIME [epoch: 25.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003336602130386118		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.003336602130386118 | validation: 0.004441796817340026]
	TIME [epoch: 26 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003490945703821494		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.003490945703821494 | validation: 0.0029759978362126557]
	TIME [epoch: 25.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003911804542628817		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.003911804542628817 | validation: 0.0034948753840956686]
	TIME [epoch: 25.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037461104077546273		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.0037461104077546273 | validation: 0.005383675505057193]
	TIME [epoch: 25.8 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030531503851925572		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.0030531503851925572 | validation: 0.0072932728265351935]
	TIME [epoch: 25.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005667333702775992		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.005667333702775992 | validation: 0.004636132695186994]
	TIME [epoch: 25.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031914616476745672		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.0031914616476745672 | validation: 0.004076038479953899]
	TIME [epoch: 25.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003705752368819106		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.003705752368819106 | validation: 0.002735115033022321]
	TIME [epoch: 26 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022268306268238645		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.0022268306268238645 | validation: 0.0029532803400082407]
	TIME [epoch: 26 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003228044096599236		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.003228044096599236 | validation: 0.004048088211806149]
	TIME [epoch: 25.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00283583160152903		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.00283583160152903 | validation: 0.004140246154943309]
	TIME [epoch: 25.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006078452133994966		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.006078452133994966 | validation: 0.0026384766366515074]
	TIME [epoch: 25.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029768453872704853		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.0029768453872704853 | validation: 0.003049652813542551]
	TIME [epoch: 25.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002685249379536646		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.002685249379536646 | validation: 0.0027616000081721916]
	TIME [epoch: 25.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002938910445065857		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.002938910445065857 | validation: 0.003422855468519002]
	TIME [epoch: 25.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035700475470790954		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.0035700475470790954 | validation: 0.004852294720739178]
	TIME [epoch: 25.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031577318758952886		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.0031577318758952886 | validation: 0.0028656861943172386]
	TIME [epoch: 25.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003216230054483024		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.003216230054483024 | validation: 0.004006340138933619]
	TIME [epoch: 25.9 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003803385135755101		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.003803385135755101 | validation: 0.0026525596978722484]
	TIME [epoch: 25.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019923038168647342		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0019923038168647342 | validation: 0.002191532725736406]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021541984429441384		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0021541984429441384 | validation: 0.003104494871462214]
	TIME [epoch: 25.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004378568935368814		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.004378568935368814 | validation: 0.0054299476212469]
	TIME [epoch: 25.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034283279263337954		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.0034283279263337954 | validation: 0.003364647602951589]
	TIME [epoch: 25.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027388057069827948		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.0027388057069827948 | validation: 0.004412961217551425]
	TIME [epoch: 25.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030230826389379007		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0030230826389379007 | validation: 0.004560963904058447]
	TIME [epoch: 26 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002869192644169467		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.002869192644169467 | validation: 0.004966451684794021]
	TIME [epoch: 25.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00332083460038049		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.00332083460038049 | validation: 0.003512707223079189]
	TIME [epoch: 25.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004576381670675225		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.004576381670675225 | validation: 0.005195515199154382]
	TIME [epoch: 25.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026045466198790293		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.0026045466198790293 | validation: 0.0025408970852772894]
	TIME [epoch: 25.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455589472359454		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.002455589472359454 | validation: 0.0035830869105951714]
	TIME [epoch: 25.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002806540199788438		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.002806540199788438 | validation: 0.0026591104990667646]
	TIME [epoch: 26 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023734302161790067		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0023734302161790067 | validation: 0.004754325846176501]
	TIME [epoch: 25.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004247907807944079		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.004247907807944079 | validation: 0.004449512091770747]
	TIME [epoch: 25.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002557147213640131		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.002557147213640131 | validation: 0.005548295144405977]
	TIME [epoch: 25.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029313235514191595		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0029313235514191595 | validation: 0.0032497215661827644]
	TIME [epoch: 25.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002329736820863627		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.002329736820863627 | validation: 0.0034230514119331142]
	TIME [epoch: 26 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037861645739993475		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0037861645739993475 | validation: 0.003073241944380644]
	TIME [epoch: 26 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00256797592339896		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.00256797592339896 | validation: 0.0021478340475131015]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002817100767992722		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.002817100767992722 | validation: 0.0022766721861907494]
	TIME [epoch: 25.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029398744810765127		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.0029398744810765127 | validation: 0.0036147031828019046]
	TIME [epoch: 25.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024541899986925035		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.0024541899986925035 | validation: 0.003343691494224397]
	TIME [epoch: 25.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002361515612933497		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.002361515612933497 | validation: 0.003358269036485936]
	TIME [epoch: 26 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027148550957727348		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.0027148550957727348 | validation: 0.0035508277845290952]
	TIME [epoch: 25.8 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032326228416328113		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.0032326228416328113 | validation: 0.0034989323393793672]
	TIME [epoch: 25.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003095597930898938		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.003095597930898938 | validation: 0.0024727084640060043]
	TIME [epoch: 25.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002093770690287291		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.002093770690287291 | validation: 0.003520004360107524]
	TIME [epoch: 25.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029054817444078463		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.0029054817444078463 | validation: 0.002737176569030875]
	TIME [epoch: 25.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026310983422872867		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.0026310983422872867 | validation: 0.003305985918629185]
	TIME [epoch: 25.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023866409751277804		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0023866409751277804 | validation: 0.003388349693310061]
	TIME [epoch: 25.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002484345547848898		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.002484345547848898 | validation: 0.005562852956997041]
	TIME [epoch: 25.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003339209748142477		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.003339209748142477 | validation: 0.001793626798370303]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002322660626509869		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.002322660626509869 | validation: 0.003991723991405651]
	TIME [epoch: 25.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002646960650589784		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.002646960650589784 | validation: 0.0029150412461784467]
	TIME [epoch: 25.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002890140504611248		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.002890140504611248 | validation: 0.004137711300733992]
	TIME [epoch: 26 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002954981996524164		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.002954981996524164 | validation: 0.0015697874064983654]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001972817070981337		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.001972817070981337 | validation: 0.004074868183142025]
	TIME [epoch: 25.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023291451697133457		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.0023291451697133457 | validation: 0.002158135606817317]
	TIME [epoch: 25.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019694558623270537		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.0019694558623270537 | validation: 0.002554922317739125]
	TIME [epoch: 25.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002557349227815465		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.002557349227815465 | validation: 0.0036881661543347226]
	TIME [epoch: 25.9 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031220191491792204		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.0031220191491792204 | validation: 0.0027288009772676682]
	TIME [epoch: 25.8 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018973396930863358		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.0018973396930863358 | validation: 0.0024126651502348064]
	TIME [epoch: 25.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002617191338101093		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.002617191338101093 | validation: 0.003436546783430167]
	TIME [epoch: 25.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029662754699611754		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.0029662754699611754 | validation: 0.004334229910815736]
	TIME [epoch: 25.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002941044491034782		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.002941044491034782 | validation: 0.0017508914741353374]
	TIME [epoch: 25.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001997437750164914		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.001997437750164914 | validation: 0.003694399600638794]
	TIME [epoch: 26 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030005883427846715		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.0030005883427846715 | validation: 0.0026337494207733193]
	TIME [epoch: 26 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020545708165665435		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0020545708165665435 | validation: 0.0018824597899739946]
	TIME [epoch: 25.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025079778778306864		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.0025079778778306864 | validation: 0.004881679322948581]
	TIME [epoch: 25.9 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005850413784166789		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.005850413784166789 | validation: 0.00279975373563535]
	TIME [epoch: 25.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025052682858123588		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0025052682858123588 | validation: 0.0031398305669563397]
	TIME [epoch: 25.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022021885417287234		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.0022021885417287234 | validation: 0.001897160701089679]
	TIME [epoch: 26 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026246914518528296		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0026246914518528296 | validation: 0.0028591484236671133]
	TIME [epoch: 25.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002272475572624176		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.002272475572624176 | validation: 0.003080296568528344]
	TIME [epoch: 25.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002798313772133997		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.002798313772133997 | validation: 0.0021174346679052043]
	TIME [epoch: 25.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001563590331382724		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.001563590331382724 | validation: 0.005852065199964505]
	TIME [epoch: 25.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029677683862517567		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0029677683862517567 | validation: 0.002270524975788877]
	TIME [epoch: 25.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002211724999125642		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.002211724999125642 | validation: 0.003031495931596427]
	TIME [epoch: 25.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00213112579919553		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.00213112579919553 | validation: 0.002593958581631373]
	TIME [epoch: 25.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018132630279114447		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0018132630279114447 | validation: 0.001520829784693368]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002381568402921295		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.002381568402921295 | validation: 0.0040218172789249175]
	TIME [epoch: 25.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025423134447963873		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.0025423134447963873 | validation: 0.0022680026393468618]
	TIME [epoch: 25.8 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017380924709892594		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0017380924709892594 | validation: 0.0032709793975520514]
	TIME [epoch: 25.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019016163426619427		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.0019016163426619427 | validation: 0.002068368842884077]
	TIME [epoch: 25.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025808379909247584		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.0025808379909247584 | validation: 0.004026284102753792]
	TIME [epoch: 25.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021961961781675307		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0021961961781675307 | validation: 0.0031677464475769472]
	TIME [epoch: 25.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017111139064093198		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.0017111139064093198 | validation: 0.007665538957660604]
	TIME [epoch: 25.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038223035793239395		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.0038223035793239395 | validation: 0.0025842372303431773]
	TIME [epoch: 26 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021099375889296833		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0021099375889296833 | validation: 0.0021324319748851615]
	TIME [epoch: 25.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022627319523829724		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.0022627319523829724 | validation: 0.002993695318906071]
	TIME [epoch: 25.8 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455863893921123		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.002455863893921123 | validation: 0.002230024293379299]
	TIME [epoch: 25.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013538300383996275		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.0013538300383996275 | validation: 0.0025224942227858012]
	TIME [epoch: 25.9 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022558763649705984		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.0022558763649705984 | validation: 0.0026083986681406926]
	TIME [epoch: 25.8 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002762216569244249		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.002762216569244249 | validation: 0.0026278896314260516]
	TIME [epoch: 25.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028150981652277736		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0028150981652277736 | validation: 0.003136759621163896]
	TIME [epoch: 25.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016589998047771947		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0016589998047771947 | validation: 0.0018934408628094594]
	TIME [epoch: 25.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023134455082920676		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0023134455082920676 | validation: 0.002375937056051389]
	TIME [epoch: 25.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021447399736555196		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.0021447399736555196 | validation: 0.002084268388417693]
	TIME [epoch: 25.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001982339472793378		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.001982339472793378 | validation: 0.0020057754122639925]
	TIME [epoch: 25.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016370714708999197		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0016370714708999197 | validation: 0.001610981120577816]
	TIME [epoch: 25.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024072884367736273		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.0024072884367736273 | validation: 0.00432575802430014]
	TIME [epoch: 25.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027486422187415244		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0027486422187415244 | validation: 0.0015280468049798514]
	TIME [epoch: 25.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001615679876567049		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.001615679876567049 | validation: 0.002075380036384744]
	TIME [epoch: 25.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001769910524837485		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.001769910524837485 | validation: 0.0035552869047612862]
	TIME [epoch: 25.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021141234448768405		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.0021141234448768405 | validation: 0.002975496968291296]
	TIME [epoch: 25.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021208288462822573		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.0021208288462822573 | validation: 0.0023019320845809772]
	TIME [epoch: 25.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002017354807707798		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.002017354807707798 | validation: 0.002086314670418714]
	TIME [epoch: 25.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019328595784206444		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.0019328595784206444 | validation: 0.0019535587881604616]
	TIME [epoch: 26 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018852459174216276		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.0018852459174216276 | validation: 0.00193786108610347]
	TIME [epoch: 25.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021914080256991435		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0021914080256991435 | validation: 0.0020967666243511653]
	TIME [epoch: 25.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002140816503232615		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.002140816503232615 | validation: 0.0015342001968608088]
	TIME [epoch: 25.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013214696651361623		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0013214696651361623 | validation: 0.0023482490198884413]
	TIME [epoch: 25.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020900497879683157		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.0020900497879683157 | validation: 0.0020144515411621607]
	TIME [epoch: 25.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018796283954175775		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0018796283954175775 | validation: 0.0018768650713936472]
	TIME [epoch: 25.9 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016904881174374996		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0016904881174374996 | validation: 0.003131082850826898]
	TIME [epoch: 25.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019763069431381953		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0019763069431381953 | validation: 0.0021010364861090924]
	TIME [epoch: 25.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001977141682010709		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.001977141682010709 | validation: 0.0031802062804780792]
	TIME [epoch: 25.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017740648418164348		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0017740648418164348 | validation: 0.00135527452036614]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013367310344990645		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.0013367310344990645 | validation: 0.002621894506695397]
	TIME [epoch: 25.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024885746511934936		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0024885746511934936 | validation: 0.0018258953900381805]
	TIME [epoch: 26 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014572219929013847		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0014572219929013847 | validation: 0.002240822398186652]
	TIME [epoch: 25.9 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021758082614029162		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0021758082614029162 | validation: 0.0011647169875097676]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017383828796253401		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0017383828796253401 | validation: 0.0021893825489563844]
	TIME [epoch: 25.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013063094752691212		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.0013063094752691212 | validation: 0.004804398450696924]
	TIME [epoch: 25.8 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003112712936217521		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.003112712936217521 | validation: 0.0021080749426488836]
	TIME [epoch: 25.8 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002064647928456345		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.002064647928456345 | validation: 0.0018816862272954245]
	TIME [epoch: 25.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001347530132252668		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.001347530132252668 | validation: 0.0018075095287524547]
	TIME [epoch: 26 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001463175758122684		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.001463175758122684 | validation: 0.0020719277113488543]
	TIME [epoch: 25.9 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017892874502136423		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0017892874502136423 | validation: 0.0022034359079529465]
	TIME [epoch: 25.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001981786282645559		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.001981786282645559 | validation: 0.003441693929296667]
	TIME [epoch: 25.9 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002614152180483175		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.002614152180483175 | validation: 0.0017332499290520502]
	TIME [epoch: 25.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00131434416622615		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.00131434416622615 | validation: 0.0016296076751140314]
	TIME [epoch: 25.9 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013965484443057992		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.0013965484443057992 | validation: 0.0015243991111836533]
	TIME [epoch: 25.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017576657881303909		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.0017576657881303909 | validation: 0.002562962855458579]
	TIME [epoch: 25.9 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024840259653385855		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0024840259653385855 | validation: 0.0025173196451694748]
	TIME [epoch: 25.9 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014261302349169555		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0014261302349169555 | validation: 0.0019502392567103098]
	TIME [epoch: 25.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017130138345964015		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0017130138345964015 | validation: 0.001055507077574963]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00147799195996394		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.00147799195996394 | validation: 0.001888773210582997]
	TIME [epoch: 25.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002052769304699659		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.002052769304699659 | validation: 0.001774484064389167]
	TIME [epoch: 25.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025017794271041866		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0025017794271041866 | validation: 0.002688466139112178]
	TIME [epoch: 25.9 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018299782867591315		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0018299782867591315 | validation: 0.001613787853304161]
	TIME [epoch: 25.8 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016379232314159714		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0016379232314159714 | validation: 0.0017587588636191784]
	TIME [epoch: 25.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014685377755543683		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0014685377755543683 | validation: 0.002728241957406739]
	TIME [epoch: 25.9 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001469451085825699		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.001469451085825699 | validation: 0.0020589570030731057]
	TIME [epoch: 26 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023552327937398808		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.0023552327937398808 | validation: 0.002145304401209255]
	TIME [epoch: 25.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014870507144959337		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.0014870507144959337 | validation: 0.0015242014984737447]
	TIME [epoch: 25.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013582520069464314		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.0013582520069464314 | validation: 0.002159848123019436]
	TIME [epoch: 25.9 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001680686307220995		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.001680686307220995 | validation: 0.0015103321355518719]
	TIME [epoch: 25.9 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002395847624285459		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.002395847624285459 | validation: 0.0011720876499598516]
	TIME [epoch: 25.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017581033444541475		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0017581033444541475 | validation: 0.0018854922218303188]
	TIME [epoch: 25.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014468552071459787		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0014468552071459787 | validation: 0.0020176055031717243]
	TIME [epoch: 25.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014746739970844017		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.0014746739970844017 | validation: 0.0017302315009007341]
	TIME [epoch: 25.8 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016679329339610329		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.0016679329339610329 | validation: 0.0021055464498385998]
	TIME [epoch: 25.8 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001200440849084691		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.001200440849084691 | validation: 0.00144268304997774]
	TIME [epoch: 25.9 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023540560207713523		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0023540560207713523 | validation: 0.00205332308796786]
	TIME [epoch: 25.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012874845841941592		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.0012874845841941592 | validation: 0.0010119321482715824]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011994893190104751		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0011994893190104751 | validation: 0.001587911616245621]
	TIME [epoch: 25.8 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019109318445334651		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0019109318445334651 | validation: 0.00156564159609965]
	TIME [epoch: 25.8 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001432321242496082		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.001432321242496082 | validation: 0.0011460172673077827]
	TIME [epoch: 25.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017725903085160544		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.0017725903085160544 | validation: 0.0022703296453744446]
	TIME [epoch: 25.9 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017436053879769902		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.0017436053879769902 | validation: 0.00233544565610329]
	TIME [epoch: 25.8 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016825774003989549		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.0016825774003989549 | validation: 0.0010771878411422638]
	TIME [epoch: 25.8 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012473804096014504		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.0012473804096014504 | validation: 0.0013797748179408985]
	TIME [epoch: 25.8 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019023516245304719		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0019023516245304719 | validation: 0.0018417505348410296]
	TIME [epoch: 25.8 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018349275619736695		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.0018349275619736695 | validation: 0.0022143413242082647]
	TIME [epoch: 25.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001246788713516621		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.001246788713516621 | validation: 0.0018844627823325927]
	TIME [epoch: 25.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013929176670383796		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0013929176670383796 | validation: 0.0013310519979479753]
	TIME [epoch: 25.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014869071289654722		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0014869071289654722 | validation: 0.0019402572305031863]
	TIME [epoch: 25.8 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014187943690007413		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0014187943690007413 | validation: 0.0022903524144311585]
	TIME [epoch: 25.8 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001603844536461048		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.001603844536461048 | validation: 0.0009530824626545948]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012916639534302661		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.0012916639534302661 | validation: 0.0014817346730750316]
	TIME [epoch: 25.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014850954532260791		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0014850954532260791 | validation: 0.0012582979199133027]
	TIME [epoch: 25.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012289536077364177		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.0012289536077364177 | validation: 0.0015808869688520718]
	TIME [epoch: 26 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014040621433725822		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.0014040621433725822 | validation: 0.0034296107129013357]
	TIME [epoch: 25.9 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015007025423580498		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.0015007025423580498 | validation: 0.0027524514126100455]
	TIME [epoch: 25.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013333604628100652		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.0013333604628100652 | validation: 0.001299889500550667]
	TIME [epoch: 25.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011897741850506807		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.0011897741850506807 | validation: 0.0017986568766157627]
	TIME [epoch: 25.9 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011163679347297078		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0011163679347297078 | validation: 0.0014568473697120748]
	TIME [epoch: 25.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015136958125150832		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.0015136958125150832 | validation: 0.0013581891063109168]
	TIME [epoch: 25.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013518385234658624		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.0013518385234658624 | validation: 0.0016123765357824267]
	TIME [epoch: 25.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013132137302536485		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.0013132137302536485 | validation: 0.0023117866162161056]
	TIME [epoch: 25.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001691488268274181		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.001691488268274181 | validation: 0.0018747949598455023]
	TIME [epoch: 25.8 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001251249230135376		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.001251249230135376 | validation: 0.0017318555240014124]
	TIME [epoch: 25.8 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010931994691862554		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.0010931994691862554 | validation: 0.0019415204195576098]
	TIME [epoch: 25.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001507012231372584		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.001507012231372584 | validation: 0.0009309534234176145]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011648405488438299		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0011648405488438299 | validation: 0.0020965577477536786]
	TIME [epoch: 25.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011395806064896934		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.0011395806064896934 | validation: 0.0021776286587963318]
	TIME [epoch: 26 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017688868825264388		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.0017688868825264388 | validation: 0.0016567090584103435]
	TIME [epoch: 25.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014852843218633345		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.0014852843218633345 | validation: 0.0015928901178710007]
	TIME [epoch: 25.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001247500612014744		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.001247500612014744 | validation: 0.0020618657195692666]
	TIME [epoch: 25.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012584801480855878		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0012584801480855878 | validation: 0.0016299021635362073]
	TIME [epoch: 25.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001413327708663837		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.001413327708663837 | validation: 0.0017205277154837607]
	TIME [epoch: 25.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016126672843287876		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.0016126672843287876 | validation: 0.0024379649941270146]
	TIME [epoch: 25.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013678133426269616		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0013678133426269616 | validation: 0.0015906828995920527]
	TIME [epoch: 25.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010633836590037563		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0010633836590037563 | validation: 0.0013374263829543523]
	TIME [epoch: 25.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013360547009563838		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0013360547009563838 | validation: 0.002266120528677842]
	TIME [epoch: 25.8 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010702676152487794		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0010702676152487794 | validation: 0.0010733623980159494]
	TIME [epoch: 25.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012560001311970148		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.0012560001311970148 | validation: 0.00223518205739182]
	TIME [epoch: 25.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010354525452496756		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0010354525452496756 | validation: 0.0013438437913777942]
	TIME [epoch: 25.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001654300990330623		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.001654300990330623 | validation: 0.0014600170069627143]
	TIME [epoch: 25.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001157483512023274		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.001157483512023274 | validation: 0.0012079598564927949]
	TIME [epoch: 25.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001574570131164904		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.001574570131164904 | validation: 0.002586101894895512]
	TIME [epoch: 25.8 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010725136012676964		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.0010725136012676964 | validation: 0.0012983501931959607]
	TIME [epoch: 25.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018488735243586706		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0018488735243586706 | validation: 0.0011451631166957289]
	TIME [epoch: 25.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010832674957457081		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0010832674957457081 | validation: 0.001242213477999945]
	TIME [epoch: 25.8 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008444361611726916		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0008444361611726916 | validation: 0.001429265961190124]
	TIME [epoch: 25.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011817876903202438		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0011817876903202438 | validation: 0.0014859465513941768]
	TIME [epoch: 25.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009646474273009118		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.0009646474273009118 | validation: 0.0016951916051310175]
	TIME [epoch: 25.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012351536296350048		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0012351536296350048 | validation: 0.0021410705378110367]
	TIME [epoch: 25.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016322127120789248		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0016322127120789248 | validation: 0.0011371349571308532]
	TIME [epoch: 25.9 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015605046086631976		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0015605046086631976 | validation: 0.0013960569417831117]
	TIME [epoch: 25.9 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009218027637586926		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.0009218027637586926 | validation: 0.0012728485846193517]
	TIME [epoch: 26 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011424483050150422		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0011424483050150422 | validation: 0.0018738723428157512]
	TIME [epoch: 25.9 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012163528585798116		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0012163528585798116 | validation: 0.0012381502268630023]
	TIME [epoch: 25.9 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001020818982966764		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.001020818982966764 | validation: 0.001847741617171658]
	TIME [epoch: 25.9 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014456199029943288		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0014456199029943288 | validation: 0.001948019905390635]
	TIME [epoch: 25.9 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010603795559378736		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0010603795559378736 | validation: 0.0016613337850485594]
	TIME [epoch: 25.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010672678536145022		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0010672678536145022 | validation: 0.0012149069418784055]
	TIME [epoch: 25.9 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011238202107255308		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.0011238202107255308 | validation: 0.0013113640730668505]
	TIME [epoch: 26 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012259631039544514		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0012259631039544514 | validation: 0.002042095793464539]
	TIME [epoch: 26 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016376292830719214		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0016376292830719214 | validation: 0.0017545845783247219]
	TIME [epoch: 25.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011620911775212103		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0011620911775212103 | validation: 0.0015775038444154257]
	TIME [epoch: 25.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009439060849293225		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0009439060849293225 | validation: 0.0016039009343233907]
	TIME [epoch: 25.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011403277413823364		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.0011403277413823364 | validation: 0.001238145283747042]
	TIME [epoch: 25.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008871344094482225		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0008871344094482225 | validation: 0.0010368965684952852]
	TIME [epoch: 25.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010604770671629265		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0010604770671629265 | validation: 0.002251307543669817]
	TIME [epoch: 25.8 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001180670822804287		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.001180670822804287 | validation: 0.001061574313329566]
	TIME [epoch: 25.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015276235399989614		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0015276235399989614 | validation: 0.0009990197504246997]
	TIME [epoch: 25.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010067876115612481		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.0010067876115612481 | validation: 0.0020432348662771486]
	TIME [epoch: 25.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010222181357010345		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.0010222181357010345 | validation: 0.0007438244157161602]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010180998957104478		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.0010180998957104478 | validation: 0.0014157930290519137]
	TIME [epoch: 25.9 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011272892535752993		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0011272892535752993 | validation: 0.0014069443755383394]
	TIME [epoch: 25.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001115782277091205		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.001115782277091205 | validation: 0.0014095279184569104]
	TIME [epoch: 25.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009658681798497366		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0009658681798497366 | validation: 0.0016381958332143621]
	TIME [epoch: 25.8 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010692061749111425		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0010692061749111425 | validation: 0.0013629729494746368]
	TIME [epoch: 25.8 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001230701127534598		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.001230701127534598 | validation: 0.0018631780403093607]
	TIME [epoch: 25.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012861225836194312		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0012861225836194312 | validation: 0.0011249127201947244]
	TIME [epoch: 25.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007611604652309542		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.0007611604652309542 | validation: 0.0012218357567724968]
	TIME [epoch: 26 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008350606722438805		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0008350606722438805 | validation: 0.0010384625566149488]
	TIME [epoch: 25.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011282923185580115		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.0011282923185580115 | validation: 0.0013497254285230022]
	TIME [epoch: 25.9 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010714442155277033		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0010714442155277033 | validation: 0.0016421479746479603]
	TIME [epoch: 25.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011293429769286512		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0011293429769286512 | validation: 0.0012564978103009549]
	TIME [epoch: 25.8 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010787164065757578		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.0010787164065757578 | validation: 0.0014764193844128092]
	TIME [epoch: 25.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008888404465202267		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.0008888404465202267 | validation: 0.0027103076130581805]
	TIME [epoch: 25.8 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012497040255064194		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0012497040255064194 | validation: 0.0011036588461137403]
	TIME [epoch: 25.8 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001082988302400125		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.001082988302400125 | validation: 0.0010571180163810405]
	TIME [epoch: 25.8 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008936100332445511		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0008936100332445511 | validation: 0.001581024774286524]
	TIME [epoch: 25.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014101631167573643		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.0014101631167573643 | validation: 0.0015050272139779954]
	TIME [epoch: 25.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008222049554795996		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.0008222049554795996 | validation: 0.0010781638177133433]
	TIME [epoch: 25.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009145571493305274		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0009145571493305274 | validation: 0.0016271034075177352]
	TIME [epoch: 25.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011845505215040254		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0011845505215040254 | validation: 0.000702325636903681]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010639925127001516		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0010639925127001516 | validation: 0.0006535540703349989]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007731894509807056		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.0007731894509807056 | validation: 0.0013696075766876721]
	TIME [epoch: 25.8 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011399109516963613		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0011399109516963613 | validation: 0.0015412350090100676]
	TIME [epoch: 25.8 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008159712227960476		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0008159712227960476 | validation: 0.0016698518148319944]
	TIME [epoch: 25.8 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009302365831297781		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.0009302365831297781 | validation: 0.0008893500810582333]
	TIME [epoch: 25.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000889273197311545		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.000889273197311545 | validation: 0.0016400903898260459]
	TIME [epoch: 26 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010290608963483107		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0010290608963483107 | validation: 0.0014325311708053138]
	TIME [epoch: 25.8 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008554753459679152		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.0008554753459679152 | validation: 0.0011159840261979496]
	TIME [epoch: 25.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008866020791490288		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.0008866020791490288 | validation: 0.0006904582049664673]
	TIME [epoch: 25.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009291597388028694		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0009291597388028694 | validation: 0.0013265266536972337]
	TIME [epoch: 25.9 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009438704401210774		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0009438704401210774 | validation: 0.0012647644437734104]
	TIME [epoch: 25.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007453085848863113		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0007453085848863113 | validation: 0.0008294026394267621]
	TIME [epoch: 25.9 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010361133851146858		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0010361133851146858 | validation: 0.0012725384201602728]
	TIME [epoch: 25.8 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007985777037798855		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0007985777037798855 | validation: 0.0019439939082558332]
	TIME [epoch: 25.8 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011371009781564686		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0011371009781564686 | validation: 0.00141794086186448]
	TIME [epoch: 25.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011553816250634158		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.0011553816250634158 | validation: 0.0009245817223488366]
	TIME [epoch: 25.9 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009772883634263968		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0009772883634263968 | validation: 0.0012244965419690708]
	TIME [epoch: 26 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011704873796287204		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.0011704873796287204 | validation: 0.0009739127571890256]
	TIME [epoch: 25.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010505649385463866		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0010505649385463866 | validation: 0.001983248764266576]
	TIME [epoch: 25.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009000230350810743		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.0009000230350810743 | validation: 0.0014425844891169488]
	TIME [epoch: 25.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011426659502346756		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0011426659502346756 | validation: 0.0017337790511404868]
	TIME [epoch: 25.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008339000468144287		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0008339000468144287 | validation: 0.001269917557115609]
	TIME [epoch: 25.9 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007206522350398705		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0007206522350398705 | validation: 0.001952785636907981]
	TIME [epoch: 25.9 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008094965689643822		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0008094965689643822 | validation: 0.0006677708055928173]
	TIME [epoch: 25.9 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006031214003878304		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.0006031214003878304 | validation: 0.0007834762993952333]
	TIME [epoch: 26 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010883783410366632		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0010883783410366632 | validation: 0.0009206585490939832]
	TIME [epoch: 25.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009728794536646734		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0009728794536646734 | validation: 0.0011733606946846766]
	TIME [epoch: 25.8 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006491606856784427		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0006491606856784427 | validation: 0.0011233009580308977]
	TIME [epoch: 25.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011003957644391733		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0011003957644391733 | validation: 0.0014338979310931962]
	TIME [epoch: 25.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009982404385787303		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.0009982404385787303 | validation: 0.0008940906074826688]
	TIME [epoch: 25.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007433665407674947		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.0007433665407674947 | validation: 0.0013605347529380392]
	TIME [epoch: 26 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008305564510025433		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0008305564510025433 | validation: 0.0011993364023798448]
	TIME [epoch: 25.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008150180990647436		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0008150180990647436 | validation: 0.0008940555189528449]
	TIME [epoch: 25.9 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007388838929321762		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0007388838929321762 | validation: 0.0009423487916767544]
	TIME [epoch: 25.9 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000993666406817192		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.000993666406817192 | validation: 0.0008871339987435833]
	TIME [epoch: 25.9 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008740018534936765		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0008740018534936765 | validation: 0.0010752941286217936]
	TIME [epoch: 25.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011233037882500527		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0011233037882500527 | validation: 0.0012327435454737162]
	TIME [epoch: 26 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008786977372033142		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0008786977372033142 | validation: 0.0013630412609006424]
	TIME [epoch: 25.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010537061017275775		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.0010537061017275775 | validation: 0.0010451545432875627]
	TIME [epoch: 25.9 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009771038350077875		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.0009771038350077875 | validation: 0.0007638364583734693]
	TIME [epoch: 25.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006531399724316245		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0006531399724316245 | validation: 0.0010821273021653158]
	TIME [epoch: 25.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008056767561847822		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.0008056767561847822 | validation: 0.000945497740613738]
	TIME [epoch: 25.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008934397119917994		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0008934397119917994 | validation: 0.0008971447325995623]
	TIME [epoch: 25.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008830760716025102		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0008830760716025102 | validation: 0.0009908186791058409]
	TIME [epoch: 26 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006683234293536967		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.0006683234293536967 | validation: 0.0010453875276368826]
	TIME [epoch: 25.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007163229187381625		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0007163229187381625 | validation: 0.0013219952030164225]
	TIME [epoch: 25.9 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011117156948642448		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0011117156948642448 | validation: 0.0008623994584974142]
	TIME [epoch: 25.8 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006240060884005402		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0006240060884005402 | validation: 0.0008052391181275213]
	TIME [epoch: 25.8 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009310091809628528		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0009310091809628528 | validation: 0.0009792940957431338]
	TIME [epoch: 25.8 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008004870789674047		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0008004870789674047 | validation: 0.0009445257161423984]
	TIME [epoch: 25.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007667674747152009		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0007667674747152009 | validation: 0.001479089150961933]
	TIME [epoch: 26 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008142275859877096		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0008142275859877096 | validation: 0.0010518189761468681]
	TIME [epoch: 26 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009784025307915463		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.0009784025307915463 | validation: 0.0012952659196710822]
	TIME [epoch: 25.9 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006338505357955521		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0006338505357955521 | validation: 0.0014150052042122745]
	TIME [epoch: 25.9 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005186506036910044		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.0005186506036910044 | validation: 0.0009343590955080821]
	TIME [epoch: 25.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008910614972048308		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.0008910614972048308 | validation: 0.0007686885892739186]
	TIME [epoch: 25.9 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012048682350788518		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0012048682350788518 | validation: 0.0011388321155112308]
	TIME [epoch: 25.9 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012002585332224138		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0012002585332224138 | validation: 0.0007657001365089933]
	TIME [epoch: 25.9 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007734458774973671		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0007734458774973671 | validation: 0.0011945975301542228]
	TIME [epoch: 26 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007614219878609103		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0007614219878609103 | validation: 0.00156079962658335]
	TIME [epoch: 25.8 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007328452584061298		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0007328452584061298 | validation: 0.0008517501472378388]
	TIME [epoch: 25.8 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005032254996709433		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0005032254996709433 | validation: 0.0009161609406182185]
	TIME [epoch: 25.8 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010583024575771941		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.0010583024575771941 | validation: 0.0012530037173569708]
	TIME [epoch: 25.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008660103890604188		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0008660103890604188 | validation: 0.0011199823782109038]
	TIME [epoch: 25.8 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006775206655014386		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.0006775206655014386 | validation: 0.0004719959431872525]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000584590434912944		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.000584590434912944 | validation: 0.000912175048707436]
	TIME [epoch: 25.8 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008556143685543898		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0008556143685543898 | validation: 0.0015612192894247656]
	TIME [epoch: 25.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000833155259830244		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.000833155259830244 | validation: 0.0005947218167168353]
	TIME [epoch: 25.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007139227464143577		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.0007139227464143577 | validation: 0.0007776872418394909]
	TIME [epoch: 25.9 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000640818217796132		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.000640818217796132 | validation: 0.0009565019054433277]
	TIME [epoch: 25.9 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009965312160700528		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.0009965312160700528 | validation: 0.0012238551254814024]
	TIME [epoch: 25.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007637151947036201		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0007637151947036201 | validation: 0.0011615459291660626]
	TIME [epoch: 25.9 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008635981921439757		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0008635981921439757 | validation: 0.0006407429499369926]
	TIME [epoch: 25.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007652478913299092		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0007652478913299092 | validation: 0.001012007621490402]
	TIME [epoch: 25.9 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008475387762104606		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0008475387762104606 | validation: 0.0009230046207477161]
	TIME [epoch: 25.9 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006199333973307622		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0006199333973307622 | validation: 0.0008143693983530174]
	TIME [epoch: 25.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000858081636070186		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.000858081636070186 | validation: 0.0011389660911001142]
	TIME [epoch: 25.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008546361744083444		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0008546361744083444 | validation: 0.0015675932575219516]
	TIME [epoch: 25.9 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008056860351641137		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0008056860351641137 | validation: 0.00045268072141084573]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_971.pth
	Model improved!!!
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006359735221868494		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0006359735221868494 | validation: 0.0006161865585049133]
	TIME [epoch: 25.9 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009001074041105224		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0009001074041105224 | validation: 0.0013472563097949477]
	TIME [epoch: 25.8 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007452473637889733		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0007452473637889733 | validation: 0.0012174666000069577]
	TIME [epoch: 25.9 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007536627104241619		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0007536627104241619 | validation: 0.0009548595961500697]
	TIME [epoch: 25.8 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048414874686429084		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.00048414874686429084 | validation: 0.0012294720550444938]
	TIME [epoch: 25.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006849494496197636		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0006849494496197636 | validation: 0.0006356805492135963]
	TIME [epoch: 25.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000738456103884084		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.000738456103884084 | validation: 0.0010524388505411225]
	TIME [epoch: 25.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005990651149383088		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0005990651149383088 | validation: 0.0010504495557753098]
	TIME [epoch: 25.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006717596881496454		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.0006717596881496454 | validation: 0.0006441133775273746]
	TIME [epoch: 25.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010352316048456887		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0010352316048456887 | validation: 0.0013801234791818607]
	TIME [epoch: 25.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006249792703489636		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0006249792703489636 | validation: 0.0009760978353562822]
	TIME [epoch: 25.9 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006332607646072975		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0006332607646072975 | validation: 0.0010950652688397461]
	TIME [epoch: 25.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008135586710615855		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0008135586710615855 | validation: 0.0006618482087644627]
	TIME [epoch: 25.8 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048607279901883096		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.00048607279901883096 | validation: 0.000688107220166649]
	TIME [epoch: 25.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006972660993644922		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.0006972660993644922 | validation: 0.0008983825705786046]
	TIME [epoch: 25.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005094247560733887		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.0005094247560733887 | validation: 0.001530330458557133]
	TIME [epoch: 25.9 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006507027667778256		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0006507027667778256 | validation: 0.0007387717560927767]
	TIME [epoch: 25.9 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008109961996277293		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0008109961996277293 | validation: 0.0012085214911490203]
	TIME [epoch: 25.9 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006968557996818794		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0006968557996818794 | validation: 0.0007139116292054988]
	TIME [epoch: 25.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000600562047919021		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.000600562047919021 | validation: 0.0005961236381359125]
	TIME [epoch: 25.8 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006724239440387174		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0006724239440387174 | validation: 0.0016846376542797082]
	TIME [epoch: 25.8 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008249984312321712		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0008249984312321712 | validation: 0.001033974849914971]
	TIME [epoch: 25.8 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008118752328749274		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.0008118752328749274 | validation: 0.0008751276988001263]
	TIME [epoch: 25.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008300770139523672		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0008300770139523672 | validation: 0.000996590624140131]
	TIME [epoch: 25.9 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007501354413810902		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0007501354413810902 | validation: 0.0007870580045739866]
	TIME [epoch: 25.9 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006363404493457377		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0006363404493457377 | validation: 0.0010308304470582526]
	TIME [epoch: 25.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007517720357605312		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0007517720357605312 | validation: 0.000586274927158776]
	TIME [epoch: 25.9 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004856754027860804		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.0004856754027860804 | validation: 0.0006209122452358864]
	TIME [epoch: 25.9 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045795851109059656		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.00045795851109059656 | validation: 0.0009435202233750113]
	TIME [epoch: 25.8 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007997209150388894		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0007997209150388894 | validation: 0.0005693728075786213]
	TIME [epoch: 475 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006304725946432992		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0006304725946432992 | validation: 0.0006834792075944404]
	TIME [epoch: 54.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008580315774457596		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0008580315774457596 | validation: 0.0006462518577239261]
	TIME [epoch: 54.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000689619221900214		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.000689619221900214 | validation: 0.0016833616674768103]
	TIME [epoch: 54.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007275196353519774		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0007275196353519774 | validation: 0.0008886192476568455]
	TIME [epoch: 55.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005285779988184933		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0005285779988184933 | validation: 0.0006074895887647092]
	TIME [epoch: 55.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005870703624390576		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0005870703624390576 | validation: 0.001067779226035909]
	TIME [epoch: 55 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006653833028435885		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0006653833028435885 | validation: 0.0012744357753636608]
	TIME [epoch: 55 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006208182751296818		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.0006208182751296818 | validation: 0.00084996735624616]
	TIME [epoch: 55.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007506712478493535		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0007506712478493535 | validation: 0.0003671771145377649]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003944293057552553		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.0003944293057552553 | validation: 0.0007203626706862015]
	TIME [epoch: 54.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006909741931754969		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0006909741931754969 | validation: 0.0005531543942555137]
	TIME [epoch: 54.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007365486423781807		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0007365486423781807 | validation: 0.0006600276000772034]
	TIME [epoch: 54.8 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004222082559011313		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0004222082559011313 | validation: 0.0006181882075976112]
	TIME [epoch: 54.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008248012090509098		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0008248012090509098 | validation: 0.0006900130493688685]
	TIME [epoch: 54.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047343205781719404		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.00047343205781719404 | validation: 0.0008102925094077521]
	TIME [epoch: 54.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004804441380368756		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0004804441380368756 | validation: 0.0014054589382594047]
	TIME [epoch: 54.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006877880542381507		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0006877880542381507 | validation: 0.0009305898583297827]
	TIME [epoch: 54.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007738949968113092		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0007738949968113092 | validation: 0.0008661925711498242]
	TIME [epoch: 54.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000475236225534405		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.000475236225534405 | validation: 0.0009078013785521248]
	TIME [epoch: 54.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045387233693654493		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.00045387233693654493 | validation: 0.0006755704334880965]
	TIME [epoch: 54.8 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008865227266543745		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.0008865227266543745 | validation: 0.0009107587593333353]
	TIME [epoch: 54.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005578689165821669		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0005578689165821669 | validation: 0.0012388864198665493]
	TIME [epoch: 54.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007881258229408616		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0007881258229408616 | validation: 0.0006113143025880642]
	TIME [epoch: 54.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005276875568884501		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0005276875568884501 | validation: 0.0010176756367429292]
	TIME [epoch: 54.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006128613915803343		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.0006128613915803343 | validation: 0.0010580606775518549]
	TIME [epoch: 54.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005141795098945775		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0005141795098945775 | validation: 0.0005264249021470376]
	TIME [epoch: 54.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003531920780288966		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0003531920780288966 | validation: 0.0005144212417578196]
	TIME [epoch: 54.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006455971404993074		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0006455971404993074 | validation: 0.0010139605033170076]
	TIME [epoch: 55 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006543025632997992		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0006543025632997992 | validation: 0.0010527418498526276]
	TIME [epoch: 54.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005481050731110132		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0005481050731110132 | validation: 0.0005401596126722695]
	TIME [epoch: 54.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006861854022770835		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0006861854022770835 | validation: 0.0011893860217010986]
	TIME [epoch: 54.8 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005615062156692011		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0005615062156692011 | validation: 0.0006400271876850896]
	TIME [epoch: 54.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000543573743233941		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.000543573743233941 | validation: 0.0005131018999623391]
	TIME [epoch: 54.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043946887748690534		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.00043946887748690534 | validation: 0.0007452174142072163]
	TIME [epoch: 54.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006087938367449733		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0006087938367449733 | validation: 0.0011683073936884724]
	TIME [epoch: 54.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00069934558542268		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.00069934558542268 | validation: 0.000921502128191329]
	TIME [epoch: 54.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048658101255785844		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.00048658101255785844 | validation: 0.001119322493755706]
	TIME [epoch: 54.8 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006078348857179884		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0006078348857179884 | validation: 0.0011884977084650457]
	TIME [epoch: 54.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046262156603224525		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.00046262156603224525 | validation: 0.0010772398792640442]
	TIME [epoch: 54.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006225021899170587		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0006225021899170587 | validation: 0.0008990990661060191]
	TIME [epoch: 54.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005152877948272585		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0005152877948272585 | validation: 0.0006445277356570358]
	TIME [epoch: 54.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006932174187207374		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0006932174187207374 | validation: 0.0011231686919650282]
	TIME [epoch: 54.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027676797590446057		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.00027676797590446057 | validation: 0.0006805255909102072]
	TIME [epoch: 54.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003240911116193992		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0003240911116193992 | validation: 0.0006075759013574462]
	TIME [epoch: 54.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005124655769462019		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0005124655769462019 | validation: 0.0008302698700312918]
	TIME [epoch: 54.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004249905277454267		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.0004249905277454267 | validation: 0.000601627062833761]
	TIME [epoch: 54.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006504936841137367		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0006504936841137367 | validation: 0.0009265607021842746]
	TIME [epoch: 54.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005546529843645967		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.0005546529843645967 | validation: 0.000778256501143911]
	TIME [epoch: 54.9 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041575599999841217		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.00041575599999841217 | validation: 0.00041766607247607903]
	TIME [epoch: 54.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046786099500813476		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.00046786099500813476 | validation: 0.0006174797272182593]
	TIME [epoch: 54.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005794494496502567		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0005794494496502567 | validation: 0.0003666859599464205]
	TIME [epoch: 54.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1052.pth
	Model improved!!!
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005578406296003386		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0005578406296003386 | validation: 0.0007341512173407292]
	TIME [epoch: 54.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040106413046145996		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.00040106413046145996 | validation: 0.0009468326984986532]
	TIME [epoch: 54.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005165297965788161		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0005165297965788161 | validation: 0.0006458025480122621]
	TIME [epoch: 54.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005511640114843197		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0005511640114843197 | validation: 0.0008729482897651658]
	TIME [epoch: 54.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010076457067311098		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0010076457067311098 | validation: 0.0005650519652171785]
	TIME [epoch: 54.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000798016654380534		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.000798016654380534 | validation: 0.0005191691403446663]
	TIME [epoch: 54.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005398419533926637		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0005398419533926637 | validation: 0.0010622442499504886]
	TIME [epoch: 54.9 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004953557828080409		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0004953557828080409 | validation: 0.0008186576876861711]
	TIME [epoch: 54.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005677273439433855		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.0005677273439433855 | validation: 0.0005276521666784096]
	TIME [epoch: 54.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000507965261412594		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.000507965261412594 | validation: 0.0005864617050294427]
	TIME [epoch: 54.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033264010631505527		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.00033264010631505527 | validation: 0.0004519912765843799]
	TIME [epoch: 54.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007055334059722417		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0007055334059722417 | validation: 0.0006595926262239234]
	TIME [epoch: 54.8 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006104703106688074		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0006104703106688074 | validation: 0.001021864645944528]
	TIME [epoch: 54.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000626571150416054		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.000626571150416054 | validation: 0.0006931311014576656]
	TIME [epoch: 54.9 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004911872737766377		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0004911872737766377 | validation: 0.0003769200364517564]
	TIME [epoch: 54.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039634624463238195		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.00039634624463238195 | validation: 0.0009751870421818811]
	TIME [epoch: 54.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000564403197473609		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.000564403197473609 | validation: 0.000714983632178515]
	TIME [epoch: 54.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003810577923845418		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0003810577923845418 | validation: 0.0006423412637674905]
	TIME [epoch: 54.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007783116662424369		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0007783116662424369 | validation: 0.0005197252332639452]
	TIME [epoch: 54.9 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047821074073255463		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.00047821074073255463 | validation: 0.0007186356544583177]
	TIME [epoch: 54.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048085391347855766		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.00048085391347855766 | validation: 0.0005828382164710409]
	TIME [epoch: 54.8 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046869705132548733		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.00046869705132548733 | validation: 0.0004989558884412899]
	TIME [epoch: 54.9 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006090701489440123		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0006090701489440123 | validation: 0.0005357122551223404]
	TIME [epoch: 54.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006111425327655005		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0006111425327655005 | validation: 0.0006013281698283528]
	TIME [epoch: 55 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003097055460567355		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0003097055460567355 | validation: 0.0003447530544606918]
	TIME [epoch: 54.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005274133528380816		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0005274133528380816 | validation: 0.0004708969274655734]
	TIME [epoch: 54.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005026875895783893		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0005026875895783893 | validation: 0.0005964068757175411]
	TIME [epoch: 54.9 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006331266169546603		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.0006331266169546603 | validation: 0.0009297461538235936]
	TIME [epoch: 54.8 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011789837708055554		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.0011789837708055554 | validation: 0.0007130539468823161]
	TIME [epoch: 54.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006509277581468553		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.0006509277581468553 | validation: 0.0005117753403945545]
	TIME [epoch: 54.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004293699441604511		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0004293699441604511 | validation: 0.0006445333579101789]
	TIME [epoch: 54.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003488342656113486		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0003488342656113486 | validation: 0.00019848657330199425]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005729916400198063		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.0005729916400198063 | validation: 0.0008335356967305865]
	TIME [epoch: 54.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002979005883335564		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0002979005883335564 | validation: 0.0006609125106198847]
	TIME [epoch: 54.8 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006009197489504145		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0006009197489504145 | validation: 0.0007150338120750685]
	TIME [epoch: 54.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006631155806602076		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0006631155806602076 | validation: 0.0004500966514500391]
	TIME [epoch: 54.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004725599809931425		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0004725599809931425 | validation: 0.0004844711970414393]
	TIME [epoch: 54.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004908079466805069		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0004908079466805069 | validation: 0.0007392685540732531]
	TIME [epoch: 54.8 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005804938186760133		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0005804938186760133 | validation: 0.000437050951751571]
	TIME [epoch: 54.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004774362233211394		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0004774362233211394 | validation: 0.0007266787675945822]
	TIME [epoch: 54.8 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008189089210250743		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0008189089210250743 | validation: 0.0004017967504024052]
	TIME [epoch: 54.9 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004519003976780962		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0004519003976780962 | validation: 0.0006741004620301885]
	TIME [epoch: 54.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022015933115080612		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.00022015933115080612 | validation: 0.0005991039720832986]
	TIME [epoch: 54.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004913854354214173		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.0004913854354214173 | validation: 0.0005629695251262117]
	TIME [epoch: 54.9 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005948892956932697		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0005948892956932697 | validation: 0.0007252375056164989]
	TIME [epoch: 54.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004232860011099864		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0004232860011099864 | validation: 0.000718310642794556]
	TIME [epoch: 54.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005356541570145974		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0005356541570145974 | validation: 0.0006896560296065113]
	TIME [epoch: 54.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004967146298015009		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0004967146298015009 | validation: 0.0005161048031976714]
	TIME [epoch: 54.8 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005294604848575384		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0005294604848575384 | validation: 0.0005730145202489654]
	TIME [epoch: 54.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004420569789905575		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.0004420569789905575 | validation: 0.0009292257387772258]
	TIME [epoch: 54.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004916485242375857		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0004916485242375857 | validation: 0.0005721186193078118]
	TIME [epoch: 54.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041381164856717364		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.00041381164856717364 | validation: 0.0004053449837918377]
	TIME [epoch: 54.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004184821001299652		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.0004184821001299652 | validation: 0.0008479092364931988]
	TIME [epoch: 54.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048032753522898446		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.00048032753522898446 | validation: 0.0008976231449915001]
	TIME [epoch: 54.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004905261268401837		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0004905261268401837 | validation: 0.0010878124353727143]
	TIME [epoch: 54.8 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005847163339186505		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0005847163339186505 | validation: 0.001308109665482439]
	TIME [epoch: 54.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007801768900805382		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0007801768900805382 | validation: 0.0009381667927183983]
	TIME [epoch: 54.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044649555997162293		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.00044649555997162293 | validation: 0.000386787650079607]
	TIME [epoch: 54.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003685289729148827		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.0003685289729148827 | validation: 0.0005265587514457906]
	TIME [epoch: 54.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004813670150870513		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0004813670150870513 | validation: 0.00037166390807708894]
	TIME [epoch: 54.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034448931202368316		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.00034448931202368316 | validation: 0.00041105856631626207]
	TIME [epoch: 54.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005177986381299327		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.0005177986381299327 | validation: 0.0006582114427204324]
	TIME [epoch: 54.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007585952085200116		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0007585952085200116 | validation: 0.00032287073192679473]
	TIME [epoch: 54.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033015698592543923		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.00033015698592543923 | validation: 0.000442812007283087]
	TIME [epoch: 54.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005296636765524258		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.0005296636765524258 | validation: 0.0005303764003915781]
	TIME [epoch: 54.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005817506607543035		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0005817506607543035 | validation: 0.0001822669070000291]
	TIME [epoch: 54.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1118.pth
	Model improved!!!
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004236697187044512		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0004236697187044512 | validation: 0.000943195462002338]
	TIME [epoch: 54.8 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044977898726157855		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.00044977898726157855 | validation: 0.0007243982566506064]
	TIME [epoch: 54.8 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038488443137953674		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.00038488443137953674 | validation: 0.0004828365623670141]
	TIME [epoch: 54.8 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002838931509729332		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0002838931509729332 | validation: 0.0005036664246620584]
	TIME [epoch: 54.8 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004229102882971401		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0004229102882971401 | validation: 0.0007200342882856922]
	TIME [epoch: 54.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005922122400786987		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0005922122400786987 | validation: 0.0007937063687856929]
	TIME [epoch: 54.8 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015050828539799467		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.00015050828539799467 | validation: 0.0005497035122210541]
	TIME [epoch: 54.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022352606671515086		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.00022352606671515086 | validation: 0.00041816368536557125]
	TIME [epoch: 54.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032971242555089966		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.00032971242555089966 | validation: 0.0006279683009898883]
	TIME [epoch: 54.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005403477177912117		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0005403477177912117 | validation: 0.0007505254259687]
	TIME [epoch: 54.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004580797810518993		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0004580797810518993 | validation: 0.0005561232730229566]
	TIME [epoch: 54.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002257370596179915		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.0002257370596179915 | validation: 0.00042334946660417484]
	TIME [epoch: 54.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00049990981584386		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.00049990981584386 | validation: 0.00047770132195782365]
	TIME [epoch: 54.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004321162012534039		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0004321162012534039 | validation: 0.0006321755247172395]
	TIME [epoch: 54.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005136207219924898		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0005136207219924898 | validation: 0.00062169858956175]
	TIME [epoch: 54.8 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003873315564877222		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0003873315564877222 | validation: 0.0009220717084700203]
	TIME [epoch: 54.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005075576430336973		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0005075576430336973 | validation: 0.0004071669413646859]
	TIME [epoch: 54.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005678931231849631		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0005678931231849631 | validation: 0.0008723488367557542]
	TIME [epoch: 54.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005371909807262518		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0005371909807262518 | validation: 0.000700041911597566]
	TIME [epoch: 54.8 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028869127588801804		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.00028869127588801804 | validation: 0.0006605389705862769]
	TIME [epoch: 54.8 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048069209651944815		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.00048069209651944815 | validation: 0.00030795142468546116]
	TIME [epoch: 54.8 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003482708997850099		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0003482708997850099 | validation: 0.0002397947333729693]
	TIME [epoch: 54.8 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023506551942743356		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.00023506551942743356 | validation: 0.0004064917033290448]
	TIME [epoch: 54.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037712535068434855		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.00037712535068434855 | validation: 0.0008124158565985109]
	TIME [epoch: 54.9 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003459735746010517		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0003459735746010517 | validation: 0.00030901853825362303]
	TIME [epoch: 54.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003930147801714214		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0003930147801714214 | validation: 0.0008617686022387874]
	TIME [epoch: 54.8 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038404408016965834		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.00038404408016965834 | validation: 0.0003054838975139891]
	TIME [epoch: 54.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035176576248900453		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.00035176576248900453 | validation: 0.0007005657460000636]
	TIME [epoch: 54.9 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030608299447934084		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.00030608299447934084 | validation: 0.00045455057541867475]
	TIME [epoch: 54.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006039259043434301		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0006039259043434301 | validation: 0.00023003153658870002]
	TIME [epoch: 54.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003297638524386326		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0003297638524386326 | validation: 0.00022988477920416806]
	TIME [epoch: 54.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023024675296422957		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.00023024675296422957 | validation: 0.0005818974942578467]
	TIME [epoch: 54.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000350911731550656		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.000350911731550656 | validation: 0.0007667041758644161]
	TIME [epoch: 54.8 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005079450357335087		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.0005079450357335087 | validation: 0.0005572594099366457]
	TIME [epoch: 54.8 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000360351861141539		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.000360351861141539 | validation: 0.0005248099595234273]
	TIME [epoch: 54.8 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031737817759750907		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.00031737817759750907 | validation: 0.0007971395626215614]
	TIME [epoch: 54.8 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005311552705867895		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.0005311552705867895 | validation: 0.00033185834884268676]
	TIME [epoch: 54.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046057423763357086		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.00046057423763357086 | validation: 0.0005187092776250948]
	TIME [epoch: 54.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040491915099849485		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.00040491915099849485 | validation: 0.00023612190326045734]
	TIME [epoch: 54.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003461424789639591		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0003461424789639591 | validation: 0.0005992176192204006]
	TIME [epoch: 54.8 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005869919593900221		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0005869919593900221 | validation: 0.00047678036627033383]
	TIME [epoch: 54.9 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005492109443179026		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.0005492109443179026 | validation: 0.0006121653037389674]
	TIME [epoch: 54.9 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003765571375356036		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0003765571375356036 | validation: 0.000548625047486726]
	TIME [epoch: 54.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004368388207445737		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0004368388207445737 | validation: 0.0007275326993013672]
	TIME [epoch: 54.9 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043929033996012467		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.00043929033996012467 | validation: 0.0005095364772777521]
	TIME [epoch: 54.8 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047379937078743256		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.00047379937078743256 | validation: 0.0004498171385461163]
	TIME [epoch: 54.9 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002588752080390268		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0002588752080390268 | validation: 0.0008410658575370463]
	TIME [epoch: 54.9 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004675344166337707		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0004675344166337707 | validation: 0.0004510265798211464]
	TIME [epoch: 54.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004572873606808477		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0004572873606808477 | validation: 0.0006046695385075172]
	TIME [epoch: 54.8 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003301054766360243		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0003301054766360243 | validation: 0.00034366142647937714]
	TIME [epoch: 54.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025913774672506443		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.00025913774672506443 | validation: 0.0004539821545935441]
	TIME [epoch: 54.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033766729254522686		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.00033766729254522686 | validation: 0.0005087101658370665]
	TIME [epoch: 54.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003763803206349361		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.0003763803206349361 | validation: 0.00035915886766527816]
	TIME [epoch: 54.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002724255504666044		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.0002724255504666044 | validation: 0.00028995587596760775]
	TIME [epoch: 54.8 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002987702771855751		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0002987702771855751 | validation: 0.00023340125280538882]
	TIME [epoch: 54.7 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003728198051915366		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0003728198051915366 | validation: 0.00041729565830898174]
	TIME [epoch: 54.8 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003305790381187241		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0003305790381187241 | validation: 0.0005590819110051228]
	TIME [epoch: 54.8 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004344997539194237		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.0004344997539194237 | validation: 0.0005276990372688024]
	TIME [epoch: 54.8 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045174329799721187		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.00045174329799721187 | validation: 0.0013022597406620938]
	TIME [epoch: 54.9 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004512460309355724		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0004512460309355724 | validation: 0.0002792723509758064]
	TIME [epoch: 54.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042508843892816395		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.00042508843892816395 | validation: 0.00028354194435682527]
	TIME [epoch: 54.8 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000445967140055934		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.000445967140055934 | validation: 0.0003133683842000146]
	TIME [epoch: 54.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028135906485102203		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.00028135906485102203 | validation: 2.1403478391198046e-05]
	TIME [epoch: 54.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1181.pth
	Model improved!!!
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005586062996907047		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0005586062996907047 | validation: 0.0008302700237874135]
	TIME [epoch: 54.8 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031325580245475116		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.00031325580245475116 | validation: 0.0004814111683533673]
	TIME [epoch: 54.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024308569257344836		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.00024308569257344836 | validation: 0.00045231269502389273]
	TIME [epoch: 54.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021237941843619222		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.00021237941843619222 | validation: 0.0005718401528946089]
	TIME [epoch: 54.8 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038521967223489374		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.00038521967223489374 | validation: 0.0005143886164854434]
	TIME [epoch: 54.7 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041382033462192		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.00041382033462192 | validation: 0.0002382821373251902]
	TIME [epoch: 54.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034315681800029706		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.00034315681800029706 | validation: 0.00024316954472915957]
	TIME [epoch: 54.7 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003043307143401466		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.0003043307143401466 | validation: 0.0002326923784065631]
	TIME [epoch: 54.9 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005851083342422625		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.0005851083342422625 | validation: 0.00040829542330244895]
	TIME [epoch: 54.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047483694018681606		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.00047483694018681606 | validation: 0.000472164961085654]
	TIME [epoch: 54.9 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006168789274935571		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0006168789274935571 | validation: 0.0003877311963467989]
	TIME [epoch: 54.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030010170116750843		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.00030010170116750843 | validation: 0.00041903546567687223]
	TIME [epoch: 54.8 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002243942909699337		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0002243942909699337 | validation: 0.0003797740565338694]
	TIME [epoch: 54.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004687556662970671		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.0004687556662970671 | validation: 0.0005349780399491264]
	TIME [epoch: 54.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006339929863890028		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0006339929863890028 | validation: 0.001333409512848082]
	TIME [epoch: 54.8 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038887548697543893		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.00038887548697543893 | validation: 0.0002418242252174041]
	TIME [epoch: 54.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035337168718463065		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.00035337168718463065 | validation: 0.0006503215378479324]
	TIME [epoch: 54.7 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004281006002547871		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.0004281006002547871 | validation: 0.00013308097061746161]
	TIME [epoch: 54.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039978576488866873		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.00039978576488866873 | validation: 0.0007821959083794931]
	TIME [epoch: 54.8 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028775263532043207		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.00028775263532043207 | validation: 0.0003568422817846626]
	TIME [epoch: 54.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005018147001006794		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0005018147001006794 | validation: 0.0002459007589847131]
	TIME [epoch: 54.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041750778480521		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.00041750778480521 | validation: 0.0003961965179792362]
	TIME [epoch: 54.8 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045092327687686474		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.00045092327687686474 | validation: 0.0008395061235913302]
	TIME [epoch: 54.8 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002462944832832286		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0002462944832832286 | validation: 0.0009227968522425733]
	TIME [epoch: 54.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004534756737082155		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.0004534756737082155 | validation: 0.0003549532179780419]
	TIME [epoch: 54.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003440039523395602		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.0003440039523395602 | validation: 0.0006051883375462169]
	TIME [epoch: 54.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042500060242136147		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.00042500060242136147 | validation: 0.0006566311725689724]
	TIME [epoch: 54.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030097570753989		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.00030097570753989 | validation: 0.00013571729368317076]
	TIME [epoch: 54.8 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003983697838439704		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0003983697838439704 | validation: 0.000531379402975574]
	TIME [epoch: 54.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003367947725669864		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.0003367947725669864 | validation: 0.00033992369999544003]
	TIME [epoch: 54.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021622772363919985		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.00021622772363919985 | validation: 0.0005744881128566365]
	TIME [epoch: 54.8 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002480087324629172		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.0002480087324629172 | validation: 0.0003576993036018896]
	TIME [epoch: 54.8 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002897839316533013		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0002897839316533013 | validation: 0.0005548392343743735]
	TIME [epoch: 54.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034336267316895693		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.00034336267316895693 | validation: 0.0008241339648357081]
	TIME [epoch: 54.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004283726980120121		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.0004283726980120121 | validation: 0.00040912580570707835]
	TIME [epoch: 54.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005606808431102071		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.0005606808431102071 | validation: 0.0006047555537865837]
	TIME [epoch: 54.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004622830834604768		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.0004622830834604768 | validation: 0.0004447229450497341]
	TIME [epoch: 54.8 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018514834246181498		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.00018514834246181498 | validation: 0.0005460046062322226]
	TIME [epoch: 54.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035832701351765377		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.00035832701351765377 | validation: 0.00026544272464383936]
	TIME [epoch: 54.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046765284791798554		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.00046765284791798554 | validation: 0.00034306792361914434]
	TIME [epoch: 54.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018329357194223308		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.00018329357194223308 | validation: 0.0005649512208672558]
	TIME [epoch: 54.7 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002872335109949271		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0002872335109949271 | validation: 0.0010050997049141797]
	TIME [epoch: 54.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018648480491440677		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.00018648480491440677 | validation: 0.00041367967102191194]
	TIME [epoch: 54.8 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019561600035347194		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.00019561600035347194 | validation: 0.0005242124806676322]
	TIME [epoch: 54.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003829003283550603		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0003829003283550603 | validation: 0.0005396729644488856]
	TIME [epoch: 54.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024276320894277783		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.00024276320894277783 | validation: 0.00035042274436487554]
	TIME [epoch: 54.8 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004127487620336319		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0004127487620336319 | validation: 0.00038727288143713246]
	TIME [epoch: 54.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029321717024327664		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.00029321717024327664 | validation: 0.0003359012756417412]
	TIME [epoch: 54.8 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029520770408585453		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.00029520770408585453 | validation: 0.00027446224805012596]
	TIME [epoch: 54.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025354906109088327		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.00025354906109088327 | validation: 7.321491762241728e-05]
	TIME [epoch: 54.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000141292596584639		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.000141292596584639 | validation: 0.0006286050691363405]
	TIME [epoch: 54.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040808088558369547		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.00040808088558369547 | validation: 0.0005716933845142434]
	TIME [epoch: 54.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003240447494985683		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.0003240447494985683 | validation: 0.0002902560440830433]
	TIME [epoch: 54.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030894733865139415		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.00030894733865139415 | validation: 0.0005794077053044431]
	TIME [epoch: 54.9 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040174440644333623		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.00040174440644333623 | validation: 0.0004061315334051452]
	TIME [epoch: 54.8 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002239411664978177		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.0002239411664978177 | validation: 0.0004993645582300674]
	TIME [epoch: 54.8 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021035672023590957		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.00021035672023590957 | validation: 0.00017733876232275313]
	TIME [epoch: 54.9 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047890737315102916		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.00047890737315102916 | validation: 0.0002840649389563303]
	TIME [epoch: 54.8 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031285839143507376		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.00031285839143507376 | validation: 0.00034999182489771027]
	TIME [epoch: 54.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029007855897818404		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.00029007855897818404 | validation: 0.0005733167088302169]
	TIME [epoch: 54.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003237428547597219		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0003237428547597219 | validation: 0.0002480919015994907]
	TIME [epoch: 54.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003392133626979732		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.0003392133626979732 | validation: 0.000438772392086455]
	TIME [epoch: 54.8 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026797476434499277		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.00026797476434499277 | validation: 0.0006275009954973579]
	TIME [epoch: 54.9 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032876764892653034		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.00032876764892653034 | validation: 0.0006442546101699288]
	TIME [epoch: 54.8 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029161791743140736		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.00029161791743140736 | validation: 0.0006524806533994311]
	TIME [epoch: 54.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002575350808553101		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.0002575350808553101 | validation: 0.0002732685444035008]
	TIME [epoch: 54.8 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003671637054519732		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.0003671637054519732 | validation: 0.0005466727118592322]
	TIME [epoch: 54.8 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023148121696308776		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.00023148121696308776 | validation: 0.00018348951694393103]
	TIME [epoch: 54.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004423691824562826		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.0004423691824562826 | validation: 0.00042468986040097964]
	TIME [epoch: 54.8 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032434891519950203		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.00032434891519950203 | validation: 0.0004070419228244653]
	TIME [epoch: 54.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002893538706637857		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.0002893538706637857 | validation: 0.0003918985301654887]
	TIME [epoch: 54.8 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002107813779431278		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0002107813779431278 | validation: 0.00024369685025227208]
	TIME [epoch: 54.8 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002936760725369001		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.0002936760725369001 | validation: 0.0008030850449376575]
	TIME [epoch: 54.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023367441983085648		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.00023367441983085648 | validation: 0.0007582734020596771]
	TIME [epoch: 54.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002643211453373655		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.0002643211453373655 | validation: 0.0003734481243094381]
	TIME [epoch: 54.8 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002976378931836421		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.0002976378931836421 | validation: 0.0004206833166354898]
	TIME [epoch: 54.8 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019462071100168044		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.00019462071100168044 | validation: 0.00013486456790954283]
	TIME [epoch: 54.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043246437317668175		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.00043246437317668175 | validation: 0.0005574703621038663]
	TIME [epoch: 54.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003113431613616815		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0003113431613616815 | validation: 0.00016699050986779973]
	TIME [epoch: 54.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002820362881090379		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.0002820362881090379 | validation: 0.0003741005711403371]
	TIME [epoch: 54.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023950081009510885		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.00023950081009510885 | validation: 0.0007182426319028626]
	TIME [epoch: 54.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004019028473969624		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.0004019028473969624 | validation: 0.0005528178860794588]
	TIME [epoch: 54.8 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030733588677284553		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.00030733588677284553 | validation: 0.000620880252314329]
	TIME [epoch: 54.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001602357405334558		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0001602357405334558 | validation: 0.00038619715471409415]
	TIME [epoch: 54.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002509294383107022		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.0002509294383107022 | validation: 0.00022117578840921405]
	TIME [epoch: 54.8 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024672932689230923		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.00024672932689230923 | validation: 0.0005587555133413557]
	TIME [epoch: 54.8 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004520816416106002		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.0004520816416106002 | validation: 0.0001822784022264621]
	TIME [epoch: 54.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003826232930384839		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.0003826232930384839 | validation: 0.0009230337645164939]
	TIME [epoch: 54.8 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000356086793158956		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.000356086793158956 | validation: 0.0001915987870244136]
	TIME [epoch: 54.7 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012261540546093985		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.00012261540546093985 | validation: 0.000235500306259528]
	TIME [epoch: 54.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003916882343135711		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.0003916882343135711 | validation: 0.00021940337350283734]
	TIME [epoch: 54.6 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046278485421912907		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.00046278485421912907 | validation: 0.00046644478884177244]
	TIME [epoch: 54.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002755131986241888		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.0002755131986241888 | validation: 0.00045011401716934694]
	TIME [epoch: 54.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038093833347738063		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.00038093833347738063 | validation: 0.0005053065280294976]
	TIME [epoch: 54.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002096462443863314		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.0002096462443863314 | validation: 0.00046766914532438174]
	TIME [epoch: 54.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030107696407740716		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.00030107696407740716 | validation: 0.0005628446962926219]
	TIME [epoch: 54.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025983877945325083		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.00025983877945325083 | validation: 0.00048595310504479005]
	TIME [epoch: 54.8 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021035977660209196		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.00021035977660209196 | validation: 0.0008429294391007387]
	TIME [epoch: 54.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003085587286113609		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0003085587286113609 | validation: 0.0004557536821493944]
	TIME [epoch: 54.8 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037737891645336387		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.00037737891645336387 | validation: 0.00042809823982067473]
	TIME [epoch: 54.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028155460097417943		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.00028155460097417943 | validation: 0.0002387195346748019]
	TIME [epoch: 54.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241208_120718/states/model_phi1_1a_v_mmd1_1282.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 35009.329 seconds.
