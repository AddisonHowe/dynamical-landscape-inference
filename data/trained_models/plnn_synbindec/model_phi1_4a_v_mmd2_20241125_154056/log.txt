Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3190096335

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.924699840323775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.924699840323775 | validation: 6.67758327248953]
	TIME [epoch: 169 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.4649696135060175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4649696135060175 | validation: 6.3780386440049925]
	TIME [epoch: 0.814 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.495161859392005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.495161859392005 | validation: 6.303210862583761]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.470006278005753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.470006278005753 | validation: 6.332130239839976]
	TIME [epoch: 0.707 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.379820108147079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.379820108147079 | validation: 6.309037546823882]
	TIME [epoch: 0.706 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.647059405498375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.647059405498375 | validation: 6.262460006205773]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.315764224926799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.315764224926799 | validation: 6.145126570233177]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.182150545138619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182150545138619 | validation: 6.103681536464007]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.139715267347827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.139715267347827 | validation: 6.11949399209058]
	TIME [epoch: 0.707 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.147779032005453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.147779032005453 | validation: 6.084725376024248]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.169898842740793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.169898842740793 | validation: 6.02703949340699]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.085228048963128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085228048963128 | validation: 5.983138142133244]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.954827895821236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.954827895821236 | validation: 5.941306605802553]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8994304217617333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8994304217617333 | validation: 5.899834834547324]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8530979515254704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8530979515254704 | validation: 5.869518098341214]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8293758030348464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8293758030348464 | validation: 5.96400116534081]
	TIME [epoch: 0.707 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009585926164231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.009585926164231 | validation: 5.879081776718215]
	TIME [epoch: 0.706 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1884069622746845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1884069622746845 | validation: 5.87724709251961]
	TIME [epoch: 0.704 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8095178845176845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8095178845176845 | validation: 5.91838490863167]
	TIME [epoch: 0.704 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.023321910589053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.023321910589053 | validation: 5.715964385541597]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7867948974365575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7867948974365575 | validation: 5.832631809440762]
	TIME [epoch: 0.705 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7497365732908277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7497365732908277 | validation: 5.785325658228206]
	TIME [epoch: 0.713 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7111415208830523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7111415208830523 | validation: 5.673153927258793]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7101831018514257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7101831018514257 | validation: 5.77715657600562]
	TIME [epoch: 0.708 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.677689085726529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.677689085726529 | validation: 5.6851902530719665]
	TIME [epoch: 0.705 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.645180607193364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.645180607193364 | validation: 5.687681289113997]
	TIME [epoch: 0.703 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.626081156498153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.626081156498153 | validation: 5.648434648605643]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6124165861744437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6124165861744437 | validation: 5.678044135723349]
	TIME [epoch: 0.707 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.605060781256974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.605060781256974 | validation: 5.5582499227702264]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6612600426985513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6612600426985513 | validation: 5.745968603106462]
	TIME [epoch: 0.708 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7014275291660232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7014275291660232 | validation: 5.544773859473345]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.637399625347522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637399625347522 | validation: 5.553501915563235]
	TIME [epoch: 0.704 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.51685083193266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.51685083193266 | validation: 5.532313660256254]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4949946776356313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4949946776356313 | validation: 5.451890072588215]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4854211687730965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4854211687730965 | validation: 5.527523741451771]
	TIME [epoch: 0.704 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4846261699549665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4846261699549665 | validation: 5.395734996988378]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5174008230537765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5174008230537765 | validation: 5.446844910186002]
	TIME [epoch: 0.708 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.411501731644341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.411501731644341 | validation: 5.329320539907872]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3505874327147853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3505874327147853 | validation: 5.312516717860622]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2921400116924575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2921400116924575 | validation: 5.288542226118155]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.271319613606919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.271319613606919 | validation: 5.234528131556537]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2340390409418878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2340390409418878 | validation: 5.2799458253708185]
	TIME [epoch: 0.708 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.288930491456591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.288930491456591 | validation: 5.154823775576381]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2071651614587857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2071651614587857 | validation: 5.155564403284513]
	TIME [epoch: 0.702 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.127075737773954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.127075737773954 | validation: 5.0917114490278]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.052718895425044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.052718895425044 | validation: 5.032785994260774]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.99783293075286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.99783293075286 | validation: 4.987111944795057]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9560688915747018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9560688915747018 | validation: 4.94714999450564]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.907419895156662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.907419895156662 | validation: 4.88491236080578]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.935243482311658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.935243482311658 | validation: 4.856257963305504]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.393695600318169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393695600318169 | validation: 4.739592839616077]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7978457608710796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7978457608710796 | validation: 4.853789249075096]
	TIME [epoch: 0.704 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0458991715335744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0458991715335744 | validation: 4.594741387644592]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8759354680986435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8759354680986435 | validation: 4.494067624689797]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7485717765671995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7485717765671995 | validation: 4.501110973803567]
	TIME [epoch: 0.708 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.692867658563808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.692867658563808 | validation: 4.244202976644352]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.551212054679716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.551212054679716 | validation: 3.955360535324235]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4443907791040527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4443907791040527 | validation: 3.5593045394904]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2484820256413447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2484820256413447 | validation: 1.8714568926422996]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8071090155317915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8071090155317915 | validation: 1.269034234737584]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4884590354493354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4884590354493354 | validation: 3.4210442908293217]
	TIME [epoch: 0.707 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.841589999724072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.841589999724072 | validation: 1.2646147721837488]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5179184997066915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5179184997066915 | validation: 1.179274001302429]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6109628479958895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6109628479958895 | validation: 1.6490070131354715]
	TIME [epoch: 0.701 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2835111455923975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2835111455923975 | validation: 1.410789913906894]
	TIME [epoch: 0.699 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1762366792618222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1762366792618222 | validation: 1.0079129053441283]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1688384196574044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1688384196574044 | validation: 1.184973717925083]
	TIME [epoch: 0.706 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1245421169980383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1245421169980383 | validation: 1.0324793652771085]
	TIME [epoch: 0.702 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0653344402215672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0653344402215672 | validation: 1.008941479235695]
	TIME [epoch: 0.703 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0270645893450538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0270645893450538 | validation: 0.9809249313269156]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0094694473340666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0094694473340666 | validation: 0.9255885710537037]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9805992897055481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9805992897055481 | validation: 0.9398635241763753]
	TIME [epoch: 0.708 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568858859223387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568858859223387 | validation: 0.792722184887225]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9649664990815683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9649664990815683 | validation: 1.2768469762663086]
	TIME [epoch: 0.709 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057198047933036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.057198047933036 | validation: 1.0151023092345743]
	TIME [epoch: 0.707 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2371616232745004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2371616232745004 | validation: 1.2328451078474427]
	TIME [epoch: 0.706 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634410971755748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0634410971755748 | validation: 0.8418740542409324]
	TIME [epoch: 0.704 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038778075048017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038778075048017 | validation: 0.7819437931491355]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9234311285133421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9234311285133421 | validation: 1.0124954274783953]
	TIME [epoch: 0.708 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519889885862296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9519889885862296 | validation: 0.7237006925283318]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137282485201134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9137282485201134 | validation: 0.9078947344507391]
	TIME [epoch: 0.709 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8914646546065561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8914646546065561 | validation: 0.7226739916375281]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8784543958860508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8784543958860508 | validation: 0.8693436387568131]
	TIME [epoch: 0.709 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752427065233559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752427065233559 | validation: 0.6910411955043452]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781659328513425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8781659328513425 | validation: 0.9088833700677743]
	TIME [epoch: 0.705 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8796105469440015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8796105469440015 | validation: 0.6929545603793367]
	TIME [epoch: 0.702 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778324649153874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8778324649153874 | validation: 1.0561657558508]
	TIME [epoch: 0.702 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228218171032854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9228218171032854 | validation: 0.6946675295150141]
	TIME [epoch: 0.699 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9517585321150215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9517585321150215 | validation: 1.0883086808894755]
	TIME [epoch: 0.699 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938954236836626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.938954236836626 | validation: 0.7012884628183791]
	TIME [epoch: 0.7 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514846165500999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8514846165500999 | validation: 0.7564758993687553]
	TIME [epoch: 0.703 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384320420664969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8384320420664969 | validation: 0.8469639222395542]
	TIME [epoch: 0.702 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565455491903927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565455491903927 | validation: 0.6694003695582146]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.946197667745856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.946197667745856 | validation: 0.936396311078341]
	TIME [epoch: 0.705 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934580431334334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934580431334334 | validation: 0.7704039912776933]
	TIME [epoch: 0.701 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871604309316792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871604309316792 | validation: 0.7648973007576096]
	TIME [epoch: 0.7 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449105508016095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449105508016095 | validation: 0.8941723723721717]
	TIME [epoch: 0.701 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469884699898037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469884699898037 | validation: 0.6800103044740271]
	TIME [epoch: 0.702 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596673921190023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8596673921190023 | validation: 0.9834053263251589]
	TIME [epoch: 0.7 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806333095749338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806333095749338 | validation: 0.6369787427343461]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729752384757575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729752384757575 | validation: 0.93176162578713]
	TIME [epoch: 0.71 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861158829259756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.861158829259756 | validation: 0.6239732449464115]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609420438299455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609420438299455 | validation: 0.9495503346041227]
	TIME [epoch: 0.709 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9045727072336408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9045727072336408 | validation: 0.7299734001074598]
	TIME [epoch: 0.702 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.988376711801235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.988376711801235 | validation: 0.7479971260452469]
	TIME [epoch: 0.701 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230165619525207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8230165619525207 | validation: 0.9202508923277336]
	TIME [epoch: 0.7 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408659288889311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408659288889311 | validation: 0.6089980965724817]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8463452000702094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8463452000702094 | validation: 0.7889468175116753]
	TIME [epoch: 0.71 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010330608184819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010330608184819 | validation: 0.7002650965155715]
	TIME [epoch: 0.761 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962552171949469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962552171949469 | validation: 0.7603565407963804]
	TIME [epoch: 0.704 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087462263104302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087462263104302 | validation: 0.7512308332863871]
	TIME [epoch: 0.705 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411149000151644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8411149000151644 | validation: 0.874010735660035]
	TIME [epoch: 0.703 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934386828887745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934386828887745 | validation: 0.881648444292999]
	TIME [epoch: 0.706 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9659189376378575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9659189376378575 | validation: 0.7257075601377244]
	TIME [epoch: 0.706 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870299896069363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7870299896069363 | validation: 0.795480757714201]
	TIME [epoch: 1.08 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993192885867036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7993192885867036 | validation: 0.5876351619359506]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758103675209286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758103675209286 | validation: 0.9949239211416121]
	TIME [epoch: 0.702 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834822389460272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834822389460272 | validation: 0.5729067452535482]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945784381497691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8945784381497691 | validation: 0.9455376315802285]
	TIME [epoch: 0.702 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356633345356196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356633345356196 | validation: 0.6694276766907526]
	TIME [epoch: 0.701 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838241867021374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7838241867021374 | validation: 0.7049672693677246]
	TIME [epoch: 0.7 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781671032367404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7781671032367404 | validation: 0.7583526592532119]
	TIME [epoch: 0.701 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788511178367267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.788511178367267 | validation: 0.637748855736175]
	TIME [epoch: 0.701 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132263650145095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132263650145095 | validation: 0.8007133317495607]
	TIME [epoch: 0.7 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187103325025192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8187103325025192 | validation: 0.746702514817544]
	TIME [epoch: 0.699 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659436804367583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659436804367583 | validation: 0.7705508210996626]
	TIME [epoch: 0.699 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137457275030809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137457275030809 | validation: 0.7971336143608032]
	TIME [epoch: 0.698 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997351727578896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7997351727578896 | validation: 0.6526762197682194]
	TIME [epoch: 0.707 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658772914848612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7658772914848612 | validation: 0.7407458768856041]
	TIME [epoch: 0.706 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606185717192835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606185717192835 | validation: 0.6058676308669418]
	TIME [epoch: 0.703 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588774833198475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588774833198475 | validation: 0.8542762284175917]
	TIME [epoch: 0.702 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136639722925658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136639722925658 | validation: 0.5659620311726521]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0068369539965687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0068369539965687 | validation: 0.9945920478047219]
	TIME [epoch: 0.702 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9151603223291644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9151603223291644 | validation: 0.6448388164176644]
	TIME [epoch: 0.702 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780482015361149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.780482015361149 | validation: 0.6068411931160385]
	TIME [epoch: 0.709 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582485972635817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582485972635817 | validation: 0.8004101854803407]
	TIME [epoch: 0.702 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621279511490349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7621279511490349 | validation: 0.6346895076464973]
	TIME [epoch: 0.701 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533736913157957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7533736913157957 | validation: 0.6814900456238262]
	TIME [epoch: 0.7 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366303981081387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366303981081387 | validation: 0.6916939671562901]
	TIME [epoch: 0.701 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335634662600368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7335634662600368 | validation: 0.6921214344510184]
	TIME [epoch: 0.701 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412192719682862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412192719682862 | validation: 0.7496374840287117]
	TIME [epoch: 0.7 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524864931990641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524864931990641 | validation: 0.9465494430113361]
	TIME [epoch: 0.701 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0024585795162892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0024585795162892 | validation: 0.687344738564785]
	TIME [epoch: 0.702 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589082542032916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589082542032916 | validation: 0.7335460351982103]
	TIME [epoch: 0.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307285227826753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7307285227826753 | validation: 0.6676901392613093]
	TIME [epoch: 0.699 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683809963973355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683809963973355 | validation: 0.7727640288069444]
	TIME [epoch: 0.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756401349598997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756401349598997 | validation: 0.591858539784248]
	TIME [epoch: 0.699 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7273417301269309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7273417301269309 | validation: 0.6854960272371132]
	TIME [epoch: 0.7 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173964317468955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173964317468955 | validation: 0.6337226650564265]
	TIME [epoch: 0.703 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065479075275016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7065479075275016 | validation: 0.6856600937277126]
	TIME [epoch: 0.701 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219064266892634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219064266892634 | validation: 0.5720424946256113]
	TIME [epoch: 0.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8561965012584049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8561965012584049 | validation: 1.125333541819218]
	TIME [epoch: 0.7 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087517792410372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.087517792410372 | validation: 0.5533884761799696]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394314963508032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394314963508032 | validation: 0.6109503985445144]
	TIME [epoch: 0.704 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150498622990421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7150498622990421 | validation: 0.7102210481538007]
	TIME [epoch: 0.704 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341307121799053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341307121799053 | validation: 0.6152504519822518]
	TIME [epoch: 0.702 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043005407091217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7043005407091217 | validation: 0.6079520946877879]
	TIME [epoch: 0.703 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6970132587278475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6970132587278475 | validation: 0.6670527525381078]
	TIME [epoch: 0.702 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966497645712352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966497645712352 | validation: 0.5806866382371094]
	TIME [epoch: 0.701 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954579616645996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954579616645996 | validation: 0.7242548505450386]
	TIME [epoch: 0.7 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085187402946184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085187402946184 | validation: 0.5767350828364999]
	TIME [epoch: 0.702 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362852409013627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362852409013627 | validation: 0.8240478877369434]
	TIME [epoch: 0.703 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794660800503278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.794660800503278 | validation: 0.6866071214336298]
	TIME [epoch: 0.702 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7925052056596738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7925052056596738 | validation: 0.5442640747642565]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827688160487194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827688160487194 | validation: 0.8978945363589118]
	TIME [epoch: 0.71 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425841469720271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425841469720271 | validation: 0.5589701212704568]
	TIME [epoch: 0.706 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687870682453434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6687870682453434 | validation: 0.5351577717683335]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966702361299403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966702361299403 | validation: 0.6901796612980048]
	TIME [epoch: 0.707 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901601454933103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6901601454933103 | validation: 0.6284752609556759]
	TIME [epoch: 0.705 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628640684088316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628640684088316 | validation: 0.5498522864942246]
	TIME [epoch: 0.706 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657323582968696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657323582968696 | validation: 0.6368069021096405]
	TIME [epoch: 0.704 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505908626042575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6505908626042575 | validation: 0.5565683737680281]
	TIME [epoch: 0.703 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371415398133641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371415398133641 | validation: 0.5660518214840169]
	TIME [epoch: 0.703 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395524208070585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6395524208070585 | validation: 0.5726370749094473]
	TIME [epoch: 0.703 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463567697639209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6463567697639209 | validation: 0.6187197618428559]
	TIME [epoch: 0.704 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048791099866535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7048791099866535 | validation: 0.896598232663095]
	TIME [epoch: 0.703 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9694142742437023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9694142742437023 | validation: 0.5768929417809379]
	TIME [epoch: 0.703 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253415361280798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253415361280798 | validation: 0.5971017382926553]
	TIME [epoch: 0.702 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478927208999494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478927208999494 | validation: 0.5385726545614239]
	TIME [epoch: 0.702 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654940452988558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6654940452988558 | validation: 0.6940822202311421]
	TIME [epoch: 0.7 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708424275663216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708424275663216 | validation: 0.4993564711889443]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291232289206019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6291232289206019 | validation: 0.5358263085333884]
	TIME [epoch: 0.706 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6067325693099598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6067325693099598 | validation: 0.5751496702657208]
	TIME [epoch: 0.705 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059014248098898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6059014248098898 | validation: 0.46225691953733705]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205070172492441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205070172492441 | validation: 0.7556379313337038]
	TIME [epoch: 0.706 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959321319013176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6959321319013176 | validation: 0.42791661696561045]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134520566194295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134520566194295 | validation: 0.6808846970940561]
	TIME [epoch: 0.705 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683596979701123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683596979701123 | validation: 0.5945955615001849]
	TIME [epoch: 0.706 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5959340303145242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5959340303145242 | validation: 0.46851992940188164]
	TIME [epoch: 0.704 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604892399957041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604892399957041 | validation: 0.5366684033402207]
	TIME [epoch: 0.704 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682767920182619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5682767920182619 | validation: 0.5505897189420221]
	TIME [epoch: 0.703 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5653457832290877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5653457832290877 | validation: 0.48299055768056975]
	TIME [epoch: 0.705 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5665180568365749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5665180568365749 | validation: 0.5630100274854382]
	TIME [epoch: 0.705 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5622762352072412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5622762352072412 | validation: 0.46258629169801824]
	TIME [epoch: 0.703 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895378942714661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5895378942714661 | validation: 0.6366864429084876]
	TIME [epoch: 0.702 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225825181806902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6225825181806902 | validation: 0.5425365080634076]
	TIME [epoch: 0.704 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341143847189111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6341143847189111 | validation: 0.4738481835750971]
	TIME [epoch: 0.703 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482803455478835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482803455478835 | validation: 0.760651967299637]
	TIME [epoch: 0.703 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6593589319279424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6593589319279424 | validation: 0.475582637224378]
	TIME [epoch: 0.703 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5628913835496041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5628913835496041 | validation: 0.4320878001771763]
	TIME [epoch: 0.705 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5351781614915996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5351781614915996 | validation: 0.527020861566327]
	TIME [epoch: 176 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5243950757583338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5243950757583338 | validation: 0.45850541309658643]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320641063844785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5320641063844785 | validation: 0.5114016201367496]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5299080176478587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5299080176478587 | validation: 0.4149358655620544]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020662158194998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5020662158194998 | validation: 0.5505389744289436]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5093146099467261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5093146099467261 | validation: 0.43912380358451064]
	TIME [epoch: 1.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574266639079978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574266639079978 | validation: 0.7784029605618717]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645435056647195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.645435056647195 | validation: 0.45327180132505696]
	TIME [epoch: 1.37 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5315183062292873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5315183062292873 | validation: 0.42779258626276456]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4817583474872605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4817583474872605 | validation: 0.5220242744188497]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4651837813532761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4651837813532761 | validation: 0.412695954058969]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43927591343784456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43927591343784456 | validation: 0.4656198289496633]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44246961351329483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44246961351329483 | validation: 0.4745277753598714]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5686497022679683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5686497022679683 | validation: 0.6776487669018778]
	TIME [epoch: 1.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522977064626392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6522977064626392 | validation: 0.44209469310136273]
	TIME [epoch: 1.37 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353742646347709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4353742646347709 | validation: 0.4909842381443853]
	TIME [epoch: 1.37 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4815093776426869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4815093776426869 | validation: 0.5732413570597961]
	TIME [epoch: 1.37 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.532133274166614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.532133274166614 | validation: 0.4147158787079347]
	TIME [epoch: 1.37 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4480378313769394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4480378313769394 | validation: 0.7400902876442279]
	TIME [epoch: 1.37 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5434823608491931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5434823608491931 | validation: 0.4215123047145779]
	TIME [epoch: 1.37 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44900132748929467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44900132748929467 | validation: 0.418972777248573]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37094345180174776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37094345180174776 | validation: 0.438832580634921]
	TIME [epoch: 1.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36142827677490375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36142827677490375 | validation: 0.37124624266237105]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37998435255913104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37998435255913104 | validation: 0.659576588416175]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179408109577832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179408109577832 | validation: 0.48904103933993287]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244588358427657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7244588358427657 | validation: 0.4428735226480708]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.403066079391404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.403066079391404 | validation: 0.6501294820861756]
	TIME [epoch: 1.37 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062580366535358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6062580366535358 | validation: 0.5245898512559369]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3996495152907075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3996495152907075 | validation: 0.49713002679763574]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47586670000562226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47586670000562226 | validation: 0.43239935520626105]
	TIME [epoch: 1.37 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42998876802014596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42998876802014596 | validation: 0.41541547322170536]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415554102689151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3415554102689151 | validation: 0.47305553320743005]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34919989296021053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34919989296021053 | validation: 0.4238455387902237]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3908499269493552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3908499269493552 | validation: 0.3764887629317856]
	TIME [epoch: 1.37 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3144664238683412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3144664238683412 | validation: 0.4659281956238694]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35417512352716046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35417512352716046 | validation: 0.3792867378667575]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5429107714184894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5429107714184894 | validation: 0.7387962729118986]
	TIME [epoch: 1.37 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982521599731166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982521599731166 | validation: 0.4481862405656152]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4018975229497045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4018975229497045 | validation: 0.42868188646633193]
	TIME [epoch: 1.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31541119018577457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31541119018577457 | validation: 0.4785187703698828]
	TIME [epoch: 1.37 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35206699906243083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35206699906243083 | validation: 0.4774001499584506]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3906780673530099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3906780673530099 | validation: 0.4949678799220828]
	TIME [epoch: 1.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3763451173798463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3763451173798463 | validation: 0.4071571681893561]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37200845376332725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37200845376332725 | validation: 0.4512020737835911]
	TIME [epoch: 1.37 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3398796352280233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3398796352280233 | validation: 0.407859756784392]
	TIME [epoch: 1.37 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3733350436584536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3733350436584536 | validation: 0.47581782048739013]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4040147348085786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4040147348085786 | validation: 0.4594993137919208]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3892071462259291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3892071462259291 | validation: 0.4222008803534088]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3093475995712677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3093475995712677 | validation: 0.5094345557441626]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031096378559485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031096378559485 | validation: 0.30944989718411553]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41829392491989126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41829392491989126 | validation: 0.5659828871919205]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059206690090021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4059206690090021 | validation: 0.3841457958459942]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2663615192098646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2663615192098646 | validation: 0.3780336624029849]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30891621129108027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30891621129108027 | validation: 0.5184120210455351]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33932974336617655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33932974336617655 | validation: 0.3761839420894133]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927009469887362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2927009469887362 | validation: 0.3717936095805239]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3786970306362119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3786970306362119 | validation: 0.4385044016228061]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3607325904643432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3607325904643432 | validation: 0.36858639688566724]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24705179054074308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24705179054074308 | validation: 0.36479150660799425]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.236536746841012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.236536746841012 | validation: 0.34848226748125183]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2592008107777708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2592008107777708 | validation: 0.48991867304893066]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774016628900283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3774016628900283 | validation: 0.377159058079941]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40410533213081595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40410533213081595 | validation: 0.7333377241415902]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44914797858773303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44914797858773303 | validation: 0.3726353520231236]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.314321733415412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.314321733415412 | validation: 0.3479278051359279]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.263491034078221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.263491034078221 | validation: 0.3998800113198999]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27015023629205015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27015023629205015 | validation: 0.37328648126023845]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3250507003498936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3250507003498936 | validation: 0.3961776329491527]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3316463041355645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3316463041355645 | validation: 0.42229683295024323]
	TIME [epoch: 1.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29354820645204416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29354820645204416 | validation: 0.33587170553230056]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2933106522543155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2933106522543155 | validation: 0.34880404754567423]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639362937801558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2639362937801558 | validation: 0.4273118507199774]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032061849412804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032061849412804 | validation: 0.35863425666965765]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27698115234154747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27698115234154747 | validation: 0.4171081914567733]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2359034116413595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2359034116413595 | validation: 0.2980087612203371]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18221845454294108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18221845454294108 | validation: 0.28662043143602145]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17059434883028182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17059434883028182 | validation: 0.3368477917028305]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17568947653596909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17568947653596909 | validation: 0.2728372293710467]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23330675751056087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23330675751056087 | validation: 0.6727287709620182]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593100707698514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.593100707698514 | validation: 0.6587772755087663]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842142184820494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842142184820494 | validation: 0.4580697400200432]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44894855854935456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44894855854935456 | validation: 0.5169539681592383]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4155380318262105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4155380318262105 | validation: 0.44756327861487577]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29740424402969323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29740424402969323 | validation: 0.36169819567707223]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22432419986716323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22432419986716323 | validation: 0.31118924800653]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20007249513492525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20007249513492525 | validation: 0.3868518531068192]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2129894852431697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2129894852431697 | validation: 0.28044736983301183]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2441014312980878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2441014312980878 | validation: 0.5323477886415285]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3914650748562903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3914650748562903 | validation: 0.32114258353906167]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3053705516448452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3053705516448452 | validation: 0.3738016107697885]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173978382940814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2173978382940814 | validation: 0.3005804570322306]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18220702680798542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18220702680798542 | validation: 0.25840906086275567]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20030831280288378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20030831280288378 | validation: 0.38289857617201345]
	TIME [epoch: 1.38 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26378671980806917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26378671980806917 | validation: 0.31446239534020004]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937877187641731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2937877187641731 | validation: 0.3404549077821041]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23908626083818824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23908626083818824 | validation: 0.4503177394143316]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502214833662887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502214833662887 | validation: 0.289133298370622]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2221369471866855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2221369471866855 | validation: 0.35742079984832187]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091428603438412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2091428603438412 | validation: 0.30785430165770716]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23882225420876524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23882225420876524 | validation: 0.33556482981617397]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3597096520544696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3597096520544696 | validation: 0.45386801297277346]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26775454953843797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26775454953843797 | validation: 0.3036072219007059]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20905032688695596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20905032688695596 | validation: 0.3315395188036162]
	TIME [epoch: 1.37 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780026995792464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780026995792464 | validation: 0.22989510113956171]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16540011738711452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16540011738711452 | validation: 0.3389847479145005]
	TIME [epoch: 1.38 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18603394284814948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18603394284814948 | validation: 0.2587982975434664]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531516334339128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2531516334339128 | validation: 0.5337040661339687]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077431909852404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3077431909852404 | validation: 0.25300074743054074]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15945319047103276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15945319047103276 | validation: 0.22730275044543152]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20607814486482517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20607814486482517 | validation: 0.3629237432443989]
	TIME [epoch: 1.38 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2373203821943222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2373203821943222 | validation: 0.45680353025037806]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3751752839199217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3751752839199217 | validation: 0.36757886520522576]
	TIME [epoch: 1.37 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26456540160242936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26456540160242936 | validation: 0.28440050890530866]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15396954895401538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15396954895401538 | validation: 0.2610436308569388]
	TIME [epoch: 1.37 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14785212943333298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14785212943333298 | validation: 0.25829359097100035]
	TIME [epoch: 1.37 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15796037010909697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15796037010909697 | validation: 0.27551495329226755]
	TIME [epoch: 1.37 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.238029327325511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.238029327325511 | validation: 0.3907444289583594]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2821470828793781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2821470828793781 | validation: 0.2843587719604694]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.259971476773636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.259971476773636 | validation: 0.5272092592182425]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28242071369859034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28242071369859034 | validation: 0.3101671043467979]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32451765427447576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32451765427447576 | validation: 0.23407713967221216]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1969765412038963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1969765412038963 | validation: 0.3134642350989859]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594178398849515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1594178398849515 | validation: 0.2139952477032248]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24043028343506784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24043028343506784 | validation: 0.4028074615266755]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2374252213597962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2374252213597962 | validation: 0.2599747244673087]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17377255274327794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17377255274327794 | validation: 0.22227913030375623]
	TIME [epoch: 1.37 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1403984010944561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1403984010944561 | validation: 0.2826551303087967]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14021003844896662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14021003844896662 | validation: 0.3961523576171073]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19505167839895823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19505167839895823 | validation: 0.2347384488427244]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637372350178303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1637372350178303 | validation: 0.2500324352957213]
	TIME [epoch: 1.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23957313728870602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23957313728870602 | validation: 0.3369413350283959]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3212040312958101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3212040312958101 | validation: 0.20786291026635456]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18296539391220448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18296539391220448 | validation: 0.35693888765181514]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1920766240624028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1920766240624028 | validation: 0.2650775089835142]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16341375836683347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16341375836683347 | validation: 0.2615495160194911]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13757439740353633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13757439740353633 | validation: 0.1906449722719008]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12847341485106006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12847341485106006 | validation: 0.29496351861977965]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14580514984364132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14580514984364132 | validation: 0.18790936763775556]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18474186489259392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18474186489259392 | validation: 0.5352049224931256]
	TIME [epoch: 1.37 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3448064500294734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3448064500294734 | validation: 0.2824272531619248]
	TIME [epoch: 1.37 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2046588680888606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2046588680888606 | validation: 0.21391941142791945]
	TIME [epoch: 1.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33918886236411433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33918886236411433 | validation: 0.433876179685341]
	TIME [epoch: 1.37 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22949139531041313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22949139531041313 | validation: 0.4841212559877974]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3900404340142677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3900404340142677 | validation: 0.23353764635273438]
	TIME [epoch: 1.37 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12196509489497828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12196509489497828 | validation: 0.2305322491999242]
	TIME [epoch: 1.37 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13779596887494452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13779596887494452 | validation: 0.25118734933565323]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13861599192741292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13861599192741292 | validation: 0.2136982892910218]
	TIME [epoch: 1.37 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10308676543719804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10308676543719804 | validation: 0.17689072451904844]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031661014615643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031661014615643 | validation: 0.20253626540072928]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13348872668735265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13348872668735265 | validation: 0.3043436538400144]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670715147054284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2670715147054284 | validation: 0.24364660460162374]
	TIME [epoch: 1.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25601863880652853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25601863880652853 | validation: 0.2969212333696258]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531807413584749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531807413584749 | validation: 0.27484816924590055]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1347341850136264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1347341850136264 | validation: 0.24394413048619606]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1856804547107739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1856804547107739 | validation: 0.49521252571723934]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24680920859816632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24680920859816632 | validation: 0.19567822281814234]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15854724941034837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15854724941034837 | validation: 0.2654948841906703]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17207648561565017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17207648561565017 | validation: 0.24345840802813612]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17619875405016452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17619875405016452 | validation: 0.20699191081394158]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16828177338851688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16828177338851688 | validation: 0.2742240256362481]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15710875185533002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15710875185533002 | validation: 0.24402794389849225]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420802347310301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1420802347310301 | validation: 0.2012148372855871]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13429862463138215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13429862463138215 | validation: 0.2635731785333261]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615784070852352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13615784070852352 | validation: 0.17214375940116403]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13338309731983297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13338309731983297 | validation: 0.19860642648487314]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1350562008391218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1350562008391218 | validation: 0.16739047883822208]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13282426685127496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13282426685127496 | validation: 0.3275535233555379]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670509039035918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670509039035918 | validation: 0.26009752365263944]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23574586805277287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23574586805277287 | validation: 0.487082551914234]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25936139452829315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25936139452829315 | validation: 0.23513988274558223]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21298889765367862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21298889765367862 | validation: 0.18436537287682817]
	TIME [epoch: 1.37 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3174495537597957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3174495537597957 | validation: 0.39067956193326614]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1864107552698685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1864107552698685 | validation: 0.28689360364525635]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19698750560200196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19698750560200196 | validation: 0.19175032466418268]
	TIME [epoch: 1.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15424783376730658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15424783376730658 | validation: 0.1753958559054686]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11200330922364787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11200330922364787 | validation: 0.24335873037182562]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805149466970416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11805149466970416 | validation: 0.1960430003218406]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838349086226587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09838349086226587 | validation: 0.16408799038185595]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09191142746785406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09191142746785406 | validation: 0.2198583628583092]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11385169598047618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11385169598047618 | validation: 0.23908030506729294]
	TIME [epoch: 1.37 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896123455339661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1896123455339661 | validation: 0.2961064442040628]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27878720632438525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27878720632438525 | validation: 0.33827131066667215]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1937483847085761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1937483847085761 | validation: 0.26451006709476366]
	TIME [epoch: 1.37 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12079660852488318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12079660852488318 | validation: 0.16057971333848334]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08804577448939775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08804577448939775 | validation: 0.1523006689058787]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11963751453670032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11963751453670032 | validation: 0.23534957022156366]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12511191883004988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12511191883004988 | validation: 0.20434885859974608]
	TIME [epoch: 1.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639442267643009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639442267643009 | validation: 0.3494897719491441]
	TIME [epoch: 1.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1990296137604543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1990296137604543 | validation: 0.2420777221123062]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15933165397833818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15933165397833818 | validation: 0.18171403136086764]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631426085383754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1631426085383754 | validation: 0.24053408427207526]
	TIME [epoch: 1.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1429318116251549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1429318116251549 | validation: 0.14128370095812773]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997602329589788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11997602329589788 | validation: 0.2788098109902805]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12676136560524748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12676136560524748 | validation: 0.13914302871982506]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10968384048298827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10968384048298827 | validation: 0.22140005545945274]
	TIME [epoch: 1.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12557561699069436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12557561699069436 | validation: 0.2823275591556919]
	TIME [epoch: 1.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21106536472003903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21106536472003903 | validation: 0.2267938430819763]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23242725541147358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23242725541147358 | validation: 0.2558819970714869]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16118353830696205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16118353830696205 | validation: 0.21605221987629686]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11180929276462698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11180929276462698 | validation: 0.13069535898134071]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07017524918600096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07017524918600096 | validation: 0.11635091297189365]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06769154258137429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06769154258137429 | validation: 0.16319394238596585]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883115421298605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06883115421298605 | validation: 0.11314159834466592]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895941068430604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06895941068430604 | validation: 0.2907541219945368]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1241629529317602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1241629529317602 | validation: 0.3152987552760694]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2511584668854026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2511584668854026 | validation: 0.5058857562793749]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26966802013584784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26966802013584784 | validation: 0.1746568896418641]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14612873481287314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14612873481287314 | validation: 0.25197562525208933]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32483232647545707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32483232647545707 | validation: 0.2250591652164828]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16238912574657252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16238912574657252 | validation: 0.23464419125378788]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17854363511511964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17854363511511964 | validation: 0.1903088444819484]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080385182606539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1080385182606539 | validation: 0.13572668586507042]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455364724310294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08455364724310294 | validation: 0.17503128103328525]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08391398148811671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08391398148811671 | validation: 0.1519226842544911]
	TIME [epoch: 1.37 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07630322445760498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07630322445760498 | validation: 0.12893719198749812]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0906316178737268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0906316178737268 | validation: 0.17007164245892425]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08189119059390328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08189119059390328 | validation: 0.14690610830184495]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959328757444956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08959328757444956 | validation: 0.20248727040591757]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147418760763849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.147418760763849 | validation: 0.4126659938429367]
	TIME [epoch: 1.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23571110519348387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23571110519348387 | validation: 0.27977156063919223]
	TIME [epoch: 1.37 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23301368732729444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23301368732729444 | validation: 0.1657882800602255]
	TIME [epoch: 1.37 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16349896086204743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16349896086204743 | validation: 0.3353580791592242]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16870309144994572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16870309144994572 | validation: 0.13962148189324433]
	TIME [epoch: 1.37 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09217553138178129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09217553138178129 | validation: 0.14436594496613347]
	TIME [epoch: 1.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08837630791170309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08837630791170309 | validation: 0.14400201016455072]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09497900726430572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09497900726430572 | validation: 0.13465631191124977]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135346673047747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10135346673047747 | validation: 0.14274372918649955]
	TIME [epoch: 1.37 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204565056778662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1204565056778662 | validation: 0.16180337564642677]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12649540856847513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12649540856847513 | validation: 0.0929800310840885]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616646752629503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09616646752629503 | validation: 0.1626647933314278]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805162127601883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07805162127601883 | validation: 0.10810948623700792]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093038413422422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.093038413422422 | validation: 0.21344645928386694]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441076709721152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1441076709721152 | validation: 0.30297112500608664]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22869582291658389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22869582291658389 | validation: 0.17825758287726445]
	TIME [epoch: 1.37 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15595485025005032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15595485025005032 | validation: 0.12892652852869935]
	TIME [epoch: 1.37 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760920640136268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760920640136268 | validation: 0.14728110143163042]
	TIME [epoch: 1.37 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07206356674144022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07206356674144022 | validation: 0.14199772019809656]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212134390100342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07212134390100342 | validation: 0.10778337471570838]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1122188230676256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1122188230676256 | validation: 0.4360842762863407]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24705508508308716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24705508508308716 | validation: 0.15949793353303968]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16288556027851195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16288556027851195 | validation: 0.14640080621976762]
	TIME [epoch: 1.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19164038795767416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19164038795767416 | validation: 0.16621854299524697]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115521434004227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09115521434004227 | validation: 0.1460418720237122]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826690048137092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08826690048137092 | validation: 0.16468604626201155]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13134082535875158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13134082535875158 | validation: 0.18979719237485074]
	TIME [epoch: 1.37 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13334151872023844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13334151872023844 | validation: 0.13370394122304047]
	TIME [epoch: 1.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10247999230665024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10247999230665024 | validation: 0.09789648776425225]
	TIME [epoch: 1.37 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13103395139040916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13103395139040916 | validation: 0.2675262592803544]
	TIME [epoch: 1.38 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12915395061266757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12915395061266757 | validation: 0.1330263027149965]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883384355380494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06883384355380494 | validation: 0.11356513880913931]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871512807103478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0871512807103478 | validation: 0.17268082705745724]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07634505191204918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07634505191204918 | validation: 0.1367443096085743]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08859092996852695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08859092996852695 | validation: 0.14207551029385782]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14462299383888572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14462299383888572 | validation: 0.30486368442519596]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16469723069754713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16469723069754713 | validation: 0.2749127178627128]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18835999335983902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18835999335983902 | validation: 0.15950313524573945]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14961095350825895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14961095350825895 | validation: 0.10616571113429335]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658583066344669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658583066344669 | validation: 0.12356247211442135]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573566617048931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0573566617048931 | validation: 0.1317504909738672]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911525131799755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911525131799755 | validation: 0.09745822105820859]
	TIME [epoch: 1.37 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420686740191253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420686740191253 | validation: 0.14523100355915827]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285349778054567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08285349778054567 | validation: 0.13049497907438087]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10151315938017415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10151315938017415 | validation: 0.26761967397661635]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608477571230828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1608477571230828 | validation: 0.15959632770387772]
	TIME [epoch: 1.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19197305229884604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19197305229884604 | validation: 0.18795270232023462]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10422477321390075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10422477321390075 | validation: 0.0820688860346776]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240731794754215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05240731794754215 | validation: 0.10260882731640311]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04882549537915395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04882549537915395 | validation: 0.09995714209453094]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591337078713572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05591337078713572 | validation: 0.16465761154450834]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688802707635477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10688802707635477 | validation: 0.40510865522226536]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2057449525758206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2057449525758206 | validation: 0.08231764149652124]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367196399971687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08367196399971687 | validation: 0.12481577915644647]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06146237438066783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06146237438066783 | validation: 0.15877751817643382]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10111952022212357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10111952022212357 | validation: 0.1287752754144755]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14583378520147303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14583378520147303 | validation: 0.10699729540009628]
	TIME [epoch: 1.38 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1602841622342887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1602841622342887 | validation: 0.25335098018017715]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14229077597705922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14229077597705922 | validation: 0.1258091624220815]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14171955134907266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14171955134907266 | validation: 0.16955652765199708]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09453007145132299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09453007145132299 | validation: 0.2159683084821883]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15962907363880113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15962907363880113 | validation: 0.18866235759241978]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553329528052817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1553329528052817 | validation: 0.11927226963594753]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422937815471203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1422937815471203 | validation: 0.23191622243812154]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09883900332001812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09883900332001812 | validation: 0.11173140435867222]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05957487459828641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05957487459828641 | validation: 0.08208057254645867]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07174826002340291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07174826002340291 | validation: 0.1355469727540025]
	TIME [epoch: 1.38 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058535720621033044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058535720621033044 | validation: 0.12175715045688676]
	TIME [epoch: 1.38 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854327714746285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05854327714746285 | validation: 0.11243653418564865]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06601667283145393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06601667283145393 | validation: 0.14125117290904912]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458458029365994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07458458029365994 | validation: 0.1204093860321066]
	TIME [epoch: 1.38 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692806054962975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692806054962975 | validation: 0.1539926634243681]
	TIME [epoch: 1.38 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16704232476657865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16704232476657865 | validation: 0.1567914077704265]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17220830082373834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17220830082373834 | validation: 0.23025697154096073]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14014840361107647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14014840361107647 | validation: 0.09965103643904306]
	TIME [epoch: 1.38 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13383081385563442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13383081385563442 | validation: 0.1805219273381038]
	TIME [epoch: 1.38 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376435103958034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07376435103958034 | validation: 0.12980631607704493]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08808074761331232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08808074761331232 | validation: 0.11614675623512954]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835520295442123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0835520295442123 | validation: 0.08088187787448992]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05862117964519104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05862117964519104 | validation: 0.10180242906180062]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04418486285309365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04418486285309365 | validation: 0.0633444760280857]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04051530738014046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04051530738014046 | validation: 0.08873839347799237]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044410690059859585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044410690059859585 | validation: 0.10227912169309368]
	TIME [epoch: 180 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08425700929415225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08425700929415225 | validation: 0.30150638926482404]
	TIME [epoch: 2.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21794211530788457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21794211530788457 | validation: 0.17713688878567385]
	TIME [epoch: 2.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19116980884490622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19116980884490622 | validation: 0.1538129890187963]
	TIME [epoch: 2.72 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21597151902097836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21597151902097836 | validation: 0.4764500048626712]
	TIME [epoch: 2.72 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30588359275089644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30588359275089644 | validation: 0.20094029574868563]
	TIME [epoch: 2.73 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15259718281469403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15259718281469403 | validation: 0.11632637138797917]
	TIME [epoch: 2.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12169120184425097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12169120184425097 | validation: 0.14395392184894126]
	TIME [epoch: 2.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11229722889566734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11229722889566734 | validation: 0.11181534897460724]
	TIME [epoch: 2.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768291454256956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0768291454256956 | validation: 0.12698494605550623]
	TIME [epoch: 2.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06712837802934588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06712837802934588 | validation: 0.1010809317429306]
	TIME [epoch: 2.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04737276306930316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04737276306930316 | validation: 0.08769453131751513]
	TIME [epoch: 2.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04193964299783939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04193964299783939 | validation: 0.09391720643969043]
	TIME [epoch: 2.72 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378382576527139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378382576527139 | validation: 0.11328741768202891]
	TIME [epoch: 2.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04651086125566648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04651086125566648 | validation: 0.08101196813063916]
	TIME [epoch: 2.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043200957476490824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043200957476490824 | validation: 0.09413962915221952]
	TIME [epoch: 2.72 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04741794492824348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04741794492824348 | validation: 0.11667423735763865]
	TIME [epoch: 2.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06566244026806198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06566244026806198 | validation: 0.12985333437193064]
	TIME [epoch: 2.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329316722894714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08329316722894714 | validation: 0.13716974276405633]
	TIME [epoch: 2.72 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15902616098431296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15902616098431296 | validation: 0.10958695652008209]
	TIME [epoch: 2.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12375871297373266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12375871297373266 | validation: 0.07216297116946138]
	TIME [epoch: 2.73 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09163166655280967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09163166655280967 | validation: 0.21832693808587134]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1317924776852535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1317924776852535 | validation: 0.14829480369017936]
	TIME [epoch: 2.72 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13751989458384267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13751989458384267 | validation: 0.17935037039198606]
	TIME [epoch: 2.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12905428792588047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12905428792588047 | validation: 0.12558785778858814]
	TIME [epoch: 2.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08371949928343643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08371949928343643 | validation: 0.07520992489277524]
	TIME [epoch: 2.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0609684886666258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0609684886666258 | validation: 0.14990796986783636]
	TIME [epoch: 2.72 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266780461607215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07266780461607215 | validation: 0.14276635795082568]
	TIME [epoch: 2.73 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160133072508823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09160133072508823 | validation: 0.21756604586258838]
	TIME [epoch: 2.72 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09320556507504377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09320556507504377 | validation: 0.06368257567190459]
	TIME [epoch: 2.72 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048003191498542985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048003191498542985 | validation: 0.1210206517797738]
	TIME [epoch: 2.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670944085129813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0670944085129813 | validation: 0.14321958434374812]
	TIME [epoch: 2.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08704154191930734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08704154191930734 | validation: 0.085853010415746]
	TIME [epoch: 2.72 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0978836497196194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0978836497196194 | validation: 0.08286085678582394]
	TIME [epoch: 2.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09333621816928647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09333621816928647 | validation: 0.10150290970559049]
	TIME [epoch: 2.72 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10366809100228612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10366809100228612 | validation: 0.1502305338047111]
	TIME [epoch: 2.72 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12623283344868155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12623283344868155 | validation: 0.16290829791792658]
	TIME [epoch: 2.72 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220027221598743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12220027221598743 | validation: 0.08662363104065146]
	TIME [epoch: 2.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0792901643874539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0792901643874539 | validation: 0.14343969480423022]
	TIME [epoch: 2.72 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08605280807666549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08605280807666549 | validation: 0.08515763659133428]
	TIME [epoch: 2.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08940214544606687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08940214544606687 | validation: 0.16223569294812729]
	TIME [epoch: 2.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08281478814234935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08281478814234935 | validation: 0.13284946864067892]
	TIME [epoch: 2.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07650411536816848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07650411536816848 | validation: 0.13124587666659737]
	TIME [epoch: 2.72 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060087285057398104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060087285057398104 | validation: 0.06126832129642284]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496225460759862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0496225460759862 | validation: 0.08433215893910466]
	TIME [epoch: 2.72 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343417809133817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06343417809133817 | validation: 0.08045775624561773]
	TIME [epoch: 2.78 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471024660975687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08471024660975687 | validation: 0.08478107110356105]
	TIME [epoch: 2.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637286631549657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637286631549657 | validation: 0.10196117968237335]
	TIME [epoch: 2.72 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265380742272516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265380742272516 | validation: 0.12468740617491605]
	TIME [epoch: 2.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09700859841151693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09700859841151693 | validation: 0.28459145424778753]
	TIME [epoch: 2.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15288440228802963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15288440228802963 | validation: 0.07979472220707634]
	TIME [epoch: 2.72 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622182674658652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07622182674658652 | validation: 0.06234615039270548]
	TIME [epoch: 2.72 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059100403483964836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059100403483964836 | validation: 0.08337303117506484]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042301391615621096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042301391615621096 | validation: 0.08394957266277348]
	TIME [epoch: 2.72 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0431106727682535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0431106727682535 | validation: 0.1091587517618815]
	TIME [epoch: 2.72 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053397186345403594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053397186345403594 | validation: 0.11808001188911599]
	TIME [epoch: 2.72 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829285167961912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829285167961912 | validation: 0.13241360792466486]
	TIME [epoch: 2.72 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757697580322298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08757697580322298 | validation: 0.07390556603226335]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115573072723109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08115573072723109 | validation: 0.12154070059425548]
	TIME [epoch: 2.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721462047572629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721462047572629 | validation: 0.11832638460019457]
	TIME [epoch: 2.72 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13776097424282668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13776097424282668 | validation: 0.3183302664786393]
	TIME [epoch: 2.72 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18937318615969723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18937318615969723 | validation: 0.10556917936467124]
	TIME [epoch: 2.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08851519945896431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08851519945896431 | validation: 0.15267526266596054]
	TIME [epoch: 2.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19501652349540435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19501652349540435 | validation: 0.3082203061621154]
	TIME [epoch: 2.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19385227697451857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19385227697451857 | validation: 0.1539371357380897]
	TIME [epoch: 2.73 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14000220923898815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14000220923898815 | validation: 0.08062415568549008]
	TIME [epoch: 2.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09676486942322195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09676486942322195 | validation: 0.07218446908406051]
	TIME [epoch: 2.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049600307981881614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049600307981881614 | validation: 0.10383606981723764]
	TIME [epoch: 2.72 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054823515008989104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054823515008989104 | validation: 0.08198706067431792]
	TIME [epoch: 2.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05080596823865118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05080596823865118 | validation: 0.09132428146366314]
	TIME [epoch: 2.72 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04919404402074431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04919404402074431 | validation: 0.10532786088049423]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05552972662923656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05552972662923656 | validation: 0.10864250011869658]
	TIME [epoch: 2.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004469578536959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07004469578536959 | validation: 0.10549866377815112]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222893126550474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10222893126550474 | validation: 0.12910620794456606]
	TIME [epoch: 2.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10992334484863736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10992334484863736 | validation: 0.10227724394398359]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306062430825087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306062430825087 | validation: 0.07233099397435734]
	TIME [epoch: 2.72 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04192776111981824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04192776111981824 | validation: 0.057413873500456306]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03175195374245567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03175195374245567 | validation: 0.063654622052098]
	TIME [epoch: 2.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03190945326386875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03190945326386875 | validation: 0.07315404465334042]
	TIME [epoch: 2.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389487835769451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0389487835769451 | validation: 0.10401664535423581]
	TIME [epoch: 2.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638496553780823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638496553780823 | validation: 0.13224719302970195]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09665904597135849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09665904597135849 | validation: 0.08837102391072704]
	TIME [epoch: 2.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10926589462889048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10926589462889048 | validation: 0.1731784299690612]
	TIME [epoch: 2.72 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08202790430473812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08202790430473812 | validation: 0.0620400883668733]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05415034641081686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05415034641081686 | validation: 0.13875692074996732]
	TIME [epoch: 2.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0640116930856558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0640116930856558 | validation: 0.111181364708034]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08194731232342008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08194731232342008 | validation: 0.08418449960872626]
	TIME [epoch: 2.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06419143536344094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06419143536344094 | validation: 0.09410259823529657]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857577640553824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03857577640553824 | validation: 0.05842966713377487]
	TIME [epoch: 2.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037778615714518626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037778615714518626 | validation: 0.09561430365156703]
	TIME [epoch: 2.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058821701227369544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058821701227369544 | validation: 0.09272631029090939]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316134683489744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1316134683489744 | validation: 0.16487051117502102]
	TIME [epoch: 2.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18004011221924632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18004011221924632 | validation: 0.19686391822775517]
	TIME [epoch: 2.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14009010122581836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14009010122581836 | validation: 0.07897044097491063]
	TIME [epoch: 2.73 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057273366902851654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057273366902851654 | validation: 0.04299850176930427]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957568054225323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03957568054225323 | validation: 0.1267358658153177]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478602343534756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478602343534756 | validation: 0.10363133712895795]
	TIME [epoch: 2.73 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1142633174420099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1142633174420099 | validation: 0.28503109760049594]
	TIME [epoch: 2.73 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15518250407954384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15518250407954384 | validation: 0.08656329345475985]
	TIME [epoch: 2.73 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719662832654323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719662832654323 | validation: 0.07298591653694585]
	TIME [epoch: 2.73 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407888103557677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09407888103557677 | validation: 0.1366161288458148]
	TIME [epoch: 2.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020547651972672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07020547651972672 | validation: 0.11796174007141312]
	TIME [epoch: 2.73 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918392156234942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0918392156234942 | validation: 0.12346798223605884]
	TIME [epoch: 2.73 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10902220671328515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10902220671328515 | validation: 0.08657465160896156]
	TIME [epoch: 2.74 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12396924560778429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12396924560778429 | validation: 0.0857509539440684]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700002916359014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05700002916359014 | validation: 0.10089314858655318]
	TIME [epoch: 2.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086088041770404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086088041770404 | validation: 0.06134110309248053]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04914914975755694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04914914975755694 | validation: 0.08817077155709768]
	TIME [epoch: 2.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04034642978071097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04034642978071097 | validation: 0.06274744621747362]
	TIME [epoch: 2.74 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039233835787383306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039233835787383306 | validation: 0.07021001679830749]
	TIME [epoch: 2.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052358805022447406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052358805022447406 | validation: 0.2116562167184217]
	TIME [epoch: 2.74 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12483273519083617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12483273519083617 | validation: 0.16082628103365187]
	TIME [epoch: 2.74 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128125628251413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128125628251413 | validation: 0.04732535621828347]
	TIME [epoch: 2.74 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680480074051894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06680480074051894 | validation: 0.10446625952563904]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258797803289616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06258797803289616 | validation: 0.12326674161027024]
	TIME [epoch: 2.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0934821600182465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0934821600182465 | validation: 0.14162827950927584]
	TIME [epoch: 2.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10847337791146693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10847337791146693 | validation: 0.06939534935701971]
	TIME [epoch: 2.74 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694715393247651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06694715393247651 | validation: 0.06917342844581309]
	TIME [epoch: 2.73 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159233351529857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04159233351529857 | validation: 0.07106521243084017]
	TIME [epoch: 2.74 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03726897768604354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03726897768604354 | validation: 0.0649100181890636]
	TIME [epoch: 2.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042193063629379156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042193063629379156 | validation: 0.1161556382293554]
	TIME [epoch: 2.73 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343031891214274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05343031891214274 | validation: 0.07645970428278073]
	TIME [epoch: 2.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558896039712934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05558896039712934 | validation: 0.09174887921330979]
	TIME [epoch: 2.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056010007908883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05056010007908883 | validation: 0.08838794526463549]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04189517155881038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04189517155881038 | validation: 0.0644151994474748]
	TIME [epoch: 2.74 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053150232800096536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053150232800096536 | validation: 0.10164526768326157]
	TIME [epoch: 2.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216825898422077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07216825898422077 | validation: 0.09126085114148297]
	TIME [epoch: 2.75 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11087605388731774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11087605388731774 | validation: 0.1169990943213368]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15419955571392133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15419955571392133 | validation: 0.2110103740161467]
	TIME [epoch: 2.74 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13571823776454692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13571823776454692 | validation: 0.05811977596197979]
	TIME [epoch: 2.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06084664586744285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06084664586744285 | validation: 0.09813354312477812]
	TIME [epoch: 2.74 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061144158346525666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061144158346525666 | validation: 0.14043564733709216]
	TIME [epoch: 2.73 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048360736595741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048360736595741 | validation: 0.13011954478651538]
	TIME [epoch: 2.74 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729105456435387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729105456435387 | validation: 0.04112139399137617]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031566596191479446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031566596191479446 | validation: 0.06057133139134316]
	TIME [epoch: 2.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201885253716829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03201885253716829 | validation: 0.0832301712392558]
	TIME [epoch: 2.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047615674290802865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047615674290802865 | validation: 0.0942652667497859]
	TIME [epoch: 2.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08629982662548026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08629982662548026 | validation: 0.1453087865377885]
	TIME [epoch: 2.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679974114371262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06679974114371262 | validation: 0.05594892541996375]
	TIME [epoch: 2.72 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02880840719335036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02880840719335036 | validation: 0.07519610882940081]
	TIME [epoch: 2.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038586680317750775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038586680317750775 | validation: 0.08670773254234831]
	TIME [epoch: 2.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059327804553847105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059327804553847105 | validation: 0.08600612047242114]
	TIME [epoch: 2.72 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08239294581362948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08239294581362948 | validation: 0.05550764569205289]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800734330617421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06800734330617421 | validation: 0.07134042705381416]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0721671638234674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0721671638234674 | validation: 0.24846012576551676]
	TIME [epoch: 2.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1893874605116388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1893874605116388 | validation: 0.2036803336197248]
	TIME [epoch: 2.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15600185128914196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15600185128914196 | validation: 0.11049518670772707]
	TIME [epoch: 2.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060072052334308616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060072052334308616 | validation: 0.0672843419987274]
	TIME [epoch: 2.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055540683719604786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055540683719604786 | validation: 0.08814987429172963]
	TIME [epoch: 2.73 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963805524292689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06963805524292689 | validation: 0.08074710485886849]
	TIME [epoch: 2.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1036841592798495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1036841592798495 | validation: 0.1445955571162552]
	TIME [epoch: 2.72 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0873795525642446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0873795525642446 | validation: 0.05545095511094225]
	TIME [epoch: 2.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614497930580544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03614497930580544 | validation: 0.06303694660803287]
	TIME [epoch: 2.72 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058482433479076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058482433479076 | validation: 0.12928711562771947]
	TIME [epoch: 2.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06615100412359473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06615100412359473 | validation: 0.0717284164628229]
	TIME [epoch: 2.72 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495999468298767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0495999468298767 | validation: 0.04983457875753235]
	TIME [epoch: 2.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04520609699199007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04520609699199007 | validation: 0.08710594046356529]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050687261970066774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050687261970066774 | validation: 0.06103804196113145]
	TIME [epoch: 2.72 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349047723417576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07349047723417576 | validation: 0.11985410603688217]
	TIME [epoch: 2.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07392255298891391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07392255298891391 | validation: 0.07237846464241814]
	TIME [epoch: 2.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046601295440783785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046601295440783785 | validation: 0.051705651742032455]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484019649255208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03484019649255208 | validation: 0.05403094142847617]
	TIME [epoch: 2.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03706299263159423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03706299263159423 | validation: 0.0786767287144761]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724163390317829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07724163390317829 | validation: 0.10157009088406943]
	TIME [epoch: 2.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12725142242658524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12725142242658524 | validation: 0.09290129242028076]
	TIME [epoch: 2.72 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08988182505097943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08988182505097943 | validation: 0.09050194890569052]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050408889968752195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050408889968752195 | validation: 0.07064482125806874]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06604771981556139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06604771981556139 | validation: 0.07433024557694037]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03958915600214443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03958915600214443 | validation: 0.037482680149417094]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036841155551038986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036841155551038986 | validation: 0.07780861145323749]
	TIME [epoch: 2.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046913582757098185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046913582757098185 | validation: 0.06087651055272612]
	TIME [epoch: 2.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04823822690445208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04823822690445208 | validation: 0.11056286985412617]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450820400497287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06450820400497287 | validation: 0.12307879716487272]
	TIME [epoch: 2.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081858775487731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08081858775487731 | validation: 0.06567093981145992]
	TIME [epoch: 2.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641499677544725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641499677544725 | validation: 0.07925120840507023]
	TIME [epoch: 2.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452776635846885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06452776635846885 | validation: 0.0872034354996722]
	TIME [epoch: 2.72 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07920440888161927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07920440888161927 | validation: 0.247695053585068]
	TIME [epoch: 2.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213600095944265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11213600095944265 | validation: 0.06912641040799683]
	TIME [epoch: 2.72 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107256283998139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107256283998139 | validation: 0.07359861797131438]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0673825174284359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0673825174284359 | validation: 0.12131693100617331]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099371627353865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099371627353865 | validation: 0.059978200948447274]
	TIME [epoch: 2.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04063432035951063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04063432035951063 | validation: 0.052128163678871464]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05295443555638981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05295443555638981 | validation: 0.08031075323505138]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07405294887679562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07405294887679562 | validation: 0.07133256038719303]
	TIME [epoch: 2.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961304130548114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07961304130548114 | validation: 0.0686631427272018]
	TIME [epoch: 2.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693669555400204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05693669555400204 | validation: 0.05536362231752753]
	TIME [epoch: 2.73 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029627101503047936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029627101503047936 | validation: 0.044660367092670794]
	TIME [epoch: 2.72 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020975479056662753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020975479056662753 | validation: 0.06470931576291647]
	TIME [epoch: 2.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03271472431684326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03271472431684326 | validation: 0.09760977354755156]
	TIME [epoch: 2.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07261203217600898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07261203217600898 | validation: 0.16928015554952547]
	TIME [epoch: 2.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312955996303632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312955996303632 | validation: 0.08795636812463561]
	TIME [epoch: 2.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10684795473741765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10684795473741765 | validation: 0.11689748264548179]
	TIME [epoch: 2.72 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07971130792307572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07971130792307572 | validation: 0.05580833115449427]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04899395585049311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04899395585049311 | validation: 0.05803846356029846]
	TIME [epoch: 2.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029735885592934885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029735885592934885 | validation: 0.05916595330413012]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029445226381076467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029445226381076467 | validation: 0.04797582695404359]
	TIME [epoch: 2.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03369668664282468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03369668664282468 | validation: 0.06382937911468511]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920979035837091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03920979035837091 | validation: 0.06709208387576013]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05889520234718122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05889520234718122 | validation: 0.0800199686515587]
	TIME [epoch: 2.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700049210347266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06700049210347266 | validation: 0.0517837340901797]
	TIME [epoch: 2.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05483358618201262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05483358618201262 | validation: 0.047165981615958655]
	TIME [epoch: 2.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891075265902504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03891075265902504 | validation: 0.09087982756782928]
	TIME [epoch: 2.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635010420441814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05635010420441814 | validation: 0.10913946792286844]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0933318394934465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0933318394934465 | validation: 0.139151225677296]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13121718055605663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13121718055605663 | validation: 0.3721069115365938]
	TIME [epoch: 2.72 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2133607873099087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2133607873099087 | validation: 0.19466608885518874]
	TIME [epoch: 2.72 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14995402746215614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14995402746215614 | validation: 0.15864822986239013]
	TIME [epoch: 2.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17780381633347225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17780381633347225 | validation: 0.13274765485752896]
	TIME [epoch: 2.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11360519471544749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11360519471544749 | validation: 0.10442971474628393]
	TIME [epoch: 2.72 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0489321976607536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0489321976607536 | validation: 0.034236582618015744]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03585609936562259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03585609936562259 | validation: 0.05485066826396345]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03037949250705566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03037949250705566 | validation: 0.05731148742280095]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02719451115285972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02719451115285972 | validation: 0.0458631181156854]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024126102153108866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024126102153108866 | validation: 0.04446067010173176]
	TIME [epoch: 2.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021753777042056676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021753777042056676 | validation: 0.06309062727821428]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02916582479910309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02916582479910309 | validation: 0.09942752700452367]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054501930176679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05054501930176679 | validation: 0.13083251842762064]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321492504894306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07321492504894306 | validation: 0.048176452157835586]
	TIME [epoch: 2.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034860748046455704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034860748046455704 | validation: 0.055961947350269985]
	TIME [epoch: 2.72 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028253403609460454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028253403609460454 | validation: 0.07503461066433505]
	TIME [epoch: 2.73 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188498857596022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04188498857596022 | validation: 0.08622177927064457]
	TIME [epoch: 2.72 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13636886803248102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13636886803248102 | validation: 0.08725247366426434]
	TIME [epoch: 2.72 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14884629470576805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14884629470576805 | validation: 0.08301765045044521]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045393086824929825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045393086824929825 | validation: 0.1013911359016737]
	TIME [epoch: 2.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061635711442887146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061635711442887146 | validation: 0.08566493028283867]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057171286944080094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057171286944080094 | validation: 0.05343981540016029]
	TIME [epoch: 2.72 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0321700363319624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0321700363319624 | validation: 0.04420696533112624]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02488098298010528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02488098298010528 | validation: 0.052879730103441026]
	TIME [epoch: 2.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417294971028863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03417294971028863 | validation: 0.05576781703196925]
	TIME [epoch: 2.73 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616999490693265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04616999490693265 | validation: 0.1192776160864063]
	TIME [epoch: 2.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305937577693594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07305937577693594 | validation: 0.11184146184470177]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868311489403552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0868311489403552 | validation: 0.07741210964130595]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638230946607841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06638230946607841 | validation: 0.10635333378073726]
	TIME [epoch: 2.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08199683158493258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08199683158493258 | validation: 0.07057680939391163]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816667955840715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0816667955840715 | validation: 0.09673524529185656]
	TIME [epoch: 2.72 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04431660680427358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04431660680427358 | validation: 0.047733295160544434]
	TIME [epoch: 2.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02316057128011803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02316057128011803 | validation: 0.05305479268682032]
	TIME [epoch: 2.73 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129977542435662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129977542435662 | validation: 0.05197919889598516]
	TIME [epoch: 2.73 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02881596832848035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02881596832848035 | validation: 0.04746290302029781]
	TIME [epoch: 2.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02749457468258915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02749457468258915 | validation: 0.04821194160159147]
	TIME [epoch: 2.73 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032113022018398156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032113022018398156 | validation: 0.08295497161757388]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881090280442158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05881090280442158 | validation: 0.05432396529851239]
	TIME [epoch: 2.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039472589150572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08039472589150572 | validation: 0.0644363817554566]
	TIME [epoch: 2.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521161845039773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06521161845039773 | validation: 0.08022680843765993]
	TIME [epoch: 2.72 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371502949806676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05371502949806676 | validation: 0.07743139802850371]
	TIME [epoch: 2.73 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04705270062262207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04705270062262207 | validation: 0.07143109771144206]
	TIME [epoch: 2.73 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804730542746502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804730542746502 | validation: 0.10875112606820947]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054955733697661574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054955733697661574 | validation: 0.09866895422839901]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08967600866630132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08967600866630132 | validation: 0.15650957633680873]
	TIME [epoch: 2.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10303208412949585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10303208412949585 | validation: 0.06301980414447013]
	TIME [epoch: 2.71 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575677624845374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04575677624845374 | validation: 0.07512897856396804]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04072048220670089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04072048220670089 | validation: 0.05743696538750003]
	TIME [epoch: 2.71 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049266324704468366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049266324704468366 | validation: 0.10169932715900123]
	TIME [epoch: 2.72 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371842426578049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06371842426578049 | validation: 0.1412207477547356]
	TIME [epoch: 2.71 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09680965902183043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09680965902183043 | validation: 0.21483277760441954]
	TIME [epoch: 2.72 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12871808043342714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12871808043342714 | validation: 0.03984522754801502]
	TIME [epoch: 2.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037812462505658465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037812462505658465 | validation: 0.07224465379775848]
	TIME [epoch: 2.72 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051234664755458126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051234664755458126 | validation: 0.10616291138603307]
	TIME [epoch: 2.71 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056086846806474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056086846806474 | validation: 0.05789204055025199]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04299103783817733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04299103783817733 | validation: 0.05109044608838784]
	TIME [epoch: 2.71 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036834909202607354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036834909202607354 | validation: 0.06329749376289208]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03261287403904395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03261287403904395 | validation: 0.04712765290411285]
	TIME [epoch: 2.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331684253299946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0331684253299946 | validation: 0.0635471988973223]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083546915823594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04083546915823594 | validation: 0.06857029372393508]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061537588297211177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061537588297211177 | validation: 0.08395225080505549]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0716015896525134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0716015896525134 | validation: 0.07139722810413904]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06242737980686435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06242737980686435 | validation: 0.09145942097961052]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0539130631930616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0539130631930616 | validation: 0.05319821929713055]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188309348855285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04188309348855285 | validation: 0.05689622704275771]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03812892354828492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03812892354828492 | validation: 0.04393685121560384]
	TIME [epoch: 2.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039456130895506966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039456130895506966 | validation: 0.07905211746216538]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054582032888294586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054582032888294586 | validation: 0.11497462694124505]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07808972116415439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07808972116415439 | validation: 0.034537355613407594]
	TIME [epoch: 2.71 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043922539887564505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043922539887564505 | validation: 0.06329834189658132]
	TIME [epoch: 2.72 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02818132498540769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02818132498540769 | validation: 0.060030835595289656]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03330360452246695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03330360452246695 | validation: 0.09659643766541176]
	TIME [epoch: 2.71 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06193772742424804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06193772742424804 | validation: 0.25487122403972057]
	TIME [epoch: 2.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12028104140295362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12028104140295362 | validation: 0.03930517280536471]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0317968747135429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0317968747135429 | validation: 0.08411985421266631]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05603270294968011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05603270294968011 | validation: 0.09683674362434404]
	TIME [epoch: 2.71 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056134804471253526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056134804471253526 | validation: 0.04810794226660153]
	TIME [epoch: 2.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036882651825686355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036882651825686355 | validation: 0.058774077836661665]
	TIME [epoch: 2.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06872154337248591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06872154337248591 | validation: 0.11535996398380102]
	TIME [epoch: 2.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248507467978797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10248507467978797 | validation: 0.11738343413727428]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09893923558478576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09893923558478576 | validation: 0.08543012778591653]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848330689941511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05848330689941511 | validation: 0.05338527436180664]
	TIME [epoch: 2.72 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03235524278060957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03235524278060957 | validation: 0.04610648874540969]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133878973813676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03133878973813676 | validation: 0.0556394064686091]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345927582154192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0345927582154192 | validation: 0.05104346026186538]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532847428884438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03532847428884438 | validation: 0.04568431342798101]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332185504155939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332185504155939 | validation: 0.06111213787917589]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957660710373971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03957660710373971 | validation: 0.08080555050554893]
	TIME [epoch: 2.71 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04410049377422732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04410049377422732 | validation: 0.08047012409191143]
	TIME [epoch: 2.72 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239123426069529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239123426069529 | validation: 0.053900009509932435]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677241334409876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03677241334409876 | validation: 0.05687407345493735]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041462250630936864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041462250630936864 | validation: 0.08399583984399293]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879540977788942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06879540977788942 | validation: 0.15413275179960767]
	TIME [epoch: 2.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552295518511096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09552295518511096 | validation: 0.07378435870466447]
	TIME [epoch: 2.71 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05316281718639023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05316281718639023 | validation: 0.04017002782243855]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03990338364579862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03990338364579862 | validation: 0.07052616961792778]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051545196208151886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051545196208151886 | validation: 0.07792010471522648]
	TIME [epoch: 2.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347823337970612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07347823337970612 | validation: 0.07643323833707508]
	TIME [epoch: 2.74 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05864888395656341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05864888395656341 | validation: 0.0601295705993272]
	TIME [epoch: 2.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168118969900322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04168118969900322 | validation: 0.05840422245055622]
	TIME [epoch: 2.74 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046456161887710186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046456161887710186 | validation: 0.09073341056274814]
	TIME [epoch: 2.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05710217649203044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05710217649203044 | validation: 0.07915841903279976]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498081839959015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498081839959015 | validation: 0.0768675680455615]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388070847547649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06388070847547649 | validation: 0.08744284790430716]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776103981918132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04776103981918132 | validation: 0.055320308823400124]
	TIME [epoch: 2.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461995412803023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03461995412803023 | validation: 0.05925136459494173]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029340907403754404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029340907403754404 | validation: 0.03902254335758925]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_154056/states/model_phi1_4a_v_mmd2_811.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2003.335 seconds.
