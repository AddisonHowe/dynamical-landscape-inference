Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3784666379

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.701082580955822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.701082580955822 | validation: 3.7875767636057667]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.517083371796115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517083371796115 | validation: 3.6059788242658275]
	TIME [epoch: 0.773 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.327126291543898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.327126291543898 | validation: 3.0553769985635943]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.833159439574737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.833159439574737 | validation: 1.830194310955764]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4389346345131564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4389346345131564 | validation: 6.636101737330662]
	TIME [epoch: 0.696 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0235464831763945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0235464831763945 | validation: 6.871680274550079]
	TIME [epoch: 0.696 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.083667828184186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.083667828184186 | validation: 6.8636875731349605]
	TIME [epoch: 0.694 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0680820983456645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0680820983456645 | validation: 6.559481388446454]
	TIME [epoch: 0.693 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.434811807460451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.434811807460451 | validation: 5.540560102889581]
	TIME [epoch: 0.693 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.658133589049926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.658133589049926 | validation: 6.110776247687091]
	TIME [epoch: 0.693 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6919404513208836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6919404513208836 | validation: 5.396621529045525]
	TIME [epoch: 0.693 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.894496845458877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.894496845458877 | validation: 2.6709905837887256]
	TIME [epoch: 0.694 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6478590512070252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6478590512070252 | validation: 1.8266351063668873]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.619458802250955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.619458802250955 | validation: 1.386482088081403]
	TIME [epoch: 0.727 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.342734573882847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.342734573882847 | validation: 1.3707000388192097]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7677424923103644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7677424923103644 | validation: 1.6914739870806894]
	TIME [epoch: 0.695 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.605087517550856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.605087517550856 | validation: 1.7252587326492936]
	TIME [epoch: 0.694 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0592484819239023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0592484819239023 | validation: 2.0029562488051202]
	TIME [epoch: 0.693 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8552664517246706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8552664517246706 | validation: 1.0949861668516487]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4171810385396395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4171810385396395 | validation: 0.9837100611541942]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4155964156717267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4155964156717267 | validation: 1.1579972023584042]
	TIME [epoch: 0.695 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3795844476760784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3795844476760784 | validation: 1.246168595084847]
	TIME [epoch: 0.693 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3685970036926356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3685970036926356 | validation: 1.0676588110738843]
	TIME [epoch: 0.695 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3397641367341118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3397641367341118 | validation: 1.049805939375964]
	TIME [epoch: 0.694 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3186620471762809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3186620471762809 | validation: 1.0909844133753797]
	TIME [epoch: 0.698 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3112920755447826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3112920755447826 | validation: 1.0733492601496692]
	TIME [epoch: 0.694 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3035771387965576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3035771387965576 | validation: 1.0147607047243212]
	TIME [epoch: 0.694 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3009323271096582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3009323271096582 | validation: 1.0934180079379494]
	TIME [epoch: 0.694 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307656989013409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.307656989013409 | validation: 1.1639762587973224]
	TIME [epoch: 0.694 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36375191015268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.36375191015268 | validation: 1.1602022244326535]
	TIME [epoch: 0.693 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4118715273377809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4118715273377809 | validation: 1.3007590055711555]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4149447733605052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4149447733605052 | validation: 0.9723240942652716]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2750914673018998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2750914673018998 | validation: 0.9799849832305174]
	TIME [epoch: 0.693 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.220730256720445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.220730256720445 | validation: 0.9546968371936105]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2128553706936327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2128553706936327 | validation: 0.9454486362431873]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207710372468836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.207710372468836 | validation: 0.9460580521415892]
	TIME [epoch: 0.693 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019067225541564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2019067225541564 | validation: 0.9445142031622421]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2087103596983855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2087103596983855 | validation: 1.0520527822405037]
	TIME [epoch: 0.693 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2643163241683033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2643163241683033 | validation: 1.0532388433712956]
	TIME [epoch: 0.691 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.353465050375858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.353465050375858 | validation: 1.296549368896524]
	TIME [epoch: 0.691 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.354510714845756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.354510714845756 | validation: 0.7708968092232267]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2140313179545361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2140313179545361 | validation: 1.2060358294992266]
	TIME [epoch: 0.692 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207759639365057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.207759639365057 | validation: 0.795010921059503]
	TIME [epoch: 0.691 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2686441888144533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2686441888144533 | validation: 1.3290210050126556]
	TIME [epoch: 0.691 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2429567256024017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2429567256024017 | validation: 0.7457080721561677]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311032372354317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1311032372354317 | validation: 0.7910811535980076]
	TIME [epoch: 0.692 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1058324459086357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1058324459086357 | validation: 0.9393586073006134]
	TIME [epoch: 0.692 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1061552485447153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1061552485447153 | validation: 0.7359855768169133]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0937001630763155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0937001630763155 | validation: 0.9409250895134793]
	TIME [epoch: 0.692 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0938500309996508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0938500309996508 | validation: 0.6959036281147739]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.094298265867502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.094298265867502 | validation: 1.2448115377653217]
	TIME [epoch: 0.693 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1669194398028317		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.1669194398028317 | validation: 0.6633994264510732]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.232533701999922		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.232533701999922 | validation: 1.112130263345459]
	TIME [epoch: 0.7 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196716616143743		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1196716616143743 | validation: 0.8088118945797572]
	TIME [epoch: 0.696 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0399929534934171		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0399929534934171 | validation: 0.6737185998778898]
	TIME [epoch: 0.696 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0746663324462309		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.0746663324462309 | validation: 1.1138500959617297]
	TIME [epoch: 0.696 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1063110335549338		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.1063110335549338 | validation: 0.7051110469128452]
	TIME [epoch: 0.696 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0773956631899986		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.0773956631899986 | validation: 0.8692446412509551]
	TIME [epoch: 0.696 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054928017222102		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.054928017222102 | validation: 0.7825063571477515]
	TIME [epoch: 0.69 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0372567463652385		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.0372567463652385 | validation: 0.7096482615567197]
	TIME [epoch: 0.69 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0294379305869064		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.0294379305869064 | validation: 0.8095938490416668]
	TIME [epoch: 0.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0141920894981926		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.0141920894981926 | validation: 0.6625483600069861]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0101105830722896		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.0101105830722896 | validation: 0.8934956894248665]
	TIME [epoch: 0.697 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0009523495869899		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.0009523495869899 | validation: 0.6359818419741076]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0025246373914027		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.0025246373914027 | validation: 1.0730295415653404]
	TIME [epoch: 0.697 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371384522808142		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.0371384522808142 | validation: 0.5710463657510768]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1028527110117465		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.1028527110117465 | validation: 1.2061612443162202]
	TIME [epoch: 0.698 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0936004760442881		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.0936004760442881 | validation: 0.6778079746374277]
	TIME [epoch: 0.697 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00543020485473		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.00543020485473 | validation: 0.7405835898884391]
	TIME [epoch: 0.696 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9757938200440225		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9757938200440225 | validation: 0.7791184815829915]
	TIME [epoch: 0.697 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.946739016463621		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.946739016463621 | validation: 0.6417792442707113]
	TIME [epoch: 0.695 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436005511218396		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.9436005511218396 | validation: 0.7998816502895151]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9154324356401317		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9154324356401317 | validation: 0.6451080596811425]
	TIME [epoch: 0.694 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312933152176219		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9312933152176219 | validation: 0.93047245888597]
	TIME [epoch: 0.696 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9456834635767782		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9456834635767782 | validation: 0.5979431911550733]
	TIME [epoch: 0.696 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012920884006267		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.012920884006267 | validation: 1.282191374468107]
	TIME [epoch: 0.697 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100753277192738		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.100753277192738 | validation: 0.5379370760488712]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0167113732951942		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.0167113732951942 | validation: 0.7549960385150846]
	TIME [epoch: 0.694 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015769123862686		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9015769123862686 | validation: 0.8366185838761336]
	TIME [epoch: 0.692 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096090508210688		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9096090508210688 | validation: 0.610073585248091]
	TIME [epoch: 0.693 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364980583430592		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.9364980583430592 | validation: 0.8468380723159943]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9116482891035669		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.9116482891035669 | validation: 0.6728620351819208]
	TIME [epoch: 0.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064394050740707		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.9064394050740707 | validation: 0.7573834155974614]
	TIME [epoch: 0.688 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918451565959467		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8918451565959467 | validation: 0.659877422965134]
	TIME [epoch: 0.689 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923461082849032		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8923461082849032 | validation: 0.9202866166468022]
	TIME [epoch: 0.689 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9275113100123445		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.9275113100123445 | validation: 0.5446337304961644]
	TIME [epoch: 0.686 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.039344416077517		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.039344416077517 | validation: 1.2250274347094239]
	TIME [epoch: 0.687 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0814001804953435		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0814001804953435 | validation: 0.5662014859529293]
	TIME [epoch: 0.687 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723783486222022		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.9723783486222022 | validation: 0.664086607982403]
	TIME [epoch: 0.686 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815035344040011		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8815035344040011 | validation: 0.930544761222498]
	TIME [epoch: 0.687 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301979340359013		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9301979340359013 | validation: 0.6046382533561083]
	TIME [epoch: 0.686 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090987990168512		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9090987990168512 | validation: 0.7460570268636451]
	TIME [epoch: 0.687 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902491948928952		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8902491948928952 | validation: 0.7625259939828082]
	TIME [epoch: 0.687 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956852975600417		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8956852975600417 | validation: 0.7006176340024446]
	TIME [epoch: 0.686 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960003400758363		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8960003400758363 | validation: 0.7569101999899233]
	TIME [epoch: 0.686 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904441548310145		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.904441548310145 | validation: 0.8030794948115396]
	TIME [epoch: 0.687 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025375230498862		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.9025375230498862 | validation: 0.6581105015352945]
	TIME [epoch: 0.688 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935971298697435		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8935971298697435 | validation: 0.9397587914973095]
	TIME [epoch: 0.689 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243614738025601		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9243614738025601 | validation: 0.5703975784253084]
	TIME [epoch: 0.689 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555307065587617		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9555307065587617 | validation: 0.9942535779105759]
	TIME [epoch: 0.688 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480087524100381		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.9480087524100381 | validation: 0.5752573774760175]
	TIME [epoch: 0.694 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904547107858863		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.904547107858863 | validation: 0.7524490109230904]
	TIME [epoch: 0.689 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86353551800624		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.86353551800624 | validation: 0.7007521117227671]
	TIME [epoch: 0.689 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607260123978456		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8607260123978456 | validation: 0.6831536990601039]
	TIME [epoch: 0.688 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631552371544168		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8631552371544168 | validation: 0.7350669153504253]
	TIME [epoch: 0.688 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692405429798535		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8692405429798535 | validation: 0.6833098366924463]
	TIME [epoch: 0.689 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698568901488084		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8698568901488084 | validation: 0.8156199563582818]
	TIME [epoch: 0.687 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034629455670029		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9034629455670029 | validation: 0.6420090564340063]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075962057960832		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9075962057960832 | validation: 0.9208602946393439]
	TIME [epoch: 0.687 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9446760685543933		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.9446760685543933 | validation: 0.5349269695781624]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0012665110447763		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.0012665110447763 | validation: 0.9701578349006618]
	TIME [epoch: 0.691 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223977767874822		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9223977767874822 | validation: 0.6784038348045023]
	TIME [epoch: 0.689 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9732794724274989		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9732794724274989 | validation: 0.9410104569253875]
	TIME [epoch: 0.688 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929771640506486		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.929771640506486 | validation: 0.6437249349004567]
	TIME [epoch: 0.688 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541190482150381		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8541190482150381 | validation: 0.6963298379619354]
	TIME [epoch: 0.689 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558813038478158		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8558813038478158 | validation: 0.750800523048067]
	TIME [epoch: 0.688 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861028372854727		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.861028372854727 | validation: 0.6505426710870497]
	TIME [epoch: 0.688 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448152322770652		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8448152322770652 | validation: 0.7445373324191331]
	TIME [epoch: 0.688 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8412493396890958		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8412493396890958 | validation: 0.6138807753618132]
	TIME [epoch: 0.689 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456742823039372		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8456742823039372 | validation: 0.8383588613257708]
	TIME [epoch: 0.687 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735136530776296		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8735136530776296 | validation: 0.5079949281549051]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0196664262589372		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.0196664262589372 | validation: 1.0749526022483462]
	TIME [epoch: 0.698 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9631880926127463		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.9631880926127463 | validation: 0.5864958024292423]
	TIME [epoch: 0.695 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722908415314019		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8722908415314019 | validation: 0.7198762273847243]
	TIME [epoch: 0.695 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8333654485857385		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8333654485857385 | validation: 0.7473877724741128]
	TIME [epoch: 0.693 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518618312213149		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8518618312213149 | validation: 0.6602354244108356]
	TIME [epoch: 0.696 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531162712046185		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8531162712046185 | validation: 0.7440885506379229]
	TIME [epoch: 0.691 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614506645810138		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8614506645810138 | validation: 0.7237008766708876]
	TIME [epoch: 0.689 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866463060833558		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8866463060833558 | validation: 0.7122738043796037]
	TIME [epoch: 0.691 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694253514542305		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8694253514542305 | validation: 0.6991255871026962]
	TIME [epoch: 0.689 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650696126553106		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8650696126553106 | validation: 0.8135369603665927]
	TIME [epoch: 0.69 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901633640720977		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.901633640720977 | validation: 0.6426270786535893]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598188522523873		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8598188522523873 | validation: 0.8656230462897402]
	TIME [epoch: 0.69 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867859515674547		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.867859515674547 | validation: 0.5175123180256448]
	TIME [epoch: 0.688 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0183814588434805		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.0183814588434805 | validation: 0.9553198276339071]
	TIME [epoch: 0.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099413935165077		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9099413935165077 | validation: 0.5918540938538622]
	TIME [epoch: 0.689 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301250079918265		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8301250079918265 | validation: 0.6463981216992535]
	TIME [epoch: 0.692 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179698077199182		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8179698077199182 | validation: 0.6840736853928457]
	TIME [epoch: 0.731 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158029359002698		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8158029359002698 | validation: 0.646840274538243]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015342883228115		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8015342883228115 | validation: 0.6830078102273828]
	TIME [epoch: 0.689 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966402775903781		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7966402775903781 | validation: 0.5558180163878256]
	TIME [epoch: 0.689 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434934418689798		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7434934418689798 | validation: 1.1441881817419446]
	TIME [epoch: 0.689 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156036026802705		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1156036026802705 | validation: 0.939843946963001]
	TIME [epoch: 0.689 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0207930989205887		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0207930989205887 | validation: 0.5017243029402914]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043405423429125		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7043405423429125 | validation: 0.6908752414244845]
	TIME [epoch: 0.696 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406397411181955		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7406397411181955 | validation: 0.60643207800006]
	TIME [epoch: 0.697 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867977027220888		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.6867977027220888 | validation: 0.5474182313681171]
	TIME [epoch: 0.694 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763378648378061		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6763378648378061 | validation: 0.6048361935727743]
	TIME [epoch: 0.692 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887826092963286		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6887826092963286 | validation: 0.49832600696627644]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636790066443854		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6636790066443854 | validation: 0.6193926301082625]
	TIME [epoch: 0.696 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469691695324303		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6469691695324303 | validation: 0.6723326163483001]
	TIME [epoch: 0.696 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093379464218893		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7093379464218893 | validation: 0.5474822806804731]
	TIME [epoch: 0.698 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990482633615139		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6990482633615139 | validation: 0.6507555396556166]
	TIME [epoch: 0.692 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849366194691376		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7849366194691376 | validation: 0.5940355284729194]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7163267416643663		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7163267416643663 | validation: 0.6361749786144412]
	TIME [epoch: 0.693 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385709153337413		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6385709153337413 | validation: 0.448966122933589]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423758879887613		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6423758879887613 | validation: 0.5320492748717571]
	TIME [epoch: 0.69 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6071198086435134		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6071198086435134 | validation: 0.5667481278975487]
	TIME [epoch: 0.691 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.603421357787656		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.603421357787656 | validation: 0.5303353543883781]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.597180010509902		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.597180010509902 | validation: 0.4822394982890618]
	TIME [epoch: 0.688 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6217566237887091		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6217566237887091 | validation: 0.5630485568017399]
	TIME [epoch: 0.689 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5938715908549742		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5938715908549742 | validation: 0.7807327821784755]
	TIME [epoch: 0.689 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737524940901923		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.737524940901923 | validation: 0.7176981701898191]
	TIME [epoch: 0.691 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705677996706761		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8705677996706761 | validation: 0.6329959304582242]
	TIME [epoch: 0.689 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985052418482755		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7985052418482755 | validation: 0.5584515235416748]
	TIME [epoch: 0.689 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247543561758671		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7247543561758671 | validation: 0.4984605082687395]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048271843631174		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7048271843631174 | validation: 0.5769559286174776]
	TIME [epoch: 0.691 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625161740242161		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.625161740242161 | validation: 0.5799317304464683]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6088950730511722		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6088950730511722 | validation: 0.551274326090587]
	TIME [epoch: 0.689 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6116343954543907		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.6116343954543907 | validation: 0.5293203576339435]
	TIME [epoch: 0.689 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6110187880510675		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.6110187880510675 | validation: 0.5007148671810983]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5893688879370081		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5893688879370081 | validation: 0.5480707882901324]
	TIME [epoch: 0.687 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5548858536953937		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.5548858536953937 | validation: 0.45665177513354804]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5703370042333675		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.5703370042333675 | validation: 0.7100290101885569]
	TIME [epoch: 0.689 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345828699409742		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6345828699409742 | validation: 0.5539949383259841]
	TIME [epoch: 0.69 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209156572987834		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7209156572987834 | validation: 0.5184102247518689]
	TIME [epoch: 0.689 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029561791253052		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6029561791253052 | validation: 0.6125291666273518]
	TIME [epoch: 0.69 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5748719876585929		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5748719876585929 | validation: 0.5255997707806247]
	TIME [epoch: 0.688 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824219069244964		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.5824219069244964 | validation: 0.5053459660046155]
	TIME [epoch: 0.689 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.569074623944641		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.569074623944641 | validation: 0.5537603311058387]
	TIME [epoch: 0.691 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384128207222234		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.5384128207222234 | validation: 0.4642855520071111]
	TIME [epoch: 0.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5486550719133604		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.5486550719133604 | validation: 0.5851770531419898]
	TIME [epoch: 0.69 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411901118133747		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.5411901118133747 | validation: 0.44664531489846687]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823002876977386		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.5823002876977386 | validation: 0.673155567597166]
	TIME [epoch: 0.697 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042463171050513		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6042463171050513 | validation: 0.47136642400736295]
	TIME [epoch: 0.698 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483458367028976		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6483458367028976 | validation: 0.4301919430644618]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726427619601238		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.5726427619601238 | validation: 0.616700724702615]
	TIME [epoch: 0.697 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711573960025509		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.5711573960025509 | validation: 0.5464925853293096]
	TIME [epoch: 0.699 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566980001011291		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.5566980001011291 | validation: 0.5238459991773262]
	TIME [epoch: 0.698 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5277770994029939		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5277770994029939 | validation: 0.46048996098824413]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175037795824464		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5175037795824464 | validation: 0.631224922994249]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499041752576875		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5499041752576875 | validation: 0.43153334545079497]
	TIME [epoch: 0.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329508828413437		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6329508828413437 | validation: 0.5287939694945525]
	TIME [epoch: 0.699 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5942619712327405		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5942619712327405 | validation: 0.6863876504003408]
	TIME [epoch: 0.697 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466441606868011		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6466441606868011 | validation: 0.600252918172618]
	TIME [epoch: 0.696 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710029427426509		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6710029427426509 | validation: 0.4874625699654842]
	TIME [epoch: 0.697 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5181596931920559		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.5181596931920559 | validation: 0.7555805851101003]
	TIME [epoch: 0.696 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706066855259872		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6706066855259872 | validation: 0.5131594133678229]
	TIME [epoch: 0.695 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204982843871578		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.6204982843871578 | validation: 0.45779122537870864]
	TIME [epoch: 0.693 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5533590510414989		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5533590510414989 | validation: 0.5936100757375707]
	TIME [epoch: 0.695 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420604590612584		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.5420604590612584 | validation: 0.4829232558481775]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139188085269446		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.5139188085269446 | validation: 0.5106380108001672]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.517269335693032		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.517269335693032 | validation: 0.4672504372584532]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5032171095912769		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.5032171095912769 | validation: 0.5084850232973117]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.494802664265858		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.494802664265858 | validation: 0.46335747385685244]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48215792780785444		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.48215792780785444 | validation: 0.45445359002701036]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48818122393960045		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.48818122393960045 | validation: 0.614124576810001]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410921543383315		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5410921543383315 | validation: 0.5006522116269237]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855813966933277		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6855813966933277 | validation: 0.40660055499199554]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704432412413917		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6704432412413917 | validation: 0.5044764105070115]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5882320230027679		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5882320230027679 | validation: 0.5756383774464737]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5492043458101268		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5492043458101268 | validation: 0.5023851278304708]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5118494096137767		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.5118494096137767 | validation: 0.4992003158728162]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094858627792256		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.5094858627792256 | validation: 0.42980308788235944]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114227046812468		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.5114227046812468 | validation: 0.5809839168579146]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5249391960695201		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5249391960695201 | validation: 0.4604739846850267]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489347626339855		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.489347626339855 | validation: 0.4549202810281685]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49386151843734594		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.49386151843734594 | validation: 0.5530099574398458]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4904047775544115		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4904047775544115 | validation: 0.3784083435921101]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320897602699179		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.5320897602699179 | validation: 0.646702183067212]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687808872573953		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5687808872573953 | validation: 0.5057471972667884]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458122008637547		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6458122008637547 | validation: 0.4117992664432009]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027011586265393		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5027011586265393 | validation: 0.6237577296280006]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5354461550367906		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.5354461550367906 | validation: 0.4507436772084904]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342550810345627		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.5342550810345627 | validation: 0.4600534864317794]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4841328586131946		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4841328586131946 | validation: 0.4469685517800109]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5134772664452301		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.5134772664452301 | validation: 0.5578810687517078]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960581461797809		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4960581461797809 | validation: 0.4199678166524827]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4677649720257877		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.4677649720257877 | validation: 0.456194280851235]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46681078696898437		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.46681078696898437 | validation: 0.5703738193123994]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128409069877253		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.5128409069877253 | validation: 0.4218586479866657]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175469774732507		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.5175469774732507 | validation: 0.6122167378869755]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311837463179111		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5311837463179111 | validation: 0.41085330783475715]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5310769490714933		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5310769490714933 | validation: 0.4377778568542267]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45810095281856733		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.45810095281856733 | validation: 0.6430810319347064]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5638493802854339		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.5638493802854339 | validation: 0.47502574927837415]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48466107489559207		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.48466107489559207 | validation: 0.4182465729840247]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4847215528129945		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.4847215528129945 | validation: 0.4801386119894202]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608499689433587		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.4608499689433587 | validation: 0.44212266560282665]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.452057498198724		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.452057498198724 | validation: 0.42416692024815056]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4368508942673167		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.4368508942673167 | validation: 0.46595474942728077]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4278620157805911		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.4278620157805911 | validation: 0.406639964738462]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4366933240243725		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.4366933240243725 | validation: 0.4252912995509477]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45921005102637696		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.45921005102637696 | validation: 0.48294978542324274]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45568505776978674		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.45568505776978674 | validation: 0.3768039071884304]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175865604873257		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.5175865604873257 | validation: 0.5890960501355419]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5199473435985805		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.5199473435985805 | validation: 0.43124919706074405]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5955060068191826		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.5955060068191826 | validation: 0.489618555212191]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43412445276526657		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.43412445276526657 | validation: 0.4445632690920768]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43006439200155244		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.43006439200155244 | validation: 0.417697215048079]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4261771090358977		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.4261771090358977 | validation: 0.4138060786673985]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41909756338515247		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.41909756338515247 | validation: 0.817688852332395]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.893210436220885		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.893210436220885 | validation: 0.6880670142011184]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206310219523131		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6206310219523131 | validation: 0.39159787697302456]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5071689950231852		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5071689950231852 | validation: 0.46173725116418274]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4882015852999221		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.4882015852999221 | validation: 0.5012213417512527]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4468104768217998		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.4468104768217998 | validation: 0.42405592096942063]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4650339565485359		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.4650339565485359 | validation: 0.44005834183002773]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221091488488115		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.4221091488488115 | validation: 0.42671233741378467]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41759500197940713		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.41759500197940713 | validation: 0.427481022392143]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41063730092645756		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.41063730092645756 | validation: 0.38802320831975545]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41273158185603825		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.41273158185603825 | validation: 0.5333449196728284]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45700031364287264		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.45700031364287264 | validation: 0.3881996084396658]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4649527910724352		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.4649527910724352 | validation: 0.44539200919246014]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4297315369241859		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4297315369241859 | validation: 0.43124153159527334]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40121683360779814		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.40121683360779814 | validation: 0.39313813341835596]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41422535315845904		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.41422535315845904 | validation: 0.5378719708654288]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4711364761695259		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.4711364761695259 | validation: 0.3867682306980103]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4336696882637224		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.4336696882637224 | validation: 0.4655674194982128]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4107579348287634		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.4107579348287634 | validation: 0.33750667529237344]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4585318143176738		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.4585318143176738 | validation: 0.5785879398267328]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5133553712760895		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5133553712760895 | validation: 0.49850017869838803]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4335001940354909		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.4335001940354909 | validation: 0.3299081445446683]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4659954102794536		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.4659954102794536 | validation: 0.4966676728272078]
	TIME [epoch: 1.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44712181233872517		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.44712181233872517 | validation: 0.3963027584544597]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4305380208779197		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.4305380208779197 | validation: 0.49963136947183184]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4357516514019825		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.4357516514019825 | validation: 0.3460094941729563]
	TIME [epoch: 1.37 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3877348229928029		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3877348229928029 | validation: 0.3676426158815411]
	TIME [epoch: 1.37 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830013058582468		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.3830013058582468 | validation: 0.4146902322376258]
	TIME [epoch: 1.37 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38514382561710736		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.38514382561710736 | validation: 0.35035998624295006]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38174094927606206		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.38174094927606206 | validation: 0.4040307611109012]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37633792365873847		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.37633792365873847 | validation: 0.3313905370649397]
	TIME [epoch: 1.37 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4131506928790002		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.4131506928790002 | validation: 0.6704168761578223]
	TIME [epoch: 1.37 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548855620898809		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6548855620898809 | validation: 0.3904751149836607]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4734992723512117		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.4734992723512117 | validation: 0.3674124788783227]
	TIME [epoch: 1.37 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39612428055570553		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.39612428055570553 | validation: 0.46989972682024084]
	TIME [epoch: 1.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4047801000338261		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4047801000338261 | validation: 0.4151918151087568]
	TIME [epoch: 1.37 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381662224112869		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.381662224112869 | validation: 0.38293575762939147]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39107355346801526		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.39107355346801526 | validation: 0.4890557539296797]
	TIME [epoch: 1.37 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40980283824570435		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.40980283824570435 | validation: 0.3663171002303766]
	TIME [epoch: 1.37 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.363441291719792		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.363441291719792 | validation: 0.3327990105764008]
	TIME [epoch: 1.37 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.389668747179485		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.389668747179485 | validation: 0.3859668532362378]
	TIME [epoch: 1.37 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3586388362606038		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.3586388362606038 | validation: 0.473123185907678]
	TIME [epoch: 1.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3893706691817595		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.3893706691817595 | validation: 0.3287307890504799]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609637504596438		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.3609637504596438 | validation: 0.39753393875857523]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.349210625302034		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.349210625302034 | validation: 0.3668771690741877]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33739328247543177		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.33739328247543177 | validation: 0.3462022335424291]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34134857879139746		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.34134857879139746 | validation: 0.3301285167243184]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34134589466809195		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.34134589466809195 | validation: 0.33499286260828465]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3467227694680138		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.3467227694680138 | validation: 0.45655109958080503]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878562374427028		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.3878562374427028 | validation: 0.34055301516084546]
	TIME [epoch: 1.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518932890599833		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3518932890599833 | validation: 0.4919721823779907]
	TIME [epoch: 1.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4138730216514787		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.4138730216514787 | validation: 0.31336767420377043]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4466999445227019		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.4466999445227019 | validation: 0.4180427191136694]
	TIME [epoch: 1.37 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33327066340817096		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.33327066340817096 | validation: 0.29228486592794084]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3222410297604209		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.3222410297604209 | validation: 0.3772298703738983]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33817572896505804		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.33817572896505804 | validation: 0.2912915971385851]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3447035125933509		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.3447035125933509 | validation: 0.4350457111226135]
	TIME [epoch: 1.37 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34665460642785006		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.34665460642785006 | validation: 0.28790598753808583]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.344954331955916		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.344954331955916 | validation: 0.40561974540287693]
	TIME [epoch: 1.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32681977024068315		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.32681977024068315 | validation: 0.2896691700036033]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29794942542347463		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.29794942542347463 | validation: 0.4684282784587914]
	TIME [epoch: 1.37 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3804700666749875		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.3804700666749875 | validation: 0.27885487178077256]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3388295347946996		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.3388295347946996 | validation: 0.3713521168632594]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28636711905383727		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.28636711905383727 | validation: 0.3249285885481536]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733246752777893		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2733246752777893 | validation: 0.30187879663511447]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26002598905955626		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.26002598905955626 | validation: 0.30237448167128933]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262927707751875		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.262927707751875 | validation: 0.2569438036826359]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27660977883181637		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.27660977883181637 | validation: 0.4540500936976723]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485228931944207		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.3485228931944207 | validation: 0.2588938557135265]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4113752131722098		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4113752131722098 | validation: 0.4619916678148432]
	TIME [epoch: 1.37 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3523420910252814		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3523420910252814 | validation: 0.2947867261107026]
	TIME [epoch: 1.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24682938852630923		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.24682938852630923 | validation: 0.23098782120784006]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2633917998876286		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.2633917998876286 | validation: 0.4004431001722207]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29552366133059915		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.29552366133059915 | validation: 0.2419261028918229]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2825715970490252		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.2825715970490252 | validation: 0.34662569189803244]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690162206190043		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.2690162206190043 | validation: 0.2186150555758123]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32692692194009815		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.32692692194009815 | validation: 0.26781649555823295]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255492548042846		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.2255492548042846 | validation: 0.29438086705743677]
	TIME [epoch: 1.37 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22651060976207407		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.22651060976207407 | validation: 0.22544041361296951]
	TIME [epoch: 1.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515943510902017		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.2515943510902017 | validation: 0.4080572975251732]
	TIME [epoch: 1.37 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3029491290148313		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3029491290148313 | validation: 0.237740521099797]
	TIME [epoch: 1.37 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23513770371219975		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.23513770371219975 | validation: 0.3273310855177129]
	TIME [epoch: 1.37 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25123466048155335		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.25123466048155335 | validation: 0.23980328206710513]
	TIME [epoch: 1.37 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814366138291782		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.2814366138291782 | validation: 0.8022891831649042]
	TIME [epoch: 1.37 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204762117750951		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8204762117750951 | validation: 0.6743972616250664]
	TIME [epoch: 1.37 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204353230573206		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6204353230573206 | validation: 0.41993074859235197]
	TIME [epoch: 1.37 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34348114976148725		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.34348114976148725 | validation: 0.20580366910565046]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926943996252964		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.2926943996252964 | validation: 0.2808322640273558]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.259838799859243		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.259838799859243 | validation: 0.2724686458842115]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22130398515390376		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.22130398515390376 | validation: 0.2566900953214627]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2213149086793233		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2213149086793233 | validation: 0.24644703386170763]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21133370317444494		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.21133370317444494 | validation: 0.22231221293829453]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20753206358277712		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.20753206358277712 | validation: 0.2346236790559152]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1999556543119339		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.1999556543119339 | validation: 0.19672614199451063]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21953055625893755		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.21953055625893755 | validation: 0.3123407477500359]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2350514640632391		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.2350514640632391 | validation: 0.19815778433346629]
	TIME [epoch: 1.37 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20294127285263094		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.20294127285263094 | validation: 0.23013522728431257]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013348095012668		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.2013348095012668 | validation: 0.18330308617643276]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20637581714439987		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20637581714439987 | validation: 0.24174336821522516]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19189972801311175		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.19189972801311175 | validation: 0.17006317707117571]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19730327133302022		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.19730327133302022 | validation: 0.23784287485064437]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19721558191838298		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.19721558191838298 | validation: 0.174068949643817]
	TIME [epoch: 1.37 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20536887516937818		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.20536887516937818 | validation: 0.276853579466028]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23292508880993146		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.23292508880993146 | validation: 0.17718147292316522]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21178342019783272		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.21178342019783272 | validation: 0.2475461453076691]
	TIME [epoch: 1.37 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1966904049841233		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.1966904049841233 | validation: 0.14889810544868964]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21129161764060145		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.21129161764060145 | validation: 0.4016587195364292]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3008411324474546		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.3008411324474546 | validation: 0.18642930410319228]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2416538098991049		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.2416538098991049 | validation: 0.194915704408251]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18535507246616406		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.18535507246616406 | validation: 0.19236496705593953]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716461287292266		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.1716461287292266 | validation: 0.16961241700490307]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20031346146500661		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.20031346146500661 | validation: 0.5833442940795294]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305778456682642		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5305778456682642 | validation: 0.26245304742219017]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22325789718139702		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.22325789718139702 | validation: 0.179809427308038]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31704208250546545		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.31704208250546545 | validation: 0.3051370996373576]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23730652025301432		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.23730652025301432 | validation: 0.19413560390445528]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182555913022585		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3182555913022585 | validation: 0.1828669355379304]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17930892225274547		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.17930892225274547 | validation: 0.2552604826021481]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20625495467019092		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.20625495467019092 | validation: 0.17474911918505961]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625477388316174		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.17625477388316174 | validation: 0.1893320310045262]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45745867555682324		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.45745867555682324 | validation: 0.15991296245895834]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29627179974694146		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.29627179974694146 | validation: 0.34073974194777124]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2560492934808276		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.2560492934808276 | validation: 0.31244511922031626]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200566357835639		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2200566357835639 | validation: 0.1870648900182506]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20220835096356674		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.20220835096356674 | validation: 0.17776832634149348]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17108465115261162		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.17108465115261162 | validation: 0.17488617810888651]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16872444533267106		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.16872444533267106 | validation: 0.16879615573795542]
	TIME [epoch: 1.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1656122825788345		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.1656122825788345 | validation: 0.17695493782998348]
	TIME [epoch: 1.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16187290905227628		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.16187290905227628 | validation: 0.17485997262064104]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15777683422627845		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.15777683422627845 | validation: 0.17554887023844334]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15566844723795434		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.15566844723795434 | validation: 0.19171710639235764]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16507177681331192		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.16507177681331192 | validation: 0.17029690392806118]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17218522456266905		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.17218522456266905 | validation: 0.27148613653336157]
	TIME [epoch: 1.37 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20840295237298448		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.20840295237298448 | validation: 0.158318223007592]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17673342456581068		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.17673342456581068 | validation: 0.2107516648488771]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749826954513489		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1749826954513489 | validation: 0.14305655831263903]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1623188774984818		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.1623188774984818 | validation: 0.18205816458314336]
	TIME [epoch: 1.37 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513022507933406		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.1513022507933406 | validation: 0.15645848014436728]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14307619297415353		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.14307619297415353 | validation: 0.13332983755652303]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18187946387212114		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.18187946387212114 | validation: 0.33551169314849344]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2718623826532928		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2718623826532928 | validation: 0.21052142096826884]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2248192337628626		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.2248192337628626 | validation: 0.2337131335870264]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18294413991135205		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.18294413991135205 | validation: 0.1466861177440936]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14903654278712367		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.14903654278712367 | validation: 0.142029651082033]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369594888138525		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.1369594888138525 | validation: 0.17571542854581523]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2472824666414274		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.2472824666414274 | validation: 0.34287244709716114]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28258436704507306		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.28258436704507306 | validation: 0.17023790665485247]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15586114532003492		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.15586114532003492 | validation: 0.12035329912355167]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.154262578953287		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.154262578953287 | validation: 0.20010184289666288]
	TIME [epoch: 1.37 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15813680689655177		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.15813680689655177 | validation: 0.13410420855409888]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306103291838005		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.1306103291838005 | validation: 0.15081341165715562]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247720909475673		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.1247720909475673 | validation: 0.1395065809775337]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12599393463217498		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.12599393463217498 | validation: 0.13365896857466925]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12465041664440867		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.12465041664440867 | validation: 0.13838605965466125]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289499432407535		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.1289499432407535 | validation: 0.12180393883089605]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234015573112729		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.1234015573112729 | validation: 0.16730123685720105]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369511883104066		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.1369511883104066 | validation: 0.16806649987958144]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17910674853043534		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.17910674853043534 | validation: 0.7121377066083138]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674240896100313		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.674240896100313 | validation: 0.39719557926981525]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33601303567898727		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.33601303567898727 | validation: 0.2276591947183634]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2418873373311958		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2418873373311958 | validation: 0.12550014886985766]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12526764986430403		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.12526764986430403 | validation: 0.21049978835331534]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15703535404601318		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.15703535404601318 | validation: 0.17070472868105024]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12678546294741141		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.12678546294741141 | validation: 0.16875288896402818]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213237015392713		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.12213237015392713 | validation: 0.14694463955913747]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12040028877995469		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.12040028877995469 | validation: 0.1310874115211031]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11447302620338314		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.11447302620338314 | validation: 0.1404896766664459]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23058892773825362		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.23058892773825362 | validation: 0.18670627755238356]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17728340611646062		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.17728340611646062 | validation: 0.14899086294615968]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11395234685770757		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.11395234685770757 | validation: 0.1572582802304683]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13134401607123805		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.13134401607123805 | validation: 0.15289602078543707]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12059472326173097		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.12059472326173097 | validation: 0.13132544973560634]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10495769689965322		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.10495769689965322 | validation: 0.13160570355825582]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613891450902987		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.10613891450902987 | validation: 0.134443996201664]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990432019482763		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.09990432019482763 | validation: 0.12048476442876131]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09870511955955945		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.09870511955955945 | validation: 0.12923692354300978]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922221847243444		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.09922221847243444 | validation: 0.13336688040986683]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728240716309997		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.09728240716309997 | validation: 0.13974644449791004]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10013518697624538		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.10013518697624538 | validation: 0.15476716086874678]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479321904371775		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.1479321904371775 | validation: 0.28722634149460136]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551945856844403		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2551945856844403 | validation: 0.1906327613604902]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20011043872313813		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.20011043872313813 | validation: 0.12253252622037927]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09670593653372606		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.09670593653372606 | validation: 0.20996368434818527]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1567296969729916		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.1567296969729916 | validation: 0.16016311351806015]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15564285432891523		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.15564285432891523 | validation: 0.12316761625372706]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0976376575896714		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.0976376575896714 | validation: 0.1408482583487942]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09876067163458457		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.09876067163458457 | validation: 0.1246253030263262]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11491957451182643		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.11491957451182643 | validation: 0.10790720206217355]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12639627489828803		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.12639627489828803 | validation: 0.12448695427940451]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09749356164347665		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.09749356164347665 | validation: 0.1518689910292325]
	TIME [epoch: 1.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11530629178847747		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.11530629178847747 | validation: 0.15968077770152075]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11480159546150191		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.11480159546150191 | validation: 0.12225465627851007]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1177330301392701		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.1177330301392701 | validation: 0.28881387204447834]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2444075708634196		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.2444075708634196 | validation: 0.1213687000343437]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10897942678677418		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.10897942678677418 | validation: 0.1271442438451574]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012134725620194		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.1012134725620194 | validation: 0.17091085451510396]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253844304544471		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.1253844304544471 | validation: 0.1385755305550003]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11652160983123366		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.11652160983123366 | validation: 0.13390080534237034]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375277679835357		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.10375277679835357 | validation: 0.11004048979515542]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09240545927136028		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.09240545927136028 | validation: 0.1137403941337115]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843374219324044		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.0843374219324044 | validation: 0.11235252902514206]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08258850082432365		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.08258850082432365 | validation: 0.12128811809800229]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987075445798053		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.0987075445798053 | validation: 0.17328588048336538]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13705029623344106		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.13705029623344106 | validation: 0.16021545414662916]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15250447739350054		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.15250447739350054 | validation: 0.18898641743529598]
	TIME [epoch: 1.37 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13109963854802673		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.13109963854802673 | validation: 0.11629597961325683]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10444811561308841		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.10444811561308841 | validation: 0.11575619964423463]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646610197449399		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.08646610197449399 | validation: 0.11585649373595548]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07941178085282331		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.07941178085282331 | validation: 0.10859158636693884]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057934295470172		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.08057934295470172 | validation: 0.10755895158595359]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08354214078288626		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.08354214078288626 | validation: 0.14950998270945084]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11294470907832313		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.11294470907832313 | validation: 0.13139601580994562]
	TIME [epoch: 1.37 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1300107346253785		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.1300107346253785 | validation: 0.20077998447505194]
	TIME [epoch: 1.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14563847466666377		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.14563847466666377 | validation: 0.10821327641939821]
	TIME [epoch: 1.37 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084429739411902		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.09084429739411902 | validation: 0.10295262467257255]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07270557207520055		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.07270557207520055 | validation: 0.10415747629900651]
	TIME [epoch: 1.37 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07464082940976724		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.07464082940976724 | validation: 0.09518324014467187]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08355782907766326		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.08355782907766326 | validation: 0.1540731046838568]
	TIME [epoch: 1.37 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11928911574799883		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.11928911574799883 | validation: 0.17732159567029143]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22766552923411		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.22766552923411 | validation: 0.12337484783049767]
	TIME [epoch: 1.37 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08750403190455364		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.08750403190455364 | validation: 0.13924496434043054]
	TIME [epoch: 1.37 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10486303911227321		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.10486303911227321 | validation: 0.10530713619224166]
	TIME [epoch: 1.37 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10321193371090327		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.10321193371090327 | validation: 0.1056377237561613]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07800218492054163		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.07800218492054163 | validation: 0.09770395794336968]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06951315970870352		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.06951315970870352 | validation: 0.09645792140015143]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745232331260144		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.0745232331260144 | validation: 0.1126616554858648]
	TIME [epoch: 1.37 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388633528257038		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.07388633528257038 | validation: 0.11345587214068178]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08600988455474738		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.08600988455474738 | validation: 0.14418179975770523]
	TIME [epoch: 1.37 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1001365800288566		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1001365800288566 | validation: 0.11475263254565689]
	TIME [epoch: 1.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09037272936086158		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.09037272936086158 | validation: 0.1325538636239508]
	TIME [epoch: 1.37 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08379907749922776		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.08379907749922776 | validation: 0.10281129024717113]
	TIME [epoch: 1.37 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07519963098052779		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.07519963098052779 | validation: 0.11460382717115986]
	TIME [epoch: 1.37 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07310033529066112		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.07310033529066112 | validation: 0.10036585336801529]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726738763024408		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.0726738763024408 | validation: 0.1316080569656841]
	TIME [epoch: 1.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921372816754829		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.0921372816754829 | validation: 0.1267124076235728]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10216781788633572		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.10216781788633572 | validation: 0.1450572736040807]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929723399279241		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.0929723399279241 | validation: 0.1035971494663645]
	TIME [epoch: 1.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07825177075751093		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.07825177075751093 | validation: 0.11310640863708238]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681235881713923		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.07681235881713923 | validation: 0.08656491716246999]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742510853733229		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.0742510853733229 | validation: 0.09701174987941895]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168219632035795		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.07168219632035795 | validation: 0.10479997861531237]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07849836945806221		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.07849836945806221 | validation: 0.1231355906463568]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764095284829507		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.07764095284829507 | validation: 0.10965187447705876]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644040587980341		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.08644040587980341 | validation: 0.1398917981125969]
	TIME [epoch: 1.37 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537029433417097		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.08537029433417097 | validation: 0.10130998246276707]
	TIME [epoch: 1.37 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0747455157526418		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.0747455157526418 | validation: 0.07115071261553992]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868306003940919		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.06868306003940919 | validation: 0.12084013358966211]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798831679250757		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.0798831679250757 | validation: 0.10897399945558675]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304450251378359		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.09304450251378359 | validation: 0.13748979216148757]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10371932361778623		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.10371932361778623 | validation: 0.08878609767505817]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794254439248889		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.0794254439248889 | validation: 0.10535994652865788]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061323610648392604		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.061323610648392604 | validation: 0.08253291406585737]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0541388147737477		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.0541388147737477 | validation: 0.07713888983631959]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06092026858215777		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.06092026858215777 | validation: 0.11238474756920894]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07398044041190968		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.07398044041190968 | validation: 0.10748984579572476]
	TIME [epoch: 2.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09349094781830884		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.09349094781830884 | validation: 0.1425342224956816]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.086004669951386		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.086004669951386 | validation: 0.09480153992556782]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267878450876789		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.07267878450876789 | validation: 0.11243542141653738]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07818180496643817		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.07818180496643817 | validation: 0.08432432804527057]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537219317055148		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.06537219317055148 | validation: 0.1039584115118558]
	TIME [epoch: 2.69 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06734193755520002		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.06734193755520002 | validation: 0.09123786611746786]
	TIME [epoch: 2.69 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06489315336333396		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.06489315336333396 | validation: 0.10743417741828412]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06592156132828292		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.06592156132828292 | validation: 0.08195046357642628]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06470687181914221		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.06470687181914221 | validation: 0.11981376092015468]
	TIME [epoch: 2.69 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157718557405193		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.07157718557405193 | validation: 0.09582922970588685]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448664056434444		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.07448664056434444 | validation: 0.10817965168686863]
	TIME [epoch: 2.69 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07525114785208882		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.07525114785208882 | validation: 0.08723697979035767]
	TIME [epoch: 2.69 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086222797747182		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.06086222797747182 | validation: 0.09142029866204421]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669614986951921		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.05669614986951921 | validation: 0.0758539335316124]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693902658297146		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.05693902658297146 | validation: 0.09048729006059188]
	TIME [epoch: 2.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07137239020180271		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07137239020180271 | validation: 0.08997286021597059]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721859862796857		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.07721859862796857 | validation: 0.1334250095967946]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07842981186896726		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.07842981186896726 | validation: 0.08142675295867885]
	TIME [epoch: 2.69 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865217062531158		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.06865217062531158 | validation: 0.08994587706095762]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853904532727865		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.05853904532727865 | validation: 0.07510200558926647]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053008192984054174		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.053008192984054174 | validation: 0.07823849745710224]
	TIME [epoch: 2.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051149316548475696		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.051149316548475696 | validation: 0.07748041105141906]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04886292870029706		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.04886292870029706 | validation: 0.068943038189643]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750786482287553		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.04750786482287553 | validation: 0.06696491962538136]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04673243387899211		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.04673243387899211 | validation: 0.07786423975650512]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668723682893944		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.04668723682893944 | validation: 0.07138708495363609]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051507679883683086		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.051507679883683086 | validation: 0.14325288544860085]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09015274931747959		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.09015274931747959 | validation: 0.11412825815603388]
	TIME [epoch: 2.69 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1014718449290204		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1014718449290204 | validation: 0.13875853103553007]
	TIME [epoch: 2.69 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1016744733667529		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1016744733667529 | validation: 0.08283217013609638]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921742794590439		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.04921742794590439 | validation: 0.07276298217307199]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04479414948560507		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.04479414948560507 | validation: 0.08987610866127137]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049633529672559754		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.049633529672559754 | validation: 0.08403928001141514]
	TIME [epoch: 2.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060481196342370785		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.060481196342370785 | validation: 0.09545063210730532]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07967011946473222		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.07967011946473222 | validation: 0.07540774529117686]
	TIME [epoch: 2.69 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056087729048403076		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.056087729048403076 | validation: 0.08764350099739773]
	TIME [epoch: 2.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862894940758908		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.04862894940758908 | validation: 0.0654097760995398]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324658684828084		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.04324658684828084 | validation: 0.06506946949769615]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04088326363758446		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04088326363758446 | validation: 0.062404353870027865]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826503227012902		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.04826503227012902 | validation: 0.09126275132635442]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395303825000223		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.05395303825000223 | validation: 0.07157777526720754]
	TIME [epoch: 2.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655395153775215		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.06655395153775215 | validation: 0.12012053997454251]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08360015254796833		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.08360015254796833 | validation: 0.07991382558123519]
	TIME [epoch: 2.69 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059986646338075184		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.059986646338075184 | validation: 0.08005611265789918]
	TIME [epoch: 2.69 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053680005810206155		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.053680005810206155 | validation: 0.06783606771438543]
	TIME [epoch: 2.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539648679244571		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.04539648679244571 | validation: 0.1216402664976952]
	TIME [epoch: 2.69 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548813799175317		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.06548813799175317 | validation: 0.07126016811321186]
	TIME [epoch: 2.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670986269549828		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.04670986269549828 | validation: 0.06378673238202233]
	TIME [epoch: 2.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472379465337438		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.04472379465337438 | validation: 0.0726248107237516]
	TIME [epoch: 2.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043190136681172574		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.043190136681172574 | validation: 0.07092051374963254]
	TIME [epoch: 2.69 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564299108427505		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.04564299108427505 | validation: 0.055288689741871434]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048090934023154466		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.048090934023154466 | validation: 0.09358596265926579]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06702063510325702		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.06702063510325702 | validation: 0.07321545173811123]
	TIME [epoch: 2.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05943672014998125		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.05943672014998125 | validation: 0.08823556885787649]
	TIME [epoch: 2.69 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06205871653026433		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.06205871653026433 | validation: 0.06495898978736894]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044553890729446766		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.044553890729446766 | validation: 0.07068115206228547]
	TIME [epoch: 2.69 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558169419145006		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.04558169419145006 | validation: 0.05772748283644065]
	TIME [epoch: 2.69 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039934003064902045		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.039934003064902045 | validation: 0.07013996150082558]
	TIME [epoch: 2.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039944635432852565		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.039944635432852565 | validation: 0.06443705209055273]
	TIME [epoch: 2.69 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384705271281174		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.04384705271281174 | validation: 0.09947677532573856]
	TIME [epoch: 2.69 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05773616530700762		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.05773616530700762 | validation: 0.07508711129341487]
	TIME [epoch: 2.69 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719764467444504		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.06719764467444504 | validation: 0.10429253844013459]
	TIME [epoch: 2.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0632088514666278		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.0632088514666278 | validation: 0.06715674960674134]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547771101281598		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.04547771101281598 | validation: 0.060871996038624365]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042171930525485665		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.042171930525485665 | validation: 0.060287923752406464]
	TIME [epoch: 2.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891132624336852		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.03891132624336852 | validation: 0.06881363949974655]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03511991506402597		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.03511991506402597 | validation: 0.05672904248591881]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03513720879180936		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.03513720879180936 | validation: 0.0636766025064045]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036732327281991106		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.036732327281991106 | validation: 0.05562351087959683]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249029672027879		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.04249029672027879 | validation: 0.09929925601669393]
	TIME [epoch: 2.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060371239794474924		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.060371239794474924 | validation: 0.07036810200449137]
	TIME [epoch: 2.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06740695687562653		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.06740695687562653 | validation: 0.10200095028258667]
	TIME [epoch: 2.69 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07692045461491104		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.07692045461491104 | validation: 0.06887534790278739]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03798906697902285		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.03798906697902285 | validation: 0.05708179432551391]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03169899343329938		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.03169899343329938 | validation: 0.06179451658675503]
	TIME [epoch: 2.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03691521867980461		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.03691521867980461 | validation: 0.06144934117132339]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129391121757116		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.04129391121757116 | validation: 0.07585630487193068]
	TIME [epoch: 2.69 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05416892611352997		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.05416892611352997 | validation: 0.057613876044011185]
	TIME [epoch: 2.69 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988177492183393		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.04988177492183393 | validation: 0.07248262325253807]
	TIME [epoch: 2.69 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04515429617368593		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.04515429617368593 | validation: 0.05600587842120022]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967905017659332		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.03967905017659332 | validation: 0.07691144216218304]
	TIME [epoch: 2.69 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042449986621347836		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.042449986621347836 | validation: 0.06230149884923171]
	TIME [epoch: 2.69 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040584760197568265		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.040584760197568265 | validation: 0.06471215422581754]
	TIME [epoch: 2.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04553166403768158		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.04553166403768158 | validation: 0.05311300258117893]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038814322147573195		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.038814322147573195 | validation: 0.06749061826077911]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905490682925402		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.03905490682925402 | validation: 0.0519589981184041]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03723065653307042		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.03723065653307042 | validation: 0.07222194236731114]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03812785023357684		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.03812785023357684 | validation: 0.050814058527795906]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04088557698155082		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.04088557698155082 | validation: 0.07790830437812502]
	TIME [epoch: 2.69 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619248593228019		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.04619248593228019 | validation: 0.06201272876515338]
	TIME [epoch: 2.69 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054010203688049		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.04054010203688049 | validation: 0.07799801392718081]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885539864506793		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.04885539864506793 | validation: 0.05288581819003005]
	TIME [epoch: 2.69 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04094037115782969		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.04094037115782969 | validation: 0.05945467168969029]
	TIME [epoch: 2.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449325321715869		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.03449325321715869 | validation: 0.05318675998761904]
	TIME [epoch: 2.69 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309694513573973		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.03309694513573973 | validation: 0.05175393764889487]
	TIME [epoch: 2.69 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03043458512457097		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.03043458512457097 | validation: 0.05210731894909857]
	TIME [epoch: 2.69 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033637698576082675		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.033637698576082675 | validation: 0.06285282554289398]
	TIME [epoch: 2.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035968194677630475		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.035968194677630475 | validation: 0.057348818670067706]
	TIME [epoch: 2.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050094044057900654		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.050094044057900654 | validation: 0.10246040036367679]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061755568484705974		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.061755568484705974 | validation: 0.0543407447422285]
	TIME [epoch: 2.69 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059909840885901		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.04059909840885901 | validation: 0.05759012346676169]
	TIME [epoch: 2.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032713663051139034		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.032713663051139034 | validation: 0.04653136267298633]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029967268419990742		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.029967268419990742 | validation: 0.0509024454513369]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02966603951403059		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.02966603951403059 | validation: 0.04822656198750345]
	TIME [epoch: 2.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352161089783627		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.03352161089783627 | validation: 0.06231645192308247]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426554233616176		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.04426554233616176 | validation: 0.05513147054544686]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054468429554052594		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.054468429554052594 | validation: 0.09089162575120008]
	TIME [epoch: 2.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823929233389321		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.05823929233389321 | validation: 0.0454125579905929]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033310248788013024		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.033310248788013024 | validation: 0.04591709788080343]
	TIME [epoch: 2.69 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02562635311397121		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.02562635311397121 | validation: 0.06069201964348142]
	TIME [epoch: 2.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03095581063563043		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.03095581063563043 | validation: 0.04612626449781028]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028321588156046715		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.028321588156046715 | validation: 0.053423691867475004]
	TIME [epoch: 2.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028839387047089743		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.028839387047089743 | validation: 0.049139315153782115]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028140604858588825		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.028140604858588825 | validation: 0.05363824513096151]
	TIME [epoch: 2.69 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705720448782429		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.03705720448782429 | validation: 0.05834079533917794]
	TIME [epoch: 2.69 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828653926979452		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.04828653926979452 | validation: 0.08584323875853629]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224193985650656		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.05224193985650656 | validation: 0.055712361827260665]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791782881581246		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.03791782881581246 | validation: 0.06311855880243165]
	TIME [epoch: 2.69 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02886408661899072		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.02886408661899072 | validation: 0.046043974329295224]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027562984894683878		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.027562984894683878 | validation: 0.04213352315393525]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026490418841339607		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.026490418841339607 | validation: 0.059115367539110744]
	TIME [epoch: 2.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02752717433201487		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.02752717433201487 | validation: 0.05263848087628428]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03466820816511715		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.03466820816511715 | validation: 0.05123244770314578]
	TIME [epoch: 2.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782722101887068		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.03782722101887068 | validation: 0.0905153850473236]
	TIME [epoch: 2.69 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04825455461570629		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.04825455461570629 | validation: 0.052037719643245585]
	TIME [epoch: 2.69 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042360915104344185		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.042360915104344185 | validation: 0.05657736280013466]
	TIME [epoch: 2.69 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03294590846597849		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.03294590846597849 | validation: 0.04092289755469241]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025578512505555318		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.025578512505555318 | validation: 0.05177942208282752]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02404695685551201		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.02404695685551201 | validation: 0.047146501824941865]
	TIME [epoch: 2.69 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024470406864509967		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.024470406864509967 | validation: 0.04335149741884262]
	TIME [epoch: 2.69 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026224829943237448		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.026224829943237448 | validation: 0.0530060270456999]
	TIME [epoch: 2.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531259168406366		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.03531259168406366 | validation: 0.06893893272118198]
	TIME [epoch: 2.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052718615770940204		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.052718615770940204 | validation: 0.0522728107778315]
	TIME [epoch: 2.69 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05060495492286979		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.05060495492286979 | validation: 0.0889338402186366]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056735166854426		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.05056735166854426 | validation: 0.04730924958192616]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024740135767682043		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.024740135767682043 | validation: 0.045917372972199264]
	TIME [epoch: 2.69 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02829643303679238		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.02829643303679238 | validation: 0.0500594055033894]
	TIME [epoch: 2.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026478699013948736		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.026478699013948736 | validation: 0.0481480723252017]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02699403593608233		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.02699403593608233 | validation: 0.04521196251504273]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02448293356279512		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.02448293356279512 | validation: 0.038380045041192724]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02402035207557989		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.02402035207557989 | validation: 0.05153882268824677]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025034225354639728		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.025034225354639728 | validation: 0.047679642735383235]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026640769055091953		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.026640769055091953 | validation: 0.05592778069785729]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034773508277486286		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.034773508277486286 | validation: 0.05011759519670397]
	TIME [epoch: 2.69 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03773776807439882		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.03773776807439882 | validation: 0.0778190379986835]
	TIME [epoch: 2.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477984427429047		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.04477984427429047 | validation: 0.04676391220122555]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029311714822831778		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.029311714822831778 | validation: 0.056139368381571554]
	TIME [epoch: 2.69 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02501112301767929		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.02501112301767929 | validation: 0.0395069806320314]
	TIME [epoch: 2.69 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02366910860881849		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.02366910860881849 | validation: 0.04641636208076926]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021797282512179394		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.021797282512179394 | validation: 0.03444010195503682]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02121573534062393		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.02121573534062393 | validation: 0.04359666070913409]
	TIME [epoch: 2.69 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023093138236571203		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.023093138236571203 | validation: 0.04490605091751424]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241814393414268		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.03241814393414268 | validation: 0.07008549864381164]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497900792430126		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.0497900792430126 | validation: 0.05389473356745004]
	TIME [epoch: 2.69 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0398218635281317		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0398218635281317 | validation: 0.05942824129309913]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028973056339606784		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.028973056339606784 | validation: 0.04051323315852417]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02202672814528663		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.02202672814528663 | validation: 0.04174814941994559]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02066999552111076		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.02066999552111076 | validation: 0.04404292247151656]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023620213366831462		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.023620213366831462 | validation: 0.03840690841433441]
	TIME [epoch: 2.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023042623169913474		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.023042623169913474 | validation: 0.042725312174207934]
	TIME [epoch: 2.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024339149919412204		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.024339149919412204 | validation: 0.04085082285379395]
	TIME [epoch: 2.69 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027154403429006636		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.027154403429006636 | validation: 0.05807939183373778]
	TIME [epoch: 2.69 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039843506417353604		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.039843506417353604 | validation: 0.04164346246590165]
	TIME [epoch: 2.7 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04070172909034674		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.04070172909034674 | validation: 0.07468554196691157]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350145418309588		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.0350145418309588 | validation: 0.036667527257989885]
	TIME [epoch: 2.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02208320851798602		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.02208320851798602 | validation: 0.035141997744219534]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019735793925987753		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.019735793925987753 | validation: 0.04261569664638686]
	TIME [epoch: 2.69 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01991988494990042		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.01991988494990042 | validation: 0.042488707746300675]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01922318844099561		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.01922318844099561 | validation: 0.03604646839590764]
	TIME [epoch: 2.7 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020354420089029438		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.020354420089029438 | validation: 0.03881096944158503]
	TIME [epoch: 2.69 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020482809681854102		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.020482809681854102 | validation: 0.04005789143127467]
	TIME [epoch: 2.69 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022206375512408713		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.022206375512408713 | validation: 0.04312570100380621]
	TIME [epoch: 2.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033246365225381366		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.033246365225381366 | validation: 0.07953414798639911]
	TIME [epoch: 2.69 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049952937903724556		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.049952937903724556 | validation: 0.04596959655970045]
	TIME [epoch: 2.69 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562201688677956		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.03562201688677956 | validation: 0.043268178121845524]
	TIME [epoch: 2.69 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02351978018167008		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.02351978018167008 | validation: 0.036789720563819975]
	TIME [epoch: 2.69 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01918641433874082		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.01918641433874082 | validation: 0.03543448012047289]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019358621683113066		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.019358621683113066 | validation: 0.03229597944508369]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020425947390522118		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.020425947390522118 | validation: 0.037418307426551944]
	TIME [epoch: 2.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0216125422505588		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.0216125422505588 | validation: 0.044033654176093256]
	TIME [epoch: 2.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026459914536939255		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.026459914536939255 | validation: 0.04443863129361341]
	TIME [epoch: 2.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0278554065983506		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.0278554065983506 | validation: 0.06353892458856938]
	TIME [epoch: 2.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349570432034708		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.03349570432034708 | validation: 0.035675784525795694]
	TIME [epoch: 2.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027210182260263213		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.027210182260263213 | validation: 0.04887812890585256]
	TIME [epoch: 2.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023388728321857708		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.023388728321857708 | validation: 0.03429737260145314]
	TIME [epoch: 2.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021987797428180005		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.021987797428180005 | validation: 0.03810758765033867]
	TIME [epoch: 2.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019695147204122382		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.019695147204122382 | validation: 0.031393634241647664]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017520109697146898		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.017520109697146898 | validation: 0.03760442240431469]
	TIME [epoch: 2.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019395983298537187		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.019395983298537187 | validation: 0.03722720932499068]
	TIME [epoch: 2.71 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020991034243357718		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.020991034243357718 | validation: 0.03833960321644402]
	TIME [epoch: 2.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018787789572625644		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.018787789572625644 | validation: 0.036600246331106735]
	TIME [epoch: 2.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023563179641510043		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.023563179641510043 | validation: 0.054299426819345575]
	TIME [epoch: 2.71 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820936522120482		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.03820936522120482 | validation: 0.042656009365100035]
	TIME [epoch: 2.71 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777656512751928		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.03777656512751928 | validation: 0.05575228220756356]
	TIME [epoch: 2.7 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02988785359867127		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.02988785359867127 | validation: 0.034766368924745765]
	TIME [epoch: 2.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018673850576751003		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.018673850576751003 | validation: 0.036791651249986325]
	TIME [epoch: 2.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018282270407762896		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.018282270407762896 | validation: 0.04455852518226534]
	TIME [epoch: 2.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019214210938507385		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.019214210938507385 | validation: 0.03313895817909109]
	TIME [epoch: 2.7 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018561336760835604		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.018561336760835604 | validation: 0.04017532145442773]
	TIME [epoch: 2.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018480563747515024		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.018480563747515024 | validation: 0.03641895586807369]
	TIME [epoch: 2.71 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021212879799001313		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.021212879799001313 | validation: 0.04050464013752167]
	TIME [epoch: 2.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024748740838756157		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.024748740838756157 | validation: 0.04329689380739771]
	TIME [epoch: 2.71 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031605350630984505		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.031605350630984505 | validation: 0.05618026236154301]
	TIME [epoch: 2.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027714026939023508		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.027714026939023508 | validation: 0.031951596660399886]
	TIME [epoch: 2.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021722530556906445		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.021722530556906445 | validation: 0.039022545404352287]
	TIME [epoch: 2.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020507049705719486		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.020507049705719486 | validation: 0.03522087015054568]
	TIME [epoch: 2.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021738507342252144		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.021738507342252144 | validation: 0.04153857419328547]
	TIME [epoch: 2.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02228873021870169		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.02228873021870169 | validation: 0.031096150134023816]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021318208852348565		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.021318208852348565 | validation: 0.0461734094911191]
	TIME [epoch: 2.69 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02227961636047219		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.02227961636047219 | validation: 0.034397462946657346]
	TIME [epoch: 2.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023663733242779518		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.023663733242779518 | validation: 0.04589797896457046]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023496964643407958		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.023496964643407958 | validation: 0.02986178593668446]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0193098701344421		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.0193098701344421 | validation: 0.037387909507507276]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018057484422548786		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.018057484422548786 | validation: 0.02787443046693008]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016889879189345054		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.016889879189345054 | validation: 0.03943338877321126]
	TIME [epoch: 2.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01680120780097097		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.01680120780097097 | validation: 0.03751076201369355]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023773316500898537		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.023773316500898537 | validation: 0.04683911202205239]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02917409380690752		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.02917409380690752 | validation: 0.03659982764299065]
	TIME [epoch: 2.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027263832707482942		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.027263832707482942 | validation: 0.053073540575936065]
	TIME [epoch: 2.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023956732585459096		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.023956732585459096 | validation: 0.03134707031353814]
	TIME [epoch: 2.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01756203091284829		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.01756203091284829 | validation: 0.03352139703321039]
	TIME [epoch: 2.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015778291127902876		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.015778291127902876 | validation: 0.03844733793763588]
	TIME [epoch: 2.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015853670364427773		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.015853670364427773 | validation: 0.03244643558098805]
	TIME [epoch: 2.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017436228169716888		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.017436228169716888 | validation: 0.03698223707023103]
	TIME [epoch: 2.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017762180536113895		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.017762180536113895 | validation: 0.03267126251088438]
	TIME [epoch: 2.71 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171453528359267		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.02171453528359267 | validation: 0.050823297523647726]
	TIME [epoch: 2.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029417745289649418		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.029417745289649418 | validation: 0.03825227478754294]
	TIME [epoch: 2.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025510674489042446		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.025510674489042446 | validation: 0.03983969459936676]
	TIME [epoch: 2.71 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018472292029099092		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.018472292029099092 | validation: 0.02872925807059129]
	TIME [epoch: 2.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016157460515442045		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.016157460515442045 | validation: 0.03400750301103157]
	TIME [epoch: 2.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015474254293689387		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.015474254293689387 | validation: 0.035297519081082744]
	TIME [epoch: 2.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015827251634168246		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.015827251634168246 | validation: 0.023906092155362782]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015293955186195158		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.015293955186195158 | validation: 0.03637685777605328]
	TIME [epoch: 2.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014909094848844697		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.014909094848844697 | validation: 0.029339294809866103]
	TIME [epoch: 2.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014739126610753078		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.014739126610753078 | validation: 0.03589846321651006]
	TIME [epoch: 2.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017219002677106198		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.017219002677106198 | validation: 0.040369703123922164]
	TIME [epoch: 2.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028871040942106678		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.028871040942106678 | validation: 0.057223957733374166]
	TIME [epoch: 2.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04210128974746396		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04210128974746396 | validation: 0.03849260777488359]
	TIME [epoch: 2.71 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02015489541744417		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.02015489541744417 | validation: 0.03601955926485311]
	TIME [epoch: 2.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014825565479543086		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.014825565479543086 | validation: 0.03856405701961571]
	TIME [epoch: 2.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0153665086854948		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.0153665086854948 | validation: 0.033836705730573194]
	TIME [epoch: 2.71 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017020596487098513		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.017020596487098513 | validation: 0.04008059321854194]
	TIME [epoch: 2.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01584832399264825		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.01584832399264825 | validation: 0.031249797956918825]
	TIME [epoch: 2.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018157664285701165		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.018157664285701165 | validation: 0.039145836555539694]
	TIME [epoch: 2.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023303633221340138		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.023303633221340138 | validation: 0.03365587059010231]
	TIME [epoch: 2.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02475257178387732		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.02475257178387732 | validation: 0.04530245704196827]
	TIME [epoch: 2.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01884160438213088		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.01884160438213088 | validation: 0.0306432319383958]
	TIME [epoch: 2.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016009550005251758		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.016009550005251758 | validation: 0.04038744668028197]
	TIME [epoch: 2.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014381698605565494		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.014381698605565494 | validation: 0.02761466125784421]
	TIME [epoch: 2.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014996765070927816		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.014996765070927816 | validation: 0.03825019391907362]
	TIME [epoch: 2.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015234338348842039		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.015234338348842039 | validation: 0.029083919112869762]
	TIME [epoch: 2.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014535819860447712		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.014535819860447712 | validation: 0.026151634396736158]
	TIME [epoch: 2.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992388948456785		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.013992388948456785 | validation: 0.02800491951495722]
	TIME [epoch: 2.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014152184144929206		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.014152184144929206 | validation: 0.040434244236326924]
	TIME [epoch: 2.71 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01988880250455364		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.01988880250455364 | validation: 0.038112473573870376]
	TIME [epoch: 2.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02795859166751888		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.02795859166751888 | validation: 0.05391019825976032]
	TIME [epoch: 2.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350656930538811		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.0350656930538811 | validation: 0.03198096408937464]
	TIME [epoch: 2.71 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018079491133854576		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.018079491133854576 | validation: 0.03187333357507256]
	TIME [epoch: 2.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013319302977852259		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.013319302977852259 | validation: 0.027043629767175906]
	TIME [epoch: 2.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013645990122120399		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.013645990122120399 | validation: 0.02840421650338597]
	TIME [epoch: 2.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014603571565433815		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.014603571565433815 | validation: 0.03365629109028138]
	TIME [epoch: 2.71 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992022004652051		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.013992022004652051 | validation: 0.02654034240568667]
	TIME [epoch: 2.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013096728741278846		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.013096728741278846 | validation: 0.03256252417390607]
	TIME [epoch: 2.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01405284722404701		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.01405284722404701 | validation: 0.031677058134245276]
	TIME [epoch: 2.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015409362124603742		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.015409362124603742 | validation: 0.030111212398861754]
	TIME [epoch: 2.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018585640078940507		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.018585640078940507 | validation: 0.044294597794363094]
	TIME [epoch: 2.7 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026701364535484058		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.026701364535484058 | validation: 0.036580109741873555]
	TIME [epoch: 2.7 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02636692475862426		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.02636692475862426 | validation: 0.03894583594248722]
	TIME [epoch: 2.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01959950685665206		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.01959950685665206 | validation: 0.025425357040652654]
	TIME [epoch: 2.7 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014449372749033		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.014449372749033 | validation: 0.029084679916684966]
	TIME [epoch: 2.7 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013600219042238577		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.013600219042238577 | validation: 0.03739133110648237]
	TIME [epoch: 2.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01402905445483556		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.01402905445483556 | validation: 0.029488981079010292]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013781959030753512		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.013781959030753512 | validation: 0.031355538591289145]
	TIME [epoch: 2.69 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013629486606959592		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.013629486606959592 | validation: 0.026209415236099688]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01405700259590654		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.01405700259590654 | validation: 0.03220378535472031]
	TIME [epoch: 2.69 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01348094399004421		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.01348094399004421 | validation: 0.03387891598140182]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014480831119895857		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.014480831119895857 | validation: 0.030077252814708844]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016148452668356263		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.016148452668356263 | validation: 0.03167846222097546]
	TIME [epoch: 2.68 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019605358695666177		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.019605358695666177 | validation: 0.031055011087623388]
	TIME [epoch: 2.69 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02419080416856673		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.02419080416856673 | validation: 0.049748257209078176]
	TIME [epoch: 2.69 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029093581427078047		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.029093581427078047 | validation: 0.02835470657451087]
	TIME [epoch: 2.69 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01707546656912846		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.01707546656912846 | validation: 0.032985487422414306]
	TIME [epoch: 2.69 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01283048881568455		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.01283048881568455 | validation: 0.029409320826350574]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013459315214806665		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.013459315214806665 | validation: 0.02887599601334502]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013312286698418125		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.013312286698418125 | validation: 0.030923618811360055]
	TIME [epoch: 2.69 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012951531921106445		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.012951531921106445 | validation: 0.022755728847849866]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01304041704836775		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.01304041704836775 | validation: 0.028343364292810837]
	TIME [epoch: 2.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012176807835794326		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.012176807835794326 | validation: 0.031581489786997166]
	TIME [epoch: 2.69 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01410705012910438		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.01410705012910438 | validation: 0.028232007584323527]
	TIME [epoch: 2.69 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019811699069741874		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.019811699069741874 | validation: 0.04481530353499613]
	TIME [epoch: 2.69 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02917134213303238		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.02917134213303238 | validation: 0.029755439500533578]
	TIME [epoch: 2.69 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015597147119439008		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.015597147119439008 | validation: 0.03128040520763921]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012061236791081648		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.012061236791081648 | validation: 0.030971088928653745]
	TIME [epoch: 2.69 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01298906090272527		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.01298906090272527 | validation: 0.027999909752608843]
	TIME [epoch: 2.69 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012980847854432351		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.012980847854432351 | validation: 0.030873058615572203]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01350766813964745		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.01350766813964745 | validation: 0.03343996284879467]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01257481955581873		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.01257481955581873 | validation: 0.035792422584346686]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012216956974333		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.012216956974333 | validation: 0.02558677894605781]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014526172260028468		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.014526172260028468 | validation: 0.03258737986800885]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01862775097776157		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.01862775097776157 | validation: 0.030416922476105304]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0157293791731973		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.0157293791731973 | validation: 0.04039969333468858]
	TIME [epoch: 2.69 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015655457488643467		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.015655457488643467 | validation: 0.02870140844696687]
	TIME [epoch: 2.69 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01423942251139316		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.01423942251139316 | validation: 0.03454653563342526]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013030782332928098		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.013030782332928098 | validation: 0.03007716296833959]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013632122761416263		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.013632122761416263 | validation: 0.02459134197879689]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01377127101653685		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.01377127101653685 | validation: 0.026185334100977876]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014494986000139903		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.014494986000139903 | validation: 0.03667141459308916]
	TIME [epoch: 2.69 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01736576316680143		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.01736576316680143 | validation: 0.024183895865403038]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019060186153091335		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.019060186153091335 | validation: 0.03600211963084291]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01667343010393139		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.01667343010393139 | validation: 0.027271008662949348]
	TIME [epoch: 2.69 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013376028196751748		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.013376028196751748 | validation: 0.026789694770001416]
	TIME [epoch: 2.69 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011845182994955619		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.011845182994955619 | validation: 0.026976282774160434]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011082921563321439		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.011082921563321439 | validation: 0.029884758853938067]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012072453317355833		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.012072453317355833 | validation: 0.03177263516708561]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012371949841182523		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.012371949841182523 | validation: 0.02342916613265361]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016075538323019615		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.016075538323019615 | validation: 0.03679698664874314]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023039054181578724		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.023039054181578724 | validation: 0.024267019683599067]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01675646640875351		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.01675646640875351 | validation: 0.03888203380536459]
	TIME [epoch: 2.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013197507653848075		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.013197507653848075 | validation: 0.02056661647051752]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011496570123286446		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.011496570123286446 | validation: 0.029231795263599893]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011286627466838893		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.011286627466838893 | validation: 0.029042334394136504]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012611664765388265		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.012611664765388265 | validation: 0.03053851172114358]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014022006939011088		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.014022006939011088 | validation: 0.03977766334902924]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015918622921378644		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.015918622921378644 | validation: 0.023295823364983804]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01694246434979708		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.01694246434979708 | validation: 0.03403376300663171]
	TIME [epoch: 2.69 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01478249304829592		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.01478249304829592 | validation: 0.023851966334175634]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010766775931684043		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.010766775931684043 | validation: 0.02477244675198044]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010693780422488119		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.010693780422488119 | validation: 0.02956869223053199]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011144964791566934		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.011144964791566934 | validation: 0.030538400443839465]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012580818793694534		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.012580818793694534 | validation: 0.03162150488921178]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015721894411657677		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.015721894411657677 | validation: 0.026164520001708536]
	TIME [epoch: 2.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014132632992609074		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.014132632992609074 | validation: 0.032927688318325536]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012735663571759835		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.012735663571759835 | validation: 0.027241965479821606]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011505677061553312		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.011505677061553312 | validation: 0.03475864264269309]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010313454628656465		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.010313454628656465 | validation: 0.026052368827931563]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011152946256787666		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.011152946256787666 | validation: 0.026840557879546735]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013707697057793448		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.013707697057793448 | validation: 0.02900369871995319]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014882711408734813		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.014882711408734813 | validation: 0.035009020670688666]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015173652125587602		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.015173652125587602 | validation: 0.02419833038346171]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016520207329688858		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.016520207329688858 | validation: 0.033563290388876336]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014889538539218981		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.014889538539218981 | validation: 0.02418344946512664]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01102382577693776		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.01102382577693776 | validation: 0.023174458687334334]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011020360556681062		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.011020360556681062 | validation: 0.03123476524668947]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011995869148828948		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.011995869148828948 | validation: 0.029202954070891518]
	TIME [epoch: 2.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015022941348190247		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.015022941348190247 | validation: 0.033524120558104]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012438110214702633		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.012438110214702633 | validation: 0.020939269666652483]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01366616988966122		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.01366616988966122 | validation: 0.029677906452165448]
	TIME [epoch: 2.68 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01348379218383709		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.01348379218383709 | validation: 0.027500209153716173]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012507402081236178		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.012507402081236178 | validation: 0.028023039514288764]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010335121284958749		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.010335121284958749 | validation: 0.02343027756889562]
	TIME [epoch: 2.69 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010420800162904038		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.010420800162904038 | validation: 0.024748592045674356]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012276877768678576		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.012276877768678576 | validation: 0.028188296411492397]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013255572897306406		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.013255572897306406 | validation: 0.022489520221120288]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014082129020918691		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.014082129020918691 | validation: 0.034972711136973665]
	TIME [epoch: 2.69 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017401264692375268		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.017401264692375268 | validation: 0.02477826782166678]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013266741300686667		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.013266741300686667 | validation: 0.02629521003831513]
	TIME [epoch: 2.69 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01237834831514089		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.01237834831514089 | validation: 0.026568948898080258]
	TIME [epoch: 2.69 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010761092576321745		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.010761092576321745 | validation: 0.026296830818946095]
	TIME [epoch: 2.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011468710729943767		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.011468710729943767 | validation: 0.023052258413224537]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00988696998381116		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.00988696998381116 | validation: 0.029504576438915842]
	TIME [epoch: 2.69 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011721932911341174		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.011721932911341174 | validation: 0.024751512834401704]
	TIME [epoch: 2.69 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013591071120465597		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.013591071120465597 | validation: 0.027656805948954866]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014772183498912912		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.014772183498912912 | validation: 0.02218678006949322]
	TIME [epoch: 2.69 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011445060535763026		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.011445060535763026 | validation: 0.030882993419714524]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009626051326169813		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.009626051326169813 | validation: 0.024105383811928085]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00964314200259353		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.00964314200259353 | validation: 0.023995312616913678]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010393766355921118		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.010393766355921118 | validation: 0.0224053139902103]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012407773689146329		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.012407773689146329 | validation: 0.028078597567396214]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012288254389413176		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.012288254389413176 | validation: 0.026295162487334135]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011569821133949101		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.011569821133949101 | validation: 0.028997335231452027]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011598976110977231		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.011598976110977231 | validation: 0.021350198519733177]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011723341056041607		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.011723341056041607 | validation: 0.030785105817609838]
	TIME [epoch: 2.69 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011662116111871242		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.011662116111871242 | validation: 0.023416563292295214]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010383519789798408		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.010383519789798408 | validation: 0.0309163938152386]
	TIME [epoch: 2.69 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010498775018585323		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.010498775018585323 | validation: 0.02300002061680181]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010059942445285977		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.010059942445285977 | validation: 0.03174033091345855]
	TIME [epoch: 2.69 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011876008710383247		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.011876008710383247 | validation: 0.026776397324254256]
	TIME [epoch: 2.69 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011495348291307429		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.011495348291307429 | validation: 0.02748850160123926]
	TIME [epoch: 2.69 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011529929008844895		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.011529929008844895 | validation: 0.02045209556392437]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_885.pth
	Model improved!!!
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009960479629757671		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.009960479629757671 | validation: 0.025539042715434335]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011024725412934954		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.011024725412934954 | validation: 0.024466802556937717]
	TIME [epoch: 2.68 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010339899337730994		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.010339899337730994 | validation: 0.023198124068077364]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01094643317948262		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.01094643317948262 | validation: 0.03160732106943174]
	TIME [epoch: 2.69 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012570209357394859		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.012570209357394859 | validation: 0.0268841877382604]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014484282452086931		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.014484282452086931 | validation: 0.03326875013258229]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015977009125593543		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.015977009125593543 | validation: 0.0210207490520847]
	TIME [epoch: 2.68 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01145961761604933		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.01145961761604933 | validation: 0.026166107820389053]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010048907935746882		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.010048907935746882 | validation: 0.029842453426929275]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010312568745841912		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.010312568745841912 | validation: 0.023629733773229546]
	TIME [epoch: 2.69 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00924827679118161		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.00924827679118161 | validation: 0.023687614849953676]
	TIME [epoch: 2.69 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009044417840124764		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.009044417840124764 | validation: 0.02360529124033092]
	TIME [epoch: 2.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010890106563308005		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.010890106563308005 | validation: 0.02512126063377054]
	TIME [epoch: 2.69 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012547257677942103		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.012547257677942103 | validation: 0.024456319703070797]
	TIME [epoch: 2.69 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012321729582589482		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.012321729582589482 | validation: 0.032055246710973054]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010979539704691588		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.010979539704691588 | validation: 0.021167484684349893]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009758902079640804		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.009758902079640804 | validation: 0.026314554672411505]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008837046719939918		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.008837046719939918 | validation: 0.024676591942354623]
	TIME [epoch: 2.69 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009127560838370448		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.009127560838370448 | validation: 0.02777616256853155]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007678276859283417		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.007678276859283417 | validation: 0.023157253480159203]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009415089871959454		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.009415089871959454 | validation: 0.02268115645380299]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010100364031070362		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.010100364031070362 | validation: 0.025029589462780125]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010573387510610913		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.010573387510610913 | validation: 0.023864526295588273]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011363880538765891		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.011363880538765891 | validation: 0.025044222579405997]
	TIME [epoch: 2.69 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012279820784336124		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.012279820784336124 | validation: 0.029175849001393497]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011029123780767858		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.011029123780767858 | validation: 0.025317389023510897]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009881469046808734		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.009881469046808734 | validation: 0.02211366607215716]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008554489978067261		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.008554489978067261 | validation: 0.02434986852563481]
	TIME [epoch: 2.69 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008393553616838906		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.008393553616838906 | validation: 0.02239472445251063]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010804954446816626		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.010804954446816626 | validation: 0.03157027476225234]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014466942433686687		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.014466942433686687 | validation: 0.025504446631753844]
	TIME [epoch: 2.69 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013924559455610499		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.013924559455610499 | validation: 0.02600761146531021]
	TIME [epoch: 2.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011091411433832597		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.011091411433832597 | validation: 0.02128107015812507]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00842053696217639		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.00842053696217639 | validation: 0.025600646732989132]
	TIME [epoch: 2.69 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008845845454205662		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.008845845454205662 | validation: 0.024716382069976308]
	TIME [epoch: 2.69 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008347841905752659		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.008347841905752659 | validation: 0.024576425461630982]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00926203094755038		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.00926203094755038 | validation: 0.024372795221814937]
	TIME [epoch: 2.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009008026968556687		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.009008026968556687 | validation: 0.01830242902067437]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008548623693771866		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.008548623693771866 | validation: 0.021697673971163314]
	TIME [epoch: 2.68 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01050390784005153		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.01050390784005153 | validation: 0.026379345741251073]
	TIME [epoch: 2.68 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0111129777084516		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.0111129777084516 | validation: 0.026453695196364414]
	TIME [epoch: 2.69 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013483931976117518		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.013483931976117518 | validation: 0.029590433828391074]
	TIME [epoch: 2.69 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013165323514756197		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.013165323514756197 | validation: 0.024260434796819677]
	TIME [epoch: 2.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009545884444684644		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.009545884444684644 | validation: 0.024378289801464528]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075167276736445715		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0075167276736445715 | validation: 0.019943469358553602]
	TIME [epoch: 2.69 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009934783346711069		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.009934783346711069 | validation: 0.02113407293086621]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008864771793206746		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.008864771793206746 | validation: 0.02469904331122974]
	TIME [epoch: 2.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007750535431236863		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.007750535431236863 | validation: 0.021376701651008856]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00941963785922118		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.00941963785922118 | validation: 0.025071302070555024]
	TIME [epoch: 2.68 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008332278442958412		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.008332278442958412 | validation: 0.020503413190262734]
	TIME [epoch: 2.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008921772743017834		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.008921772743017834 | validation: 0.025395139078171328]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008443684894972417		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.008443684894972417 | validation: 0.024481925993392176]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008111267097195432		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.008111267097195432 | validation: 0.023953505028602995]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009443921430030313		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.009443921430030313 | validation: 0.02083492639125796]
	TIME [epoch: 2.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01266412687872598		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.01266412687872598 | validation: 0.0343297945634377]
	TIME [epoch: 2.69 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0181641867746534		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.0181641867746534 | validation: 0.02296806470249916]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009372992710422396		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.009372992710422396 | validation: 0.022673798198241557]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008713891032409625		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.008713891032409625 | validation: 0.025207253311293]
	TIME [epoch: 2.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009083516160444078		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.009083516160444078 | validation: 0.02117568217188437]
	TIME [epoch: 2.69 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008647026452547016		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.008647026452547016 | validation: 0.026582204837613976]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008232354149751082		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.008232354149751082 | validation: 0.02105602536811183]
	TIME [epoch: 2.69 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009276057782755996		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.009276057782755996 | validation: 0.02046490425713761]
	TIME [epoch: 2.69 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010677380527981764		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.010677380527981764 | validation: 0.02924425571361218]
	TIME [epoch: 2.69 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010937474519541445		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.010937474519541445 | validation: 0.023156584318575892]
	TIME [epoch: 2.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008562653661709323		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.008562653661709323 | validation: 0.020368372767919474]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008413315270310954		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.008413315270310954 | validation: 0.02369953739503039]
	TIME [epoch: 2.69 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009146084777657661		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.009146084777657661 | validation: 0.026559031357303964]
	TIME [epoch: 2.69 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008545836808112559		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.008545836808112559 | validation: 0.023760838069943092]
	TIME [epoch: 2.69 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008406004717104355		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.008406004717104355 | validation: 0.018638956068154078]
	TIME [epoch: 2.69 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009925539382137982		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.009925539382137982 | validation: 0.024181456816967463]
	TIME [epoch: 2.69 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009617543065226414		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.009617543065226414 | validation: 0.02162675938680707]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009188040047089644		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.009188040047089644 | validation: 0.024720166686721192]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008557902387865106		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.008557902387865106 | validation: 0.020139474930623704]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008209098513348154		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.008209098513348154 | validation: 0.030346100549526325]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007927900499445327		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.007927900499445327 | validation: 0.01923538641335586]
	TIME [epoch: 2.68 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008977452210432207		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.008977452210432207 | validation: 0.019792650604510045]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009323835793831711		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.009323835793831711 | validation: 0.0178428968234437]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009651212054371411		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.009651212054371411 | validation: 0.025378750504045068]
	TIME [epoch: 2.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011275147493566928		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.011275147493566928 | validation: 0.02223896209666565]
	TIME [epoch: 2.69 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008104445926925729		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.008104445926925729 | validation: 0.020334189866969266]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008278055423111049		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.008278055423111049 | validation: 0.020955862800287828]
	TIME [epoch: 2.69 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076413364839640644		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.0076413364839640644 | validation: 0.02390404979098525]
	TIME [epoch: 2.69 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008418113475606627		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.008418113475606627 | validation: 0.026663935731322544]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008552291316811607		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.008552291316811607 | validation: 0.017396698925739706]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010220602356610337		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.010220602356610337 | validation: 0.024338175958870934]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012098635751336455		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.012098635751336455 | validation: 0.022558474866800085]
	TIME [epoch: 2.69 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008524141916992127		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.008524141916992127 | validation: 0.02269782112122065]
	TIME [epoch: 2.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00740866583282509		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.00740866583282509 | validation: 0.019554747029701593]
	TIME [epoch: 2.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008040977639641511		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.008040977639641511 | validation: 0.02112419951971104]
	TIME [epoch: 2.69 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007855449702976981		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.007855449702976981 | validation: 0.017636353335610745]
	TIME [epoch: 2.69 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00860137105910834		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.00860137105910834 | validation: 0.019111136817102817]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007788021614061574		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.007788021614061574 | validation: 0.02175562909309764]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007532801638403287		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.007532801638403287 | validation: 0.018637404630643985]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008903244288808178		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.008903244288808178 | validation: 0.02394271965846947]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009411679097281581		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.009411679097281581 | validation: 0.022847070396226767]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007889092440353875		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.007889092440353875 | validation: 0.025734924805061312]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008363408127925177		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.008363408127925177 | validation: 0.019421360672728406]
	TIME [epoch: 2.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0077927880582236795		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0077927880582236795 | validation: 0.018681922846196]
	TIME [epoch: 2.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007662165668987204		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.007662165668987204 | validation: 0.028025512651797146]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008085106845570249		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.008085106845570249 | validation: 0.01931334745010005]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00890719308700323		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.00890719308700323 | validation: 0.021505433079388972]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009105301808335506		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.009105301808335506 | validation: 0.023759940299322282]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008619789706479907		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.008619789706479907 | validation: 0.024851615405733208]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008471523997956977		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.008471523997956977 | validation: 0.020040107789523066]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007709068449237486		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.007709068449237486 | validation: 0.023534976989730862]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008106153532965031		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.008106153532965031 | validation: 0.020539679128079216]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008734401600449812		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.008734401600449812 | validation: 0.02018435595735756]
	TIME [epoch: 2.68 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006898981648765103		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.006898981648765103 | validation: 0.021525756409438615]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00796524508689759		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.00796524508689759 | validation: 0.02051983847004866]
	TIME [epoch: 2.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007632428807757333		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.007632428807757333 | validation: 0.021903057893669434]
	TIME [epoch: 2.69 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007522674082151948		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.007522674082151948 | validation: 0.019372968199469542]
	TIME [epoch: 2.68 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006904347198294022		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.006904347198294022 | validation: 0.02711490838467481]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00817235300964369		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.00817235300964369 | validation: 0.020079792672358644]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010859899579995736		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.010859899579995736 | validation: 0.02719434758583077]
	TIME [epoch: 2.68 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012250521808486157		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.012250521808486157 | validation: 0.019084538198195656]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008552648345710977		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.008552648345710977 | validation: 0.02062424808939598]
	TIME [epoch: 187 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006771728276308202		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.006771728276308202 | validation: 0.026313593540053328]
	TIME [epoch: 5.75 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008191119422048334		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.008191119422048334 | validation: 0.01953588019085092]
	TIME [epoch: 5.74 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006836498154608227		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.006836498154608227 | validation: 0.018821390461709987]
	TIME [epoch: 5.73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007972120547437248		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.007972120547437248 | validation: 0.02295880083829459]
	TIME [epoch: 5.73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007285339757058485		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.007285339757058485 | validation: 0.016104439841226604]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1006.pth
	Model improved!!!
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007108499061645337		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.007108499061645337 | validation: 0.020813878110967257]
	TIME [epoch: 5.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008214587231248328		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.008214587231248328 | validation: 0.016839481924185273]
	TIME [epoch: 5.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007761635935708241		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.007761635935708241 | validation: 0.020815722287634997]
	TIME [epoch: 5.73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00854964040298966		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.00854964040298966 | validation: 0.022297967591662707]
	TIME [epoch: 5.73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011560622739838938		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.011560622739838938 | validation: 0.020552353172311966]
	TIME [epoch: 5.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009479150148006446		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.009479150148006446 | validation: 0.022174754691944534]
	TIME [epoch: 5.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007848195618840145		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.007848195618840145 | validation: 0.023128519724186072]
	TIME [epoch: 5.73 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075369396129640245		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.0075369396129640245 | validation: 0.021594858589451028]
	TIME [epoch: 5.73 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006823474748027914		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.006823474748027914 | validation: 0.023069970762068015]
	TIME [epoch: 5.74 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00696011955421579		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.00696011955421579 | validation: 0.019659561223133972]
	TIME [epoch: 5.75 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007661396356070206		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.007661396356070206 | validation: 0.024070122673481565]
	TIME [epoch: 5.75 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008312690017845304		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.008312690017845304 | validation: 0.021266694354794183]
	TIME [epoch: 5.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00820648332032274		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.00820648332032274 | validation: 0.021218199060214982]
	TIME [epoch: 5.75 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006590736803138997		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.006590736803138997 | validation: 0.01817335494058303]
	TIME [epoch: 5.75 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007071832603443391		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.007071832603443391 | validation: 0.02055474808777078]
	TIME [epoch: 5.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006842687667057428		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.006842687667057428 | validation: 0.019807670412999613]
	TIME [epoch: 5.76 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008093748616735742		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.008093748616735742 | validation: 0.021612327609179218]
	TIME [epoch: 5.76 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008115540952337623		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.008115540952337623 | validation: 0.02159640616195434]
	TIME [epoch: 5.75 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009196985926002184		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.009196985926002184 | validation: 0.019774675046082348]
	TIME [epoch: 5.75 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008567595179574616		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.008567595179574616 | validation: 0.018978867454994996]
	TIME [epoch: 5.76 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007375114037595006		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.007375114037595006 | validation: 0.018143765968537017]
	TIME [epoch: 5.75 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00693001646958002		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.00693001646958002 | validation: 0.022439512852429556]
	TIME [epoch: 5.76 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007486316466363598		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.007486316466363598 | validation: 0.01606772452065731]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1029.pth
	Model improved!!!
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00769103889524071		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.00769103889524071 | validation: 0.020549581283015386]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007678947100539102		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.007678947100539102 | validation: 0.018709507698273266]
	TIME [epoch: 5.76 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006311395990752996		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.006311395990752996 | validation: 0.02091833876768925]
	TIME [epoch: 5.76 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007080300412522923		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.007080300412522923 | validation: 0.018603121968891623]
	TIME [epoch: 5.77 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007098273630242682		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.007098273630242682 | validation: 0.01702910036243176]
	TIME [epoch: 5.77 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00788647578631511		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.00788647578631511 | validation: 0.022720417751563995]
	TIME [epoch: 5.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00881103631885934		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.00881103631885934 | validation: 0.021633599395902727]
	TIME [epoch: 5.77 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007455807643111188		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.007455807643111188 | validation: 0.016984530639138386]
	TIME [epoch: 5.77 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007718499107684047		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.007718499107684047 | validation: 0.023846836405733197]
	TIME [epoch: 5.77 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006462157104488643		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.006462157104488643 | validation: 0.020477317167027886]
	TIME [epoch: 5.77 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006980676107498414		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.006980676107498414 | validation: 0.019006180432337705]
	TIME [epoch: 5.76 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007254687786761061		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.007254687786761061 | validation: 0.020210199618760617]
	TIME [epoch: 5.75 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007053804282552721		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.007053804282552721 | validation: 0.02017573585649615]
	TIME [epoch: 5.75 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00932466931380352		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.00932466931380352 | validation: 0.020225573375392228]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006616150282522307		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.006616150282522307 | validation: 0.016003857715921988]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1044.pth
	Model improved!!!
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007920930656344964		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.007920930656344964 | validation: 0.024030883868888166]
	TIME [epoch: 5.75 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007687667625757981		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.007687667625757981 | validation: 0.022601318397749495]
	TIME [epoch: 5.75 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007886901771717907		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.007886901771717907 | validation: 0.017686505205287195]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008111873904652652		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.008111873904652652 | validation: 0.01867720516558954]
	TIME [epoch: 5.75 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006009625426474755		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.006009625426474755 | validation: 0.018000044587331433]
	TIME [epoch: 5.76 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0068174597407601135		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.0068174597407601135 | validation: 0.022941685524347267]
	TIME [epoch: 5.76 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0083068725862004		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.0083068725862004 | validation: 0.01723820344743612]
	TIME [epoch: 5.75 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072611647749975675		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0072611647749975675 | validation: 0.01823264609970119]
	TIME [epoch: 5.77 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007224379564475046		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.007224379564475046 | validation: 0.019100760429655507]
	TIME [epoch: 5.76 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006728486083206181		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.006728486083206181 | validation: 0.023600705426957504]
	TIME [epoch: 5.75 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007195536633600011		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.007195536633600011 | validation: 0.019545451100181712]
	TIME [epoch: 5.75 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006930434262525382		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.006930434262525382 | validation: 0.0215587193372918]
	TIME [epoch: 5.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007161970097548208		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.007161970097548208 | validation: 0.01729232763079157]
	TIME [epoch: 5.75 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007827735163334402		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.007827735163334402 | validation: 0.02061218039666586]
	TIME [epoch: 5.75 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008088538983280068		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.008088538983280068 | validation: 0.01991465898217063]
	TIME [epoch: 5.75 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00785984991285332		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.00785984991285332 | validation: 0.023159740167834376]
	TIME [epoch: 5.75 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006819211204319875		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.006819211204319875 | validation: 0.02058053541098186]
	TIME [epoch: 5.75 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006187332483055759		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.006187332483055759 | validation: 0.016190899194098197]
	TIME [epoch: 5.75 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006354574608040598		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.006354574608040598 | validation: 0.019659997507680528]
	TIME [epoch: 5.76 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005901714220900365		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.005901714220900365 | validation: 0.017378217211428095]
	TIME [epoch: 5.76 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006505029852944924		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.006505029852944924 | validation: 0.01866902419306148]
	TIME [epoch: 5.75 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006324901889549634		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.006324901889549634 | validation: 0.020525358975079434]
	TIME [epoch: 5.76 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00636443889481285		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.00636443889481285 | validation: 0.015836353004640768]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007123141636107801		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.007123141636107801 | validation: 0.01969341277326632]
	TIME [epoch: 5.75 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007065330362626061		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.007065330362626061 | validation: 0.0172607211547425]
	TIME [epoch: 5.75 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006527812047356255		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.006527812047356255 | validation: 0.01637208131381661]
	TIME [epoch: 5.76 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006290963005057853		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.006290963005057853 | validation: 0.016278335221811723]
	TIME [epoch: 5.75 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006688177011995847		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.006688177011995847 | validation: 0.02019695799993171]
	TIME [epoch: 5.75 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006488652547349971		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.006488652547349971 | validation: 0.020733845201786463]
	TIME [epoch: 5.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007112605230637526		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.007112605230637526 | validation: 0.020166218222348223]
	TIME [epoch: 5.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0074842157460224355		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.0074842157460224355 | validation: 0.01687492056231311]
	TIME [epoch: 5.75 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006105145139666428		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.006105145139666428 | validation: 0.019406374148658992]
	TIME [epoch: 5.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005821256318363849		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.005821256318363849 | validation: 0.019939561108623695]
	TIME [epoch: 5.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0068735457920933865		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.0068735457920933865 | validation: 0.01796990535394386]
	TIME [epoch: 5.75 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008522356573511178		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.008522356573511178 | validation: 0.018929328032041327]
	TIME [epoch: 5.74 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007430062645469588		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.007430062645469588 | validation: 0.01614039490938243]
	TIME [epoch: 5.75 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005371069171080856		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.005371069171080856 | validation: 0.01630321964117586]
	TIME [epoch: 5.75 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005917499923541845		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.005917499923541845 | validation: 0.01686567861473275]
	TIME [epoch: 5.75 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006794796105789536		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.006794796105789536 | validation: 0.01902088416871649]
	TIME [epoch: 5.74 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006976589456124953		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.006976589456124953 | validation: 0.018361239411938648]
	TIME [epoch: 5.74 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006643569374587305		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.006643569374587305 | validation: 0.018433420712519365]
	TIME [epoch: 5.73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005954274333343846		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.005954274333343846 | validation: 0.018750969779426332]
	TIME [epoch: 5.74 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006665837077202367		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.006665837077202367 | validation: 0.01810923598465498]
	TIME [epoch: 5.73 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006000065399845943		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.006000065399845943 | validation: 0.01816065732847587]
	TIME [epoch: 5.73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006733366141599231		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.006733366141599231 | validation: 0.01950641396819125]
	TIME [epoch: 5.73 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058521477735598276		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0058521477735598276 | validation: 0.019171781886763453]
	TIME [epoch: 5.73 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007457826750433685		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.007457826750433685 | validation: 0.020460268253584712]
	TIME [epoch: 5.73 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007167738680897221		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.007167738680897221 | validation: 0.018594884819886526]
	TIME [epoch: 5.74 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007699974193068774		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.007699974193068774 | validation: 0.018000224513944642]
	TIME [epoch: 5.74 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006203935167925009		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.006203935167925009 | validation: 0.0161605542770601]
	TIME [epoch: 5.73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006863002782918041		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.006863002782918041 | validation: 0.022917175427548432]
	TIME [epoch: 5.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005730583430937062		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.005730583430937062 | validation: 0.017018481729872536]
	TIME [epoch: 5.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006458854855454458		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.006458854855454458 | validation: 0.019662891797557005]
	TIME [epoch: 5.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006514528483187543		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.006514528483187543 | validation: 0.016479743327348206]
	TIME [epoch: 5.74 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006396355087553132		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.006396355087553132 | validation: 0.02218147122993036]
	TIME [epoch: 5.74 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005593008820471399		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.005593008820471399 | validation: 0.016106286290426455]
	TIME [epoch: 5.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006724404941389154		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.006724404941389154 | validation: 0.016424297623922557]
	TIME [epoch: 5.74 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006184561949625892		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.006184561949625892 | validation: 0.0191811228477496]
	TIME [epoch: 5.75 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006272453704846509		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.006272453704846509 | validation: 0.020175750387687198]
	TIME [epoch: 5.75 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008249824443696334		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.008249824443696334 | validation: 0.016134894852090378]
	TIME [epoch: 5.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008425522282186423		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.008425522282186423 | validation: 0.018511312246374256]
	TIME [epoch: 5.74 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0065577154900440865		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.0065577154900440865 | validation: 0.020525362487555723]
	TIME [epoch: 5.76 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005927729275034086		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.005927729275034086 | validation: 0.02287322706750874]
	TIME [epoch: 5.75 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006705689443389426		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.006705689443389426 | validation: 0.01816530910810741]
	TIME [epoch: 5.76 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005991742431703751		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.005991742431703751 | validation: 0.01784766134350273]
	TIME [epoch: 5.75 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005577431692525176		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.005577431692525176 | validation: 0.01995121764107949]
	TIME [epoch: 5.76 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006917445280694144		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.006917445280694144 | validation: 0.017281454548514697]
	TIME [epoch: 5.74 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006122807173694261		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.006122807173694261 | validation: 0.017320241867153818]
	TIME [epoch: 5.75 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006296523638838546		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.006296523638838546 | validation: 0.01645248857989128]
	TIME [epoch: 5.76 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005916397554052904		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.005916397554052904 | validation: 0.015300433738351571]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00649722368442244		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.00649722368442244 | validation: 0.01851739799216535]
	TIME [epoch: 5.75 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006330342473380007		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.006330342473380007 | validation: 0.019025696802949854]
	TIME [epoch: 5.75 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006363945081979186		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.006363945081979186 | validation: 0.015752615624895216]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005645827787005584		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.005645827787005584 | validation: 0.015140260208676948]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1118.pth
	Model improved!!!
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005930035820380877		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.005930035820380877 | validation: 0.016227678157583914]
	TIME [epoch: 5.77 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005637081338794722		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.005637081338794722 | validation: 0.017131707313861877]
	TIME [epoch: 5.78 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006911564622566237		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.006911564622566237 | validation: 0.014114490901232613]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007172882565369148		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.007172882565369148 | validation: 0.01958525187943977]
	TIME [epoch: 5.77 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006940700786590109		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.006940700786590109 | validation: 0.016346186525048857]
	TIME [epoch: 5.79 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005770592605594487		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.005770592605594487 | validation: 0.014740220993018206]
	TIME [epoch: 5.78 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005999592774431585		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.005999592774431585 | validation: 0.020155161885414365]
	TIME [epoch: 5.8 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006132594419763865		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.006132594419763865 | validation: 0.015105702406688994]
	TIME [epoch: 5.78 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005823169853108398		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.005823169853108398 | validation: 0.015683513214817126]
	TIME [epoch: 5.77 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005504259876109247		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.005504259876109247 | validation: 0.017236011069123237]
	TIME [epoch: 5.76 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006270585515919672		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.006270585515919672 | validation: 0.018553630751499683]
	TIME [epoch: 5.77 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005308159181104705		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.005308159181104705 | validation: 0.01702381254101856]
	TIME [epoch: 5.76 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006012808337549861		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.006012808337549861 | validation: 0.017706765771106827]
	TIME [epoch: 5.78 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005577198592736545		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.005577198592736545 | validation: 0.015988730707274534]
	TIME [epoch: 5.76 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006073904225336372		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.006073904225336372 | validation: 0.019062354048123033]
	TIME [epoch: 5.78 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005586594062819594		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.005586594062819594 | validation: 0.016288243951770587]
	TIME [epoch: 5.77 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005821725003838236		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.005821725003838236 | validation: 0.018764648948371612]
	TIME [epoch: 5.77 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005379036393624862		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.005379036393624862 | validation: 0.01937930653648519]
	TIME [epoch: 5.77 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006360870330807876		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.006360870330807876 | validation: 0.019343736910397858]
	TIME [epoch: 5.77 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00700610192702853		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.00700610192702853 | validation: 0.015337967521623475]
	TIME [epoch: 5.77 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005729600762751514		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.005729600762751514 | validation: 0.017066875205090714]
	TIME [epoch: 5.78 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005641343581908161		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.005641343581908161 | validation: 0.01849883967988646]
	TIME [epoch: 5.78 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005509638862873074		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.005509638862873074 | validation: 0.01607019761991222]
	TIME [epoch: 5.78 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005714943954901074		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.005714943954901074 | validation: 0.018021999342583084]
	TIME [epoch: 5.78 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006500454795193833		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.006500454795193833 | validation: 0.015397945134505143]
	TIME [epoch: 5.78 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006193571513299925		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.006193571513299925 | validation: 0.01499912964750484]
	TIME [epoch: 5.78 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006012474015885196		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.006012474015885196 | validation: 0.013945167004274717]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1145.pth
	Model improved!!!
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006255817820305962		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.006255817820305962 | validation: 0.021282644385370576]
	TIME [epoch: 5.77 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005595682288171014		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.005595682288171014 | validation: 0.018953497807006836]
	TIME [epoch: 5.76 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005738158118796217		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.005738158118796217 | validation: 0.018137033330342212]
	TIME [epoch: 5.77 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005739986471436859		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.005739986471436859 | validation: 0.016734501068509888]
	TIME [epoch: 5.76 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007242277913876143		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.007242277913876143 | validation: 0.01787307253255861]
	TIME [epoch: 5.76 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007108001950899705		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.007108001950899705 | validation: 0.020374370296263978]
	TIME [epoch: 5.76 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006335054452187104		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.006335054452187104 | validation: 0.014951598384151677]
	TIME [epoch: 5.77 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005867674260200187		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.005867674260200187 | validation: 0.018796509716873945]
	TIME [epoch: 5.76 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005752302822311673		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.005752302822311673 | validation: 0.016418512425302824]
	TIME [epoch: 5.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005376956278087498		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.005376956278087498 | validation: 0.018306997322763]
	TIME [epoch: 5.77 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049755356317678036		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.0049755356317678036 | validation: 0.016604892595445276]
	TIME [epoch: 5.76 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005016749284354002		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.005016749284354002 | validation: 0.016222713341031713]
	TIME [epoch: 5.76 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006160037603448756		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.006160037603448756 | validation: 0.016310347529562976]
	TIME [epoch: 5.76 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0056929903774694725		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.0056929903774694725 | validation: 0.016901303703560976]
	TIME [epoch: 5.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005886832468792073		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.005886832468792073 | validation: 0.018721049894127064]
	TIME [epoch: 5.76 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006618025528897245		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.006618025528897245 | validation: 0.018730567662110478]
	TIME [epoch: 5.76 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00546567736557238		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.00546567736557238 | validation: 0.016199157171112654]
	TIME [epoch: 5.76 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006012341631069462		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.006012341631069462 | validation: 0.017938123659923765]
	TIME [epoch: 5.77 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006200653997723118		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.006200653997723118 | validation: 0.01712205604558268]
	TIME [epoch: 5.77 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005750863341716439		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.005750863341716439 | validation: 0.018885007159319026]
	TIME [epoch: 5.77 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005458204815180656		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.005458204815180656 | validation: 0.016492272769295113]
	TIME [epoch: 5.77 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005410463658504949		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.005410463658504949 | validation: 0.017432926263005234]
	TIME [epoch: 5.78 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0047495920190607855		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.0047495920190607855 | validation: 0.01701427652963592]
	TIME [epoch: 5.77 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005355233013624632		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.005355233013624632 | validation: 0.016886279091631973]
	TIME [epoch: 5.77 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058389266325759685		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.0058389266325759685 | validation: 0.016458761883524888]
	TIME [epoch: 5.77 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058869921434619		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.0058869921434619 | validation: 0.01530736212890983]
	TIME [epoch: 5.77 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0055863571610134015		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.0055863571610134015 | validation: 0.01582722769405689]
	TIME [epoch: 5.75 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005643812189592244		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.005643812189592244 | validation: 0.017621062044189863]
	TIME [epoch: 5.76 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0052262521407377485		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.0052262521407377485 | validation: 0.01634796181462358]
	TIME [epoch: 5.76 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005573703124179101		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.005573703124179101 | validation: 0.017229978073417886]
	TIME [epoch: 5.76 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005720080573893656		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.005720080573893656 | validation: 0.01663315838039303]
	TIME [epoch: 5.76 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006282095990937141		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.006282095990937141 | validation: 0.01699583156096466]
	TIME [epoch: 5.76 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005676301936741778		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.005676301936741778 | validation: 0.01663896275879625]
	TIME [epoch: 5.77 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005240758798879375		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.005240758798879375 | validation: 0.01299520182181675]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1179.pth
	Model improved!!!
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006243299164496936		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.006243299164496936 | validation: 0.01761958289141178]
	TIME [epoch: 5.76 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050306117266971		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.0050306117266971 | validation: 0.01670722983415427]
	TIME [epoch: 5.77 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004652872619690146		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.004652872619690146 | validation: 0.017794907749215085]
	TIME [epoch: 5.77 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004962091481516996		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.004962091481516996 | validation: 0.015938337429261762]
	TIME [epoch: 5.76 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00621268762024983		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.00621268762024983 | validation: 0.01688322678319949]
	TIME [epoch: 5.76 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0064297243891483545		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.0064297243891483545 | validation: 0.013046112185190263]
	TIME [epoch: 5.76 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00517645111054787		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.00517645111054787 | validation: 0.017144091038504828]
	TIME [epoch: 5.76 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005526624235741383		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.005526624235741383 | validation: 0.01994008014660449]
	TIME [epoch: 5.73 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006475447842051066		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.006475447842051066 | validation: 0.01832847180166496]
	TIME [epoch: 5.76 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005471197111848931		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.005471197111848931 | validation: 0.01482308598934412]
	TIME [epoch: 5.76 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005279395683244366		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.005279395683244366 | validation: 0.016187805730003592]
	TIME [epoch: 5.77 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005201700211021513		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.005201700211021513 | validation: 0.017768319522607157]
	TIME [epoch: 5.76 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005139128802514032		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.005139128802514032 | validation: 0.017516382172146117]
	TIME [epoch: 5.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005402315898173602		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.005402315898173602 | validation: 0.019798987673245873]
	TIME [epoch: 5.76 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049899165465505544		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.0049899165465505544 | validation: 0.01760494537320988]
	TIME [epoch: 5.76 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058170768429532576		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.0058170768429532576 | validation: 0.017398486422936812]
	TIME [epoch: 5.76 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0054622950504467326		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.0054622950504467326 | validation: 0.015985680093278276]
	TIME [epoch: 5.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005864038559607702		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.005864038559607702 | validation: 0.018283596224856435]
	TIME [epoch: 5.75 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0047079540338659884		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.0047079540338659884 | validation: 0.018170911775036824]
	TIME [epoch: 5.75 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005413535107014954		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.005413535107014954 | validation: 0.018025660360575447]
	TIME [epoch: 5.75 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005452268141566212		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.005452268141566212 | validation: 0.01209809273196627]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1200.pth
	Model improved!!!
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0054188849051423635		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.0054188849051423635 | validation: 0.017436286288755976]
	TIME [epoch: 5.73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005926494641492981		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.005926494641492981 | validation: 0.015039464582744345]
	TIME [epoch: 5.73 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005884717619435511		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.005884717619435511 | validation: 0.02213448642197028]
	TIME [epoch: 5.74 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005256503209390192		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.005256503209390192 | validation: 0.01276190997086234]
	TIME [epoch: 5.74 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006149214336207051		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.006149214336207051 | validation: 0.014129167393982811]
	TIME [epoch: 5.74 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006679535985087899		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.006679535985087899 | validation: 0.01695901358281551]
	TIME [epoch: 5.74 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059702694966153845		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.0059702694966153845 | validation: 0.01672863059356965]
	TIME [epoch: 5.74 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004832306601764403		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.004832306601764403 | validation: 0.018971258731622444]
	TIME [epoch: 5.76 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005184333769965358		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.005184333769965358 | validation: 0.01761891776407416]
	TIME [epoch: 5.77 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005323426945704825		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.005323426945704825 | validation: 0.014609891439322842]
	TIME [epoch: 5.76 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005704736376438522		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.005704736376438522 | validation: 0.014464411528517462]
	TIME [epoch: 5.76 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004946462601591613		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.004946462601591613 | validation: 0.017895338467482282]
	TIME [epoch: 5.76 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005655929365527498		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.005655929365527498 | validation: 0.016951685138632367]
	TIME [epoch: 5.76 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0057108824429621865		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.0057108824429621865 | validation: 0.013412416580017406]
	TIME [epoch: 5.76 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005194306098950017		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.005194306098950017 | validation: 0.017438438165646476]
	TIME [epoch: 5.76 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004944700081630801		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.004944700081630801 | validation: 0.014428765959113611]
	TIME [epoch: 5.76 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004523090985576326		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.004523090985576326 | validation: 0.014381569331443201]
	TIME [epoch: 5.76 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004657010928922275		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.004657010928922275 | validation: 0.02062228872560066]
	TIME [epoch: 5.76 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00506774615195366		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.00506774615195366 | validation: 0.019192117058365477]
	TIME [epoch: 5.77 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0061745437379748585		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.0061745437379748585 | validation: 0.013880463412940714]
	TIME [epoch: 5.77 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006300957034847731		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.006300957034847731 | validation: 0.014461505768274142]
	TIME [epoch: 5.76 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004417603977803949		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.004417603977803949 | validation: 0.016399022934742935]
	TIME [epoch: 5.76 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004671555239288245		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.004671555239288245 | validation: 0.014639022698426363]
	TIME [epoch: 5.76 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004458737218831468		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.004458737218831468 | validation: 0.015584139557432208]
	TIME [epoch: 5.76 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00484929940956267		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.00484929940956267 | validation: 0.015808978507230266]
	TIME [epoch: 5.77 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005081535147892418		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.005081535147892418 | validation: 0.01718181133725092]
	TIME [epoch: 5.77 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038970762470192833		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.0038970762470192833 | validation: 0.015263992828435336]
	TIME [epoch: 5.77 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004919210578408242		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.004919210578408242 | validation: 0.014750194233431735]
	TIME [epoch: 5.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00497841976878309		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.00497841976878309 | validation: 0.01474541875211488]
	TIME [epoch: 5.78 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004537481143338552		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.004537481143338552 | validation: 0.011870614834935423]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1230.pth
	Model improved!!!
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004146600044681933		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.004146600044681933 | validation: 0.013326839874998242]
	TIME [epoch: 5.77 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050681802638777905		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.0050681802638777905 | validation: 0.014591136240678438]
	TIME [epoch: 5.77 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005686963723100417		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.005686963723100417 | validation: 0.015219640144562119]
	TIME [epoch: 5.77 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005281085540063158		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.005281085540063158 | validation: 0.018408955464290822]
	TIME [epoch: 5.77 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050883757849427185		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.0050883757849427185 | validation: 0.01574954390853214]
	TIME [epoch: 5.77 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004646711510659848		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.004646711510659848 | validation: 0.018173729225164217]
	TIME [epoch: 5.77 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005511499918529852		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.005511499918529852 | validation: 0.01833441565591566]
	TIME [epoch: 5.76 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005535361509295887		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.005535361509295887 | validation: 0.019239043499950394]
	TIME [epoch: 5.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00616750848196829		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.00616750848196829 | validation: 0.014956674053046438]
	TIME [epoch: 5.76 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005816191825796322		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.005816191825796322 | validation: 0.015727700392831535]
	TIME [epoch: 5.76 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004731537689658953		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.004731537689658953 | validation: 0.015126864283460384]
	TIME [epoch: 5.76 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005674179396947523		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.005674179396947523 | validation: 0.015756707883936304]
	TIME [epoch: 5.77 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005424014619800155		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.005424014619800155 | validation: 0.015861098830703902]
	TIME [epoch: 5.77 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004465673968421377		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.004465673968421377 | validation: 0.014978259311239207]
	TIME [epoch: 5.77 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00461962876949585		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.00461962876949585 | validation: 0.019244605005536444]
	TIME [epoch: 5.79 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005281343238255354		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.005281343238255354 | validation: 0.016524721620843164]
	TIME [epoch: 5.78 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005916300734340077		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.005916300734340077 | validation: 0.017823601108455957]
	TIME [epoch: 5.79 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00517221182096197		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.00517221182096197 | validation: 0.0145380527003807]
	TIME [epoch: 5.78 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004573101796084796		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.004573101796084796 | validation: 0.013809871803237783]
	TIME [epoch: 5.78 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005188443786917696		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.005188443786917696 | validation: 0.017848384287218157]
	TIME [epoch: 5.78 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004843035597967721		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.004843035597967721 | validation: 0.013892410949356183]
	TIME [epoch: 5.79 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0061437544015515875		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.0061437544015515875 | validation: 0.014241609847278049]
	TIME [epoch: 5.78 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005292744985125834		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.005292744985125834 | validation: 0.01537066528559733]
	TIME [epoch: 5.78 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00489890334061359		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.00489890334061359 | validation: 0.014996334521540767]
	TIME [epoch: 5.78 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004922509419844306		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.004922509419844306 | validation: 0.015528356183085146]
	TIME [epoch: 5.78 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004882323993572435		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.004882323993572435 | validation: 0.013048580168716706]
	TIME [epoch: 5.78 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004264308748578331		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.004264308748578331 | validation: 0.018079116561369492]
	TIME [epoch: 5.78 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0048855542868561485		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.0048855542868561485 | validation: 0.017761190004613393]
	TIME [epoch: 5.78 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00483503830893997		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.00483503830893997 | validation: 0.01471689489689736]
	TIME [epoch: 5.78 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00495657301267593		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.00495657301267593 | validation: 0.015314220621735587]
	TIME [epoch: 5.77 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049244772741968736		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.0049244772741968736 | validation: 0.014494620766844712]
	TIME [epoch: 5.75 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005644551828368063		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.005644551828368063 | validation: 0.014425833662680144]
	TIME [epoch: 5.75 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004730126446693428		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.004730126446693428 | validation: 0.014066589136585362]
	TIME [epoch: 5.75 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005187796796040592		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.005187796796040592 | validation: 0.014294114105822542]
	TIME [epoch: 5.75 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004082068590198347		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.004082068590198347 | validation: 0.015153903500852018]
	TIME [epoch: 5.76 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005468562807362084		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.005468562807362084 | validation: 0.015036389121321792]
	TIME [epoch: 5.75 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005043030701166641		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.005043030701166641 | validation: 0.011220043661836399]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1267.pth
	Model improved!!!
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005054311130782353		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.005054311130782353 | validation: 0.01594067029213918]
	TIME [epoch: 5.79 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004604942856463566		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.004604942856463566 | validation: 0.016882235660044865]
	TIME [epoch: 5.8 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00482862156471217		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.00482862156471217 | validation: 0.01585923573383167]
	TIME [epoch: 5.79 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004963656555247582		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.004963656555247582 | validation: 0.017636952494844007]
	TIME [epoch: 5.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005233235818911621		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.005233235818911621 | validation: 0.016558790405787917]
	TIME [epoch: 5.79 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005159299034065364		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.005159299034065364 | validation: 0.014974337406710026]
	TIME [epoch: 5.8 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004866867585966185		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.004866867585966185 | validation: 0.016399057766108973]
	TIME [epoch: 5.78 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004847277790141455		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.004847277790141455 | validation: 0.016802380525680127]
	TIME [epoch: 5.8 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004917808121850411		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.004917808121850411 | validation: 0.015036008905179456]
	TIME [epoch: 5.79 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004921495187615623		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.004921495187615623 | validation: 0.016313480673075687]
	TIME [epoch: 5.79 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005038070243089236		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.005038070243089236 | validation: 0.016900586065252854]
	TIME [epoch: 5.78 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0043807963179289845		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.0043807963179289845 | validation: 0.018521694188659277]
	TIME [epoch: 5.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0039181444551573		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0039181444551573 | validation: 0.01561711202760967]
	TIME [epoch: 5.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004890665864069117		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.004890665864069117 | validation: 0.01704872949874933]
	TIME [epoch: 5.79 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004892315849612169		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.004892315849612169 | validation: 0.015802333710934625]
	TIME [epoch: 5.78 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005603779294185242		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.005603779294185242 | validation: 0.020028698008901436]
	TIME [epoch: 5.79 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005087373755078637		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.005087373755078637 | validation: 0.01498600949374741]
	TIME [epoch: 5.78 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051131687946628845		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.0051131687946628845 | validation: 0.015839253193753722]
	TIME [epoch: 5.79 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004874235082862389		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.004874235082862389 | validation: 0.02002267154776223]
	TIME [epoch: 5.79 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005027476907050294		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.005027476907050294 | validation: 0.014722837834582747]
	TIME [epoch: 5.77 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005673789642562153		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.005673789642562153 | validation: 0.01528915585941253]
	TIME [epoch: 5.78 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004904263369423709		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.004904263369423709 | validation: 0.014151237332727697]
	TIME [epoch: 5.77 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004583885174783177		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.004583885174783177 | validation: 0.016125332009071226]
	TIME [epoch: 5.76 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004162362701891647		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.004162362701891647 | validation: 0.015101422886446558]
	TIME [epoch: 5.78 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0052803228730774		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.0052803228730774 | validation: 0.015190875714113695]
	TIME [epoch: 5.76 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003973237165338845		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.003973237165338845 | validation: 0.017506455637633346]
	TIME [epoch: 5.77 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004921971818413964		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.004921971818413964 | validation: 0.01157890198722087]
	TIME [epoch: 5.76 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050251852367817615		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.0050251852367817615 | validation: 0.015895943535379122]
	TIME [epoch: 5.77 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003883534938165747		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.003883534938165747 | validation: 0.016728531795477397]
	TIME [epoch: 5.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004889659448810055		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.004889659448810055 | validation: 0.013326280085794874]
	TIME [epoch: 5.77 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004595604974067626		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.004595604974067626 | validation: 0.017417183828346016]
	TIME [epoch: 5.77 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004522672781627803		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.004522672781627803 | validation: 0.014708864204899098]
	TIME [epoch: 5.76 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003962212343608563		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.003962212343608563 | validation: 0.01580366421354995]
	TIME [epoch: 5.76 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005347969664082926		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.005347969664082926 | validation: 0.014136612407066785]
	TIME [epoch: 5.79 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005310579718922269		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.005310579718922269 | validation: 0.016499533323587112]
	TIME [epoch: 5.81 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005209006412024623		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.005209006412024623 | validation: 0.018973315964689687]
	TIME [epoch: 5.81 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004640434724569457		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.004640434724569457 | validation: 0.013668466535476898]
	TIME [epoch: 5.82 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049526583184930795		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.0049526583184930795 | validation: 0.016962360940689713]
	TIME [epoch: 5.81 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004493763838969793		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.004493763838969793 | validation: 0.014691385261692759]
	TIME [epoch: 5.8 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005322054006748923		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.005322054006748923 | validation: 0.017001790361517412]
	TIME [epoch: 5.8 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004530643490980768		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.004530643490980768 | validation: 0.016310122241742876]
	TIME [epoch: 5.81 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005164960937384872		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.005164960937384872 | validation: 0.01920437372095366]
	TIME [epoch: 5.79 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004913517872571858		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.004913517872571858 | validation: 0.015948813244035697]
	TIME [epoch: 5.8 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0042874645590737035		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.0042874645590737035 | validation: 0.015300971710378653]
	TIME [epoch: 5.79 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00482177300053937		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.00482177300053937 | validation: 0.013986385666216062]
	TIME [epoch: 5.8 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005125808497207652		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.005125808497207652 | validation: 0.013378361638482106]
	TIME [epoch: 5.8 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004755758318061844		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.004755758318061844 | validation: 0.017758427373974584]
	TIME [epoch: 5.8 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004278394701641669		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.004278394701641669 | validation: 0.019473106713444467]
	TIME [epoch: 5.8 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00495561085584274		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.00495561085584274 | validation: 0.016883993512642883]
	TIME [epoch: 5.8 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00490552310138727		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.00490552310138727 | validation: 0.013622255311960974]
	TIME [epoch: 5.8 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0046383853703892185		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.0046383853703892185 | validation: 0.01335415955744772]
	TIME [epoch: 5.81 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004466870090053642		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.004466870090053642 | validation: 0.012989547054535477]
	TIME [epoch: 5.8 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0041639892788713296		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.0041639892788713296 | validation: 0.01636191577954519]
	TIME [epoch: 5.8 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004609319387853449		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.004609319387853449 | validation: 0.012251601519186129]
	TIME [epoch: 5.8 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005043307875579593		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.005043307875579593 | validation: 0.01432882331750004]
	TIME [epoch: 5.8 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0054736312648350416		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.0054736312648350416 | validation: 0.012773143770893348]
	TIME [epoch: 5.8 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00422490935014057		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.00422490935014057 | validation: 0.016006133389155165]
	TIME [epoch: 5.8 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004710747564389296		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.004710747564389296 | validation: 0.013460260569903371]
	TIME [epoch: 5.8 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004289632529834298		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.004289632529834298 | validation: 0.013671613013021444]
	TIME [epoch: 5.8 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005015181680040375		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.005015181680040375 | validation: 0.01456174454080419]
	TIME [epoch: 5.8 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0041760702823580055		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.0041760702823580055 | validation: 0.013571199267922752]
	TIME [epoch: 5.8 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005075214025667216		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.005075214025667216 | validation: 0.013561334118530511]
	TIME [epoch: 5.81 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004575458222349973		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.004575458222349973 | validation: 0.014811519237094473]
	TIME [epoch: 5.81 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004993414874239404		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.004993414874239404 | validation: 0.013590221754794962]
	TIME [epoch: 5.81 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005069931206795195		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.005069931206795195 | validation: 0.015174421874009893]
	TIME [epoch: 5.8 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004566996599183854		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.004566996599183854 | validation: 0.012616250603103875]
	TIME [epoch: 5.81 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004100232676112055		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.004100232676112055 | validation: 0.016219071993572788]
	TIME [epoch: 5.8 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004341262522088094		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.004341262522088094 | validation: 0.016973625301184393]
	TIME [epoch: 5.8 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004817763501567651		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.004817763501567651 | validation: 0.01297809000976511]
	TIME [epoch: 5.79 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004063857120876528		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.004063857120876528 | validation: 0.014595562299539455]
	TIME [epoch: 5.8 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004654667012036874		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.004654667012036874 | validation: 0.0172307408004159]
	TIME [epoch: 5.79 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0040572258559460304		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.0040572258559460304 | validation: 0.018223423943247986]
	TIME [epoch: 5.8 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004225285678498567		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.004225285678498567 | validation: 0.014932627411485001]
	TIME [epoch: 5.8 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004014250935188904		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.004014250935188904 | validation: 0.01181487013984327]
	TIME [epoch: 5.81 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005115366473958462		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.005115366473958462 | validation: 0.01713408890573346]
	TIME [epoch: 5.8 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004515677935427771		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.004515677935427771 | validation: 0.01605878236370335]
	TIME [epoch: 5.81 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004793496961452279		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.004793496961452279 | validation: 0.014956123364076025]
	TIME [epoch: 5.81 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004426893051852155		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.004426893051852155 | validation: 0.01653286397784739]
	TIME [epoch: 5.81 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0043364260831739035		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.0043364260831739035 | validation: 0.01506383152611972]
	TIME [epoch: 5.8 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004751848343153592		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.004751848343153592 | validation: 0.014115265750008266]
	TIME [epoch: 5.8 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004880888645970155		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.004880888645970155 | validation: 0.0124147667541756]
	TIME [epoch: 5.79 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00463110535760872		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.00463110535760872 | validation: 0.01763659515772925]
	TIME [epoch: 5.79 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004939790500037391		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.004939790500037391 | validation: 0.01456535132469381]
	TIME [epoch: 5.78 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004582835192096028		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.004582835192096028 | validation: 0.013530596753163615]
	TIME [epoch: 5.8 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004275698596919215		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.004275698596919215 | validation: 0.016403383421213204]
	TIME [epoch: 5.79 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004249964895276864		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.004249964895276864 | validation: 0.01651956463383727]
	TIME [epoch: 5.79 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004352160747059186		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.004352160747059186 | validation: 0.016044082315064358]
	TIME [epoch: 5.79 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004461465457559233		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.004461465457559233 | validation: 0.014673122133445182]
	TIME [epoch: 5.79 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0045058892707003344		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.0045058892707003344 | validation: 0.013770752657416063]
	TIME [epoch: 5.78 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004497929536743895		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.004497929536743895 | validation: 0.01204668831519763]
	TIME [epoch: 5.78 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003831037795069543		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.003831037795069543 | validation: 0.015407895986417897]
	TIME [epoch: 5.78 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004202753920286002		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.004202753920286002 | validation: 0.01247440528110022]
	TIME [epoch: 5.79 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003978243583968156		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.003978243583968156 | validation: 0.016162254521906904]
	TIME [epoch: 5.78 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0036037048607048382		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.0036037048607048382 | validation: 0.012804377592699291]
	TIME [epoch: 5.79 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003553861730965662		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.003553861730965662 | validation: 0.014355848988870878]
	TIME [epoch: 5.78 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004250359687771979		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.004250359687771979 | validation: 0.015933816192284435]
	TIME [epoch: 5.79 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004636448148534074		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.004636448148534074 | validation: 0.017706149803228866]
	TIME [epoch: 5.78 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00447517529037881		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.00447517529037881 | validation: 0.016382624377823552]
	TIME [epoch: 5.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004249429584553145		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.004249429584553145 | validation: 0.014435755613157453]
	TIME [epoch: 5.78 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0046486104640414885		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.0046486104640414885 | validation: 0.014424267241970767]
	TIME [epoch: 5.78 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004704993367179661		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.004704993367179661 | validation: 0.017193516441232005]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_132908/states/model_phi1_4a_v_mmd1_1368.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4816.061 seconds.
