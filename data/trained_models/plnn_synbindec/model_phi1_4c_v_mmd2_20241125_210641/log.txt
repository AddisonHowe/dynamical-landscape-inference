Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3508326725

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.802873513031597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.802873513031597 | validation: 5.130248354027152]
	TIME [epoch: 178 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.146059451652648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.146059451652648 | validation: 4.197791933964231]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.352024160510527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.352024160510527 | validation: 4.926609404566974]
	TIME [epoch: 2.83 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.146973655159264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.146973655159264 | validation: 4.154966715826073]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.31838587874634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.31838587874634 | validation: 4.089679462299023]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.254290445602411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.254290445602411 | validation: 4.037647488618249]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342900028132911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342900028132911 | validation: 4.138950067860682]
	TIME [epoch: 2.82 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.333568500354062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.333568500354062 | validation: 4.016474684018152]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.182420793181092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182420793181092 | validation: 3.8296995589124823]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.113080906743301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113080906743301 | validation: 3.828577136281115]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.050401415484503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.050401415484503 | validation: 3.8104592802460675]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.033476946973683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033476946973683 | validation: 3.7343659228790544]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021369014024101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.021369014024101 | validation: 3.759345691855227]
	TIME [epoch: 2.83 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024196998718331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.024196998718331 | validation: 3.7281118364807706]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008709010491516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008709010491516 | validation: 3.72436472934818]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.039500333471741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.039500333471741 | validation: 3.6736864867793146]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.949583370923714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.949583370923714 | validation: 3.569903124354603]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9129192652194495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9129192652194495 | validation: 3.579848420245709]
	TIME [epoch: 2.82 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.882083426846393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.882083426846393 | validation: 3.524981115569527]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.859350795771984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859350795771984 | validation: 3.5265115474411246]
	TIME [epoch: 2.82 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8505820556948063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8505820556948063 | validation: 3.518931089300763]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8633349597329913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8633349597329913 | validation: 3.534033122650871]
	TIME [epoch: 2.83 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8693644079596954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8693644079596954 | validation: 3.55770796074583]
	TIME [epoch: 2.83 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9186696622602804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9186696622602804 | validation: 3.437352266670817]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7785448718202668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7785448718202668 | validation: 3.4084940907257257]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.767153829036758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.767153829036758 | validation: 3.4310366345812797]
	TIME [epoch: 2.83 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.781069122065444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.781069122065444 | validation: 3.3866707609538036]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7594151014326878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7594151014326878 | validation: 3.374472735547954]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.741581743939222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.741581743939222 | validation: 3.358690978247267]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7141366834488907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7141366834488907 | validation: 3.3355070262945796]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.727327993309345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727327993309345 | validation: 3.404627324054824]
	TIME [epoch: 2.82 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7342386359465243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7342386359465243 | validation: 3.311816782521038]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.713795410453132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.713795410453132 | validation: 3.264424846524182]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.643227240497127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.643227240497127 | validation: 3.2469910956366976]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6201474495237735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6201474495237735 | validation: 3.218145462360702]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6010625274893138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6010625274893138 | validation: 3.186186224889633]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5836160063791307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5836160063791307 | validation: 3.184770144441973]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5625767568538236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5625767568538236 | validation: 3.149233342829999]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5459961356577674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5459961356577674 | validation: 3.1429408285184004]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5282817496251444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5282817496251444 | validation: 3.1328403093185253]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5208078102317133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5208078102317133 | validation: 3.1632484456608276]
	TIME [epoch: 2.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.524089281697345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.524089281697345 | validation: 3.2373909713475]
	TIME [epoch: 2.84 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6094552319901205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6094552319901205 | validation: 3.068632430602433]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4580812479104197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4580812479104197 | validation: 3.081516733336931]
	TIME [epoch: 2.84 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.414996841816826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.414996841816826 | validation: 3.049681981852962]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.396114243306022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.396114243306022 | validation: 2.910298645824903]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.271221107609319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.271221107609319 | validation: 2.6108409583882577]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9738550875608274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9738550875608274 | validation: 1.8042356578300494]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1576824879946854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1576824879946854 | validation: 1.5068494856250254]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7511508407032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7511508407032 | validation: 1.3757365783031705]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7111809605218669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7111809605218669 | validation: 0.9320147545013638]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2106381265219315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2106381265219315 | validation: 1.0051447780678633]
	TIME [epoch: 2.83 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133128183576854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.133128183576854 | validation: 1.4732634886622027]
	TIME [epoch: 2.83 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7545964335133895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7545964335133895 | validation: 0.8105994592363666]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0008711696472514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0008711696472514 | validation: 0.8564489592527007]
	TIME [epoch: 2.82 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0560337714378647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0560337714378647 | validation: 0.7857577576485959]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9752680783107535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9752680783107535 | validation: 0.7452842598192226]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580156215374978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580156215374978 | validation: 0.8337892256142756]
	TIME [epoch: 2.81 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0854321504998286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0854321504998286 | validation: 0.7985224822252139]
	TIME [epoch: 2.82 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0115579126616852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0115579126616852 | validation: 0.7427805006971814]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262162582124372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9262162582124372 | validation: 0.8302466016142582]
	TIME [epoch: 2.82 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0829171589804034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0829171589804034 | validation: 0.7779072751487438]
	TIME [epoch: 2.82 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9544372970057806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9544372970057806 | validation: 0.7775718920269296]
	TIME [epoch: 2.82 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9178791987589722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9178791987589722 | validation: 0.7831852845338478]
	TIME [epoch: 2.81 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210631000019649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9210631000019649 | validation: 0.7590544353510392]
	TIME [epoch: 2.81 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267265152055678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9267265152055678 | validation: 0.828826127365695]
	TIME [epoch: 2.83 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.005168712389802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.005168712389802 | validation: 0.7483387912074203]
	TIME [epoch: 2.82 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9169096490725279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9169096490725279 | validation: 0.7849433986327572]
	TIME [epoch: 2.81 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9169372470805351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9169372470805351 | validation: 0.7594519286711727]
	TIME [epoch: 2.82 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290831245406435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9290831245406435 | validation: 0.7466751235044513]
	TIME [epoch: 2.83 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9145260685328314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9145260685328314 | validation: 0.838685335609485]
	TIME [epoch: 2.83 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040990696023072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.040990696023072 | validation: 0.7584776883173195]
	TIME [epoch: 2.83 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460461864078535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9460461864078535 | validation: 0.8136075690588677]
	TIME [epoch: 2.83 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004862036010553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.004862036010553 | validation: 1.104155621109199]
	TIME [epoch: 2.82 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5252184074378266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5252184074378266 | validation: 1.050412076953695]
	TIME [epoch: 2.82 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2734790760639403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2734790760639403 | validation: 0.7477744287822929]
	TIME [epoch: 2.82 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9720888161131683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9720888161131683 | validation: 0.8876500473919153]
	TIME [epoch: 2.83 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583470016326917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0583470016326917 | validation: 0.7597443172718349]
	TIME [epoch: 2.83 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938162771911722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.938162771911722 | validation: 0.7562699308640926]
	TIME [epoch: 2.82 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388280158339651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9388280158339651 | validation: 0.7846630095786065]
	TIME [epoch: 2.83 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053027453295808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053027453295808 | validation: 0.7673443154837766]
	TIME [epoch: 2.83 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028191787115114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9028191787115114 | validation: 0.8305610843598874]
	TIME [epoch: 2.83 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9773680035193306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9773680035193306 | validation: 0.7550914453454729]
	TIME [epoch: 2.83 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057437574829581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9057437574829581 | validation: 0.7667630478327386]
	TIME [epoch: 2.84 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918026090609851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.918026090609851 | validation: 0.7653098659729838]
	TIME [epoch: 2.84 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896684065640296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896684065640296 | validation: 0.7503607583762328]
	TIME [epoch: 2.83 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997581170178437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997581170178437 | validation: 0.7664250142731621]
	TIME [epoch: 2.83 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963872002642175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8963872002642175 | validation: 0.7544188381677768]
	TIME [epoch: 2.83 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005771746235128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005771746235128 | validation: 0.763693255708966]
	TIME [epoch: 2.82 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111697354598303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9111697354598303 | validation: 0.7590790765177948]
	TIME [epoch: 2.82 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949194115914122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8949194115914122 | validation: 0.7383263184671057]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9079942217019306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9079942217019306 | validation: 0.7442336799541006]
	TIME [epoch: 2.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8897565744852581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8897565744852581 | validation: 0.8059054657582039]
	TIME [epoch: 2.84 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9418066054691124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9418066054691124 | validation: 0.8958245316989942]
	TIME [epoch: 2.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1318677832669115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1318677832669115 | validation: 0.7845212062643458]
	TIME [epoch: 2.84 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9414119936243434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9414119936243434 | validation: 0.7610827780761412]
	TIME [epoch: 2.84 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929449188364508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929449188364508 | validation: 0.7666035081452818]
	TIME [epoch: 2.85 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258098705158213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258098705158213 | validation: 0.816490216101612]
	TIME [epoch: 2.85 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9528803156275153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9528803156275153 | validation: 0.8190577022172909]
	TIME [epoch: 2.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9928852389389843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9928852389389843 | validation: 0.7607853602888789]
	TIME [epoch: 2.84 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918385168192671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.918385168192671 | validation: 0.7600480576000074]
	TIME [epoch: 2.84 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943109831336673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943109831336673 | validation: 0.7765298568155732]
	TIME [epoch: 2.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248621489082013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248621489082013 | validation: 0.7693234290254439]
	TIME [epoch: 2.84 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057722017279187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9057722017279187 | validation: 0.7657627483000534]
	TIME [epoch: 2.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9303771547314285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9303771547314285 | validation: 0.763012762737038]
	TIME [epoch: 2.85 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243269049170959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9243269049170959 | validation: 0.7676796521243291]
	TIME [epoch: 2.84 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9147444798419159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9147444798419159 | validation: 0.772631061770157]
	TIME [epoch: 2.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923098496095475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.923098496095475 | validation: 0.7731973707702372]
	TIME [epoch: 2.85 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9432879617812208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9432879617812208 | validation: 0.8450720328805308]
	TIME [epoch: 2.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0134590882922012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0134590882922012 | validation: 0.7339843411473801]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873647804913292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873647804913292 | validation: 0.7470377922295539]
	TIME [epoch: 2.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143010921685343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143010921685343 | validation: 0.8155044862871522]
	TIME [epoch: 2.83 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728171614942258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9728171614942258 | validation: 0.7378674903258869]
	TIME [epoch: 2.83 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861562411964693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861562411964693 | validation: 0.7456887337804465]
	TIME [epoch: 2.83 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8955312559949107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8955312559949107 | validation: 0.7817126749152221]
	TIME [epoch: 2.83 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143484163106375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143484163106375 | validation: 0.7363328442945287]
	TIME [epoch: 2.83 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080832360734478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9080832360734478 | validation: 0.809808330283072]
	TIME [epoch: 2.83 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9753020736660539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9753020736660539 | validation: 0.7453576136935117]
	TIME [epoch: 2.84 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9040900877529032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9040900877529032 | validation: 0.8263524997577909]
	TIME [epoch: 2.84 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9880812806546923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9880812806546923 | validation: 0.8437799873105347]
	TIME [epoch: 2.83 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017536006048087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017536006048087 | validation: 0.7504337163713761]
	TIME [epoch: 2.83 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039774044102415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9039774044102415 | validation: 0.8231315715348277]
	TIME [epoch: 2.84 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302876942588952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9302876942588952 | validation: 0.7593515538527736]
	TIME [epoch: 2.83 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985674100194885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8985674100194885 | validation: 0.7599630015512506]
	TIME [epoch: 2.82 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259406438272099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9259406438272099 | validation: 0.7506969109604922]
	TIME [epoch: 2.83 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825470972033561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8825470972033561 | validation: 0.7339195915725736]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8822353312605836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8822353312605836 | validation: 0.7315816454273469]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9059669103331035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9059669103331035 | validation: 0.7439472783921316]
	TIME [epoch: 2.83 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896571282996771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896571282996771 | validation: 0.8209660876576269]
	TIME [epoch: 2.83 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0264060916828242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0264060916828242 | validation: 0.7401213163230831]
	TIME [epoch: 2.83 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891178170535271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891178170535271 | validation: 0.7619365269670011]
	TIME [epoch: 2.83 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947771646290346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8947771646290346 | validation: 0.7520571823597252]
	TIME [epoch: 2.84 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127537442432601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127537442432601 | validation: 0.8259625823957841]
	TIME [epoch: 2.83 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0321687906346653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0321687906346653 | validation: 0.7514580464074506]
	TIME [epoch: 2.83 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015385163931494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015385163931494 | validation: 0.8335699882720228]
	TIME [epoch: 2.83 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0240547756085543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0240547756085543 | validation: 0.8102011754772944]
	TIME [epoch: 2.83 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9745260098762571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9745260098762571 | validation: 0.7897429021295819]
	TIME [epoch: 2.83 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9563805792310228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9563805792310228 | validation: 0.7630667883518859]
	TIME [epoch: 2.83 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347581752524307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9347581752524307 | validation: 0.7513272320481893]
	TIME [epoch: 2.84 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902049792334811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902049792334811 | validation: 0.743348595544091]
	TIME [epoch: 2.83 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881319928606785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8881319928606785 | validation: 0.7503418486738905]
	TIME [epoch: 2.83 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8989984533704705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989984533704705 | validation: 0.7435376573401868]
	TIME [epoch: 2.84 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191913451877507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9191913451877507 | validation: 0.7881507712162725]
	TIME [epoch: 2.84 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366582963902951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9366582963902951 | validation: 0.7390565113257548]
	TIME [epoch: 2.83 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896346671797287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.896346671797287 | validation: 0.7391210489940714]
	TIME [epoch: 2.83 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840370937160982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8840370937160982 | validation: 0.768806899129102]
	TIME [epoch: 2.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032664107099732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032664107099732 | validation: 0.7345798704158435]
	TIME [epoch: 2.83 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075472197406864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075472197406864 | validation: 0.7593234758451592]
	TIME [epoch: 2.83 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9071867465715115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9071867465715115 | validation: 0.7900944710648548]
	TIME [epoch: 2.83 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451340914679257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451340914679257 | validation: 0.8260178851580199]
	TIME [epoch: 2.84 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0353717310694774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0353717310694774 | validation: 0.7885176592655865]
	TIME [epoch: 2.84 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9234035851227245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9234035851227245 | validation: 0.7886450649641988]
	TIME [epoch: 2.84 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9418284379554899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9418284379554899 | validation: 0.822776511209044]
	TIME [epoch: 2.84 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0112925358304676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0112925358304676 | validation: 0.7576547921418162]
	TIME [epoch: 2.84 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139910445039928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139910445039928 | validation: 0.7898528308293025]
	TIME [epoch: 2.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585276981872356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9585276981872356 | validation: 0.7442283461514476]
	TIME [epoch: 2.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854621650076291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854621650076291 | validation: 0.8098339998987385]
	TIME [epoch: 2.83 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568362275436563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568362275436563 | validation: 0.7553836690315919]
	TIME [epoch: 2.83 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033202073651937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9033202073651937 | validation: 0.7361423650550961]
	TIME [epoch: 2.83 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9170654245103487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9170654245103487 | validation: 0.7791642051283827]
	TIME [epoch: 2.84 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9307864689162455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9307864689162455 | validation: 0.7440141550773444]
	TIME [epoch: 2.83 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831608376100866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831608376100866 | validation: 0.7713761097482896]
	TIME [epoch: 2.83 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391450668506878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9391450668506878 | validation: 0.734456101476175]
	TIME [epoch: 2.83 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719422873658536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8719422873658536 | validation: 0.7430929616064473]
	TIME [epoch: 2.84 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834114357886901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834114357886901 | validation: 0.7618334483085749]
	TIME [epoch: 2.83 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041081382237007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9041081382237007 | validation: 0.7473747820262133]
	TIME [epoch: 2.82 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9120939606647747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9120939606647747 | validation: 0.7478906398685039]
	TIME [epoch: 2.83 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961238085302304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8961238085302304 | validation: 0.7247361819920423]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682077430859912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8682077430859912 | validation: 0.7523549413389929]
	TIME [epoch: 2.82 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917292383510781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917292383510781 | validation: 0.8477364081072363]
	TIME [epoch: 2.83 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017472320277455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017472320277455 | validation: 0.7435251114984945]
	TIME [epoch: 2.83 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885374446959687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885374446959687 | validation: 0.7704395800456159]
	TIME [epoch: 2.82 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9401374298167864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9401374298167864 | validation: 0.7842103859524061]
	TIME [epoch: 2.82 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9306336854183158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9306336854183158 | validation: 0.776475453775977]
	TIME [epoch: 2.84 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908416184343613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908416184343613 | validation: 0.7588901948878761]
	TIME [epoch: 2.83 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9097402232900472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9097402232900472 | validation: 0.7442803590164945]
	TIME [epoch: 2.83 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998264702929129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998264702929129 | validation: 0.7305459797481021]
	TIME [epoch: 2.83 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746703263905176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746703263905176 | validation: 0.7395457402138134]
	TIME [epoch: 2.83 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8872405160419635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8872405160419635 | validation: 0.7385855348660496]
	TIME [epoch: 2.83 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823635506460678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823635506460678 | validation: 0.797269792870528]
	TIME [epoch: 2.82 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9317297154031343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9317297154031343 | validation: 0.7274414366214949]
	TIME [epoch: 2.83 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711031203991099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8711031203991099 | validation: 0.7590726583934502]
	TIME [epoch: 2.83 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831945787755848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831945787755848 | validation: 0.7299278337627716]
	TIME [epoch: 2.83 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811598034509782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811598034509782 | validation: 0.7581525923297615]
	TIME [epoch: 2.83 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892732562178006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8892732562178006 | validation: 0.7693449994077042]
	TIME [epoch: 2.83 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224918158040418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224918158040418 | validation: 0.7504612531400608]
	TIME [epoch: 2.84 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869948394380603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869948394380603 | validation: 0.726214974723074]
	TIME [epoch: 2.83 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841424464064733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841424464064733 | validation: 0.740644777695]
	TIME [epoch: 2.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8690148687372482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8690148687372482 | validation: 0.7553208584587159]
	TIME [epoch: 2.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740227451173749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740227451173749 | validation: 0.7482512261142763]
	TIME [epoch: 2.82 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869341344881375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869341344881375 | validation: 0.7496902613697531]
	TIME [epoch: 2.83 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487442677002517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9487442677002517 | validation: 0.8548794375866712]
	TIME [epoch: 2.84 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0097840006511032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0097840006511032 | validation: 0.7549519244822291]
	TIME [epoch: 2.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8954541325612215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8954541325612215 | validation: 0.7664186347615254]
	TIME [epoch: 2.83 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425267942028167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9425267942028167 | validation: 0.7433360944896572]
	TIME [epoch: 2.84 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780903881345443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780903881345443 | validation: 0.727826048589953]
	TIME [epoch: 2.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605233238883996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605233238883996 | validation: 0.716673044079759]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649381302379752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649381302379752 | validation: 0.7553530513601509]
	TIME [epoch: 2.85 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919586968112794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8919586968112794 | validation: 0.7345094321843508]
	TIME [epoch: 2.84 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855820767009007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8855820767009007 | validation: 0.7256960696782992]
	TIME [epoch: 2.84 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613479935869341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8613479935869341 | validation: 0.7140113100028963]
	TIME [epoch: 190 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698752683576828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8698752683576828 | validation: 0.7696287321411507]
	TIME [epoch: 6.07 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191535472645653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9191535472645653 | validation: 0.878400214983373]
	TIME [epoch: 6.06 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.090365722574841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.090365722574841 | validation: 0.820030704799701]
	TIME [epoch: 6.06 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9861584783053993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9861584783053993 | validation: 0.760594431719089]
	TIME [epoch: 6.05 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9104374001688182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9104374001688182 | validation: 0.7420175723514135]
	TIME [epoch: 6.05 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958372271616998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958372271616998 | validation: 0.7645724690089934]
	TIME [epoch: 6.06 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870219924156872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8870219924156872 | validation: 0.7624722853336499]
	TIME [epoch: 6.06 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8941982014237456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8941982014237456 | validation: 0.7528482906716487]
	TIME [epoch: 6.06 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725374044918838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8725374044918838 | validation: 0.7326519874778641]
	TIME [epoch: 6.06 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605312921817132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605312921817132 | validation: 0.7293679436305438]
	TIME [epoch: 6.06 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646590655733313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646590655733313 | validation: 0.7263026102697059]
	TIME [epoch: 6.07 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585574624922154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8585574624922154 | validation: 0.7688951231142008]
	TIME [epoch: 6.07 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746231805168493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746231805168493 | validation: 0.7409973477217243]
	TIME [epoch: 6.06 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758895703989787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758895703989787 | validation: 0.821140779499226]
	TIME [epoch: 6.07 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9341824294901645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9341824294901645 | validation: 0.794932906054629]
	TIME [epoch: 6.07 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9306280052295965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9306280052295965 | validation: 0.728439074211461]
	TIME [epoch: 6.08 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543382618888601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8543382618888601 | validation: 0.7243578801939076]
	TIME [epoch: 6.08 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569700803848584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8569700803848584 | validation: 0.7293962967060076]
	TIME [epoch: 6.08 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639238982778301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639238982778301 | validation: 0.7219486068631247]
	TIME [epoch: 6.09 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885864807357433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885864807357433 | validation: 0.7212008573291039]
	TIME [epoch: 6.09 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520064489985196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520064489985196 | validation: 0.7381701875944147]
	TIME [epoch: 6.09 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738660168957819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738660168957819 | validation: 0.7144449042495852]
	TIME [epoch: 6.09 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804308291788356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8804308291788356 | validation: 0.6933539369395371]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453139420668637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453139420668637 | validation: 0.8070340636215212]
	TIME [epoch: 6.08 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127680741944264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127680741944264 | validation: 0.9306373244317654]
	TIME [epoch: 6.08 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630942621585604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0630942621585604 | validation: 0.7672982165033408]
	TIME [epoch: 6.11 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919050635142827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8919050635142827 | validation: 0.7258607737747889]
	TIME [epoch: 6.09 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461616014842369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461616014842369 | validation: 0.7198247888879484]
	TIME [epoch: 6.09 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8718371515910667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8718371515910667 | validation: 0.7196568798047673]
	TIME [epoch: 6.11 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572076005301034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572076005301034 | validation: 0.7036689352620802]
	TIME [epoch: 6.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448756960649538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448756960649538 | validation: 0.7011011849798627]
	TIME [epoch: 6.11 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8523085402984333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8523085402984333 | validation: 0.743049757343525]
	TIME [epoch: 6.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576008408442115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8576008408442115 | validation: 0.7009304564589313]
	TIME [epoch: 6.08 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332531886444241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8332531886444241 | validation: 0.7103663711481258]
	TIME [epoch: 6.09 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229512274304536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229512274304536 | validation: 0.6802699650923327]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963396381077479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963396381077479 | validation: 0.7174110441031538]
	TIME [epoch: 6.09 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606127814383937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606127814383937 | validation: 1.4366759845798445]
	TIME [epoch: 6.07 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.670038247158064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.670038247158064 | validation: 0.708813814724226]
	TIME [epoch: 6.06 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9464511995771124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9464511995771124 | validation: 0.8348901560819004]
	TIME [epoch: 6.08 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9855319570239377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9855319570239377 | validation: 0.764553288345826]
	TIME [epoch: 6.08 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943027436136011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943027436136011 | validation: 0.7253673887814379]
	TIME [epoch: 6.08 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8941567749736161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8941567749736161 | validation: 0.6805192339435799]
	TIME [epoch: 6.07 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937907845693149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937907845693149 | validation: 0.6995382308043032]
	TIME [epoch: 6.07 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552181538094605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8552181538094605 | validation: 0.7377498290775388]
	TIME [epoch: 6.07 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866285313733325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.866285313733325 | validation: 0.6940549461273038]
	TIME [epoch: 6.07 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597621684776001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597621684776001 | validation: 0.7081517371143461]
	TIME [epoch: 6.08 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470359460189373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470359460189373 | validation: 0.6936995331044106]
	TIME [epoch: 6.07 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391277853369496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8391277853369496 | validation: 0.6952511000296753]
	TIME [epoch: 6.06 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428895690801628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8428895690801628 | validation: 0.6994082592608791]
	TIME [epoch: 6.07 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389012494066447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389012494066447 | validation: 0.6963914421163918]
	TIME [epoch: 6.08 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8217294901109505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8217294901109505 | validation: 0.6968369581978696]
	TIME [epoch: 6.08 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8285466374009497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8285466374009497 | validation: 0.7307728936627982]
	TIME [epoch: 6.08 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8503858106263646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8503858106263646 | validation: 0.8806930941735657]
	TIME [epoch: 6.08 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9565690631699828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9565690631699828 | validation: 1.4904273886601325]
	TIME [epoch: 6.09 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4372395955420194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4372395955420194 | validation: 0.8976463206314865]
	TIME [epoch: 6.09 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439167447098817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1439167447098817 | validation: 0.740853249697663]
	TIME [epoch: 6.07 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9441646627159077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9441646627159077 | validation: 0.75971858512469]
	TIME [epoch: 6.07 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934148350853011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934148350853011 | validation: 0.74420248368656]
	TIME [epoch: 6.07 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875670774535857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875670774535857 | validation: 0.7047203946635261]
	TIME [epoch: 6.07 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554247814494045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8554247814494045 | validation: 0.7173093508800431]
	TIME [epoch: 6.08 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8619089539067474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8619089539067474 | validation: 0.6899207376672103]
	TIME [epoch: 6.09 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574717231810587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8574717231810587 | validation: 0.6933357876549265]
	TIME [epoch: 6.07 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338398864679346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8338398864679346 | validation: 0.712257658629456]
	TIME [epoch: 6.08 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602684954212774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8602684954212774 | validation: 0.6996218579236925]
	TIME [epoch: 6.08 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372648094470713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372648094470713 | validation: 0.6989179479949758]
	TIME [epoch: 6.09 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545381880774682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545381880774682 | validation: 0.6859275437937364]
	TIME [epoch: 6.08 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8343858825601518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8343858825601518 | validation: 0.6960559468478597]
	TIME [epoch: 6.08 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377607204988894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377607204988894 | validation: 0.6841491061232524]
	TIME [epoch: 6.09 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8282172913353051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8282172913353051 | validation: 0.6893889207989367]
	TIME [epoch: 6.09 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237169034829402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8237169034829402 | validation: 0.6916584597874531]
	TIME [epoch: 6.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177362080872896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177362080872896 | validation: 0.6841688008280583]
	TIME [epoch: 6.1 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094355853124762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8094355853124762 | validation: 0.6742255245839307]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076202529462149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076202529462149 | validation: 0.6675860646857992]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840046499228563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7840046499228563 | validation: 0.6609792298192785]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690330812531595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690330812531595 | validation: 0.6420725562353686]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310661883127878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310661883127878 | validation: 1.9741863602032195]
	TIME [epoch: 6.09 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0862538923906517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0862538923906517 | validation: 1.2216976761363958]
	TIME [epoch: 6.09 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3434794845900553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3434794845900553 | validation: 0.7913552160218679]
	TIME [epoch: 6.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545830280916917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545830280916917 | validation: 0.8212134985042487]
	TIME [epoch: 6.09 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580577604960273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580577604960273 | validation: 0.7147470092247028]
	TIME [epoch: 6.08 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848002985758665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848002985758665 | validation: 0.734809950653077]
	TIME [epoch: 6.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259305156775526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8259305156775526 | validation: 0.6900543607666614]
	TIME [epoch: 6.08 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783579267675745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.783579267675745 | validation: 0.6843998570076553]
	TIME [epoch: 6.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787756506995193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787756506995193 | validation: 0.6454439264172352]
	TIME [epoch: 6.09 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451046942103983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451046942103983 | validation: 0.6396020107407188]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7322273161934655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322273161934655 | validation: 0.6907555686476331]
	TIME [epoch: 6.09 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120279468249872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7120279468249872 | validation: 0.7034605285850816]
	TIME [epoch: 6.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418127201074143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7418127201074143 | validation: 0.9783702666328238]
	TIME [epoch: 6.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9767840102505297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9767840102505297 | validation: 1.0343118089432906]
	TIME [epoch: 6.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1219954973988213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1219954973988213 | validation: 0.7285006275389958]
	TIME [epoch: 6.09 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807179410865261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807179410865261 | validation: 0.7517168421461049]
	TIME [epoch: 6.08 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640796421203775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8640796421203775 | validation: 0.6588926419831704]
	TIME [epoch: 6.08 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768366236211516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768366236211516 | validation: 0.7011128154032503]
	TIME [epoch: 6.09 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499892965510192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499892965510192 | validation: 0.6576649363871874]
	TIME [epoch: 6.09 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927787549167829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927787549167829 | validation: 0.6059423728978991]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377781690635166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6377781690635166 | validation: 0.6624023542389352]
	TIME [epoch: 6.07 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657332918378232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657332918378232 | validation: 0.8385618005828038]
	TIME [epoch: 6.08 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8941141315113937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8941141315113937 | validation: 0.7742449356780873]
	TIME [epoch: 6.08 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965073049436382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965073049436382 | validation: 0.71160590052484]
	TIME [epoch: 6.09 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606644116742394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606644116742394 | validation: 0.6144528018703336]
	TIME [epoch: 6.09 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763835237656093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6763835237656093 | validation: 0.6247933792257337]
	TIME [epoch: 6.06 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6619470785876357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6619470785876357 | validation: 0.6431910159557819]
	TIME [epoch: 6.07 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678056204299158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6678056204299158 | validation: 1.1221264117199996]
	TIME [epoch: 6.07 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1080589464865176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1080589464865176 | validation: 1.002916192174842]
	TIME [epoch: 6.08 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1055151602578335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1055151602578335 | validation: 0.7233863689476654]
	TIME [epoch: 6.08 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960651075364451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960651075364451 | validation: 0.7390360067939895]
	TIME [epoch: 6.07 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551503194005531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551503194005531 | validation: 0.6328750530799725]
	TIME [epoch: 6.07 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457171191462243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457171191462243 | validation: 0.6604684446563729]
	TIME [epoch: 6.07 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213212000115788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7213212000115788 | validation: 0.6074878968787387]
	TIME [epoch: 6.08 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574382862630529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6574382862630529 | validation: 0.6438654046181237]
	TIME [epoch: 6.08 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504870782719431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6504870782719431 | validation: 0.690428939114223]
	TIME [epoch: 6.07 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209566008867069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7209566008867069 | validation: 0.7970010103150085]
	TIME [epoch: 6.09 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7825547621454491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7825547621454491 | validation: 0.8184026828174309]
	TIME [epoch: 6.08 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593996524406822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8593996524406822 | validation: 0.6069893146842382]
	TIME [epoch: 6.09 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620162081743844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620162081743844 | validation: 0.6366319282847934]
	TIME [epoch: 6.09 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886410114568601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886410114568601 | validation: 0.5916067527608344]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214949837481715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6214949837481715 | validation: 0.6336229530613677]
	TIME [epoch: 6.06 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6090200957388998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6090200957388998 | validation: 0.6673945859744015]
	TIME [epoch: 6.06 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672717800158996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672717800158996 | validation: 0.8187963545685364]
	TIME [epoch: 6.08 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998802926807059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998802926807059 | validation: 0.8256189293857862]
	TIME [epoch: 6.07 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890775593253494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8890775593253494 | validation: 0.6461697504080641]
	TIME [epoch: 6.08 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6930850741677154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6930850741677154 | validation: 0.642794911738062]
	TIME [epoch: 6.11 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379391092728793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379391092728793 | validation: 0.5695103938094042]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6127399302399582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6127399302399582 | validation: 0.5611174602418848]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421524493971742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5421524493971742 | validation: 0.567184062488511]
	TIME [epoch: 6.08 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480861617832505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480861617832505 | validation: 0.5358117010504078]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5552258594787187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5552258594787187 | validation: 0.812705634698752]
	TIME [epoch: 6.07 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962482956583001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962482956583001 | validation: 0.8765398069636433]
	TIME [epoch: 6.08 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537006716722624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9537006716722624 | validation: 0.6046638092319983]
	TIME [epoch: 6.07 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912633308472204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912633308472204 | validation: 0.6135898054795136]
	TIME [epoch: 6.06 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992501706321831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6992501706321831 | validation: 0.542384496006262]
	TIME [epoch: 6.07 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534509585065783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5534509585065783 | validation: 0.6544534433749338]
	TIME [epoch: 6.07 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104502410489385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6104502410489385 | validation: 0.7980279826773092]
	TIME [epoch: 6.06 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852438910167048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852438910167048 | validation: 0.51077709266039]
	TIME [epoch: 6.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5676682191927362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5676682191927362 | validation: 0.5747889157771073]
	TIME [epoch: 6.06 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6168708681573147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6168708681573147 | validation: 0.6651440403364636]
	TIME [epoch: 6.07 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6674497620501589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6674497620501589 | validation: 0.5395458070670348]
	TIME [epoch: 6.06 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5549054829846444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5549054829846444 | validation: 0.5810906992989884]
	TIME [epoch: 6.06 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232832140520823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6232832140520823 | validation: 0.5294178692098317]
	TIME [epoch: 6.06 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5380978565732758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5380978565732758 | validation: 0.4726639894830532]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49625744449447307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49625744449447307 | validation: 0.5516081701487812]
	TIME [epoch: 6.05 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5284915718677254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5284915718677254 | validation: 0.7631444589976827]
	TIME [epoch: 6.05 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290851625152054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8290851625152054 | validation: 0.5394432194321209]
	TIME [epoch: 6.07 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312211674736582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6312211674736582 | validation: 0.4850636172885109]
	TIME [epoch: 6.05 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316515510351291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5316515510351291 | validation: 0.5886572610339961]
	TIME [epoch: 6.06 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587296380308288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587296380308288 | validation: 1.0757099177667955]
	TIME [epoch: 6.07 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9860326734734716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9860326734734716 | validation: 0.7673685168356242]
	TIME [epoch: 6.06 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760287394586446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760287394586446 | validation: 0.6086181387421435]
	TIME [epoch: 6.06 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181783540679914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181783540679914 | validation: 0.6312459825052656]
	TIME [epoch: 6.07 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7121814934814213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7121814934814213 | validation: 0.5393856273744854]
	TIME [epoch: 6.06 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619098050521809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619098050521809 | validation: 0.45573467832756676]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5049166749772946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5049166749772946 | validation: 0.5368121515885547]
	TIME [epoch: 6.07 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179662591570184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179662591570184 | validation: 0.6083013983198827]
	TIME [epoch: 6.06 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592685529541501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592685529541501 | validation: 0.7338599973633418]
	TIME [epoch: 6.07 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781291026728791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781291026728791 | validation: 0.6439236527555988]
	TIME [epoch: 6.07 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6890301261429853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6890301261429853 | validation: 0.4584022188181216]
	TIME [epoch: 6.09 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48172329033513905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48172329033513905 | validation: 0.41260806435238423]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46030684028062746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46030684028062746 | validation: 0.46486271798451995]
	TIME [epoch: 6.08 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48709919984700945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48709919984700945 | validation: 0.7337842264607919]
	TIME [epoch: 6.08 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701508045206951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701508045206951 | validation: 0.7297845821502943]
	TIME [epoch: 6.08 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781857762798594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.781857762798594 | validation: 0.4596004632987617]
	TIME [epoch: 6.09 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48990603455471216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48990603455471216 | validation: 0.49958446748017293]
	TIME [epoch: 6.08 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505921643606819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.505921643606819 | validation: 0.5194212946483788]
	TIME [epoch: 6.07 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5451033177704188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5451033177704188 | validation: 0.5810522758276758]
	TIME [epoch: 6.09 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5214510859276151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5214510859276151 | validation: 0.5767878994011737]
	TIME [epoch: 6.08 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284800990550555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284800990550555 | validation: 0.48755608094127806]
	TIME [epoch: 6.08 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4729233665588124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4729233665588124 | validation: 0.36460542156610926]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3884192881980106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3884192881980106 | validation: 0.39538552861249876]
	TIME [epoch: 6.13 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3742307700765671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3742307700765671 | validation: 0.3351579318088819]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35693543469911765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35693543469911765 | validation: 0.4255382670515636]
	TIME [epoch: 6.08 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40330089485841814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40330089485841814 | validation: 0.634519132204159]
	TIME [epoch: 6.08 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058924778268977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058924778268977 | validation: 0.44158745186131887]
	TIME [epoch: 6.09 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42337161685327224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42337161685327224 | validation: 0.3570478775443867]
	TIME [epoch: 6.09 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37692440564786966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37692440564786966 | validation: 0.6727066603350598]
	TIME [epoch: 6.08 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043011378363773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043011378363773 | validation: 0.7645342223521845]
	TIME [epoch: 6.09 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074411593815161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074411593815161 | validation: 0.5895085085411135]
	TIME [epoch: 6.08 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789290502044956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.789290502044956 | validation: 0.6887799596807638]
	TIME [epoch: 6.11 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002340752477974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002340752477974 | validation: 0.4740355608579385]
	TIME [epoch: 6.13 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5781284862882872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5781284862882872 | validation: 0.40850913141486256]
	TIME [epoch: 6.12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560791122034135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4560791122034135 | validation: 0.6635598989363136]
	TIME [epoch: 6.13 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919984634091415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5919984634091415 | validation: 0.6636802753508034]
	TIME [epoch: 6.11 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761122242619598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7761122242619598 | validation: 0.3724747256274805]
	TIME [epoch: 6.12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941608545166757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3941608545166757 | validation: 0.696432755642453]
	TIME [epoch: 6.12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430651597673318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6430651597673318 | validation: 0.4064402447767821]
	TIME [epoch: 6.11 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45283868340626726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45283868340626726 | validation: 0.30837600526907627]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3271578919179882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3271578919179882 | validation: 0.3667290132560494]
	TIME [epoch: 6.09 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335326041500815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.335326041500815 | validation: 0.34905857781193905]
	TIME [epoch: 6.09 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39231310321138885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39231310321138885 | validation: 0.5798990591060978]
	TIME [epoch: 6.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4825933248931989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4825933248931989 | validation: 0.479906651561725]
	TIME [epoch: 6.11 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5450383425540445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5450383425540445 | validation: 0.3248468316090274]
	TIME [epoch: 6.12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41772017900294617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41772017900294617 | validation: 0.42127196067643446]
	TIME [epoch: 6.12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3516117083348855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3516117083348855 | validation: 0.474249447772217]
	TIME [epoch: 6.12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49992937295095213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49992937295095213 | validation: 0.47033025997783257]
	TIME [epoch: 6.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239174800223109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4239174800223109 | validation: 0.42927930075547444]
	TIME [epoch: 6.09 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4852486815327106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4852486815327106 | validation: 0.38641223874270314]
	TIME [epoch: 6.09 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3572987737504207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3572987737504207 | validation: 0.26985027680457957]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3104529914690636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3104529914690636 | validation: 0.4315523604002298]
	TIME [epoch: 6.07 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33552233355541505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33552233355541505 | validation: 0.2873885130023453]
	TIME [epoch: 6.08 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3860528838817336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3860528838817336 | validation: 0.6238287272467153]
	TIME [epoch: 6.08 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5087760347117346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5087760347117346 | validation: 0.47797183662808235]
	TIME [epoch: 6.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5697319963550431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5697319963550431 | validation: 0.2662916760395006]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3562239444657499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3562239444657499 | validation: 0.6680557101161761]
	TIME [epoch: 6.08 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5577713473609694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5577713473609694 | validation: 0.5729626095865966]
	TIME [epoch: 6.07 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588327675816676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588327675816676 | validation: 0.33760779845220196]
	TIME [epoch: 6.09 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40434136358650646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40434136358650646 | validation: 0.5928659530622642]
	TIME [epoch: 6.07 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5541773490968898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5541773490968898 | validation: 0.2940218231658832]
	TIME [epoch: 6.09 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3286716744586857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3286716744586857 | validation: 0.2977557776248971]
	TIME [epoch: 6.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2686838140610317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2686838140610317 | validation: 0.23094762517665282]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27979026707625104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27979026707625104 | validation: 0.374411513479733]
	TIME [epoch: 6.08 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29611679847373257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29611679847373257 | validation: 0.28245298166305466]
	TIME [epoch: 6.09 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.342901582107147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.342901582107147 | validation: 0.4247453806657586]
	TIME [epoch: 6.09 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34391845670444526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34391845670444526 | validation: 0.3681221491813562]
	TIME [epoch: 6.09 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43122798482281427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43122798482281427 | validation: 0.23202911975449886]
	TIME [epoch: 6.07 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2672226390003566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2672226390003566 | validation: 0.4095327113173635]
	TIME [epoch: 6.1 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3037261591642562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3037261591642562 | validation: 0.40667582806030067]
	TIME [epoch: 6.09 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4806902822212877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4806902822212877 | validation: 0.30450960143878564]
	TIME [epoch: 6.09 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26380487107678324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26380487107678324 | validation: 0.24827419446601287]
	TIME [epoch: 6.09 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26682917342818424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26682917342818424 | validation: 0.41866758303351825]
	TIME [epoch: 6.08 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3633399279626858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3633399279626858 | validation: 0.43686332248995]
	TIME [epoch: 6.09 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372741134432384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5372741134432384 | validation: 0.23682259146134657]
	TIME [epoch: 6.09 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28901810430898983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28901810430898983 | validation: 0.49192422281271253]
	TIME [epoch: 6.07 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3641719719176787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3641719719176787 | validation: 0.46698511228516026]
	TIME [epoch: 6.08 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5559676729958346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5559676729958346 | validation: 0.44168862663219993]
	TIME [epoch: 6.07 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31526504930667776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31526504930667776 | validation: 0.23085425985641025]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24288289321739967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24288289321739967 | validation: 0.2599400746816101]
	TIME [epoch: 6.06 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983545859477404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2983545859477404 | validation: 0.4572827309849672]
	TIME [epoch: 6.06 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3770650988370871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3770650988370871 | validation: 0.3412083108662442]
	TIME [epoch: 6.08 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42434979055615446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42434979055615446 | validation: 0.237473271305804]
	TIME [epoch: 6.08 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24646114992143162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24646114992143162 | validation: 0.36085254703836567]
	TIME [epoch: 6.08 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28924444585903225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28924444585903225 | validation: 0.4718199399482842]
	TIME [epoch: 6.08 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5224385205027177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5224385205027177 | validation: 0.27088430224454546]
	TIME [epoch: 6.07 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22290887962875733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22290887962875733 | validation: 0.22005287636622053]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687455816296386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2687455816296386 | validation: 0.42334948551140195]
	TIME [epoch: 6.08 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488621173207592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3488621173207592 | validation: 0.25614570109272233]
	TIME [epoch: 6.08 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29204636539076817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29204636539076817 | validation: 0.21982271465643702]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2154984349075476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2154984349075476 | validation: 0.40558419004913215]
	TIME [epoch: 6.08 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30096477469224125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30096477469224125 | validation: 0.45870175043398764]
	TIME [epoch: 6.08 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6009949465034159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6009949465034159 | validation: 0.5966811944264866]
	TIME [epoch: 6.09 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4394102520219261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4394102520219261 | validation: 0.34478704778931263]
	TIME [epoch: 6.08 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2521703734153927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2521703734153927 | validation: 0.483952084449183]
	TIME [epoch: 6.09 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224542846333877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224542846333877 | validation: 0.21144927721141418]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21150351955073265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21150351955073265 | validation: 0.4690612883388037]
	TIME [epoch: 6.08 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36402287479608963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36402287479608963 | validation: 0.4476489427068298]
	TIME [epoch: 6.1 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5212120084958953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5212120084958953 | validation: 0.20944526692346283]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23607311463266162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23607311463266162 | validation: 0.5126286592929908]
	TIME [epoch: 6.08 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40600696180541834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40600696180541834 | validation: 0.4281514357559525]
	TIME [epoch: 6.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.482487529281813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482487529281813 | validation: 0.20230604588823056]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23689498370822967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23689498370822967 | validation: 0.5312859616942432]
	TIME [epoch: 6.08 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45081983843239853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45081983843239853 | validation: 0.34814967023242166]
	TIME [epoch: 6.09 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38220031904669044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38220031904669044 | validation: 0.2324092762805127]
	TIME [epoch: 6.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2079769609819765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2079769609819765 | validation: 0.2036224031121604]
	TIME [epoch: 6.09 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2503403570271748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2503403570271748 | validation: 0.2756140596166356]
	TIME [epoch: 6.09 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2615059474252726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2615059474252726 | validation: 0.22063609753279054]
	TIME [epoch: 6.1 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24072824721569733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24072824721569733 | validation: 0.23898435117737132]
	TIME [epoch: 6.09 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830368128649283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830368128649283 | validation: 0.4839567470541353]
	TIME [epoch: 6.07 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3591959521673793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3591959521673793 | validation: 0.30211807982645544]
	TIME [epoch: 6.09 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4115389022404192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4115389022404192 | validation: 0.23652260633581582]
	TIME [epoch: 6.08 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23774526919384875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23774526919384875 | validation: 0.31282319501074846]
	TIME [epoch: 6.09 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25268127273455854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25268127273455854 | validation: 0.41049043692646453]
	TIME [epoch: 6.08 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4682366325022075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4682366325022075 | validation: 0.2819362569254807]
	TIME [epoch: 6.08 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562579235766784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2562579235766784 | validation: 0.22183214460772255]
	TIME [epoch: 6.08 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24000255482310154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24000255482310154 | validation: 0.3213575472635943]
	TIME [epoch: 6.08 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752608362906797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2752608362906797 | validation: 0.2796019317781359]
	TIME [epoch: 6.08 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488728274550925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3488728274550925 | validation: 0.33870375776265305]
	TIME [epoch: 6.07 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24309530172695046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24309530172695046 | validation: 0.1808082334486311]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24677141260562938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24677141260562938 | validation: 0.4168627290290685]
	TIME [epoch: 6.06 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2842085317762766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2842085317762766 | validation: 0.21114388478492688]
	TIME [epoch: 6.06 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27380426544817477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27380426544817477 | validation: 0.358622941288244]
	TIME [epoch: 6.07 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2793489899146617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2793489899146617 | validation: 0.31335907150654596]
	TIME [epoch: 6.07 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34405213989519395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34405213989519395 | validation: 0.2644586152565429]
	TIME [epoch: 6.07 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27965127822379743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27965127822379743 | validation: 0.27734107139966363]
	TIME [epoch: 6.07 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26441480069522955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26441480069522955 | validation: 0.3355100154464354]
	TIME [epoch: 6.07 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2741515399302767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2741515399302767 | validation: 0.3014342342933559]
	TIME [epoch: 6.07 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753876902138699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3753876902138699 | validation: 0.39401486411589337]
	TIME [epoch: 6.07 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905706252516228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2905706252516228 | validation: 0.18338450302123738]
	TIME [epoch: 6.08 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23755587234539363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23755587234539363 | validation: 0.24898367276459404]
	TIME [epoch: 6.06 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19560660650320474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19560660650320474 | validation: 0.1418751113397689]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1904377407430855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1904377407430855 | validation: 0.3511939702486348]
	TIME [epoch: 6.07 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26126567481142304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26126567481142304 | validation: 0.3691814476299802]
	TIME [epoch: 6.09 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365587540179906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4365587540179906 | validation: 0.22359182967183544]
	TIME [epoch: 6.07 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19573173331975519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19573173331975519 | validation: 0.38476346942188866]
	TIME [epoch: 6.06 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3409751843004855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3409751843004855 | validation: 0.47995404677477344]
	TIME [epoch: 6.08 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5510464076399836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5510464076399836 | validation: 0.33352645087058064]
	TIME [epoch: 6.08 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3035564936558193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3035564936558193 | validation: 0.2761341639829456]
	TIME [epoch: 6.06 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3107619675236047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3107619675236047 | validation: 0.29133909767371313]
	TIME [epoch: 6.07 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25791251784670893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25791251784670893 | validation: 0.30135877971405456]
	TIME [epoch: 6.06 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23330900761849527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23330900761849527 | validation: 0.2952244145669802]
	TIME [epoch: 6.08 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4254804866884335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4254804866884335 | validation: 0.24168266185423756]
	TIME [epoch: 6.06 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21434129857473103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21434129857473103 | validation: 0.17658482908936024]
	TIME [epoch: 6.06 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18032853487788933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18032853487788933 | validation: 0.27333707110325867]
	TIME [epoch: 6.07 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24637215110869298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24637215110869298 | validation: 0.406962358269503]
	TIME [epoch: 6.06 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48988516200547116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48988516200547116 | validation: 0.5162863317719664]
	TIME [epoch: 6.07 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780150691807975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3780150691807975 | validation: 0.19150186264364133]
	TIME [epoch: 6.08 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22450025477075325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22450025477075325 | validation: 0.23724529545858497]
	TIME [epoch: 6.07 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22868788852128683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22868788852128683 | validation: 0.4443630215755332]
	TIME [epoch: 6.08 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34423705285241585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34423705285241585 | validation: 0.30991620674304743]
	TIME [epoch: 6.07 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42931262662100894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42931262662100894 | validation: 0.2752408326305652]
	TIME [epoch: 6.09 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30864859475248435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30864859475248435 | validation: 0.25124350028046266]
	TIME [epoch: 6.08 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21248432639240214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21248432639240214 | validation: 0.300205706183994]
	TIME [epoch: 6.08 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3307005466610343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307005466610343 | validation: 0.37115802569207057]
	TIME [epoch: 198 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26116130536363663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26116130536363663 | validation: 0.22514775597333891]
	TIME [epoch: 12.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.287730831249345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.287730831249345 | validation: 0.2674351157123813]
	TIME [epoch: 12.9 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23723431640717454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23723431640717454 | validation: 0.20489940913465032]
	TIME [epoch: 12.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20527732209345467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20527732209345467 | validation: 0.32439087088857893]
	TIME [epoch: 12.9 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771542908227771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2771542908227771 | validation: 0.3451330541323067]
	TIME [epoch: 12.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.387292833788447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.387292833788447 | validation: 0.27882676992470906]
	TIME [epoch: 12.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21558328509744368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21558328509744368 | validation: 0.15163470988005529]
	TIME [epoch: 12.9 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18500657419501132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18500657419501132 | validation: 0.3342316424554914]
	TIME [epoch: 12.9 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22389236594130146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22389236594130146 | validation: 0.22181717479312787]
	TIME [epoch: 12.9 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3242130508643833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3242130508643833 | validation: 0.37695325832126886]
	TIME [epoch: 12.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27169947114707127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27169947114707127 | validation: 0.2835058282099605]
	TIME [epoch: 12.9 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032564690455806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032564690455806 | validation: 0.2827976937641585]
	TIME [epoch: 12.9 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713298287224775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2713298287224775 | validation: 0.27599299555732615]
	TIME [epoch: 12.9 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30314559658188317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30314559658188317 | validation: 0.26331860278757796]
	TIME [epoch: 12.9 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19733642551277966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19733642551277966 | validation: 0.17040614408001284]
	TIME [epoch: 12.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20796720879753422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20796720879753422 | validation: 0.2439331210241231]
	TIME [epoch: 12.9 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18706322208040804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18706322208040804 | validation: 0.13736279262196854]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21073643892191685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21073643892191685 | validation: 0.5968548392423475]
	TIME [epoch: 12.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45585557235193136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45585557235193136 | validation: 0.3629889219114031]
	TIME [epoch: 12.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4491710786811325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4491710786811325 | validation: 0.25738967100573623]
	TIME [epoch: 12.9 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31485366262399866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31485366262399866 | validation: 0.3946725019521964]
	TIME [epoch: 12.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27843327619949215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27843327619949215 | validation: 0.2904563823803906]
	TIME [epoch: 12.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121153823956117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3121153823956117 | validation: 0.2541647974978993]
	TIME [epoch: 12.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21105185614865993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21105185614865993 | validation: 0.22751466329375383]
	TIME [epoch: 12.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586023268381521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2586023268381521 | validation: 0.24650259625338308]
	TIME [epoch: 12.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2368128208489537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2368128208489537 | validation: 0.17078837833925253]
	TIME [epoch: 12.9 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19363986083720292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19363986083720292 | validation: 0.4367399259481056]
	TIME [epoch: 12.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30654139900748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30654139900748 | validation: 0.26087447375053446]
	TIME [epoch: 12.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33433836068774997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33433836068774997 | validation: 0.24907965285178177]
	TIME [epoch: 12.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19828036584079844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19828036584079844 | validation: 0.1507481488040084]
	TIME [epoch: 12.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17504562395160528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17504562395160528 | validation: 0.17436212721491706]
	TIME [epoch: 12.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582589350358942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582589350358942 | validation: 0.1941317842283205]
	TIME [epoch: 12.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19029119779310413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19029119779310413 | validation: 0.2857142321126677]
	TIME [epoch: 12.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30497075015218444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30497075015218444 | validation: 0.3517351689079966]
	TIME [epoch: 12.9 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847541218659542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2847541218659542 | validation: 0.23208495934164947]
	TIME [epoch: 12.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904868156713087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2904868156713087 | validation: 0.3439080226143064]
	TIME [epoch: 12.9 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.246142032088796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.246142032088796 | validation: 0.1672967924700096]
	TIME [epoch: 12.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23784484723673793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23784484723673793 | validation: 0.40301672986691595]
	TIME [epoch: 12.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2799877302414329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2799877302414329 | validation: 0.23397175335198736]
	TIME [epoch: 12.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31134012253648186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31134012253648186 | validation: 0.20830703017899915]
	TIME [epoch: 12.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20134443506037467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20134443506037467 | validation: 0.16189344540970577]
	TIME [epoch: 12.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1587909962537735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1587909962537735 | validation: 0.3060666685972551]
	TIME [epoch: 12.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21755370102310145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21755370102310145 | validation: 0.29792475102799315]
	TIME [epoch: 12.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771910272880669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3771910272880669 | validation: 0.21627486459634016]
	TIME [epoch: 12.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20626456909231522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20626456909231522 | validation: 0.21052897437857176]
	TIME [epoch: 12.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1874070265913594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1874070265913594 | validation: 0.2574026517739162]
	TIME [epoch: 12.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29144805667755924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29144805667755924 | validation: 0.3384630272017729]
	TIME [epoch: 12.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2907501697623311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2907501697623311 | validation: 0.17317390267863056]
	TIME [epoch: 12.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22930605393077913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22930605393077913 | validation: 0.34930802641769676]
	TIME [epoch: 12.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23135872324422246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23135872324422246 | validation: 0.18588752579067563]
	TIME [epoch: 12.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24357802461800218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24357802461800218 | validation: 0.2300079678878536]
	TIME [epoch: 12.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17605911027659552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17605911027659552 | validation: 0.1954948873570348]
	TIME [epoch: 12.9 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2103044312374997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2103044312374997 | validation: 0.31302388881035514]
	TIME [epoch: 12.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25503437284466574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25503437284466574 | validation: 0.2753299484064244]
	TIME [epoch: 12.9 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2753665132196844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2753665132196844 | validation: 0.1850136435929283]
	TIME [epoch: 12.9 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638685137226545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638685137226545 | validation: 0.13460716469008518]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731686323559127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1731686323559127 | validation: 0.42955027855966005]
	TIME [epoch: 12.9 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771036571990704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2771036571990704 | validation: 0.21179627658494976]
	TIME [epoch: 12.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30183733358350073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30183733358350073 | validation: 0.19717116976294427]
	TIME [epoch: 12.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17144846963780438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17144846963780438 | validation: 0.19196082467475795]
	TIME [epoch: 12.9 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205564636311477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.205564636311477 | validation: 0.33811470609473326]
	TIME [epoch: 12.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27721939360198816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27721939360198816 | validation: 0.26155229793682616]
	TIME [epoch: 12.9 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121637568202033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3121637568202033 | validation: 0.262640539782244]
	TIME [epoch: 12.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20264055603677913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20264055603677913 | validation: 0.22591800709953952]
	TIME [epoch: 12.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25469949388422347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25469949388422347 | validation: 0.22211709525552542]
	TIME [epoch: 12.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20543896095561087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20543896095561087 | validation: 0.3036644940763827]
	TIME [epoch: 12.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23550336278779965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23550336278779965 | validation: 0.22812765097776067]
	TIME [epoch: 12.9 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937282730077864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2937282730077864 | validation: 0.3896747188480156]
	TIME [epoch: 12.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24938009806725028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24938009806725028 | validation: 0.12002597450785353]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17492076685302352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17492076685302352 | validation: 0.2226043368671753]
	TIME [epoch: 12.9 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18471986378745633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18471986378745633 | validation: 0.2449224347868193]
	TIME [epoch: 12.9 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625216465528961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625216465528961 | validation: 0.3126085935691123]
	TIME [epoch: 12.9 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282301752788546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2282301752788546 | validation: 0.17988304808315103]
	TIME [epoch: 12.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21538927540275804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21538927540275804 | validation: 0.226973845951313]
	TIME [epoch: 12.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16804541700226222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16804541700226222 | validation: 0.168327590201159]
	TIME [epoch: 12.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19379761758193043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19379761758193043 | validation: 0.33999344000280857]
	TIME [epoch: 12.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23241184828191913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23241184828191913 | validation: 0.18958779437337886]
	TIME [epoch: 12.9 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25726239504599624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25726239504599624 | validation: 0.26206073422446596]
	TIME [epoch: 12.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17485614138807237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17485614138807237 | validation: 0.12042327344544229]
	TIME [epoch: 12.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15888040513896953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15888040513896953 | validation: 0.2729640519501266]
	TIME [epoch: 12.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677421558398006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1677421558398006 | validation: 0.1738211843397377]
	TIME [epoch: 12.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23508086681968327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23508086681968327 | validation: 0.3253191712653867]
	TIME [epoch: 12.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20853756194817494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20853756194817494 | validation: 0.2852714343444759]
	TIME [epoch: 12.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3040968807373826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3040968807373826 | validation: 0.19277132951335216]
	TIME [epoch: 12.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20166225614733557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20166225614733557 | validation: 0.16534187006202206]
	TIME [epoch: 12.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14458466185722657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14458466185722657 | validation: 0.24545104765370454]
	TIME [epoch: 12.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14904397851872317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14904397851872317 | validation: 0.1970640800254424]
	TIME [epoch: 12.9 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28657973199865483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28657973199865483 | validation: 0.39753010188829835]
	TIME [epoch: 12.9 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24949291749277588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24949291749277588 | validation: 0.20639818494958578]
	TIME [epoch: 12.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24721938623738168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24721938623738168 | validation: 0.22161977959246887]
	TIME [epoch: 12.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2217633070317877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2217633070317877 | validation: 0.23569942729057025]
	TIME [epoch: 12.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2172467248465287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2172467248465287 | validation: 0.3035117604776919]
	TIME [epoch: 12.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1917247897226158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1917247897226158 | validation: 0.17351007125557893]
	TIME [epoch: 12.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24958301089969817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24958301089969817 | validation: 0.27192328842648245]
	TIME [epoch: 12.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16693290098010408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16693290098010408 | validation: 0.12465787711087667]
	TIME [epoch: 12.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16758114999815266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16758114999815266 | validation: 0.34761567083186456]
	TIME [epoch: 12.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23495849695135185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23495849695135185 | validation: 0.2592930217069459]
	TIME [epoch: 12.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31177725550039903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31177725550039903 | validation: 0.2827396475644758]
	TIME [epoch: 12.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23260102451248071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23260102451248071 | validation: 0.3079176204620222]
	TIME [epoch: 12.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29596902546584863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29596902546584863 | validation: 0.19427782884441816]
	TIME [epoch: 12.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22407840498413037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22407840498413037 | validation: 0.3286527431164209]
	TIME [epoch: 12.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24382495114725827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24382495114725827 | validation: 0.1364854072080689]
	TIME [epoch: 12.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19421875969858043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19421875969858043 | validation: 0.32961708397522416]
	TIME [epoch: 12.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19805994530596316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19805994530596316 | validation: 0.1350585433479033]
	TIME [epoch: 12.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19969390205497056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19969390205497056 | validation: 0.2256986253983727]
	TIME [epoch: 12.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17671378526316228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17671378526316228 | validation: 0.21020293472212925]
	TIME [epoch: 12.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23037376379018376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23037376379018376 | validation: 0.2483004364639633]
	TIME [epoch: 12.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2002858833828213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2002858833828213 | validation: 0.18634162751170052]
	TIME [epoch: 12.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20277894085737758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20277894085737758 | validation: 0.2193227416948612]
	TIME [epoch: 12.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571744639484542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1571744639484542 | validation: 0.12813523815566447]
	TIME [epoch: 12.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15837521011769448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15837521011769448 | validation: 0.3022916195953736]
	TIME [epoch: 12.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18452711516727477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18452711516727477 | validation: 0.17311886180872083]
	TIME [epoch: 12.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2289954181473458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2289954181473458 | validation: 0.18158196131522192]
	TIME [epoch: 12.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15817757220221684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15817757220221684 | validation: 0.16126655937202108]
	TIME [epoch: 12.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16960542123072728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16960542123072728 | validation: 0.25092607735083866]
	TIME [epoch: 12.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727501743059625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1727501743059625 | validation: 0.17984306941826442]
	TIME [epoch: 12.9 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23139019783810508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23139019783810508 | validation: 0.37103506317666846]
	TIME [epoch: 12.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22209949606017215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22209949606017215 | validation: 0.13113191041942093]
	TIME [epoch: 12.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20547564215925007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20547564215925007 | validation: 0.24612669345625943]
	TIME [epoch: 12.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1587120301898383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1587120301898383 | validation: 0.1348836205946848]
	TIME [epoch: 12.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687323829916161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687323829916161 | validation: 0.29375909732109723]
	TIME [epoch: 12.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21882875706667249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21882875706667249 | validation: 0.3239950466625596]
	TIME [epoch: 12.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30353938300161687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30353938300161687 | validation: 0.13469710149753744]
	TIME [epoch: 12.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15609812887469915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15609812887469915 | validation: 0.24917672373969912]
	TIME [epoch: 12.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16753386829715056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16753386829715056 | validation: 0.2214984579230943]
	TIME [epoch: 12.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26432733321856244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26432733321856244 | validation: 0.3063891016871928]
	TIME [epoch: 12.9 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2220876434313619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2220876434313619 | validation: 0.1137507363709987]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560845370390466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1560845370390466 | validation: 0.29609422765236637]
	TIME [epoch: 12.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814792182821186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18814792182821186 | validation: 0.2510586459146321]
	TIME [epoch: 12.9 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693811189472037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2693811189472037 | validation: 0.17809018463373572]
	TIME [epoch: 12.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1529831435749225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1529831435749225 | validation: 0.14942250144924515]
	TIME [epoch: 12.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15432158944668495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15432158944668495 | validation: 0.1641452674949972]
	TIME [epoch: 12.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298794903317357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298794903317357 | validation: 0.16178224002384242]
	TIME [epoch: 12.9 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20379106262700852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20379106262700852 | validation: 0.42852865671702545]
	TIME [epoch: 12.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504050772956596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504050772956596 | validation: 0.21316644234032314]
	TIME [epoch: 12.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29420659146709977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29420659146709977 | validation: 0.33067073271107184]
	TIME [epoch: 12.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22665121928259507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22665121928259507 | validation: 0.20565851073593386]
	TIME [epoch: 12.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2124149091482916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2124149091482916 | validation: 0.21671548240867652]
	TIME [epoch: 12.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14985687994521477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14985687994521477 | validation: 0.1374201546713243]
	TIME [epoch: 12.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17326036069056153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17326036069056153 | validation: 0.3380264793845504]
	TIME [epoch: 12.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071254707727709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2071254707727709 | validation: 0.1894805659419042]
	TIME [epoch: 12.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21373150109953976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21373150109953976 | validation: 0.17829187019648196]
	TIME [epoch: 12.9 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15529981837711368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15529981837711368 | validation: 0.16322001180049397]
	TIME [epoch: 12.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15701950248145327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15701950248145327 | validation: 0.30122168502142]
	TIME [epoch: 12.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165996288975923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2165996288975923 | validation: 0.214724249855315]
	TIME [epoch: 12.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27089308523951455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27089308523951455 | validation: 0.3641198652990792]
	TIME [epoch: 12.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2329333738574276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2329333738574276 | validation: 0.16250097682901818]
	TIME [epoch: 12.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18734652668494356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18734652668494356 | validation: 0.10544201011397077]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.107077880983902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.107077880983902 | validation: 0.20057939580161685]
	TIME [epoch: 12.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12190553712948561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12190553712948561 | validation: 0.17814076423850622]
	TIME [epoch: 12.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282133604368331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2282133604368331 | validation: 0.39462625412499086]
	TIME [epoch: 12.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24992547696402703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24992547696402703 | validation: 0.1641673939455633]
	TIME [epoch: 12.9 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20502841504585995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20502841504585995 | validation: 0.24929667251007467]
	TIME [epoch: 12.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2247485357410423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2247485357410423 | validation: 0.2131161378164022]
	TIME [epoch: 12.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24495503950660577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24495503950660577 | validation: 0.29759509568642145]
	TIME [epoch: 12.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17474630817331196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17474630817331196 | validation: 0.12540402060500777]
	TIME [epoch: 12.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18869811759844307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18869811759844307 | validation: 0.28784900798121155]
	TIME [epoch: 12.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16819793284754822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16819793284754822 | validation: 0.1694508613139168]
	TIME [epoch: 12.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21632977715098617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21632977715098617 | validation: 0.3172125717370642]
	TIME [epoch: 12.9 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1897264193468879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1897264193468879 | validation: 0.09681358924185605]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13566992968939182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13566992968939182 | validation: 0.21652998226944087]
	TIME [epoch: 13 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499685569197976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499685569197976 | validation: 0.19269642868658382]
	TIME [epoch: 12.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22733970332434492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22733970332434492 | validation: 0.27341699836109284]
	TIME [epoch: 13 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20818141215301722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20818141215301722 | validation: 0.18067885541903733]
	TIME [epoch: 13 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20279104730937556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20279104730937556 | validation: 0.2525645552104396]
	TIME [epoch: 13 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482862578059303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1482862578059303 | validation: 0.11700009573648931]
	TIME [epoch: 13 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1645731702205071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645731702205071 | validation: 0.33865900921686487]
	TIME [epoch: 13 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2070891833750764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2070891833750764 | validation: 0.1955652080566825]
	TIME [epoch: 12.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24110430983847797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24110430983847797 | validation: 0.2078830866231207]
	TIME [epoch: 12.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13468340557394817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13468340557394817 | validation: 0.14729335970711444]
	TIME [epoch: 12.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15873381891034297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15873381891034297 | validation: 0.20704133524837182]
	TIME [epoch: 12.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15344330263884484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15344330263884484 | validation: 0.11767469518009516]
	TIME [epoch: 12.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552179076211644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14552179076211644 | validation: 0.11824462551182191]
	TIME [epoch: 12.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11356958122776332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11356958122776332 | validation: 0.3787269525335611]
	TIME [epoch: 12.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23309690157782675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23309690157782675 | validation: 0.2461873958768572]
	TIME [epoch: 12.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220620465967978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3220620465967978 | validation: 0.12833243298917993]
	TIME [epoch: 12.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11633513260034717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11633513260034717 | validation: 0.14227770742568752]
	TIME [epoch: 12.9 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12013253130378246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12013253130378246 | validation: 0.23216569256914507]
	TIME [epoch: 12.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614090668309175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1614090668309175 | validation: 0.2256140476118884]
	TIME [epoch: 12.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.299477122364984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.299477122364984 | validation: 0.1928764183990166]
	TIME [epoch: 12.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12126610638643533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12126610638643533 | validation: 0.1065046652771997]
	TIME [epoch: 12.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11541475060788933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11541475060788933 | validation: 0.32681886045779174]
	TIME [epoch: 12.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17821372041368044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17821372041368044 | validation: 0.20804005228275343]
	TIME [epoch: 12.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2985686561795724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2985686561795724 | validation: 0.4040987671786921]
	TIME [epoch: 12.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364848993711075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364848993711075 | validation: 0.6138147982098595]
	TIME [epoch: 12.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44643070119209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44643070119209 | validation: 0.2585199796615414]
	TIME [epoch: 12.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33100229545983456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33100229545983456 | validation: 0.15709352667848167]
	TIME [epoch: 12.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15741157327494687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15741157327494687 | validation: 0.13985556634992238]
	TIME [epoch: 12.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13845752910135237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13845752910135237 | validation: 0.14914246556221086]
	TIME [epoch: 12.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307556166288818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307556166288818 | validation: 0.24931356778584]
	TIME [epoch: 12.8 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16938770421808275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16938770421808275 | validation: 0.26465167251338023]
	TIME [epoch: 12.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32797773446560435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32797773446560435 | validation: 0.5360517712202694]
	TIME [epoch: 12.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33567825057463585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33567825057463585 | validation: 0.11479957476923314]
	TIME [epoch: 12.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181596919709492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1181596919709492 | validation: 0.18917599885600622]
	TIME [epoch: 12.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.143118087849674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.143118087849674 | validation: 0.2139797959074056]
	TIME [epoch: 12.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.243438295079362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243438295079362 | validation: 0.34144288177000026]
	TIME [epoch: 12.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2101902297175637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2101902297175637 | validation: 0.13109550712413046]
	TIME [epoch: 12.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688734604878039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688734604878039 | validation: 0.22730749090160926]
	TIME [epoch: 12.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1278551338044709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1278551338044709 | validation: 0.13875767079227583]
	TIME [epoch: 12.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1623816894734129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1623816894734129 | validation: 0.28534704615690487]
	TIME [epoch: 13 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17976009985445798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17976009985445798 | validation: 0.16162721403840088]
	TIME [epoch: 12.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20644023353583052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20644023353583052 | validation: 0.18113824800073028]
	TIME [epoch: 12.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12613811562767005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12613811562767005 | validation: 0.0900877312413566]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12984917952746247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12984917952746247 | validation: 0.40321901583194664]
	TIME [epoch: 12.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25570285499842194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25570285499842194 | validation: 0.22401314199494973]
	TIME [epoch: 13 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706472935936446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2706472935936446 | validation: 0.1543860115330086]
	TIME [epoch: 13 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13711276511737416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13711276511737416 | validation: 0.3322547578344014]
	TIME [epoch: 13 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614622863073619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614622863073619 | validation: 0.16155644256550264]
	TIME [epoch: 13 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17618016190661764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17618016190661764 | validation: 0.20564584019989748]
	TIME [epoch: 13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17790574564699804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17790574564699804 | validation: 0.12016612651651176]
	TIME [epoch: 13 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599126570979765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1599126570979765 | validation: 0.3319682809296034]
	TIME [epoch: 12.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18218357447286418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18218357447286418 | validation: 0.13323419029278166]
	TIME [epoch: 12.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19135677145728744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19135677145728744 | validation: 0.27310555808379106]
	TIME [epoch: 12.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16013455652146516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16013455652146516 | validation: 0.1436996194818781]
	TIME [epoch: 12.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15002920274027295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15002920274027295 | validation: 0.47074556330026174]
	TIME [epoch: 12.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337580325744721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3337580325744721 | validation: 0.12049681756927494]
	TIME [epoch: 12.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17287935982542954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17287935982542954 | validation: 0.2883149206904152]
	TIME [epoch: 12.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16057458662999047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16057458662999047 | validation: 0.13323550207386803]
	TIME [epoch: 12.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17641804826216728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17641804826216728 | validation: 0.2177910002928306]
	TIME [epoch: 12.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12824536269572268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12824536269572268 | validation: 0.1493607521461313]
	TIME [epoch: 12.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18597072619533428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18597072619533428 | validation: 0.3379206131780508]
	TIME [epoch: 12.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1835771387903139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1835771387903139 | validation: 0.11457370318485288]
	TIME [epoch: 12.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15051335785507053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15051335785507053 | validation: 0.2927824138248253]
	TIME [epoch: 12.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153987483969679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.153987483969679 | validation: 0.15584788714801986]
	TIME [epoch: 12.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19842181533174505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19842181533174505 | validation: 0.22198133813447876]
	TIME [epoch: 12.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22794159176587683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22794159176587683 | validation: 0.12512342292133558]
	TIME [epoch: 12.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13676318409106922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13676318409106922 | validation: 0.33144620199478375]
	TIME [epoch: 12.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18902446493106795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18902446493106795 | validation: 0.12837636912858164]
	TIME [epoch: 12.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18883254393639518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18883254393639518 | validation: 0.24581407512965195]
	TIME [epoch: 12.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13501105819901002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13501105819901002 | validation: 0.11267194185090511]
	TIME [epoch: 12.9 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13088901171904915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13088901171904915 | validation: 0.3085815304579817]
	TIME [epoch: 12.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756921350787109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756921350787109 | validation: 0.13933466261401548]
	TIME [epoch: 12.9 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16223259124648606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16223259124648606 | validation: 0.33285507770488726]
	TIME [epoch: 13 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18272045861247527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18272045861247527 | validation: 0.11543912715505684]
	TIME [epoch: 12.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16101167286685955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16101167286685955 | validation: 0.2647094925605207]
	TIME [epoch: 13 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15704947197040559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15704947197040559 | validation: 0.14376836129215387]
	TIME [epoch: 12.9 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1989390547936525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1989390547936525 | validation: 0.23274958806947865]
	TIME [epoch: 13 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12674880688467155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12674880688467155 | validation: 0.15847571870393362]
	TIME [epoch: 12.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15474174529976178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15474174529976178 | validation: 0.29220923585059055]
	TIME [epoch: 12.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22896250830090176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22896250830090176 | validation: 0.16781005092987866]
	TIME [epoch: 12.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18883509442715954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18883509442715954 | validation: 0.2107718252174573]
	TIME [epoch: 12.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14224496912610302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14224496912610302 | validation: 0.10400540504414993]
	TIME [epoch: 12.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13576055957320873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13576055957320873 | validation: 0.28009901141623295]
	TIME [epoch: 12.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15169037629917018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15169037629917018 | validation: 0.1256454347547676]
	TIME [epoch: 12.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16726199550431228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16726199550431228 | validation: 0.5251083965019272]
	TIME [epoch: 12.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3386752918836801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3386752918836801 | validation: 0.08955807505702905]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10634830624646462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10634830624646462 | validation: 0.09230720748497816]
	TIME [epoch: 13 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10271993955212966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10271993955212966 | validation: 0.16004913401791263]
	TIME [epoch: 13 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11015510503188758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11015510503188758 | validation: 0.0906822755958565]
	TIME [epoch: 13 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11756060952855979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11756060952855979 | validation: 0.3741880475985636]
	TIME [epoch: 12.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23259162253730445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23259162253730445 | validation: 0.2163323641315802]
	TIME [epoch: 13 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27031902080655246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27031902080655246 | validation: 0.1484075090908326]
	TIME [epoch: 12.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12935047852959491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12935047852959491 | validation: 0.12147424154525172]
	TIME [epoch: 12.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146498393066196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146498393066196 | validation: 0.26990033390463647]
	TIME [epoch: 12.9 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601492948772976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601492948772976 | validation: 0.17494902477865007]
	TIME [epoch: 12.9 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24505852956275448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24505852956275448 | validation: 0.21070346743165744]
	TIME [epoch: 12.9 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12043468007322145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12043468007322145 | validation: 0.09722667701656562]
	TIME [epoch: 12.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12217807398136372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12217807398136372 | validation: 0.2562786097761744]
	TIME [epoch: 12.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1650377939445828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1650377939445828 | validation: 0.2231324147132921]
	TIME [epoch: 12.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22254381872509757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22254381872509757 | validation: 0.26556115270702735]
	TIME [epoch: 12.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15117023652205205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15117023652205205 | validation: 0.10092821696894423]
	TIME [epoch: 12.9 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14206091487140024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14206091487140024 | validation: 0.4715518031585809]
	TIME [epoch: 12.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27432369990446354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27432369990446354 | validation: 0.10678070042041975]
	TIME [epoch: 12.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15055877657030411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15055877657030411 | validation: 0.208936441377356]
	TIME [epoch: 12.9 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15241757595361696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15241757595361696 | validation: 0.2176339108510339]
	TIME [epoch: 12.9 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20582990832608444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20582990832608444 | validation: 0.16157243143768812]
	TIME [epoch: 12.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13519011121073843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13519011121073843 | validation: 0.07845359740513301]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11017867844865603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11017867844865603 | validation: 0.32718281388491255]
	TIME [epoch: 13 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16818946503126753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16818946503126753 | validation: 0.12986198893972697]
	TIME [epoch: 12.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1787393106159885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1787393106159885 | validation: 0.2494468993196407]
	TIME [epoch: 12.9 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15642455177754838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15642455177754838 | validation: 0.1419548510005256]
	TIME [epoch: 12.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490498685901585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1490498685901585 | validation: 0.5071294520423573]
	TIME [epoch: 12.9 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351022182329133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3351022182329133 | validation: 0.16470955975448967]
	TIME [epoch: 12.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13151707573054636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13151707573054636 | validation: 0.10183353976925696]
	TIME [epoch: 13 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706767870579789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706767870579789 | validation: 0.30334698898661494]
	TIME [epoch: 12.9 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18910221735357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18910221735357 | validation: 0.16200511404752505]
	TIME [epoch: 12.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16060123543126817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16060123543126817 | validation: 0.2647193940840944]
	TIME [epoch: 12.9 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18163856106622825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18163856106622825 | validation: 0.17665453971604625]
	TIME [epoch: 12.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21502801364628105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21502801364628105 | validation: 0.12892730391535037]
	TIME [epoch: 13 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477377945837104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477377945837104 | validation: 0.5891357548735202]
	TIME [epoch: 13 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40576968200620506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40576968200620506 | validation: 0.1310732309063176]
	TIME [epoch: 13 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1869073826801174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1869073826801174 | validation: 0.19618824159030793]
	TIME [epoch: 12.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15678480295399116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15678480295399116 | validation: 0.09357375534157045]
	TIME [epoch: 12.9 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10121420839355033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10121420839355033 | validation: 0.2255733251892559]
	TIME [epoch: 13 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11734422844139658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11734422844139658 | validation: 0.17107786576995304]
	TIME [epoch: 12.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21995508568865144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21995508568865144 | validation: 0.1890708709941545]
	TIME [epoch: 13 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17440311628148622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17440311628148622 | validation: 0.11027055974092886]
	TIME [epoch: 12.9 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11505102358835323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11505102358835323 | validation: 0.23736168069820912]
	TIME [epoch: 13 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12750190545463513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12750190545463513 | validation: 0.12299328216163241]
	TIME [epoch: 12.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17887749920072576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17887749920072576 | validation: 0.37126615801003354]
	TIME [epoch: 12.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19805675967011305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19805675967011305 | validation: 0.11515392810279633]
	TIME [epoch: 12.9 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1383220042059115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1383220042059115 | validation: 0.30421451889045453]
	TIME [epoch: 12.9 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1796207500070064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1796207500070064 | validation: 0.12711297233808602]
	TIME [epoch: 12.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17542131698975336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17542131698975336 | validation: 0.17534314957086422]
	TIME [epoch: 13 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11240339356310272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11240339356310272 | validation: 0.15118123695794392]
	TIME [epoch: 12.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14406900983378887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14406900983378887 | validation: 0.22600532393471418]
	TIME [epoch: 12.9 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16043252103906624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16043252103906624 | validation: 0.10142245653040792]
	TIME [epoch: 12.9 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13517151422978077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13517151422978077 | validation: 0.41471166371211965]
	TIME [epoch: 13 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20006417551402453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20006417551402453 | validation: 0.1411201025560922]
	TIME [epoch: 12.9 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17571130374188307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17571130374188307 | validation: 0.12197497410658774]
	TIME [epoch: 12.9 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11476995701263835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11476995701263835 | validation: 0.14931158867394154]
	TIME [epoch: 12.9 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13853778389077015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13853778389077015 | validation: 0.2886327179710455]
	TIME [epoch: 12.9 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20801933943355677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20801933943355677 | validation: 0.18010327479639404]
	TIME [epoch: 12.9 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21800922619588176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21800922619588176 | validation: 0.20932916284836295]
	TIME [epoch: 12.9 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310588853699977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310588853699977 | validation: 0.24044107791498273]
	TIME [epoch: 12.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2254625477200338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2254625477200338 | validation: 0.08359557857767544]
	TIME [epoch: 12.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0864918760037002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0864918760037002 | validation: 0.09438729092970519]
	TIME [epoch: 12.9 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07232526967881328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07232526967881328 | validation: 0.15660778346097892]
	TIME [epoch: 12.9 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885790838430707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0885790838430707 | validation: 0.1277430897709367]
	TIME [epoch: 12.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178023110191737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178023110191737 | validation: 0.2814217578893485]
	TIME [epoch: 12.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26389950580708893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26389950580708893 | validation: 0.14954756806679065]
	TIME [epoch: 12.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17980814399443523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17980814399443523 | validation: 0.23522071067324613]
	TIME [epoch: 12.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12770618296252312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12770618296252312 | validation: 0.1442931233453217]
	TIME [epoch: 12.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17713552143884653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17713552143884653 | validation: 0.37349946589266786]
	TIME [epoch: 12.9 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20025539037848497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20025539037848497 | validation: 0.14346941457866103]
	TIME [epoch: 12.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17280360845693518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17280360845693518 | validation: 0.1593256527240215]
	TIME [epoch: 12.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11008501786229744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11008501786229744 | validation: 0.10144459454046827]
	TIME [epoch: 12.9 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1102706589649121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1102706589649121 | validation: 0.2878262383815267]
	TIME [epoch: 12.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181751871733117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181751871733117 | validation: 0.12503981589089466]
	TIME [epoch: 12.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1654009258477744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1654009258477744 | validation: 0.40283403858207223]
	TIME [epoch: 12.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749436651703751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749436651703751 | validation: 0.2116855603331938]
	TIME [epoch: 12.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936760328149874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936760328149874 | validation: 0.17327160130691788]
	TIME [epoch: 12.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255063294471659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255063294471659 | validation: 0.24084943395131184]
	TIME [epoch: 12.9 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16153562206177843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16153562206177843 | validation: 0.11540341929873321]
	TIME [epoch: 12.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13435720442456994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13435720442456994 | validation: 0.27238371876938394]
	TIME [epoch: 12.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12221148466384976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12221148466384976 | validation: 0.14794055859031882]
	TIME [epoch: 12.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17939036777066153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17939036777066153 | validation: 0.38836927313521186]
	TIME [epoch: 12.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815416210278272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815416210278272 | validation: 0.10949373365640863]
	TIME [epoch: 12.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14518869578632937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14518869578632937 | validation: 0.24107226052654263]
	TIME [epoch: 12.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292940287916782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292940287916782 | validation: 0.1587631676717743]
	TIME [epoch: 12.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17720991118043128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17720991118043128 | validation: 0.1598018599379669]
	TIME [epoch: 12.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096662959146395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096662959146395 | validation: 0.1105393504489975]
	TIME [epoch: 12.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037482474135204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1037482474135204 | validation: 0.2256402948575226]
	TIME [epoch: 12.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12529711992476114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12529711992476114 | validation: 0.16762206156155235]
	TIME [epoch: 12.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21164668730684127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21164668730684127 | validation: 0.18673123369930625]
	TIME [epoch: 12.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10451566264656485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10451566264656485 | validation: 0.10115376887908085]
	TIME [epoch: 12.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13348827465070673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13348827465070673 | validation: 0.18516145862661795]
	TIME [epoch: 12.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09921848787721495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09921848787721495 | validation: 0.16918296160965782]
	TIME [epoch: 12.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18937757402643954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18937757402643954 | validation: 0.23883839635507478]
	TIME [epoch: 12.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14080128708148767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14080128708148767 | validation: 0.20508300469452556]
	TIME [epoch: 12.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19987922236134736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19987922236134736 | validation: 0.23266314760199927]
	TIME [epoch: 12.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11735152326191017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11735152326191017 | validation: 0.12097566267499248]
	TIME [epoch: 12.9 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15618106353812164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15618106353812164 | validation: 0.26156445759326247]
	TIME [epoch: 12.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1391328664097294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1391328664097294 | validation: 0.14198672779145718]
	TIME [epoch: 12.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16932205225387187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16932205225387187 | validation: 0.22142867403677458]
	TIME [epoch: 12.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148192579974591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.148192579974591 | validation: 0.14676761878778924]
	TIME [epoch: 12.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15193926207840625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15193926207840625 | validation: 0.22801446256811247]
	TIME [epoch: 12.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12511526344490925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12511526344490925 | validation: 0.09620850466525589]
	TIME [epoch: 12.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1408345802045977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1408345802045977 | validation: 0.3448637209297772]
	TIME [epoch: 12.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17039496785146013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17039496785146013 | validation: 0.14447882897850994]
	TIME [epoch: 12.9 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17002434927634866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17002434927634866 | validation: 0.17956049928865683]
	TIME [epoch: 12.9 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13667333548701074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13667333548701074 | validation: 0.16820388146817525]
	TIME [epoch: 12.9 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15687146857128478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15687146857128478 | validation: 0.1673047738898202]
	TIME [epoch: 13 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11237182750228847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11237182750228847 | validation: 0.07737383464992198]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824711507815028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10824711507815028 | validation: 0.27412004889099045]
	TIME [epoch: 12.9 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13284114378069467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13284114378069467 | validation: 0.11747507305609035]
	TIME [epoch: 12.9 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16534149777514612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16534149777514612 | validation: 0.30414731816737467]
	TIME [epoch: 12.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486894306244276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486894306244276 | validation: 0.09925433357060075]
	TIME [epoch: 12.9 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12183843742260074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12183843742260074 | validation: 0.19772094097850149]
	TIME [epoch: 12.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10237726156482616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10237726156482616 | validation: 0.14615749670772207]
	TIME [epoch: 12.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413579408976968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1413579408976968 | validation: 0.22794234675397407]
	TIME [epoch: 12.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15173983596236282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15173983596236282 | validation: 0.13676114886796725]
	TIME [epoch: 12.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12134042834217595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12134042834217595 | validation: 0.29218987544007835]
	TIME [epoch: 12.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12933833729391744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12933833729391744 | validation: 0.12207095281514105]
	TIME [epoch: 12.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16488877481289424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16488877481289424 | validation: 0.18769198747417756]
	TIME [epoch: 12.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1097993901525051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1097993901525051 | validation: 0.09667579022816986]
	TIME [epoch: 12.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192443014790284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10192443014790284 | validation: 0.24568286427935482]
	TIME [epoch: 12.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15194046275870032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15194046275870032 | validation: 0.13640129459861036]
	TIME [epoch: 12.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1597853127831036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1597853127831036 | validation: 0.2080779450265355]
	TIME [epoch: 12.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10280098354982464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10280098354982464 | validation: 0.08757170635381376]
	TIME [epoch: 12.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11714290476277286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11714290476277286 | validation: 0.2548736675843715]
	TIME [epoch: 12.9 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13988358923396274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13988358923396274 | validation: 0.11120575162282267]
	TIME [epoch: 12.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14859209980289742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14859209980289742 | validation: 0.24679280572337597]
	TIME [epoch: 12.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261024046819577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261024046819577 | validation: 0.08147349272118226]
	TIME [epoch: 12.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10425118341135578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10425118341135578 | validation: 0.25570181943981085]
	TIME [epoch: 12.9 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571706400268851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1571706400268851 | validation: 0.171313159193283]
	TIME [epoch: 12.8 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16720558351744685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16720558351744685 | validation: 0.1280763563943201]
	TIME [epoch: 12.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10004452459032656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10004452459032656 | validation: 0.17701608824547055]
	TIME [epoch: 12.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1683275093158638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1683275093158638 | validation: 0.10211082560922446]
	TIME [epoch: 12.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09045576713561417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09045576713561417 | validation: 0.061762088177988464]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062427177875827536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062427177875827536 | validation: 0.09277281198913133]
	TIME [epoch: 12.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058203137713676856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058203137713676856 | validation: 0.055396088656422794]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06126797615000729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06126797615000729 | validation: 0.06302547324516967]
	TIME [epoch: 12.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677617473814345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07677617473814345 | validation: 0.44264668593852347]
	TIME [epoch: 12.9 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24970016630012828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24970016630012828 | validation: 0.2789701005358554]
	TIME [epoch: 13 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32856652678643905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32856652678643905 | validation: 0.39108272981841236]
	TIME [epoch: 12.9 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27374680116233197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27374680116233197 | validation: 0.19297640462313645]
	TIME [epoch: 12.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14372599983683082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14372599983683082 | validation: 0.0843030034441082]
	TIME [epoch: 12.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835144717120975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08835144717120975 | validation: 0.09286097195406146]
	TIME [epoch: 12.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0972546255758007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0972546255758007 | validation: 0.20264402152684116]
	TIME [epoch: 12.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20931145721308014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20931145721308014 | validation: 0.3457341784931104]
	TIME [epoch: 12.9 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23689023038918186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23689023038918186 | validation: 0.13840494102513326]
	TIME [epoch: 12.9 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17022506206296137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17022506206296137 | validation: 0.23110304088705044]
	TIME [epoch: 12.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258035567633315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1258035567633315 | validation: 0.10005776005457584]
	TIME [epoch: 12.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11619200601582368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11619200601582368 | validation: 0.16388284909891315]
	TIME [epoch: 12.9 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10601073398075503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10601073398075503 | validation: 0.11500198787250014]
	TIME [epoch: 12.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308081352989828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1308081352989828 | validation: 0.2150358565815838]
	TIME [epoch: 12.9 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12074295284541543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12074295284541543 | validation: 0.09859466206573182]
	TIME [epoch: 12.9 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12587260811906062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12587260811906062 | validation: 0.2700463545596736]
	TIME [epoch: 12.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749551090584028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1749551090584028 | validation: 0.10818601205625909]
	TIME [epoch: 12.9 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09412346727336862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09412346727336862 | validation: 0.08408320497717285]
	TIME [epoch: 12.9 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839605526877198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07839605526877198 | validation: 0.08398611806051076]
	TIME [epoch: 12.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479474414108131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07479474414108131 | validation: 0.2328376689412468]
	TIME [epoch: 12.9 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11881207870228472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11881207870228472 | validation: 0.14827549275761662]
	TIME [epoch: 12.9 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21576118281129972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21576118281129972 | validation: 0.3590832742047062]
	TIME [epoch: 12.9 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16643109107078816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16643109107078816 | validation: 0.08613520120949124]
	TIME [epoch: 12.9 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308847075375005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09308847075375005 | validation: 0.21848112016612867]
	TIME [epoch: 12.9 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1633708908434285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1633708908434285 | validation: 0.09785160567011801]
	TIME [epoch: 12.9 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299977600988403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12299977600988403 | validation: 0.22302626165536235]
	TIME [epoch: 12.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11662997730955249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11662997730955249 | validation: 0.08366924522064363]
	TIME [epoch: 12.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11527811719012916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11527811719012916 | validation: 0.5267953403158299]
	TIME [epoch: 12.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079130547410417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6079130547410417 | validation: 0.3265539654122731]
	TIME [epoch: 12.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3608698894066065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3608698894066065 | validation: 0.3754148981335923]
	TIME [epoch: 12.9 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3206434762464004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3206434762464004 | validation: 0.3141915854134973]
	TIME [epoch: 12.9 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25359095703443063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25359095703443063 | validation: 0.22943453936304048]
	TIME [epoch: 12.9 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25402210559968214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25402210559968214 | validation: 0.21114266644652427]
	TIME [epoch: 12.9 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2176039722414982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2176039722414982 | validation: 0.1911476926776095]
	TIME [epoch: 12.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15625041657631278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15625041657631278 | validation: 0.2444239196355684]
	TIME [epoch: 12.9 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17820745101222152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17820745101222152 | validation: 0.2494441635832182]
	TIME [epoch: 12.9 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27371075956750857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27371075956750857 | validation: 1.0349578800873696]
	TIME [epoch: 12.9 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1997378528857678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1997378528857678 | validation: 0.6141350316966051]
	TIME [epoch: 12.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274545641421962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6274545641421962 | validation: 0.2687396515214055]
	TIME [epoch: 12.9 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28330620309615434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28330620309615434 | validation: 0.1895827628685341]
	TIME [epoch: 12.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2341136817882793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2341136817882793 | validation: 0.1239514976204116]
	TIME [epoch: 12.9 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16251593918219556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16251593918219556 | validation: 0.10908323833899183]
	TIME [epoch: 12.9 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14325759758980172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14325759758980172 | validation: 0.12919089519427493]
	TIME [epoch: 12.9 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12716505458961488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12716505458961488 | validation: 0.09270974271993185]
	TIME [epoch: 12.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13767278324541207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13767278324541207 | validation: 0.19660112122966367]
	TIME [epoch: 12.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349747488307981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349747488307981 | validation: 0.0994581448362874]
	TIME [epoch: 12.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12398813161884416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12398813161884416 | validation: 0.13567434161512879]
	TIME [epoch: 12.9 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229119566986921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09229119566986921 | validation: 0.08948643892080431]
	TIME [epoch: 12.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556962460544004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09556962460544004 | validation: 0.13871534773373104]
	TIME [epoch: 12.9 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096661349774571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08096661349774571 | validation: 0.08258018260256675]
	TIME [epoch: 12.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08168546580017244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08168546580017244 | validation: 0.11282126741929832]
	TIME [epoch: 12.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09255989054962412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09255989054962412 | validation: 0.09755390215325428]
	TIME [epoch: 12.9 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12266617392694007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12266617392694007 | validation: 0.24951874298489787]
	TIME [epoch: 12.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14601057833049158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14601057833049158 | validation: 0.08655012081245718]
	TIME [epoch: 12.9 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11708805741918536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11708805741918536 | validation: 0.3867345958623065]
	TIME [epoch: 12.9 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2412967637929674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2412967637929674 | validation: 0.18766244604902638]
	TIME [epoch: 12.9 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16145051348623363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16145051348623363 | validation: 0.17238955651130353]
	TIME [epoch: 12.9 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385618819031291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385618819031291 | validation: 0.07777000238130535]
	TIME [epoch: 12.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11963447885824495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11963447885824495 | validation: 0.21205869394810417]
	TIME [epoch: 12.9 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12247580506700789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12247580506700789 | validation: 0.11674319776064329]
	TIME [epoch: 12.9 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1330039956018964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1330039956018964 | validation: 0.13307267578348844]
	TIME [epoch: 12.9 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08595330248807923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08595330248807923 | validation: 0.07938963444778452]
	TIME [epoch: 12.9 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788897954600317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09788897954600317 | validation: 0.36849761638783635]
	TIME [epoch: 12.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823277091504252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2823277091504252 | validation: 0.13067134502801445]
	TIME [epoch: 12.9 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18502940854910827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18502940854910827 | validation: 0.7828007613831766]
	TIME [epoch: 12.9 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5956634196732823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5956634196732823 | validation: 0.5450199246327255]
	TIME [epoch: 12.9 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3293892128734192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3293892128734192 | validation: 0.22838263625069524]
	TIME [epoch: 12.9 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824410971120542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1824410971120542 | validation: 0.14152548808462137]
	TIME [epoch: 12.9 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458568470717108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458568470717108 | validation: 0.11425212478568854]
	TIME [epoch: 12.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101307704287074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.101307704287074 | validation: 0.10084473586356851]
	TIME [epoch: 12.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1230002215876219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1230002215876219 | validation: 0.1722316177512961]
	TIME [epoch: 12.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10171400531997477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10171400531997477 | validation: 0.07525200311955411]
	TIME [epoch: 12.9 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153393264214574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09153393264214574 | validation: 0.10993288388854433]
	TIME [epoch: 12.9 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235687963481725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07235687963481725 | validation: 0.06550596918084994]
	TIME [epoch: 12.9 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08202430301489542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08202430301489542 | validation: 0.1868076227299055]
	TIME [epoch: 12.9 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1146717248275521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1146717248275521 | validation: 0.07627484900718469]
	TIME [epoch: 12.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031604734061086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1031604734061086 | validation: 0.15151666827386126]
	TIME [epoch: 12.9 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928766781420607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0928766781420607 | validation: 0.07303006136698757]
	TIME [epoch: 12.9 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038494532364382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10038494532364382 | validation: 0.19204351602305703]
	TIME [epoch: 12.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111951529700323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111951529700323 | validation: 0.06537760075442903]
	TIME [epoch: 12.9 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08309074594697614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08309074594697614 | validation: 0.11865644847158029]
	TIME [epoch: 12.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08473450845193979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08473450845193979 | validation: 0.0773646409308515]
	TIME [epoch: 12.9 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08941010542935705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08941010542935705 | validation: 0.13876823270655675]
	TIME [epoch: 12.9 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08349406265305007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08349406265305007 | validation: 0.06867623232656743]
	TIME [epoch: 12.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08275004256260271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08275004256260271 | validation: 0.18214677277953337]
	TIME [epoch: 12.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1159195537571765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1159195537571765 | validation: 0.10558146904043234]
	TIME [epoch: 12.9 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15216884682448026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15216884682448026 | validation: 0.25617749326128897]
	TIME [epoch: 12.9 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12242284615796437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12242284615796437 | validation: 0.06462984190777611]
	TIME [epoch: 12.9 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951972479967634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951972479967634 | validation: 0.1051536392117905]
	TIME [epoch: 12.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780195138477916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0780195138477916 | validation: 0.07727920191970657]
	TIME [epoch: 12.9 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08730022621865528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08730022621865528 | validation: 0.12211718416710529]
	TIME [epoch: 12.9 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08395898925135083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08395898925135083 | validation: 0.09101922843791373]
	TIME [epoch: 12.9 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12247958042042165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12247958042042165 | validation: 0.2438576096448352]
	TIME [epoch: 12.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13312478337263403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13312478337263403 | validation: 0.07167258680113962]
	TIME [epoch: 12.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0996267095929246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0996267095929246 | validation: 0.4681029177130088]
	TIME [epoch: 12.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162824868121051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5162824868121051 | validation: 0.37281901606784545]
	TIME [epoch: 12.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097046090450849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4097046090450849 | validation: 0.30843431823260703]
	TIME [epoch: 12.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35925617385405345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35925617385405345 | validation: 0.6573511103647205]
	TIME [epoch: 12.9 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4038316683023765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4038316683023765 | validation: 0.20962389425205688]
	TIME [epoch: 12.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25915588220854013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25915588220854013 | validation: 0.19559383562260885]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_210641/states/model_phi1_4c_v_mmd2_984.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 9269.816 seconds.
