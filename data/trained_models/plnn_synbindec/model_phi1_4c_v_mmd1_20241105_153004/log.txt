Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1010320439

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.757423830600552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.757423830600552 | validation: 4.3030881561438]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.058058789054392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.058058789054392 | validation: 3.1855630036246234]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1872076057847534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1872076057847534 | validation: 2.1909313611178898]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.359696685897344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.359696685897344 | validation: 3.0592140022595506]
	TIME [epoch: 2.77 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.881507383257905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.881507383257905 | validation: 2.8505921159995053]
	TIME [epoch: 2.77 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.433778627094126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.433778627094126 | validation: 3.760491608158148]
	TIME [epoch: 2.77 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.652362438584547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.652362438584547 | validation: 2.106874681320909]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.132063618232049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.132063618232049 | validation: 1.8085297549575405]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.774364652328054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.774364652328054 | validation: 1.7721410079072681]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.778158838095415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.778158838095415 | validation: 1.4982922261520928]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5770351841243866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5770351841243866 | validation: 1.5678347867011597]
	TIME [epoch: 2.78 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.468553336809293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.468553336809293 | validation: 1.442578112321458]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4633471767588107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4633471767588107 | validation: 1.5141412995840726]
	TIME [epoch: 2.77 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4470413552253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4470413552253 | validation: 1.4653862181252155]
	TIME [epoch: 2.77 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3730883608980669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3730883608980669 | validation: 1.3500538883065187]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3947456596685353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3947456596685353 | validation: 1.4186781027787339]
	TIME [epoch: 2.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3590681343840287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3590681343840287 | validation: 1.3662691583430289]
	TIME [epoch: 2.77 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314093639153395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.314093639153395 | validation: 1.3390593287862937]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3020421601896857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3020421601896857 | validation: 1.3905029389411232]
	TIME [epoch: 2.77 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3140236311524893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3140236311524893 | validation: 1.256106770185947]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2484238133939751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2484238133939751 | validation: 1.2419161165202048]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2297532886821834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2297532886821834 | validation: 1.2490818167674533]
	TIME [epoch: 2.78 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1936294941279506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1936294941279506 | validation: 1.2544828418534302]
	TIME [epoch: 2.78 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1991286774167274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1991286774167274 | validation: 1.2292987912431526]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2109605457790003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2109605457790003 | validation: 1.238819604157438]
	TIME [epoch: 2.77 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2131193536326765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2131193536326765 | validation: 1.121252828825545]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2014186016816066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2014186016816066 | validation: 1.2137455151191694]
	TIME [epoch: 2.78 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190596559602346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.190596559602346 | validation: 1.2397377777616174]
	TIME [epoch: 2.78 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690820012207002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1690820012207002 | validation: 1.029263636969816]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102505840844399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1102505840844399 | validation: 0.986235388436697]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107179903191909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.107179903191909 | validation: 1.1081990816173963]
	TIME [epoch: 2.78 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111431882539623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.111431882539623 | validation: 1.0601472290893117]
	TIME [epoch: 2.78 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0957042023397188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0957042023397188 | validation: 0.9845528753297446]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079557176312189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.079557176312189 | validation: 1.0923933611841912]
	TIME [epoch: 2.77 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1524651633987117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1524651633987117 | validation: 1.0824337293610762]
	TIME [epoch: 2.76 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1080844386528683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1080844386528683 | validation: 1.2605196938750696]
	TIME [epoch: 2.76 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2048106220445916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2048106220445916 | validation: 0.9023618458711571]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0496732765648966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0496732765648966 | validation: 0.9395407685696071]
	TIME [epoch: 2.78 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0666674146501587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0666674146501587 | validation: 0.9125787960404605]
	TIME [epoch: 2.78 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0503752011962721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0503752011962721 | validation: 1.0354274058060928]
	TIME [epoch: 2.78 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.088785629116813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.088785629116813 | validation: 0.9765794044234476]
	TIME [epoch: 2.78 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1090805802130068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1090805802130068 | validation: 0.9880067554835024]
	TIME [epoch: 2.79 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0952156581331307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0952156581331307 | validation: 0.9304157213786935]
	TIME [epoch: 2.79 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0613005874201218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0613005874201218 | validation: 0.8250781600099059]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002701315466591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0002701315466591 | validation: 0.8934537976400471]
	TIME [epoch: 2.76 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9920273802521393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9920273802521393 | validation: 0.838749089876832]
	TIME [epoch: 2.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9837726120135674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9837726120135674 | validation: 0.830211344645224]
	TIME [epoch: 2.76 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0008643406322395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0008643406322395 | validation: 0.9380106074922354]
	TIME [epoch: 2.77 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02975870351924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.02975870351924 | validation: 0.941603127865041]
	TIME [epoch: 2.76 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589873214720569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0589873214720569 | validation: 0.8974740043945286]
	TIME [epoch: 2.78 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0118569224368075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0118569224368075 | validation: 0.8295619625747014]
	TIME [epoch: 2.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682221365587591		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9682221365587591 | validation: 0.7982298166626851]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.963215785591689		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.963215785591689 | validation: 0.8089647476741892]
	TIME [epoch: 2.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451090889485451		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.9451090889485451 | validation: 0.7733249852257432]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9497680590357612		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.9497680590357612 | validation: 0.8872102735646973]
	TIME [epoch: 2.78 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199642276329257		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.0199642276329257 | validation: 0.9539685409789134]
	TIME [epoch: 2.77 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0755322593569785		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.0755322593569785 | validation: 0.7810257448670874]
	TIME [epoch: 2.77 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9421033051388659		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9421033051388659 | validation: 0.9105502655586794]
	TIME [epoch: 2.77 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0674753009679128		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.0674753009679128 | validation: 0.7668539732647659]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290227958073802		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9290227958073802 | validation: 0.7826978426387214]
	TIME [epoch: 2.79 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9558244992080027		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.9558244992080027 | validation: 0.8194300452542329]
	TIME [epoch: 2.77 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9691213702632476		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.9691213702632476 | validation: 0.8072333040041992]
	TIME [epoch: 2.77 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614778982146782		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9614778982146782 | validation: 0.7859567930827933]
	TIME [epoch: 2.76 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335712516953331		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9335712516953331 | validation: 0.7441944558630867]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9062809566040693		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9062809566040693 | validation: 0.7442341521658125]
	TIME [epoch: 2.76 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077221648368367		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9077221648368367 | validation: 0.7869480526761067]
	TIME [epoch: 2.76 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066794982967308		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.9066794982967308 | validation: 0.7529419624926903]
	TIME [epoch: 2.77 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269185688706528		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.9269185688706528 | validation: 0.7918083820709265]
	TIME [epoch: 2.77 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386224802733568		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9386224802733568 | validation: 0.7723305036831625]
	TIME [epoch: 2.77 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9631782875717271		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9631782875717271 | validation: 0.8933283327987627]
	TIME [epoch: 2.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0641184179014755		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.0641184179014755 | validation: 0.753832722976168]
	TIME [epoch: 2.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9192208367298056		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.9192208367298056 | validation: 0.8396125576725911]
	TIME [epoch: 2.78 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0111619160253296		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.0111619160253296 | validation: 0.7783260623804128]
	TIME [epoch: 2.78 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210255851935082		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9210255851935082 | validation: 0.7512172271516808]
	TIME [epoch: 2.78 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933686212479028		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8933686212479028 | validation: 0.8131926528240296]
	TIME [epoch: 2.76 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9727235328058944		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9727235328058944 | validation: 0.7489939246475334]
	TIME [epoch: 2.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060949156965589		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.9060949156965589 | validation: 0.7315571370468412]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906927580369544		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8906927580369544 | validation: 0.7404946592400387]
	TIME [epoch: 2.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017413654246121		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9017413654246121 | validation: 0.745048594754973]
	TIME [epoch: 2.77 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980282592987989		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8980282592987989 | validation: 0.7455549478595982]
	TIME [epoch: 2.76 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880926960793082		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8880926960793082 | validation: 0.74261748860684]
	TIME [epoch: 2.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886624731561034		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.886624731561034 | validation: 0.7313040230102019]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884187248106544		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.884187248106544 | validation: 0.7480765268633439]
	TIME [epoch: 2.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994881597028543		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8994881597028543 | validation: 0.8532370801034255]
	TIME [epoch: 2.78 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785325819997357		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.9785325819997357 | validation: 0.842620696995414]
	TIME [epoch: 2.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983983253109561		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.983983253109561 | validation: 0.7539456344202744]
	TIME [epoch: 2.77 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134002093678302		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.9134002093678302 | validation: 0.8158012954726557]
	TIME [epoch: 2.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0030107834231574		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0030107834231574 | validation: 0.7884076340003016]
	TIME [epoch: 2.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253212155106243		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.9253212155106243 | validation: 0.7751791325365902]
	TIME [epoch: 2.77 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9237783061961992		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9237783061961992 | validation: 0.7499038524060891]
	TIME [epoch: 2.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218345865365677		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9218345865365677 | validation: 0.7372419221165862]
	TIME [epoch: 2.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881086622755752		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8881086622755752 | validation: 0.7523642149953784]
	TIME [epoch: 2.77 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885433596027571		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8885433596027571 | validation: 0.7793798147489465]
	TIME [epoch: 2.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236880263468089		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9236880263468089 | validation: 0.7300023707325288]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875133722292995		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8875133722292995 | validation: 0.7241435521162556]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848527198075743		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8848527198075743 | validation: 0.7277023887385762]
	TIME [epoch: 2.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838163755794364		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8838163755794364 | validation: 0.7362602135140884]
	TIME [epoch: 2.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888101430576839		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8888101430576839 | validation: 0.7459125517239316]
	TIME [epoch: 2.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841071342548008		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8841071342548008 | validation: 0.7396918149289914]
	TIME [epoch: 2.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886667338673209		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.886667338673209 | validation: 0.7407885348539802]
	TIME [epoch: 2.77 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8843881020564266		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8843881020564266 | validation: 0.726382862515718]
	TIME [epoch: 2.78 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765252504193037		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8765252504193037 | validation: 0.74900773356975]
	TIME [epoch: 2.78 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017396762054578		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9017396762054578 | validation: 0.8177717471703364]
	TIME [epoch: 2.77 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013545250181355		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.013545250181355 | validation: 0.8737243792793606]
	TIME [epoch: 2.78 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0280577669903668		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.0280577669903668 | validation: 0.7329065825168075]
	TIME [epoch: 2.78 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8970352801185582		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8970352801185582 | validation: 0.7716459410619706]
	TIME [epoch: 2.78 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9633681603463055		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9633681603463055 | validation: 0.723664431004848]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917150786017866		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8917150786017866 | validation: 0.7951358069096799]
	TIME [epoch: 2.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453590998605751		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9453590998605751 | validation: 0.7635873068479176]
	TIME [epoch: 2.78 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300826629489664		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.9300826629489664 | validation: 0.726929132772744]
	TIME [epoch: 2.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8801137582805793		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8801137582805793 | validation: 0.7789920721963393]
	TIME [epoch: 2.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9402210791511106		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9402210791511106 | validation: 0.7287588809234186]
	TIME [epoch: 2.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891499958125718		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8891499958125718 | validation: 0.741912670022251]
	TIME [epoch: 2.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134794964204181		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9134794964204181 | validation: 0.7563519309347678]
	TIME [epoch: 2.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9151068951193833		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9151068951193833 | validation: 0.7282003303348393]
	TIME [epoch: 2.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733723349521699		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8733723349521699 | validation: 0.7628760941335129]
	TIME [epoch: 2.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9418618185479386		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.9418618185479386 | validation: 0.7323406289496834]
	TIME [epoch: 2.78 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794481433121986		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8794481433121986 | validation: 0.76717147722855]
	TIME [epoch: 2.78 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887143041182342		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.887143041182342 | validation: 0.7460384674976093]
	TIME [epoch: 2.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075524585889113		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.9075524585889113 | validation: 0.7475608940467399]
	TIME [epoch: 2.78 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885609526068074		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.885609526068074 | validation: 0.7388125785714948]
	TIME [epoch: 2.78 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8807897354830905		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8807897354830905 | validation: 0.7139195062232213]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757731550467682		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8757731550467682 | validation: 0.7420035568440708]
	TIME [epoch: 2.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840810335813438		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8840810335813438 | validation: 0.729315961458346]
	TIME [epoch: 2.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861702166314019		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8861702166314019 | validation: 0.7754812122878854]
	TIME [epoch: 2.76 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287035968444346		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9287035968444346 | validation: 0.7536808667792275]
	TIME [epoch: 2.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982193869278203		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8982193869278203 | validation: 0.7458753152741976]
	TIME [epoch: 2.76 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981073579753343		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8981073579753343 | validation: 0.7682458737207732]
	TIME [epoch: 2.75 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9145222816250236		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.9145222816250236 | validation: 0.7496207663085354]
	TIME [epoch: 2.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8845026304740707		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8845026304740707 | validation: 0.7383204047772739]
	TIME [epoch: 2.76 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993343786551736		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8993343786551736 | validation: 0.7440527398372769]
	TIME [epoch: 2.75 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026481262947468		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9026481262947468 | validation: 0.7339700625490186]
	TIME [epoch: 2.75 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857801347901304		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8857801347901304 | validation: 0.7273624114161983]
	TIME [epoch: 2.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769151352583211		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8769151352583211 | validation: 0.7588213731147068]
	TIME [epoch: 2.75 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9019060268325683		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9019060268325683 | validation: 0.7349679237310497]
	TIME [epoch: 2.77 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832757806686448		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8832757806686448 | validation: 0.7366660383634238]
	TIME [epoch: 2.77 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761015879080312		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8761015879080312 | validation: 0.7624810892913481]
	TIME [epoch: 2.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070927008961117		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.9070927008961117 | validation: 0.7748015944342432]
	TIME [epoch: 2.78 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9338164523772347		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.9338164523772347 | validation: 0.7328225605370055]
	TIME [epoch: 2.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9035461294189945		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.9035461294189945 | validation: 0.8024813277924867]
	TIME [epoch: 2.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.947185505904118		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.947185505904118 | validation: 0.7241337376092165]
	TIME [epoch: 2.76 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893197757505709		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8893197757505709 | validation: 0.7560643457238667]
	TIME [epoch: 2.8 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122632327863416		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.9122632327863416 | validation: 0.7321127992527156]
	TIME [epoch: 2.78 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8812140169824321		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8812140169824321 | validation: 0.7267116658922071]
	TIME [epoch: 2.79 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676665699725835		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8676665699725835 | validation: 0.7233623317805185]
	TIME [epoch: 2.77 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850400796216877		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8850400796216877 | validation: 0.7139786008417853]
	TIME [epoch: 2.78 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798391020064782		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8798391020064782 | validation: 0.7520323907879285]
	TIME [epoch: 2.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998751638468554		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8998751638468554 | validation: 0.7680059735273753]
	TIME [epoch: 2.78 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465811178665018		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9465811178665018 | validation: 0.7326227512421053]
	TIME [epoch: 2.77 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761555982706346		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8761555982706346 | validation: 0.7499696160246618]
	TIME [epoch: 2.78 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957866291263803		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8957866291263803 | validation: 0.7290032230038549]
	TIME [epoch: 2.77 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8909357909427084		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8909357909427084 | validation: 0.7413445312201954]
	TIME [epoch: 2.78 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123422505403863		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.9123422505403863 | validation: 0.8100271000118572]
	TIME [epoch: 2.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9942100594106836		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9942100594106836 | validation: 0.8053765017039276]
	TIME [epoch: 2.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9771703120247429		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.9771703120247429 | validation: 0.7086621034152488]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776086352888396		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8776086352888396 | validation: 0.7497339704471471]
	TIME [epoch: 2.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9188337776166731		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.9188337776166731 | validation: 0.7407583546608386]
	TIME [epoch: 2.79 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877448633052297		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.877448633052297 | validation: 0.7506900166551422]
	TIME [epoch: 2.79 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888980733557788		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8888980733557788 | validation: 0.7317399655144771]
	TIME [epoch: 2.78 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890695333799457		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8890695333799457 | validation: 0.7236436405843243]
	TIME [epoch: 2.78 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8787246823574311		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8787246823574311 | validation: 0.7466983015137552]
	TIME [epoch: 2.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887865883089403		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8887865883089403 | validation: 0.7214828393506261]
	TIME [epoch: 2.78 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811746934286133		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8811746934286133 | validation: 0.7345367361841292]
	TIME [epoch: 2.77 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765384786040514		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8765384786040514 | validation: 0.74637787230138]
	TIME [epoch: 2.78 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887369258268524		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8887369258268524 | validation: 0.7257812616964436]
	TIME [epoch: 2.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758212026103124		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8758212026103124 | validation: 0.7400737223034992]
	TIME [epoch: 2.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770402120563674		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8770402120563674 | validation: 0.7242849491066956]
	TIME [epoch: 2.78 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938922887709467		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8938922887709467 | validation: 0.7661210471440931]
	TIME [epoch: 2.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974125294975633		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8974125294975633 | validation: 0.7583671410381314]
	TIME [epoch: 2.78 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058480672642286		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.9058480672642286 | validation: 0.7174736008796998]
	TIME [epoch: 2.78 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8928496122378541		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8928496122378541 | validation: 0.7514913763676119]
	TIME [epoch: 2.78 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781565639448426		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8781565639448426 | validation: 0.7196848349301999]
	TIME [epoch: 2.79 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774959163568689		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.8774959163568689 | validation: 0.7343612536088919]
	TIME [epoch: 2.78 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836366360905649		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.8836366360905649 | validation: 0.7334189586747639]
	TIME [epoch: 2.78 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680696715868521		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.8680696715868521 | validation: 0.7222847983781036]
	TIME [epoch: 2.78 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925095181710614		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8925095181710614 | validation: 0.7287996285901697]
	TIME [epoch: 2.78 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749289400404261		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8749289400404261 | validation: 0.7384048154495755]
	TIME [epoch: 2.78 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789125100586591		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8789125100586591 | validation: 0.7476385740507328]
	TIME [epoch: 2.78 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8977631888553304		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8977631888553304 | validation: 0.7696626605671493]
	TIME [epoch: 2.78 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9341144102762041		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9341144102762041 | validation: 0.7519735419612147]
	TIME [epoch: 2.78 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870507574689858		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8870507574689858 | validation: 0.7205890267982865]
	TIME [epoch: 2.78 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811924360559297		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8811924360559297 | validation: 0.7309696776065885]
	TIME [epoch: 2.78 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772481319923862		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8772481319923862 | validation: 0.7408982693938733]
	TIME [epoch: 2.78 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8867120467817421		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8867120467817421 | validation: 0.7318086430454045]
	TIME [epoch: 2.78 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714089543723494		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8714089543723494 | validation: 0.722331393231932]
	TIME [epoch: 2.78 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836250509484161		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8836250509484161 | validation: 0.7336625066015889]
	TIME [epoch: 2.78 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870867087799725		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.870867087799725 | validation: 0.728053512587256]
	TIME [epoch: 2.79 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704411466413291		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8704411466413291 | validation: 0.7301377968632983]
	TIME [epoch: 2.78 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854093741060959		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8854093741060959 | validation: 0.7508821820847786]
	TIME [epoch: 2.78 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221941567007451		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9221941567007451 | validation: 0.7890274757679893]
	TIME [epoch: 2.78 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168940965724433		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9168940965724433 | validation: 0.734392082815279]
	TIME [epoch: 2.78 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041242488291685		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9041242488291685 | validation: 0.7221047437712209]
	TIME [epoch: 2.78 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689567379425072		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8689567379425072 | validation: 0.7780004535962148]
	TIME [epoch: 2.78 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907781779621234		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.907781779621234 | validation: 0.7252204037482161]
	TIME [epoch: 2.78 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676314946983185		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8676314946983185 | validation: 0.7194408371034449]
	TIME [epoch: 2.78 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940370081629558		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8940370081629558 | validation: 0.729199115175336]
	TIME [epoch: 2.78 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727482469353198		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.8727482469353198 | validation: 0.7603527204221511]
	TIME [epoch: 2.79 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791583588398834		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8791583588398834 | validation: 0.7220310214705608]
	TIME [epoch: 2.78 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666206951822151		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8666206951822151 | validation: 0.7226989955271172]
	TIME [epoch: 2.78 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673993435309092		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.8673993435309092 | validation: 0.7355297482175485]
	TIME [epoch: 2.77 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656572801173654		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8656572801173654 | validation: 0.7232634255759756]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554765311683198		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.8554765311683198 | validation: 0.7295331867090763]
	TIME [epoch: 6.04 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644737065082632		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.8644737065082632 | validation: 0.7059534758371664]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8619560128699786		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8619560128699786 | validation: 0.7622435771014665]
	TIME [epoch: 6.02 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866104027924192		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8866104027924192 | validation: 0.7161500877388823]
	TIME [epoch: 6.02 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8521968404456121		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8521968404456121 | validation: 0.7236847833229029]
	TIME [epoch: 6.02 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675312185587731		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8675312185587731 | validation: 0.7448789462085509]
	TIME [epoch: 6.03 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951190946553679		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8951190946553679 | validation: 1.0340442727919024]
	TIME [epoch: 6.02 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453552229020927		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1453552229020927 | validation: 0.7080852230465184]
	TIME [epoch: 6.02 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8571131641036246		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8571131641036246 | validation: 0.7403233054694001]
	TIME [epoch: 6.02 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8820249986117432		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.8820249986117432 | validation: 0.7422525452882995]
	TIME [epoch: 6.02 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731268823492453		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.8731268823492453 | validation: 0.7293021277555621]
	TIME [epoch: 6.02 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557244613991621		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.8557244613991621 | validation: 0.7332210134454156]
	TIME [epoch: 6.02 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575990240513077		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.8575990240513077 | validation: 0.7174610090443073]
	TIME [epoch: 6.02 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853781635887336		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.853781635887336 | validation: 0.6854291627163405]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437716534849181		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8437716534849181 | validation: 0.7107724787538002]
	TIME [epoch: 6.02 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468790277670799		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.8468790277670799 | validation: 0.7129022481057528]
	TIME [epoch: 6.03 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363039989086629		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.8363039989086629 | validation: 0.7043188233584557]
	TIME [epoch: 6.01 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394614408834795		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.8394614408834795 | validation: 0.7154507723652677]
	TIME [epoch: 6.02 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294023816531313		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8294023816531313 | validation: 0.7050037648254822]
	TIME [epoch: 6.01 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820603128015982		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.820603128015982 | validation: 0.6949956730744413]
	TIME [epoch: 6.02 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040410828793434		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.8040410828793434 | validation: 2.692190602896983]
	TIME [epoch: 6.02 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7656100377301973		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 2.7656100377301973 | validation: 1.0043085966412848]
	TIME [epoch: 6.03 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0740526156125139		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0740526156125139 | validation: 0.7897971474958297]
	TIME [epoch: 6.02 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919179969698371		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.8919179969698371 | validation: 0.8562469051311468]
	TIME [epoch: 6.03 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589832966708105		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9589832966708105 | validation: 0.6925978079947526]
	TIME [epoch: 6.01 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918717696776929		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7918717696776929 | validation: 0.7317798181530026]
	TIME [epoch: 6.02 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260092335335258		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8260092335335258 | validation: 0.7023020495104639]
	TIME [epoch: 6.01 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952757663544887		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7952757663544887 | validation: 0.6958047027694939]
	TIME [epoch: 6.03 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884730922910831		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7884730922910831 | validation: 0.6919444759507365]
	TIME [epoch: 6.02 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767182642136188		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7767182642136188 | validation: 0.6930536916206623]
	TIME [epoch: 6.03 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672329659976336		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7672329659976336 | validation: 0.6814337897920024]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681821779224431		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7681821779224431 | validation: 0.6898937007616222]
	TIME [epoch: 6.03 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587710296695324		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7587710296695324 | validation: 0.7429438970611151]
	TIME [epoch: 6.02 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881838732823593		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7881838732823593 | validation: 0.82537591006423]
	TIME [epoch: 6.01 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320786743252819		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9320786743252819 | validation: 0.7047613782495356]
	TIME [epoch: 6.01 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636244327451582		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7636244327451582 | validation: 0.669700210884041]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464986034396347		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7464986034396347 | validation: 0.6725438389987025]
	TIME [epoch: 6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396948981450128		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7396948981450128 | validation: 0.6906601904651652]
	TIME [epoch: 6.01 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574703243204113		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7574703243204113 | validation: 0.7618156447185158]
	TIME [epoch: 6.01 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8201537573403169		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.8201537573403169 | validation: 0.9904651162948103]
	TIME [epoch: 6.01 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0563979967859427		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.0563979967859427 | validation: 0.6868792936956876]
	TIME [epoch: 6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654089951023261		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7654089951023261 | validation: 0.8513163393929374]
	TIME [epoch: 6.01 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172060521356185		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9172060521356185 | validation: 0.6914017797343555]
	TIME [epoch: 6.02 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038644991516489		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.8038644991516489 | validation: 0.6887809730071779]
	TIME [epoch: 6.01 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946480764647268		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7946480764647268 | validation: 0.6801811526000462]
	TIME [epoch: 6.02 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621550302864318		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7621550302864318 | validation: 0.6410400472364335]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747608618450719		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.747608618450719 | validation: 0.6427679800970503]
	TIME [epoch: 6.02 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7241462716589172		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7241462716589172 | validation: 0.6559298851384462]
	TIME [epoch: 6.02 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979446778919939		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6979446778919939 | validation: 0.6787244341670408]
	TIME [epoch: 6.02 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212834362857151		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7212834362857151 | validation: 1.074617174909088]
	TIME [epoch: 6.03 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0647809913807267		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0647809913807267 | validation: 1.1196129158816082]
	TIME [epoch: 6.02 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2542577074394277		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.2542577074394277 | validation: 0.9048311651775871]
	TIME [epoch: 6.03 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0388318216761498		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.0388318216761498 | validation: 0.6704724164994892]
	TIME [epoch: 6.02 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015797577486024		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.8015797577486024 | validation: 0.7296441862940954]
	TIME [epoch: 6.02 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507226386634392		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8507226386634392 | validation: 0.6735301756880395]
	TIME [epoch: 6.02 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980933416084546		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7980933416084546 | validation: 0.6888905269468749]
	TIME [epoch: 6.03 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872763205317775		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7872763205317775 | validation: 0.6723648005415487]
	TIME [epoch: 6.02 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784079864975706		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7784079864975706 | validation: 0.6447348647122099]
	TIME [epoch: 6.02 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753501058753679		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.753501058753679 | validation: 0.6533499427371185]
	TIME [epoch: 6.01 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319821873384066		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7319821873384066 | validation: 0.6409283356129641]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707928547909458		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.707928547909458 | validation: 0.6526781718629505]
	TIME [epoch: 6.01 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691796446888553		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.691796446888553 | validation: 0.6731254461943103]
	TIME [epoch: 6.02 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72774015558514		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.72774015558514 | validation: 1.046787087721549]
	TIME [epoch: 6.01 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049224546454053		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.049224546454053 | validation: 1.0213465315152834]
	TIME [epoch: 6.01 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127130297204957		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.127130297204957 | validation: 0.8649308168513774]
	TIME [epoch: 6.01 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9493340811008142		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9493340811008142 | validation: 0.6743782910601115]
	TIME [epoch: 6.02 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777826065467179		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.777826065467179 | validation: 0.7100784474760885]
	TIME [epoch: 6.01 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176499872536941		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8176499872536941 | validation: 0.6541806626476819]
	TIME [epoch: 6.02 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633869399644732		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7633869399644732 | validation: 0.6777355796763636]
	TIME [epoch: 6.01 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572687768300054		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7572687768300054 | validation: 0.6456009396227638]
	TIME [epoch: 6.02 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717791197294813		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.717791197294813 | validation: 0.652178609325433]
	TIME [epoch: 6.01 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7114841560651712		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7114841560651712 | validation: 0.6365331471947594]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906653872673246		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6906653872673246 | validation: 0.6784568267371278]
	TIME [epoch: 6.01 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900106761705774		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6900106761705774 | validation: 0.7162847556757099]
	TIME [epoch: 6.02 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809213030926013		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7809213030926013 | validation: 0.8372265204265272]
	TIME [epoch: 6.02 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472188319983041		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.8472188319983041 | validation: 0.8263806485345055]
	TIME [epoch: 6.02 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111887537203391		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9111887537203391 | validation: 0.6716187906432696]
	TIME [epoch: 6.01 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7539247058516096		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7539247058516096 | validation: 0.721133721400816]
	TIME [epoch: 6.02 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039941658934958		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8039941658934958 | validation: 0.6408111938606527]
	TIME [epoch: 6.02 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109521020116856		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7109521020116856 | validation: 0.6386475201488712]
	TIME [epoch: 6.01 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705236471587519		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.705236471587519 | validation: 0.6305103095296808]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860978210352152		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6860978210352152 | validation: 0.628669483301374]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678817539405497		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6678817539405497 | validation: 0.659935761506381]
	TIME [epoch: 6.03 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820410556482969		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6820410556482969 | validation: 0.760673599330018]
	TIME [epoch: 6.02 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8258084291281489		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8258084291281489 | validation: 0.6354092139360272]
	TIME [epoch: 6.02 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658061623886089		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.658061623886089 | validation: 0.6005212681883836]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488467096481627		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6488467096481627 | validation: 0.6254863896295911]
	TIME [epoch: 5.97 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658507318359402		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.658507318359402 | validation: 0.7103497725857288]
	TIME [epoch: 5.99 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773068588645974		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.773068588645974 | validation: 0.6327124007506003]
	TIME [epoch: 5.98 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648839306078979		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6648839306078979 | validation: 0.6301964940934336]
	TIME [epoch: 5.99 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691907322770397		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.691907322770397 | validation: 0.6264236747726779]
	TIME [epoch: 5.98 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765576813605508		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6765576813605508 | validation: 0.6996981361409884]
	TIME [epoch: 5.99 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673857003112312		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7673857003112312 | validation: 0.5929947737107665]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370303241033779		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6370303241033779 | validation: 0.5731396446409315]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618125793887726		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.618125793887726 | validation: 0.5740881304439717]
	TIME [epoch: 5.98 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229353346073324		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6229353346073324 | validation: 0.7280400940664609]
	TIME [epoch: 6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734145094848908		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.734145094848908 | validation: 0.961496469953492]
	TIME [epoch: 5.99 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0628837408898122		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.0628837408898122 | validation: 0.7279584553099552]
	TIME [epoch: 6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8053870095473064		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8053870095473064 | validation: 0.6701353911317818]
	TIME [epoch: 5.98 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475999061289256		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7475999061289256 | validation: 0.6129950205356183]
	TIME [epoch: 6.01 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779621694912197		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6779621694912197 | validation: 0.6220466277824435]
	TIME [epoch: 6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669730559702716		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6669730559702716 | validation: 0.5804355877125069]
	TIME [epoch: 6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181651429334959		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6181651429334959 | validation: 0.6048988568349265]
	TIME [epoch: 6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416471752959049		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6416471752959049 | validation: 0.7164969064702706]
	TIME [epoch: 6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246653343164284		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7246653343164284 | validation: 0.8220664747953141]
	TIME [epoch: 6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009154208458929		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.9009154208458929 | validation: 0.6693522332517057]
	TIME [epoch: 6.01 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372556882131522		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.7372556882131522 | validation: 0.6270938703232778]
	TIME [epoch: 6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038675971623638		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7038675971623638 | validation: 0.5607756024345505]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639686424971051		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.639686424971051 | validation: 0.5804864170800046]
	TIME [epoch: 6.01 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277976644930091		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6277976644930091 | validation: 0.6009819866519677]
	TIME [epoch: 6 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320983659678822		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6320983659678822 | validation: 0.6732545567615031]
	TIME [epoch: 5.99 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217815179918676		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.7217815179918676 | validation: 0.5698116615008454]
	TIME [epoch: 5.99 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6081603524320262		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6081603524320262 | validation: 0.6095275878843953]
	TIME [epoch: 6 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418521418789636		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6418521418789636 | validation: 0.6110083760782115]
	TIME [epoch: 5.99 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337739039558644		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6337739039558644 | validation: 0.6514674179851605]
	TIME [epoch: 6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168731533545889		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7168731533545889 | validation: 0.5277488658542344]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580773665657664		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.580773665657664 | validation: 0.6011297328038558]
	TIME [epoch: 6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310725323499063		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6310725323499063 | validation: 0.7556289261479157]
	TIME [epoch: 6.01 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960821777137562		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7960821777137562 | validation: 0.5285432452230218]
	TIME [epoch: 6.01 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5808667178053928		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.5808667178053928 | validation: 0.7710718796902108]
	TIME [epoch: 6 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868400856127198		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.7868400856127198 | validation: 0.7691949903743951]
	TIME [epoch: 6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344819887653857		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8344819887653857 | validation: 0.6713878269562294]
	TIME [epoch: 5.99 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378618641547455		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.7378618641547455 | validation: 0.5740323514546811]
	TIME [epoch: 5.99 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502229725943905		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6502229725943905 | validation: 0.5605519375270384]
	TIME [epoch: 6 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406165403159986		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6406165403159986 | validation: 0.5766833413560345]
	TIME [epoch: 5.99 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001709149312027		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6001709149312027 | validation: 0.5428621986933239]
	TIME [epoch: 5.99 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629432447253394		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.5629432447253394 | validation: 0.5534540139264937]
	TIME [epoch: 5.99 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5691818033295417		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.5691818033295417 | validation: 0.5823155571201658]
	TIME [epoch: 6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082624584682074		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6082624584682074 | validation: 0.7385142531428488]
	TIME [epoch: 6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837972342143573		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.7837972342143573 | validation: 0.5336231521059108]
	TIME [epoch: 5.99 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617709724611888		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5617709724611888 | validation: 0.670496655084051]
	TIME [epoch: 5.99 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806604208569629		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6806604208569629 | validation: 0.6634778234622989]
	TIME [epoch: 5.99 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492241018471861		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.7492241018471861 | validation: 0.5762068640429184]
	TIME [epoch: 5.99 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100229499109773		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6100229499109773 | validation: 0.6982625502755438]
	TIME [epoch: 5.99 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283187957298205		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.7283187957298205 | validation: 0.6024351775272802]
	TIME [epoch: 5.98 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682530629951328		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6682530629951328 | validation: 0.5677148090750652]
	TIME [epoch: 6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6070312531760704		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6070312531760704 | validation: 0.5818583563803784]
	TIME [epoch: 5.98 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6180135837848306		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6180135837848306 | validation: 0.5498764511177909]
	TIME [epoch: 5.99 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5788663965052468		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.5788663965052468 | validation: 0.5069169189981538]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307958875443278		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.5307958875443278 | validation: 0.5283556220458004]
	TIME [epoch: 6.01 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424594247007783		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5424594247007783 | validation: 0.6172064739357052]
	TIME [epoch: 6.01 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414568262721898		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6414568262721898 | validation: 0.5751389088586331]
	TIME [epoch: 6.01 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857557507593917		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5857557507593917 | validation: 0.6128568817183999]
	TIME [epoch: 6.02 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390160149959937		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6390160149959937 | validation: 0.5032878804459658]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375279297547891		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5375279297547891 | validation: 0.5301778661463461]
	TIME [epoch: 6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508927709760743		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5508927709760743 | validation: 0.6179306906346957]
	TIME [epoch: 5.99 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344272399685056		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6344272399685056 | validation: 0.528222500820022]
	TIME [epoch: 6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441184220977755		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5441184220977755 | validation: 0.5660280176171799]
	TIME [epoch: 5.99 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867010981360903		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5867010981360903 | validation: 0.5256032589703645]
	TIME [epoch: 6 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5450546199013917		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.5450546199013917 | validation: 0.5514887491101533]
	TIME [epoch: 5.99 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5705371788112281		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5705371788112281 | validation: 0.5497754818987753]
	TIME [epoch: 6.01 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682951931441794		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5682951931441794 | validation: 0.6152196820648442]
	TIME [epoch: 5.99 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6641913790268896		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6641913790268896 | validation: 0.47235719523532294]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5103018203657458		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5103018203657458 | validation: 0.5477162073581798]
	TIME [epoch: 6.01 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499463370896207		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5499463370896207 | validation: 0.641451989885037]
	TIME [epoch: 6.01 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899297049773265		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6899297049773265 | validation: 0.47888828352436513]
	TIME [epoch: 6.02 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49664732199311845		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.49664732199311845 | validation: 0.6467238717348397]
	TIME [epoch: 6.01 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667341333543066		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6667341333543066 | validation: 0.7112324594770714]
	TIME [epoch: 6.02 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787013935561425		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7787013935561425 | validation: 0.5834913142610335]
	TIME [epoch: 6.02 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583086242532273		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6583086242532273 | validation: 0.5641782472540859]
	TIME [epoch: 6.03 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6117453675805666		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6117453675805666 | validation: 0.47700778624405943]
	TIME [epoch: 6.02 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5262270225372979		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5262270225372979 | validation: 0.48321618483810874]
	TIME [epoch: 6.02 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076775946253193		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5076775946253193 | validation: 0.5640997043673651]
	TIME [epoch: 6.02 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726369474263815		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5726369474263815 | validation: 0.6753630337842971]
	TIME [epoch: 6.02 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086089973959366		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7086089973959366 | validation: 0.49338506858327646]
	TIME [epoch: 6.01 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367304970980918		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.5367304970980918 | validation: 0.7012921170482603]
	TIME [epoch: 6.02 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002629843467753		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7002629843467753 | validation: 0.5758331725667774]
	TIME [epoch: 6.02 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.611941594931383		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.611941594931383 | validation: 0.5143015222869491]
	TIME [epoch: 6.02 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546658837694389		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.546658837694389 | validation: 0.5242050346061902]
	TIME [epoch: 6.02 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444528790226881		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5444528790226881 | validation: 0.4932530226123527]
	TIME [epoch: 6.02 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4984440664399196		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4984440664399196 | validation: 0.4736697266125412]
	TIME [epoch: 6.01 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46858972090224027		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.46858972090224027 | validation: 0.4633416790888199]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46954951775856707		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.46954951775856707 | validation: 0.47601056636996547]
	TIME [epoch: 5.98 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46852747036400144		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.46852747036400144 | validation: 0.5247396469042684]
	TIME [epoch: 5.99 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069896357990864		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5069896357990864 | validation: 0.5922633574915322]
	TIME [epoch: 6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223093774674794		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6223093774674794 | validation: 0.45211416266954957]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4713728254314953		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.4713728254314953 | validation: 0.44337418588539546]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.459849466201744		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.459849466201744 | validation: 0.4643489593030351]
	TIME [epoch: 5.98 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46972979089167716		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.46972979089167716 | validation: 0.5125949149647984]
	TIME [epoch: 6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5040254382942639		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5040254382942639 | validation: 0.6382258646356115]
	TIME [epoch: 5.99 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932379877798053		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6932379877798053 | validation: 0.46336127105331004]
	TIME [epoch: 6 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4850949639481094		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.4850949639481094 | validation: 0.7336939509750016]
	TIME [epoch: 6.01 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153764611790984		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7153764611790984 | validation: 0.613916413084284]
	TIME [epoch: 6 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713542652668616		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6713542652668616 | validation: 0.5491873962313017]
	TIME [epoch: 6 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5778794974102981		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5778794974102981 | validation: 0.5055041005484407]
	TIME [epoch: 6 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391571807736463		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5391571807736463 | validation: 0.42940257487645117]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.469164578130945		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.469164578130945 | validation: 0.41971704342395033]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4547936977348491		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.4547936977348491 | validation: 0.521214014072015]
	TIME [epoch: 5.98 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068076714565123		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5068076714565123 | validation: 0.6174097182436723]
	TIME [epoch: 5.99 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398205524397075		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.6398205524397075 | validation: 0.44226172368261474]
	TIME [epoch: 6 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44879797105843594		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.44879797105843594 | validation: 0.651745711537142]
	TIME [epoch: 5.99 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319860315795311		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6319860315795311 | validation: 0.5677809920974365]
	TIME [epoch: 6 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377293454595849		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6377293454595849 | validation: 0.4853008612999988]
	TIME [epoch: 5.99 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5176278898854682		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5176278898854682 | validation: 0.5114550446313587]
	TIME [epoch: 5.99 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5386136982176931		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.5386136982176931 | validation: 0.44805207993696716]
	TIME [epoch: 5.99 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46663088017646354		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.46663088017646354 | validation: 0.4339909188775539]
	TIME [epoch: 6 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43775175821249596		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.43775175821249596 | validation: 0.4682654600922188]
	TIME [epoch: 5.99 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47172409781141406		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.47172409781141406 | validation: 0.5088745590076391]
	TIME [epoch: 6 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5208685167682391		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5208685167682391 | validation: 0.4435406345839368]
	TIME [epoch: 5.99 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469955594626984		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4469955594626984 | validation: 0.4462310833010783]
	TIME [epoch: 6 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365860435993399		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.4365860435993399 | validation: 0.4259962391631145]
	TIME [epoch: 5.98 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4440869254124472		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.4440869254124472 | validation: 0.4664897175405431]
	TIME [epoch: 5.99 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4859604336342352		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.4859604336342352 | validation: 0.4422001634772391]
	TIME [epoch: 5.98 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4446827136972962		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.4446827136972962 | validation: 0.4393045665010886]
	TIME [epoch: 5.99 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.448130920910659		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.448130920910659 | validation: 0.43157788023692717]
	TIME [epoch: 5.99 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4334335280972246		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4334335280972246 | validation: 0.4571201895027217]
	TIME [epoch: 5.98 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46606455562887933		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.46606455562887933 | validation: 0.4443738291987629]
	TIME [epoch: 5.99 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44021900950119147		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.44021900950119147 | validation: 0.4660549395407846]
	TIME [epoch: 5.99 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4736321278319822		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4736321278319822 | validation: 0.4916698508120069]
	TIME [epoch: 5.99 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45828258110893144		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.45828258110893144 | validation: 0.503611172217437]
	TIME [epoch: 5.99 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222515741031009		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5222515741031009 | validation: 0.4008411704683226]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4108617728327322		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.4108617728327322 | validation: 0.4212551023172192]
	TIME [epoch: 5.99 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4118687877109001		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.4118687877109001 | validation: 0.4636853802065204]
	TIME [epoch: 5.99 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4857597208118715		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.4857597208118715 | validation: 0.6055764873764314]
	TIME [epoch: 6 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.560263542978856		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.560263542978856 | validation: 0.5476123553335318]
	TIME [epoch: 5.99 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651970374920862		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.5651970374920862 | validation: 0.4151555519850938]
	TIME [epoch: 5.99 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43063986081879413		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.43063986081879413 | validation: 0.4906048126423449]
	TIME [epoch: 5.98 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877736692328324		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.4877736692328324 | validation: 0.45287274326951654]
	TIME [epoch: 5.99 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4533061938764699		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.4533061938764699 | validation: 0.3818577104851413]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38611739555902547		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.38611739555902547 | validation: 0.4387029012941474]
	TIME [epoch: 5.99 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4315004088624065		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.4315004088624065 | validation: 0.4980780800954353]
	TIME [epoch: 6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49635358412230496		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.49635358412230496 | validation: 0.3797269161567244]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833597203122645		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3833597203122645 | validation: 0.3833167421821686]
	TIME [epoch: 5.98 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830025493866897		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.3830025493866897 | validation: 0.3728949721935044]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3851814124643558		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3851814124643558 | validation: 0.39684742891397246]
	TIME [epoch: 5.95 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39625744224201753		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.39625744224201753 | validation: 0.438169020546739]
	TIME [epoch: 5.97 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46372736735932635		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.46372736735932635 | validation: 0.4378831900316851]
	TIME [epoch: 5.97 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42431749329073315		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.42431749329073315 | validation: 0.43723337844111]
	TIME [epoch: 5.96 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44090215604824623		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.44090215604824623 | validation: 0.3870621601033698]
	TIME [epoch: 5.98 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37712711459487536		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.37712711459487536 | validation: 0.3509508744303502]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35883145439912184		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.35883145439912184 | validation: 0.3631957041971112]
	TIME [epoch: 5.99 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587130983574785		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3587130983574785 | validation: 0.4005301711479987]
	TIME [epoch: 5.94 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38935071430227686		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.38935071430227686 | validation: 0.4623045578887428]
	TIME [epoch: 5.99 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44747861483963247		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.44747861483963247 | validation: 0.49369456722150273]
	TIME [epoch: 5.98 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624863802645916		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5624863802645916 | validation: 0.3988264247782348]
	TIME [epoch: 5.96 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.401921227464297		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.401921227464297 | validation: 0.5680043007288108]
	TIME [epoch: 5.98 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369469749669847		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.5369469749669847 | validation: 0.4549238110902259]
	TIME [epoch: 5.99 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4834984284106866		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.4834984284106866 | validation: 0.368696141174745]
	TIME [epoch: 5.98 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710883527061115		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.3710883527061115 | validation: 0.5102592910953075]
	TIME [epoch: 5.98 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4660095360801769		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.4660095360801769 | validation: 0.4289982185628019]
	TIME [epoch: 5.97 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4599689155752617		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.4599689155752617 | validation: 0.37861761225435275]
	TIME [epoch: 5.98 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3489078836477288		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.3489078836477288 | validation: 0.4413739785109959]
	TIME [epoch: 5.98 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4035697254209973		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.4035697254209973 | validation: 0.44493460209251356]
	TIME [epoch: 6 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4458503406045688		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4458503406045688 | validation: 0.35479598124710254]
	TIME [epoch: 5.99 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33455397573525686		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.33455397573525686 | validation: 0.392515885064111]
	TIME [epoch: 5.98 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3685622973813675		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.3685622973813675 | validation: 0.4360665421354023]
	TIME [epoch: 5.97 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4530120580697774		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.4530120580697774 | validation: 0.32351534075711125]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33443728302652115		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.33443728302652115 | validation: 0.3336507393251397]
	TIME [epoch: 5.98 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32549866068760336		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.32549866068760336 | validation: 0.33703416776407996]
	TIME [epoch: 5.98 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473105343580787		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.3473105343580787 | validation: 0.41373865489225176]
	TIME [epoch: 5.97 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3838995463357205		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3838995463357205 | validation: 0.4169176286424097]
	TIME [epoch: 5.99 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46657003654710627		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.46657003654710627 | validation: 0.32026569040925423]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3324451603422004		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.3324451603422004 | validation: 0.4305548640367769]
	TIME [epoch: 5.99 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4075309915248309		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.4075309915248309 | validation: 0.4551282223801607]
	TIME [epoch: 6 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4901911824190161		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.4901911824190161 | validation: 0.31775434615331255]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32803779900955277		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.32803779900955277 | validation: 0.5486966670404858]
	TIME [epoch: 5.98 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.515759847303785		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.515759847303785 | validation: 0.4379000447506199]
	TIME [epoch: 5.99 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4651145102803902		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4651145102803902 | validation: 0.36910809552850565]
	TIME [epoch: 5.98 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38667605550032463		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.38667605550032463 | validation: 0.4480569796010199]
	TIME [epoch: 5.99 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42862795312985674		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.42862795312985674 | validation: 0.3371441635310491]
	TIME [epoch: 6 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3455664734189482		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.3455664734189482 | validation: 0.31307116274676305]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30349493051468984		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.30349493051468984 | validation: 0.3163204305949841]
	TIME [epoch: 6 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32197025546723906		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.32197025546723906 | validation: 0.36618215967419787]
	TIME [epoch: 6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36871803463611613		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.36871803463611613 | validation: 0.31950335207702674]
	TIME [epoch: 5.99 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129339892979006		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.3129339892979006 | validation: 0.2911644073565777]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3079197219764429		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.3079197219764429 | validation: 0.35049889957447977]
	TIME [epoch: 5.99 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33528674675468556		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.33528674675468556 | validation: 0.37323429318345747]
	TIME [epoch: 5.99 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3931104829405949		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.3931104829405949 | validation: 0.3172271429503388]
	TIME [epoch: 6 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3097312936789623		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.3097312936789623 | validation: 0.27681955792198293]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905005685042341		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2905005685042341 | validation: 0.31341745497257395]
	TIME [epoch: 6.01 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970537924832311		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2970537924832311 | validation: 0.3474165448545171]
	TIME [epoch: 6.01 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3655685573795233		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.3655685573795233 | validation: 0.394896580663865]
	TIME [epoch: 6.01 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3551501894312453		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.3551501894312453 | validation: 0.3580095552658394]
	TIME [epoch: 6 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40236885141665013		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.40236885141665013 | validation: 0.3020325129345355]
	TIME [epoch: 5.98 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2933935979176709		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2933935979176709 | validation: 0.40727476478715974]
	TIME [epoch: 6.01 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36013371858117366		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.36013371858117366 | validation: 0.45040225982483534]
	TIME [epoch: 6.01 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4737824298801774		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.4737824298801774 | validation: 0.3000844662264728]
	TIME [epoch: 6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2988095167704754		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.2988095167704754 | validation: 0.6427603457065721]
	TIME [epoch: 6.02 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5692953395459422		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.5692953395459422 | validation: 0.3529536873939435]
	TIME [epoch: 5.99 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38486178898880397		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.38486178898880397 | validation: 0.3454748506007848]
	TIME [epoch: 5.99 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3569789818634781		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.3569789818634781 | validation: 0.3650460397515257]
	TIME [epoch: 6 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33575921653191165		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.33575921653191165 | validation: 0.28956384660044726]
	TIME [epoch: 5.97 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28991175392797436		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.28991175392797436 | validation: 0.27479307461859986]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733607215767307		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.2733607215767307 | validation: 0.2716040949428901]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739296597645917		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2739296597645917 | validation: 0.2818873804765966]
	TIME [epoch: 6.01 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2865151943430447		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.2865151943430447 | validation: 0.3889518742416038]
	TIME [epoch: 6.01 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35272127858795044		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.35272127858795044 | validation: 0.3519636042194074]
	TIME [epoch: 6.01 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37995410914670663		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.37995410914670663 | validation: 0.2807692217303858]
	TIME [epoch: 6.02 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27552431806913696		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.27552431806913696 | validation: 0.3943362005123341]
	TIME [epoch: 6 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3608936766341398		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.3608936766341398 | validation: 0.3901679335785382]
	TIME [epoch: 6.02 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41424962535255516		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.41424962535255516 | validation: 0.28230381474515387]
	TIME [epoch: 6.01 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823327904065339		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.2823327904065339 | validation: 0.49407098009386224]
	TIME [epoch: 6.01 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4489300107854798		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4489300107854798 | validation: 0.3366434782149288]
	TIME [epoch: 6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36609080499846386		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.36609080499846386 | validation: 0.28510342795802884]
	TIME [epoch: 6 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29131446405012484		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.29131446405012484 | validation: 0.38042267891406406]
	TIME [epoch: 5.98 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3572254534717168		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.3572254534717168 | validation: 0.3341424151658119]
	TIME [epoch: 6 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.329999361438198		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.329999361438198 | validation: 0.25403953808155116]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258704921789911		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.258704921789911 | validation: 0.29640635187297093]
	TIME [epoch: 6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27138973356553076		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.27138973356553076 | validation: 0.28800478370112315]
	TIME [epoch: 6 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312809193167084		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.312809193167084 | validation: 0.290903298222933]
	TIME [epoch: 185 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26982447146487265		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.26982447146487265 | validation: 0.27706374847066756]
	TIME [epoch: 12.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2694953743894271		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.2694953743894271 | validation: 0.29280285791975585]
	TIME [epoch: 12.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.272383997201286		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.272383997201286 | validation: 0.30723985443487817]
	TIME [epoch: 12.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30560682995046357		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.30560682995046357 | validation: 0.2803556130185813]
	TIME [epoch: 12.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666822509814267		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.2666822509814267 | validation: 0.27047555272936474]
	TIME [epoch: 12.8 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27945605401390916		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.27945605401390916 | validation: 0.33298345980362476]
	TIME [epoch: 12.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2995020634172085		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.2995020634172085 | validation: 0.32631800672432937]
	TIME [epoch: 12.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33422884879169235		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.33422884879169235 | validation: 0.2559392402549206]
	TIME [epoch: 12.8 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25000662628502435		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.25000662628502435 | validation: 0.24474465552260566]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2408655831662403		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.2408655831662403 | validation: 0.23985744446775434]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24498441230951196		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.24498441230951196 | validation: 0.2976115282962462]
	TIME [epoch: 12.8 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2842082970522732		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.2842082970522732 | validation: 0.3325342817289049]
	TIME [epoch: 12.9 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39443943768287254		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.39443943768287254 | validation: 0.28208513461409546]
	TIME [epoch: 12.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574504345588138		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2574504345588138 | validation: 0.4383631398836322]
	TIME [epoch: 12.9 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953702608574325		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.3953702608574325 | validation: 0.34718090378440974]
	TIME [epoch: 12.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40576827863848414		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.40576827863848414 | validation: 0.30470545770542423]
	TIME [epoch: 12.9 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052018290101323		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.3052018290101323 | validation: 0.49914069092318836]
	TIME [epoch: 12.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42557515385681227		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.42557515385681227 | validation: 0.28259471841627787]
	TIME [epoch: 12.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3026744171170558		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.3026744171170558 | validation: 0.24258563504031994]
	TIME [epoch: 12.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25386072211516864		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.25386072211516864 | validation: 0.31980898280293846]
	TIME [epoch: 12.9 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2979348151869499		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.2979348151869499 | validation: 0.280804697603303]
	TIME [epoch: 12.8 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30567632789167787		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.30567632789167787 | validation: 0.24331476060536855]
	TIME [epoch: 12.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23305051752392253		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.23305051752392253 | validation: 0.2688853402643509]
	TIME [epoch: 12.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2589450587641041		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.2589450587641041 | validation: 0.2744774670729299]
	TIME [epoch: 12.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30332471815291223		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.30332471815291223 | validation: 0.2585123900093457]
	TIME [epoch: 12.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24528294987086138		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.24528294987086138 | validation: 0.2370544931993911]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2384563148814155		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.2384563148814155 | validation: 0.25140319930637384]
	TIME [epoch: 12.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24596678369756875		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.24596678369756875 | validation: 0.2690111222616322]
	TIME [epoch: 12.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2899783694592089		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.2899783694592089 | validation: 0.2839069036200031]
	TIME [epoch: 12.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665813374226964		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.2665813374226964 | validation: 0.2672750613800913]
	TIME [epoch: 12.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29081873977810513		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.29081873977810513 | validation: 0.25018311240529273]
	TIME [epoch: 12.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23605300445708569		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.23605300445708569 | validation: 0.21953333137584397]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22787502542936167		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.22787502542936167 | validation: 0.2458473395744091]
	TIME [epoch: 12.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23421893028297547		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.23421893028297547 | validation: 0.246719906018856]
	TIME [epoch: 12.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811860257806984		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.2811860257806984 | validation: 0.3091881355457784]
	TIME [epoch: 12.8 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2729890468953717		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.2729890468953717 | validation: 0.2812027485898569]
	TIME [epoch: 12.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308512874533444		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.308512874533444 | validation: 0.23792486402202415]
	TIME [epoch: 12.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22466226235934408		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.22466226235934408 | validation: 0.22278420524035197]
	TIME [epoch: 12.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2183260201844079		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.2183260201844079 | validation: 0.21695652662203213]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2238802952820895		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.2238802952820895 | validation: 0.2887606811433476]
	TIME [epoch: 12.8 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25855599250925376		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.25855599250925376 | validation: 0.3159072052404601]
	TIME [epoch: 12.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36482759347330335		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.36482759347330335 | validation: 0.2376010660597613]
	TIME [epoch: 12.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22369645722888704		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.22369645722888704 | validation: 0.24532365059445135]
	TIME [epoch: 12.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22838606274308898		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.22838606274308898 | validation: 0.26459309169841977]
	TIME [epoch: 12.8 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30046444545345496		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.30046444545345496 | validation: 0.3116544422096309]
	TIME [epoch: 12.8 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765620578187893		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.2765620578187893 | validation: 0.2717501582062501]
	TIME [epoch: 12.8 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034004087360504		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.3034004087360504 | validation: 0.22835070344040798]
	TIME [epoch: 12.8 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180464976550484		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.2180464976550484 | validation: 0.25256703822249743]
	TIME [epoch: 12.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2279887013565596		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.2279887013565596 | validation: 0.26197721122786694]
	TIME [epoch: 12.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2984545073901292		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.2984545073901292 | validation: 0.3166585866846056]
	TIME [epoch: 12.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26127948203246854		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.26127948203246854 | validation: 0.2664107754966553]
	TIME [epoch: 12.8 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28399371561344		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.28399371561344 | validation: 0.24270805920622518]
	TIME [epoch: 12.9 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22423221423480527		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.22423221423480527 | validation: 0.21085764029382573]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225196465530929		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.225196465530929 | validation: 0.24835384503685518]
	TIME [epoch: 12.8 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23138467089883716		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.23138467089883716 | validation: 0.2374657991237668]
	TIME [epoch: 12.8 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2723738955477908		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.2723738955477908 | validation: 0.2750964130965906]
	TIME [epoch: 12.8 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24124988635691277		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.24124988635691277 | validation: 0.23534360433173562]
	TIME [epoch: 12.8 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24621402790792815		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.24621402790792815 | validation: 0.2530697198664739]
	TIME [epoch: 12.8 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22585970444252823		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.22585970444252823 | validation: 0.23772578498230168]
	TIME [epoch: 12.8 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23861134440530285		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.23861134440530285 | validation: 0.2687814420257936]
	TIME [epoch: 12.8 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23769342931137125		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.23769342931137125 | validation: 0.24776931767404653]
	TIME [epoch: 12.8 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26078652993493173		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.26078652993493173 | validation: 0.26664946521212135]
	TIME [epoch: 12.8 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24050176563204517		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.24050176563204517 | validation: 0.22203947962454126]
	TIME [epoch: 12.8 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558968600619243		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.2558968600619243 | validation: 0.25590233671993917]
	TIME [epoch: 12.8 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2294809068681697		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.2294809068681697 | validation: 0.22517583589264173]
	TIME [epoch: 12.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2379328037097186		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.2379328037097186 | validation: 0.2902905363217455]
	TIME [epoch: 12.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24957632038667285		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.24957632038667285 | validation: 0.25818163670551736]
	TIME [epoch: 12.8 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2828533501009352		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.2828533501009352 | validation: 0.23902019563824442]
	TIME [epoch: 12.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21119466775259071		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.21119466775259071 | validation: 0.20070398835750464]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034117321526005		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.2034117321526005 | validation: 0.21413803266140388]
	TIME [epoch: 12.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20358656376876771		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.20358656376876771 | validation: 0.21010963425822712]
	TIME [epoch: 12.8 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21503671986520245		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.21503671986520245 | validation: 0.27996361353139765]
	TIME [epoch: 12.8 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564459186210755		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.2564459186210755 | validation: 0.2891566678131334]
	TIME [epoch: 12.8 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3299864211769753		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.3299864211769753 | validation: 0.22175123721219325]
	TIME [epoch: 12.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2082537785484592		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.2082537785484592 | validation: 0.3129579600788932]
	TIME [epoch: 12.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25687876438629664		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.25687876438629664 | validation: 0.30767972631528956]
	TIME [epoch: 12.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34356168370524215		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.34356168370524215 | validation: 0.19554288096635553]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2023780972673063		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.2023780972673063 | validation: 0.4222219814593549]
	TIME [epoch: 12.8 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459046534773273		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.3459046534773273 | validation: 0.29195235127101987]
	TIME [epoch: 12.8 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30880047938431254		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.30880047938431254 | validation: 0.23059281915903865]
	TIME [epoch: 12.8 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24140758850767355		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.24140758850767355 | validation: 0.37621740681038013]
	TIME [epoch: 12.8 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32178826433746055		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.32178826433746055 | validation: 0.22046843966576804]
	TIME [epoch: 12.8 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24109040350458485		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.24109040350458485 | validation: 0.19285558689462645]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2052800975070837		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.2052800975070837 | validation: 0.25267029213116954]
	TIME [epoch: 12.8 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23562996535340128		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.23562996535340128 | validation: 0.2171287080153789]
	TIME [epoch: 12.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2495268093271391		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.2495268093271391 | validation: 0.22241212323577875]
	TIME [epoch: 12.8 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21249787676055135		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.21249787676055135 | validation: 0.1959877957541428]
	TIME [epoch: 12.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1980753932138869		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1980753932138869 | validation: 0.21458188341928475]
	TIME [epoch: 12.8 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20303197649004898		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.20303197649004898 | validation: 0.22586235863714538]
	TIME [epoch: 12.8 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22273818093833364		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.22273818093833364 | validation: 0.25009756819020856]
	TIME [epoch: 12.8 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21863467101698966		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.21863467101698966 | validation: 0.22048072823218662]
	TIME [epoch: 12.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23179772028156337		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.23179772028156337 | validation: 0.24054667379822262]
	TIME [epoch: 12.8 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21540017253927204		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.21540017253927204 | validation: 0.1983648610739929]
	TIME [epoch: 12.8 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21957532614329103		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.21957532614329103 | validation: 0.26176472706819276]
	TIME [epoch: 12.8 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22147653880356286		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.22147653880356286 | validation: 0.2387312609642415]
	TIME [epoch: 12.8 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24439868914465682		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.24439868914465682 | validation: 0.2227740036223098]
	TIME [epoch: 12.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2027785927001907		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.2027785927001907 | validation: 0.1924248021507997]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1999780503033617		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1999780503033617 | validation: 0.2122263803701575]
	TIME [epoch: 12.8 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20292291049929026		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.20292291049929026 | validation: 0.22575502394747435]
	TIME [epoch: 12.8 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23269110732886547		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.23269110732886547 | validation: 0.25164536101101437]
	TIME [epoch: 12.8 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22214429627821727		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.22214429627821727 | validation: 0.21218798316378545]
	TIME [epoch: 12.8 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2223994378722953		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.2223994378722953 | validation: 0.23365779732139186]
	TIME [epoch: 12.8 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20730311388015607		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.20730311388015607 | validation: 0.19106567640570182]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21550357760695704		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.21550357760695704 | validation: 0.2308143527520776]
	TIME [epoch: 12.8 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2187928810737288		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.2187928810737288 | validation: 0.2337870351559982]
	TIME [epoch: 12.8 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24299613038828252		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.24299613038828252 | validation: 0.23595182952416366]
	TIME [epoch: 12.8 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20747634043948857		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.20747634043948857 | validation: 0.19296277822756636]
	TIME [epoch: 12.8 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21702032276126967		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.21702032276126967 | validation: 0.24575885294833588]
	TIME [epoch: 12.8 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21520439958078194		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.21520439958078194 | validation: 0.23749261322771018]
	TIME [epoch: 12.8 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24992099375842558		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.24992099375842558 | validation: 0.2298803279586756]
	TIME [epoch: 12.8 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19457661832035433		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.19457661832035433 | validation: 0.20705045067776473]
	TIME [epoch: 12.8 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18313268873553532		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.18313268873553532 | validation: 0.20459122275987768]
	TIME [epoch: 12.8 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18718890375940114		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.18718890375940114 | validation: 0.26642316153250406]
	TIME [epoch: 12.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22320674268591612		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.22320674268591612 | validation: 0.25321732403758307]
	TIME [epoch: 12.8 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28613933837067884		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.28613933837067884 | validation: 0.19861676560880692]
	TIME [epoch: 12.8 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18748096455171087		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.18748096455171087 | validation: 0.2522920916991717]
	TIME [epoch: 12.8 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21287166799885918		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.21287166799885918 | validation: 0.2622996438993768]
	TIME [epoch: 12.8 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27871949640606003		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.27871949640606003 | validation: 0.2082420283234354]
	TIME [epoch: 12.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19196136530705452		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.19196136530705452 | validation: 0.21459968097994417]
	TIME [epoch: 12.8 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19265794903990305		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.19265794903990305 | validation: 0.2073519131259415]
	TIME [epoch: 12.8 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22126943852374653		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.22126943852374653 | validation: 0.2879314618722002]
	TIME [epoch: 12.8 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25159509836541255		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.25159509836541255 | validation: 0.2577587440503607]
	TIME [epoch: 12.8 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28004021028766746		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.28004021028766746 | validation: 0.20583656850555584]
	TIME [epoch: 12.8 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18841805559892283		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.18841805559892283 | validation: 0.33500589787808654]
	TIME [epoch: 12.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27295740578203254		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.27295740578203254 | validation: 0.2676840735562113]
	TIME [epoch: 12.8 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27922089479131407		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.27922089479131407 | validation: 0.19742109937140992]
	TIME [epoch: 12.8 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18686096800090524		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.18686096800090524 | validation: 0.3697441906008867]
	TIME [epoch: 12.8 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810725697720287		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.2810725697720287 | validation: 0.24650358101179554]
	TIME [epoch: 12.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24540042561295736		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.24540042561295736 | validation: 0.18916671293747978]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18828343466169614		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.18828343466169614 | validation: 0.25870587928154504]
	TIME [epoch: 12.8 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23333722261910733		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.23333722261910733 | validation: 0.2436122836922678]
	TIME [epoch: 12.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2353569992761627		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.2353569992761627 | validation: 0.19345731959073753]
	TIME [epoch: 12.8 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18367854668006856		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.18367854668006856 | validation: 0.2230180739516545]
	TIME [epoch: 12.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19558885632576445		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.19558885632576445 | validation: 0.22113075581562197]
	TIME [epoch: 12.8 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22786140505484626		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.22786140505484626 | validation: 0.229961888967534]
	TIME [epoch: 12.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18876923559967826		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.18876923559967826 | validation: 0.17941550621348795]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18930969151731186		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.18930969151731186 | validation: 0.21201097548766482]
	TIME [epoch: 12.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17945250654379735		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.17945250654379735 | validation: 0.19033817034407957]
	TIME [epoch: 12.8 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18226395624776026		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.18226395624776026 | validation: 0.19230828585611803]
	TIME [epoch: 12.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815562293587793		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.1815562293587793 | validation: 0.1867082708787543]
	TIME [epoch: 12.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18267176731098317		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.18267176731098317 | validation: 0.19120046876440694]
	TIME [epoch: 12.8 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18043697532941327		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.18043697532941327 | validation: 0.18321983958427768]
	TIME [epoch: 12.8 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17859835209822575		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.17859835209822575 | validation: 0.2299442865770156]
	TIME [epoch: 12.8 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18576301583072208		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.18576301583072208 | validation: 0.23339351274218312]
	TIME [epoch: 12.8 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25331572074357445		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.25331572074357445 | validation: 0.2864568071478895]
	TIME [epoch: 12.8 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305912633611709		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.2305912633611709 | validation: 0.21796726735344968]
	TIME [epoch: 12.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24504466094360117		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.24504466094360117 | validation: 0.20697085716081745]
	TIME [epoch: 12.8 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18104728313023757		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.18104728313023757 | validation: 0.19960705839641252]
	TIME [epoch: 12.8 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17448744902441393		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.17448744902441393 | validation: 0.1900708516954601]
	TIME [epoch: 12.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20645699463901182		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.20645699463901182 | validation: 0.2608458693887114]
	TIME [epoch: 12.8 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23974714931295973		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.23974714931295973 | validation: 0.24950674013513915]
	TIME [epoch: 12.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26002038664473937		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.26002038664473937 | validation: 0.20986026992932266]
	TIME [epoch: 12.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1776827794927567		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.1776827794927567 | validation: 0.21157295196913953]
	TIME [epoch: 12.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19008569611831277		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.19008569611831277 | validation: 0.1959313241962436]
	TIME [epoch: 12.8 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22089032692227947		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.22089032692227947 | validation: 0.2465331219676916]
	TIME [epoch: 12.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2009109325668279		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.2009109325668279 | validation: 0.20728712730222162]
	TIME [epoch: 12.8 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992835953590439		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.1992835953590439 | validation: 0.20821434161135302]
	TIME [epoch: 12.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18228748386805443		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.18228748386805443 | validation: 0.1794971877742617]
	TIME [epoch: 12.8 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804017858937887		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.1804017858937887 | validation: 0.18854396813228053]
	TIME [epoch: 12.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17348180686167083		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.17348180686167083 | validation: 0.17901793419458972]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18203083121268407		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.18203083121268407 | validation: 0.2218016764172447]
	TIME [epoch: 12.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19014074456611355		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.19014074456611355 | validation: 0.21888588812953913]
	TIME [epoch: 12.8 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23538919136698205		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.23538919136698205 | validation: 0.24733291246152878]
	TIME [epoch: 12.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21282358046761027		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.21282358046761027 | validation: 0.2118903727869148]
	TIME [epoch: 12.8 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2056705093399473		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.2056705093399473 | validation: 0.20385121402506226]
	TIME [epoch: 12.8 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.179036097163655		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.179036097163655 | validation: 0.19034067164310933]
	TIME [epoch: 12.8 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18511144305213292		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.18511144305213292 | validation: 0.21598791925276295]
	TIME [epoch: 12.8 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18284836638848334		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.18284836638848334 | validation: 0.1827786531050839]
	TIME [epoch: 12.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18722971814151435		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.18722971814151435 | validation: 0.23720196916019676]
	TIME [epoch: 12.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19122459684643733		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.19122459684643733 | validation: 0.195321153164393]
	TIME [epoch: 12.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20394756288651586		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.20394756288651586 | validation: 0.2447396577035904]
	TIME [epoch: 12.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1934088957719652		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.1934088957719652 | validation: 0.20598274484471416]
	TIME [epoch: 12.8 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20502350049494739		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.20502350049494739 | validation: 0.2050778471570962]
	TIME [epoch: 12.8 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18187664746341703		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.18187664746341703 | validation: 0.1858410495186304]
	TIME [epoch: 12.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17072807981245666		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.17072807981245666 | validation: 0.21803626953331534]
	TIME [epoch: 12.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804183615630462		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.1804183615630462 | validation: 0.18913329953368194]
	TIME [epoch: 12.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19455923500991532		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.19455923500991532 | validation: 0.2709346633610665]
	TIME [epoch: 12.8 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21182405855616065		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.21182405855616065 | validation: 0.20983050280756138]
	TIME [epoch: 12.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2150904839529302		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.2150904839529302 | validation: 0.21188125672729075]
	TIME [epoch: 12.8 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804268620739599		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.1804268620739599 | validation: 0.18815827188244308]
	TIME [epoch: 12.8 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17008399306263455		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.17008399306263455 | validation: 0.17103176659553654]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17450552763511282		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.17450552763511282 | validation: 0.20490476564397359]
	TIME [epoch: 12.8 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17257326330282222		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.17257326330282222 | validation: 0.20406192412758914]
	TIME [epoch: 12.8 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20314108272274758		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.20314108272274758 | validation: 0.2737397556756029]
	TIME [epoch: 12.8 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071624845067559		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.2071624845067559 | validation: 0.21426128553649743]
	TIME [epoch: 12.8 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22627648567234523		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.22627648567234523 | validation: 0.18940112139649642]
	TIME [epoch: 12.8 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17436295766794693		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.17436295766794693 | validation: 0.17730766403858042]
	TIME [epoch: 12.8 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17062505928727814		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.17062505928727814 | validation: 0.16634269616762262]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734875756265813		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.1734875756265813 | validation: 0.23032476137567237]
	TIME [epoch: 12.8 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18601652713132324		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.18601652713132324 | validation: 0.2082621394343538]
	TIME [epoch: 12.8 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2140589557127592		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.2140589557127592 | validation: 0.22992306129777218]
	TIME [epoch: 12.8 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18778005683863902		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.18778005683863902 | validation: 0.18037986607755171]
	TIME [epoch: 12.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1801735699654303		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.1801735699654303 | validation: 0.23276277225823946]
	TIME [epoch: 12.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1759776563673877		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.1759776563673877 | validation: 0.17591373609829683]
	TIME [epoch: 12.8 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18188845603068152		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.18188845603068152 | validation: 0.21734876598042227]
	TIME [epoch: 12.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1883559791676233		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.1883559791676233 | validation: 0.2141485788458362]
	TIME [epoch: 12.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20808682521049865		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.20808682521049865 | validation: 0.21737705460583837]
	TIME [epoch: 12.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1818328903581307		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1818328903581307 | validation: 0.1751639082218418]
	TIME [epoch: 12.8 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16789808559075708		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.16789808559075708 | validation: 0.17911689615048226]
	TIME [epoch: 12.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169457526874314		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.169457526874314 | validation: 0.1906538244039586]
	TIME [epoch: 12.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18154052685056704		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.18154052685056704 | validation: 0.2291204772230179]
	TIME [epoch: 12.8 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18549240594594518		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.18549240594594518 | validation: 0.19254533255605358]
	TIME [epoch: 12.8 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1889237059770948		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.1889237059770948 | validation: 0.22003939526003943]
	TIME [epoch: 12.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17824158498831566		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.17824158498831566 | validation: 0.1774547182291717]
	TIME [epoch: 12.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18867640643788564		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.18867640643788564 | validation: 0.23600657232876637]
	TIME [epoch: 12.8 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18394917322847207		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.18394917322847207 | validation: 0.18081704321653436]
	TIME [epoch: 12.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2010944445495332		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.2010944445495332 | validation: 0.2156343389349766]
	TIME [epoch: 12.9 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16897209592685716		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.16897209592685716 | validation: 0.18472646169404544]
	TIME [epoch: 12.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16667021268102922		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.16667021268102922 | validation: 0.18041696841747795]
	TIME [epoch: 12.9 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717480398898074		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.1717480398898074 | validation: 0.17023018413459085]
	TIME [epoch: 12.8 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17620905347709845		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.17620905347709845 | validation: 0.19630806830329886]
	TIME [epoch: 12.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18296584347892453		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.18296584347892453 | validation: 0.20165224436153278]
	TIME [epoch: 12.8 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19400896782847737		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.19400896782847737 | validation: 0.21129331298076776]
	TIME [epoch: 12.8 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785906295032538		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.1785906295032538 | validation: 0.18150569097933486]
	TIME [epoch: 12.8 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17676455229687624		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.17676455229687624 | validation: 0.21232540858956886]
	TIME [epoch: 12.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17527069885547938		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.17527069885547938 | validation: 0.17863954872364593]
	TIME [epoch: 12.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1762979796619289		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.1762979796619289 | validation: 0.1956302402357029]
	TIME [epoch: 12.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16797019517624062		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.16797019517624062 | validation: 0.18250299202735876]
	TIME [epoch: 12.8 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16294304647511315		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.16294304647511315 | validation: 0.20583286711992851]
	TIME [epoch: 12.8 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726809457895572		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.1726809457895572 | validation: 0.19512240229523214]
	TIME [epoch: 12.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038343965466204		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.2038343965466204 | validation: 0.22066287885365063]
	TIME [epoch: 12.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19281701701216694		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.19281701701216694 | validation: 0.18679011953277166]
	TIME [epoch: 12.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19743634931665965		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.19743634931665965 | validation: 0.1986560710614076]
	TIME [epoch: 12.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16179297232416665		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.16179297232416665 | validation: 0.18223356597229295]
	TIME [epoch: 12.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15599656480050691		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.15599656480050691 | validation: 0.18143754316116809]
	TIME [epoch: 12.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15810670328759927		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.15810670328759927 | validation: 0.1753782717539797]
	TIME [epoch: 12.8 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1643473925083645		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.1643473925083645 | validation: 0.17752079641966592]
	TIME [epoch: 12.8 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17081553189559312		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.17081553189559312 | validation: 0.24366375827284067]
	TIME [epoch: 12.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19159812493774744		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.19159812493774744 | validation: 0.19686167912566377]
	TIME [epoch: 12.8 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21389003613100258		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.21389003613100258 | validation: 0.23183303208116365]
	TIME [epoch: 12.8 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17825007782962096		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.17825007782962096 | validation: 0.18149672289762664]
	TIME [epoch: 12.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16446847928577568		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.16446847928577568 | validation: 0.1926806959046867]
	TIME [epoch: 12.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16217771167381148		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.16217771167381148 | validation: 0.163674198497328]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16680884694363785		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.16680884694363785 | validation: 0.19986552206136785]
	TIME [epoch: 12.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672579751861842		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.1672579751861842 | validation: 0.1892456365015452]
	TIME [epoch: 12.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17815341143030494		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.17815341143030494 | validation: 0.22323821453815682]
	TIME [epoch: 12.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711642945913117		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.1711642945913117 | validation: 0.17910983883560028]
	TIME [epoch: 12.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17999880708192406		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.17999880708192406 | validation: 0.22215701920915906]
	TIME [epoch: 12.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18128311359949897		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.18128311359949897 | validation: 0.1841268334343314]
	TIME [epoch: 12.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1890132092021475		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.1890132092021475 | validation: 0.2072290645111036]
	TIME [epoch: 12.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16943939904127042		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.16943939904127042 | validation: 0.16241885372893697]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16544482493526264		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.16544482493526264 | validation: 0.19734417767642853]
	TIME [epoch: 12.8 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15937136837849314		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.15937136837849314 | validation: 0.168803911567185]
	TIME [epoch: 12.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16745522242627914		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.16745522242627914 | validation: 0.23096849025709468]
	TIME [epoch: 12.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1748467283241427		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.1748467283241427 | validation: 0.18706822964409292]
	TIME [epoch: 12.8 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18756183151938513		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.18756183151938513 | validation: 0.2048528957819783]
	TIME [epoch: 12.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680313013079669		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.1680313013079669 | validation: 0.15651821805229865]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16513937795755254		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.16513937795755254 | validation: 0.19993425559784359]
	TIME [epoch: 12.8 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598679511124536		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.1598679511124536 | validation: 0.17356538558242088]
	TIME [epoch: 12.8 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16384874471657998		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.16384874471657998 | validation: 0.173564896121244]
	TIME [epoch: 12.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15367705118028924		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.15367705118028924 | validation: 0.17976553234629777]
	TIME [epoch: 12.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15802471213396832		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.15802471213396832 | validation: 0.17424619248053472]
	TIME [epoch: 12.8 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575209989264594		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.1575209989264594 | validation: 0.1960180902753729]
	TIME [epoch: 12.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1624360891363328		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.1624360891363328 | validation: 0.18475961163462915]
	TIME [epoch: 12.8 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18781184875506593		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.18781184875506593 | validation: 0.23526568486446608]
	TIME [epoch: 12.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1897980187680348		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.1897980187680348 | validation: 0.19500149067371372]
	TIME [epoch: 12.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20300160055418448		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.20300160055418448 | validation: 0.1835866095068539]
	TIME [epoch: 12.8 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15733512723576745		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.15733512723576745 | validation: 0.21264969348243792]
	TIME [epoch: 12.8 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659253142137697		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.1659253142137697 | validation: 0.1953468694228796]
	TIME [epoch: 12.8 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978101418966166		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.1978101418966166 | validation: 0.19402432557810279]
	TIME [epoch: 12.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1604722061379232		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.1604722061379232 | validation: 0.17796594474837413]
	TIME [epoch: 12.8 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15723473232725219		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.15723473232725219 | validation: 0.17588748004009563]
	TIME [epoch: 12.8 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524286529025781		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.1524286529025781 | validation: 0.18431780156856534]
	TIME [epoch: 12.8 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15687234668254627		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.15687234668254627 | validation: 0.16946038057834625]
	TIME [epoch: 12.8 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17026321053957966		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.17026321053957966 | validation: 0.24681869429422917]
	TIME [epoch: 12.8 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18834836667759847		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.18834836667759847 | validation: 0.2022611596946031]
	TIME [epoch: 12.8 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20217313487467217		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.20217313487467217 | validation: 0.18241900965693625]
	TIME [epoch: 12.9 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556287583052136		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.1556287583052136 | validation: 0.18901326137259963]
	TIME [epoch: 12.8 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1636079561025846		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.1636079561025846 | validation: 0.17192418720940467]
	TIME [epoch: 12.8 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18250569874967357		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.18250569874967357 | validation: 0.2018024721648402]
	TIME [epoch: 12.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16301310550022813		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.16301310550022813 | validation: 0.17907282974320005]
	TIME [epoch: 12.8 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15705336267030132		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.15705336267030132 | validation: 0.17373971774459235]
	TIME [epoch: 12.8 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15405710376635404		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.15405710376635404 | validation: 0.16930226972920592]
	TIME [epoch: 12.8 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1555821705279556		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.1555821705279556 | validation: 0.1951641879587449]
	TIME [epoch: 12.8 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1633592069868105		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.1633592069868105 | validation: 0.18200864415783405]
	TIME [epoch: 12.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18614316472456735		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.18614316472456735 | validation: 0.20128387899532318]
	TIME [epoch: 12.8 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16351045588764185		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.16351045588764185 | validation: 0.16841779532883794]
	TIME [epoch: 12.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1628572587752163		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.1628572587752163 | validation: 0.19941430587641912]
	TIME [epoch: 12.8 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1554454751255515		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.1554454751255515 | validation: 0.1713777704951766]
	TIME [epoch: 12.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16569751463105667		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.16569751463105667 | validation: 0.20321101736909764]
	TIME [epoch: 12.8 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16178262294035656		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.16178262294035656 | validation: 0.16980277558795281]
	TIME [epoch: 12.8 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669130225903155		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.1669130225903155 | validation: 0.18768813119020736]
	TIME [epoch: 12.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651281620660339		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.1651281620660339 | validation: 0.16422475723291502]
	TIME [epoch: 12.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15787987219768973		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.15787987219768973 | validation: 0.19846119241119237]
	TIME [epoch: 12.8 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15888674824815013		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.15888674824815013 | validation: 0.17779400430199]
	TIME [epoch: 12.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16449147375041082		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.16449147375041082 | validation: 0.21090815311745034]
	TIME [epoch: 12.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16595306886357544		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.16595306886357544 | validation: 0.16004556440619844]
	TIME [epoch: 12.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17525656871130843		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.17525656871130843 | validation: 0.19169166638372892]
	TIME [epoch: 12.8 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15451276506883663		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.15451276506883663 | validation: 0.15886534490857507]
	TIME [epoch: 12.8 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14840545564870283		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.14840545564870283 | validation: 0.1873226617560642]
	TIME [epoch: 12.8 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484408381254252		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.1484408381254252 | validation: 0.16260114267223347]
	TIME [epoch: 12.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15256882925144338		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.15256882925144338 | validation: 0.20465255858873277]
	TIME [epoch: 12.8 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16071447944597644		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.16071447944597644 | validation: 0.18841043942405042]
	TIME [epoch: 12.8 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19900226988473008		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.19900226988473008 | validation: 0.2145353318409467]
	TIME [epoch: 12.8 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16127403496010345		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.16127403496010345 | validation: 0.16613392697499652]
	TIME [epoch: 12.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16242370730459332		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.16242370730459332 | validation: 0.20913030834142987]
	TIME [epoch: 12.8 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841710831632377		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.15841710831632377 | validation: 0.177260337717753]
	TIME [epoch: 12.8 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17250572419335505		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.17250572419335505 | validation: 0.20610623469767939]
	TIME [epoch: 12.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163511864303553		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.163511864303553 | validation: 0.17008592740532272]
	TIME [epoch: 12.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15746937606981257		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.15746937606981257 | validation: 0.19471126408009987]
	TIME [epoch: 12.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16000883274919975		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.16000883274919975 | validation: 0.16732493991326655]
	TIME [epoch: 12.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576462493942676		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.1576462493942676 | validation: 0.21545755004169181]
	TIME [epoch: 12.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15987853464305155		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.15987853464305155 | validation: 0.16512366869538694]
	TIME [epoch: 12.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16653547341906585		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.16653547341906585 | validation: 0.19129912359750142]
	TIME [epoch: 12.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15744958332288264		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.15744958332288264 | validation: 0.1629023305358957]
	TIME [epoch: 12.8 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15378442454382696		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.15378442454382696 | validation: 0.17491702823374222]
	TIME [epoch: 12.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14879827166001294		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.14879827166001294 | validation: 0.16458127171707115]
	TIME [epoch: 12.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15264893089709358		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.15264893089709358 | validation: 0.21467872170964766]
	TIME [epoch: 12.8 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16728418991319863		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.16728418991319863 | validation: 0.18736021734260144]
	TIME [epoch: 12.8 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1774864419615105		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1774864419615105 | validation: 0.1728859692404106]
	TIME [epoch: 12.8 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14952458581461409		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.14952458581461409 | validation: 0.17968084032359113]
	TIME [epoch: 12.8 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490099903115328		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.1490099903115328 | validation: 0.16193599595813635]
	TIME [epoch: 12.8 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15145249099009297		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.15145249099009297 | validation: 0.1796837848702533]
	TIME [epoch: 12.8 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15088482688149502		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.15088482688149502 | validation: 0.17483692630801478]
	TIME [epoch: 12.8 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519316817266376		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.15519316817266376 | validation: 0.2132794834011183]
	TIME [epoch: 12.8 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15790947948282155		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.15790947948282155 | validation: 0.1822037541511913]
	TIME [epoch: 12.8 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19179243670410276		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.19179243670410276 | validation: 0.19060883354855876]
	TIME [epoch: 12.8 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15338510237995087		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.15338510237995087 | validation: 0.17532944084122082]
	TIME [epoch: 12.8 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14761537155782833		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.14761537155782833 | validation: 0.17298533854007658]
	TIME [epoch: 12.8 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14650826450186213		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.14650826450186213 | validation: 0.17622501912934305]
	TIME [epoch: 12.8 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14993715779483338		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.14993715779483338 | validation: 0.19658680664782896]
	TIME [epoch: 12.8 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15242586360542335		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.15242586360542335 | validation: 0.17422518705995638]
	TIME [epoch: 12.8 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17889504601817607		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.17889504601817607 | validation: 0.22735574116887838]
	TIME [epoch: 12.8 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16650886973042436		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.16650886973042436 | validation: 0.16210936375279525]
	TIME [epoch: 12.8 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1633199418954299		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.1633199418954299 | validation: 0.18625267547539703]
	TIME [epoch: 12.8 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14846810402732533		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.14846810402732533 | validation: 0.17052046724084757]
	TIME [epoch: 12.8 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15341257472067468		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.15341257472067468 | validation: 0.20373921599426092]
	TIME [epoch: 12.8 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15523393538864544		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.15523393538864544 | validation: 0.17081231786845547]
	TIME [epoch: 12.8 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16323345370353448		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.16323345370353448 | validation: 0.19179333510650803]
	TIME [epoch: 12.8 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506892279932944		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.1506892279932944 | validation: 0.16125786138113105]
	TIME [epoch: 12.8 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14728394838823716		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.14728394838823716 | validation: 0.19299717504354796]
	TIME [epoch: 12.8 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14867492518285158		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.14867492518285158 | validation: 0.17151803720352987]
	TIME [epoch: 12.8 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15288726518164242		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.15288726518164242 | validation: 0.19358443901770905]
	TIME [epoch: 12.8 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15250856164566867		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.15250856164566867 | validation: 0.16193681412546296]
	TIME [epoch: 12.8 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16286193514286268		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.16286193514286268 | validation: 0.1803611813025363]
	TIME [epoch: 12.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15215277376312375		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.15215277376312375 | validation: 0.15444332624805313]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1489789163207461		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.1489789163207461 | validation: 0.1688654955080755]
	TIME [epoch: 12.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14547892219471378		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.14547892219471378 | validation: 0.15886253917762405]
	TIME [epoch: 12.8 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14820154595777468		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.14820154595777468 | validation: 0.20111167551681555]
	TIME [epoch: 12.8 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14867849380701068		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.14867849380701068 | validation: 0.16569705430633755]
	TIME [epoch: 12.8 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556061015848638		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.1556061015848638 | validation: 0.23079131225230803]
	TIME [epoch: 12.8 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696139827880276		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.1696139827880276 | validation: 0.16975421085833473]
	TIME [epoch: 12.8 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16794674265986806		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.16794674265986806 | validation: 0.18583078401367842]
	TIME [epoch: 12.8 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14427754385149136		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.14427754385149136 | validation: 0.1575087103732149]
	TIME [epoch: 12.8 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410794407436924		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.1410794407436924 | validation: 0.17811019669652273]
	TIME [epoch: 12.8 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14713103993982887		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.14713103993982887 | validation: 0.1639639513270671]
	TIME [epoch: 12.8 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14493323335614597		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.14493323335614597 | validation: 0.18275676408375627]
	TIME [epoch: 12.8 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14424948815649213		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.14424948815649213 | validation: 0.15560203731312347]
	TIME [epoch: 12.8 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14980646750472335		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.14980646750472335 | validation: 0.22833682209049433]
	TIME [epoch: 12.8 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15838864107534592		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.15838864107534592 | validation: 0.16561183596590345]
	TIME [epoch: 12.8 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16713985095912545		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.16713985095912545 | validation: 0.18701918417131794]
	TIME [epoch: 12.8 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15027612411386246		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.15027612411386246 | validation: 0.16114662591488876]
	TIME [epoch: 12.8 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14340200082065396		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.14340200082065396 | validation: 0.17328346594518373]
	TIME [epoch: 12.8 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474576798949688		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.1474576798949688 | validation: 0.15781896745853172]
	TIME [epoch: 12.8 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14719774028221985		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.14719774028221985 | validation: 0.1904733528080207]
	TIME [epoch: 12.8 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14344031417716563		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.14344031417716563 | validation: 0.15731600949568633]
	TIME [epoch: 12.8 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14865730275087935		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.14865730275087935 | validation: 0.2217084946232522]
	TIME [epoch: 12.8 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15698607143228743		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.15698607143228743 | validation: 0.16659201401892487]
	TIME [epoch: 12.8 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1695905025468697		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.1695905025468697 | validation: 0.17452156569262806]
	TIME [epoch: 12.8 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471602671622797		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.1471602671622797 | validation: 0.16243734988303926]
	TIME [epoch: 12.8 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14308146792178866		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.14308146792178866 | validation: 0.17345463997170238]
	TIME [epoch: 12.8 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14182998969216962		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.14182998969216962 | validation: 0.1755679793680105]
	TIME [epoch: 12.8 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14484651788419392		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.14484651788419392 | validation: 0.1736815142539225]
	TIME [epoch: 12.8 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14139889194293828		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.14139889194293828 | validation: 0.16038043934975554]
	TIME [epoch: 12.8 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413316979732021		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.1413316979732021 | validation: 0.15457080327196174]
	TIME [epoch: 12.8 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13923084709936473		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.13923084709936473 | validation: 0.15599234135835294]
	TIME [epoch: 12.8 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244523436909995		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.14244523436909995 | validation: 0.19273239657430433]
	TIME [epoch: 12.8 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14728852045213703		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.14728852045213703 | validation: 0.17102671873626837]
	TIME [epoch: 12.8 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18906595185714076		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.18906595185714076 | validation: 0.209668673758533]
	TIME [epoch: 12.8 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582698032718309		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.1582698032718309 | validation: 0.16621229595072926]
	TIME [epoch: 12.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15205798521609246		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.15205798521609246 | validation: 0.17859425026377646]
	TIME [epoch: 12.8 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14200056007050868		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.14200056007050868 | validation: 0.16267746778592862]
	TIME [epoch: 12.8 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13885532832545328		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.13885532832545328 | validation: 0.1476517227652723]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_874.pth
	Model improved!!!
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14362984846341548		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.14362984846341548 | validation: 0.1989298756800918]
	TIME [epoch: 12.8 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1532809503496507		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.1532809503496507 | validation: 0.17553648247740167]
	TIME [epoch: 12.8 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16450616489402914		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.16450616489402914 | validation: 0.18330372924829064]
	TIME [epoch: 12.8 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420016832789236		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.1420016832789236 | validation: 0.16112488498496985]
	TIME [epoch: 12.8 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14583356836066197		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.14583356836066197 | validation: 0.1962060785222152]
	TIME [epoch: 12.8 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14623561345645567		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.14623561345645567 | validation: 0.167166138230561]
	TIME [epoch: 12.8 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15034626855271704		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.15034626855271704 | validation: 0.17822774788669402]
	TIME [epoch: 12.8 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1353974685172291		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.1353974685172291 | validation: 0.1699230548521464]
	TIME [epoch: 12.8 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14328562777915743		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.14328562777915743 | validation: 0.1725511508720774]
	TIME [epoch: 12.8 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14666238570417028		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.14666238570417028 | validation: 0.15270823486162152]
	TIME [epoch: 12.8 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15850824160711133		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.15850824160711133 | validation: 0.21371396258889647]
	TIME [epoch: 12.8 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14928914163971838		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.14928914163971838 | validation: 0.15878238397934155]
	TIME [epoch: 12.8 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484069472050976		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1484069472050976 | validation: 0.17901848555526223]
	TIME [epoch: 12.8 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14352649159348432		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.14352649159348432 | validation: 0.16668273579798157]
	TIME [epoch: 12.8 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14110847990935177		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.14110847990935177 | validation: 0.1743413697820986]
	TIME [epoch: 12.8 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384535878040987		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.1384535878040987 | validation: 0.1560931854510019]
	TIME [epoch: 12.8 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14111194379732653		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.14111194379732653 | validation: 0.16148855121132444]
	TIME [epoch: 12.8 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407223550002208		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.1407223550002208 | validation: 0.1912903303979202]
	TIME [epoch: 12.8 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1429040471339193		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.1429040471339193 | validation: 0.16344464580768076]
	TIME [epoch: 12.8 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15962394708532893		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.15962394708532893 | validation: 0.21264581272659458]
	TIME [epoch: 12.8 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617459787778181		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.1617459787778181 | validation: 0.15782032215461897]
	TIME [epoch: 12.8 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15500687340948144		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.15500687340948144 | validation: 0.15743109472112962]
	TIME [epoch: 12.8 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13638272415936478		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.13638272415936478 | validation: 0.18625785176005405]
	TIME [epoch: 12.8 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14630311236545818		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.14630311236545818 | validation: 0.15138674195172047]
	TIME [epoch: 12.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1539070092486074		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.1539070092486074 | validation: 0.1682690474040873]
	TIME [epoch: 12.8 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14344269154508635		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.14344269154508635 | validation: 0.1719946897598839]
	TIME [epoch: 12.8 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1358984080694329		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.1358984080694329 | validation: 0.16252239697780024]
	TIME [epoch: 12.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411877773455334		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.1411877773455334 | validation: 0.18978769961586403]
	TIME [epoch: 12.8 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410576870839205		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.1410576870839205 | validation: 0.15127799100914913]
	TIME [epoch: 12.8 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15091850525406772		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.15091850525406772 | validation: 0.1956958752297428]
	TIME [epoch: 12.8 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14551991650648818		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.14551991650648818 | validation: 0.16200021770186745]
	TIME [epoch: 12.8 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1432646532234471		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.1432646532234471 | validation: 0.1684279835800011]
	TIME [epoch: 12.8 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13495437634845103		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.13495437634845103 | validation: 0.1485475741997239]
	TIME [epoch: 12.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13957213218564624		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.13957213218564624 | validation: 0.17321331416458746]
	TIME [epoch: 12.8 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13704408507113744		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.13704408507113744 | validation: 0.1612219868397016]
	TIME [epoch: 12.8 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14568685034700302		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.14568685034700302 | validation: 0.1674397530818145]
	TIME [epoch: 12.8 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13626603253318845		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.13626603253318845 | validation: 0.16084411188372624]
	TIME [epoch: 12.8 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436916496373116		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.1436916496373116 | validation: 0.18168175201574438]
	TIME [epoch: 12.8 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1454015374576273		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.1454015374576273 | validation: 0.1772065067890649]
	TIME [epoch: 12.8 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15814815660262865		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.15814815660262865 | validation: 0.20396644855135673]
	TIME [epoch: 12.8 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14255743599004514		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.14255743599004514 | validation: 0.16071284894066157]
	TIME [epoch: 12.8 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13678313115317786		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.13678313115317786 | validation: 0.18173633486609608]
	TIME [epoch: 12.8 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14148308662542466		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.14148308662542466 | validation: 0.16159495342398178]
	TIME [epoch: 12.8 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1346708016959828		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.1346708016959828 | validation: 0.19217834702622058]
	TIME [epoch: 12.8 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400892684584985		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.1400892684584985 | validation: 0.16253125052501333]
	TIME [epoch: 12.8 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15002207480349308		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.15002207480349308 | validation: 0.1881122395044003]
	TIME [epoch: 12.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14162447090229924		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.14162447090229924 | validation: 0.14694241079837253]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14748605249574245		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.14748605249574245 | validation: 0.18126413000762856]
	TIME [epoch: 12.8 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14019524899763394		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.14019524899763394 | validation: 0.1554790540976389]
	TIME [epoch: 12.8 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14266606763225148		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.14266606763225148 | validation: 0.1710920559121709]
	TIME [epoch: 12.8 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.133816559516674		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.133816559516674 | validation: 0.14841345261816485]
	TIME [epoch: 12.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1344144098744862		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.1344144098744862 | validation: 0.17397287484540996]
	TIME [epoch: 12.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13443166380297		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.13443166380297 | validation: 0.1757782454657337]
	TIME [epoch: 12.8 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14693298937949556		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.14693298937949556 | validation: 0.22499823735247937]
	TIME [epoch: 12.8 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15654629537643056		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.15654629537643056 | validation: 0.15674658283578732]
	TIME [epoch: 12.8 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15281658912625204		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.15281658912625204 | validation: 0.1692755952932165]
	TIME [epoch: 12.8 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136947753996749		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.136947753996749 | validation: 0.1684858464100223]
	TIME [epoch: 12.8 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341228006917863		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.1341228006917863 | validation: 0.16639648419381375]
	TIME [epoch: 12.8 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349451296852918		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.1349451296852918 | validation: 0.1673128379620521]
	TIME [epoch: 12.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13733073734300713		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.13733073734300713 | validation: 0.1520522849433049]
	TIME [epoch: 12.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335365056796781		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.1335365056796781 | validation: 0.17604969605315704]
	TIME [epoch: 12.8 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377563566878202		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.1377563566878202 | validation: 0.14440088978239154]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_936.pth
	Model improved!!!
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13718593518857533		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.13718593518857533 | validation: 0.18343417690186914]
	TIME [epoch: 12.8 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14099986628466754		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.14099986628466754 | validation: 0.14834308346320543]
	TIME [epoch: 12.8 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14332705097709858		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.14332705097709858 | validation: 0.20534672968174533]
	TIME [epoch: 12.8 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490883343361935		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.1490883343361935 | validation: 0.16068818797062587]
	TIME [epoch: 12.8 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14620681362898555		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.14620681362898555 | validation: 0.1735753073335587]
	TIME [epoch: 12.8 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377539220648995		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1377539220648995 | validation: 0.15637928367038964]
	TIME [epoch: 12.8 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13435393171649412		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.13435393171649412 | validation: 0.1504387353274782]
	TIME [epoch: 12.8 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13474174933465696		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.13474174933465696 | validation: 0.19919622473520207]
	TIME [epoch: 12.8 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145031797625845		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.145031797625845 | validation: 0.1516333455030028]
	TIME [epoch: 12.8 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15386857289042435		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.15386857289042435 | validation: 0.17450145579167647]
	TIME [epoch: 12.8 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1412990240192271		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.1412990240192271 | validation: 0.16872078847423091]
	TIME [epoch: 12.8 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13222390931512631		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.13222390931512631 | validation: 0.1524840774324735]
	TIME [epoch: 12.8 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13282480948641892		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.13282480948641892 | validation: 0.19964365886929814]
	TIME [epoch: 12.8 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500144298108088		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.1500144298108088 | validation: 0.16783075968403127]
	TIME [epoch: 12.8 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14669646108690138		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.14669646108690138 | validation: 0.16312981411707012]
	TIME [epoch: 12.8 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13387998982960173		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.13387998982960173 | validation: 0.16895947748592244]
	TIME [epoch: 12.8 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12717901710074273		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.12717901710074273 | validation: 0.1639339674654965]
	TIME [epoch: 12.8 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1303876857305238		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.1303876857305238 | validation: 0.17149800631773868]
	TIME [epoch: 12.8 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13461568369204246		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.13461568369204246 | validation: 0.16808511457455538]
	TIME [epoch: 12.8 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13646376665038593		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.13646376665038593 | validation: 0.16378413949103476]
	TIME [epoch: 12.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12778099724430061		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.12778099724430061 | validation: 0.15591146790994886]
	TIME [epoch: 12.8 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13684136183009712		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.13684136183009712 | validation: 0.1895054210525509]
	TIME [epoch: 12.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15039943765407046		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.15039943765407046 | validation: 0.1601700732765324]
	TIME [epoch: 12.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16447062487369757		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.16447062487369757 | validation: 0.18723782832571886]
	TIME [epoch: 12.8 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13938048439780093		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.13938048439780093 | validation: 0.1736829894932039]
	TIME [epoch: 12.8 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338114408128202		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.1338114408128202 | validation: 0.15656438379719084]
	TIME [epoch: 12.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13030754175587403		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.13030754175587403 | validation: 0.1661626853054409]
	TIME [epoch: 12.8 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13018347902031852		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.13018347902031852 | validation: 0.15759076412788087]
	TIME [epoch: 12.8 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13143884762090838		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.13143884762090838 | validation: 0.16234237729981668]
	TIME [epoch: 12.8 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13134690933628912		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.13134690933628912 | validation: 0.161764637494869]
	TIME [epoch: 12.8 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310107133991609		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.1310107133991609 | validation: 0.158098674443083]
	TIME [epoch: 12.8 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375627033770935		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.1375627033770935 | validation: 0.22640743432802496]
	TIME [epoch: 12.8 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16011128055582305		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.16011128055582305 | validation: 0.15233244184939065]
	TIME [epoch: 12.8 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15314710256139413		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.15314710256139413 | validation: 0.15832951534733125]
	TIME [epoch: 12.8 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13016081095867532		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.13016081095867532 | validation: 0.19161057510354984]
	TIME [epoch: 12.8 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394266809043327		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.1394266809043327 | validation: 0.15062510306773935]
	TIME [epoch: 12.8 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14149509949191225		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.14149509949191225 | validation: 0.16461553270980606]
	TIME [epoch: 12.8 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1291237672850098		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.1291237672850098 | validation: 0.18167096069696076]
	TIME [epoch: 12.8 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13216665076727194		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.13216665076727194 | validation: 0.15383417005539182]
	TIME [epoch: 12.8 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13568830153179717		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.13568830153179717 | validation: 0.17734879732715525]
	TIME [epoch: 12.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310613935703535		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.1310613935703535 | validation: 0.14530630780852327]
	TIME [epoch: 12.8 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12834739754264937		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.12834739754264937 | validation: 0.1616271774483239]
	TIME [epoch: 12.8 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268515287723439		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.1268515287723439 | validation: 0.18289542616639418]
	TIME [epoch: 12.8 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13240697085663414		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.13240697085663414 | validation: 0.15443392749501517]
	TIME [epoch: 12.8 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14145976219359896		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.14145976219359896 | validation: 0.1641391287679932]
	TIME [epoch: 12.8 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1311967994909454		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.1311967994909454 | validation: 0.16681030467203492]
	TIME [epoch: 12.8 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313099665999528		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.1313099665999528 | validation: 0.1459379127191879]
	TIME [epoch: 12.8 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12930430055438538		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.12930430055438538 | validation: 0.1705174493350063]
	TIME [epoch: 12.8 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12937839009444685		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.12937839009444685 | validation: 0.15786845371513003]
	TIME [epoch: 12.8 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13027581664585874		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.13027581664585874 | validation: 0.15384918701561442]
	TIME [epoch: 12.8 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297708976791689		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.1297708976791689 | validation: 0.17397579496590154]
	TIME [epoch: 12.8 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13999303784024872		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.13999303784024872 | validation: 0.15862549319142727]
	TIME [epoch: 12.8 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15442597626603305		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.15442597626603305 | validation: 0.16487730372971354]
	TIME [epoch: 12.8 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13156510891469328		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.13156510891469328 | validation: 0.17066780000085977]
	TIME [epoch: 12.8 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12918404908877762		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.12918404908877762 | validation: 0.1465040960937194]
	TIME [epoch: 12.8 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339600859428567		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.1339600859428567 | validation: 0.19825029134714567]
	TIME [epoch: 12.8 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136072317931076		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.136072317931076 | validation: 0.16257133852338307]
	TIME [epoch: 12.8 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723576651802952		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.13723576651802952 | validation: 0.16786902561415865]
	TIME [epoch: 12.8 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13065251864776645		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.13065251864776645 | validation: 0.1556785740461654]
	TIME [epoch: 12.8 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13088120832064623		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.13088120832064623 | validation: 0.15933689239754356]
	TIME [epoch: 12.8 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12936083438214266		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.12936083438214266 | validation: 0.15539442772847098]
	TIME [epoch: 12.8 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12970668067861454		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.12970668067861454 | validation: 0.1641785836748361]
	TIME [epoch: 12.8 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12852553718229134		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.12852553718229134 | validation: 0.1639699051883432]
	TIME [epoch: 12.8 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285993877037468		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.1285993877037468 | validation: 0.14507182733403012]
	TIME [epoch: 12.8 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348680645331202		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.1348680645331202 | validation: 0.1917842226974004]
	TIME [epoch: 196 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14357179508568904		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.14357179508568904 | validation: 0.1573818377456017]
	TIME [epoch: 26.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13478540544397047		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.13478540544397047 | validation: 0.16977834233472489]
	TIME [epoch: 26.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394083542057495		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.12394083542057495 | validation: 0.14555945312913968]
	TIME [epoch: 26.5 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13005636562880488		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.13005636562880488 | validation: 0.16854607303100722]
	TIME [epoch: 26.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12379904367469234		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.12379904367469234 | validation: 0.1651246096795558]
	TIME [epoch: 26.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12917789089888873		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.12917789089888873 | validation: 0.16368764031504834]
	TIME [epoch: 26.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12830267291955835		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.12830267291955835 | validation: 0.1610926337870216]
	TIME [epoch: 26.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289691351369374		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.1289691351369374 | validation: 0.17701791984053117]
	TIME [epoch: 26.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1238807723811486		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.1238807723811486 | validation: 0.14935315769080432]
	TIME [epoch: 26.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375718171047693		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.1375718171047693 | validation: 0.1872284731568588]
	TIME [epoch: 26.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1314162491051019		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.1314162491051019 | validation: 0.1556893944099622]
	TIME [epoch: 26.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379174791247402		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.1379174791247402 | validation: 0.18031427120369012]
	TIME [epoch: 26.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13090478551692267		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.13090478551692267 | validation: 0.15135099048871045]
	TIME [epoch: 26.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12556168164395992		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.12556168164395992 | validation: 0.14844284961684315]
	TIME [epoch: 26.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12917602885790613		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.12917602885790613 | validation: 0.1515999407307311]
	TIME [epoch: 26.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12681402095551772		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.12681402095551772 | validation: 0.14616091745926227]
	TIME [epoch: 26.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12866485840698408		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.12866485840698408 | validation: 0.163971845315767]
	TIME [epoch: 26.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130201874021397		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.130201874021397 | validation: 0.1443727220241288]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1019.pth
	Model improved!!!
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13394320328386722		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.13394320328386722 | validation: 0.16607229502634024]
	TIME [epoch: 26.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12915603948346985		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.12915603948346985 | validation: 0.14894238017220093]
	TIME [epoch: 26.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14054296495289217		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.14054296495289217 | validation: 0.1882252825405561]
	TIME [epoch: 26.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13592774394156656		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.13592774394156656 | validation: 0.14720785003024087]
	TIME [epoch: 26.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1358927569014067		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.1358927569014067 | validation: 0.17051392779261268]
	TIME [epoch: 26.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13012433725238257		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.13012433725238257 | validation: 0.15996098771498396]
	TIME [epoch: 26.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12349697188972705		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.12349697188972705 | validation: 0.13593726061230255]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292252214400888		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.1292252214400888 | validation: 0.17069089098944254]
	TIME [epoch: 26.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12608769537585374		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.12608769537585374 | validation: 0.1437763340271843]
	TIME [epoch: 26.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12762527785358863		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.12762527785358863 | validation: 0.15542562332284773]
	TIME [epoch: 26.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12450483874321767		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.12450483874321767 | validation: 0.154173525003158]
	TIME [epoch: 26.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299470840249739		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.12299470840249739 | validation: 0.15928169355179117]
	TIME [epoch: 26.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12389123217844701		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.12389123217844701 | validation: 0.1524120017712341]
	TIME [epoch: 26.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276743514466722		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.1276743514466722 | validation: 0.16672038224375307]
	TIME [epoch: 26.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394514650704912		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.12394514650704912 | validation: 0.15207763432293878]
	TIME [epoch: 26.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12598248912941631		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.12598248912941631 | validation: 0.18590288897685547]
	TIME [epoch: 26.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13234880435414595		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.13234880435414595 | validation: 0.1546657557520503]
	TIME [epoch: 26.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1472994900609895		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.1472994900609895 | validation: 0.16978971689724123]
	TIME [epoch: 26.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13228196080398347		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.13228196080398347 | validation: 0.15919572814948935]
	TIME [epoch: 26.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12405190796062067		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.12405190796062067 | validation: 0.1551239676213828]
	TIME [epoch: 26.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12873752000164995		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.12873752000164995 | validation: 0.18228716122077687]
	TIME [epoch: 26.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288623715279606		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.1288623715279606 | validation: 0.14021542215182478]
	TIME [epoch: 26.5 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878813661609326		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.12878813661609326 | validation: 0.14539709015778998]
	TIME [epoch: 26.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12939842016036962		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.12939842016036962 | validation: 0.16534980750322742]
	TIME [epoch: 26.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12634806876953467		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.12634806876953467 | validation: 0.14422244770510934]
	TIME [epoch: 26.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12234528789596602		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.12234528789596602 | validation: 0.16794104905120633]
	TIME [epoch: 26.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12876563095338917		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.12876563095338917 | validation: 0.14240146007326768]
	TIME [epoch: 26.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637225914248696		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.13637225914248696 | validation: 0.164408354103187]
	TIME [epoch: 26.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12679021378859084		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.12679021378859084 | validation: 0.1668202155019699]
	TIME [epoch: 26.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11897457581917355		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.11897457581917355 | validation: 0.14976924618451068]
	TIME [epoch: 26.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12351610011369447		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.12351610011369447 | validation: 0.15467928739624917]
	TIME [epoch: 26.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12141235139897448		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.12141235139897448 | validation: 0.15303724665399776]
	TIME [epoch: 26.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12283053194179402		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.12283053194179402 | validation: 0.1511243748209653]
	TIME [epoch: 26.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12199416077755014		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.12199416077755014 | validation: 0.15844498398847817]
	TIME [epoch: 26.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12423303916572163		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.12423303916572163 | validation: 0.1537812234269885]
	TIME [epoch: 26.5 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12495013638559563		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.12495013638559563 | validation: 0.16517225980621789]
	TIME [epoch: 26.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12341847011912904		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.12341847011912904 | validation: 0.15484928268878032]
	TIME [epoch: 26.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217070474306746		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.1217070474306746 | validation: 0.17702851478067905]
	TIME [epoch: 26.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12943636741316916		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.12943636741316916 | validation: 0.16431991240235852]
	TIME [epoch: 26.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14822651801314307		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.14822651801314307 | validation: 0.1846044037564315]
	TIME [epoch: 26.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12617420425282233		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.12617420425282233 | validation: 0.1509473764208053]
	TIME [epoch: 26.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327745494798667		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.12327745494798667 | validation: 0.15789329436802726]
	TIME [epoch: 26.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128004531096822		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.128004531096822 | validation: 0.176270858106382]
	TIME [epoch: 26.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12499747074698689		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.12499747074698689 | validation: 0.13812876971006066]
	TIME [epoch: 26.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164259825889431		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.12164259825889431 | validation: 0.16434233385408528]
	TIME [epoch: 26.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12283736058127911		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.12283736058127911 | validation: 0.15709402237749837]
	TIME [epoch: 26.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12260175598673223		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.12260175598673223 | validation: 0.16226489902870148]
	TIME [epoch: 26.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090564920841983		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.12090564920841983 | validation: 0.14868245112938672]
	TIME [epoch: 26.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12125944874956641		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.12125944874956641 | validation: 0.14468572534868934]
	TIME [epoch: 26.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231582371461802		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.1231582371461802 | validation: 0.1610208137349144]
	TIME [epoch: 26.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12256288457456299		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.12256288457456299 | validation: 0.1383272385216521]
	TIME [epoch: 26.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12720009437256918		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.12720009437256918 | validation: 0.19676697521720024]
	TIME [epoch: 26.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13276109235003858		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.13276109235003858 | validation: 0.14373711328432345]
	TIME [epoch: 26.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12895180848487672		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.12895180848487672 | validation: 0.1664412738434603]
	TIME [epoch: 26.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1244544567804212		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.1244544567804212 | validation: 0.17147429448090254]
	TIME [epoch: 26.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989784227218586		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.11989784227218586 | validation: 0.14658947496293032]
	TIME [epoch: 26.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12571002074434923		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.12571002074434923 | validation: 0.1680398151262537]
	TIME [epoch: 26.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272902740185696		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.1272902740185696 | validation: 0.14068721972292467]
	TIME [epoch: 26.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12628437577868792		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.12628437577868792 | validation: 0.16837577693987457]
	TIME [epoch: 26.5 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11833713011733689		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.11833713011733689 | validation: 0.1542542081169098]
	TIME [epoch: 26.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12191690120635258		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.12191690120635258 | validation: 0.16515581112284367]
	TIME [epoch: 26.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997087708234111		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.11997087708234111 | validation: 0.1472768236170209]
	TIME [epoch: 26.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12576605047121325		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.12576605047121325 | validation: 0.18084298753028008]
	TIME [epoch: 26.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13128700675350813		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.13128700675350813 | validation: 0.14278082038754283]
	TIME [epoch: 26.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1235557685655035		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.1235557685655035 | validation: 0.1541232185632528]
	TIME [epoch: 26.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12328313124330736		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.12328313124330736 | validation: 0.13358792786612808]
	TIME [epoch: 26.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11782571604838277		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.11782571604838277 | validation: 0.16006896325069855]
	TIME [epoch: 26.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12130122836671879		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.12130122836671879 | validation: 0.16181846641275235]
	TIME [epoch: 26.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1201697239174975		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.1201697239174975 | validation: 0.15774023789122707]
	TIME [epoch: 26.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11926751536607903		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.11926751536607903 | validation: 0.16251272588537966]
	TIME [epoch: 26.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12076435606469126		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.12076435606469126 | validation: 0.18581437639380938]
	TIME [epoch: 26.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12899357828105118		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.12899357828105118 | validation: 0.13241788409981645]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13219492893283125		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.13219492893283125 | validation: 0.14745347531290573]
	TIME [epoch: 26.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11897019163363393		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.11897019163363393 | validation: 0.17630283058081192]
	TIME [epoch: 26.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12194260824496503		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.12194260824496503 | validation: 0.14834779553649657]
	TIME [epoch: 26.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12256850705977482		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.12256850705977482 | validation: 0.16598649718442154]
	TIME [epoch: 26.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12233632742616153		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.12233632742616153 | validation: 0.1318695027734049]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1096.pth
	Model improved!!!
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12211626665440786		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.12211626665440786 | validation: 0.16952034432446497]
	TIME [epoch: 26.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1225821587791441		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.1225821587791441 | validation: 0.1578789889553487]
	TIME [epoch: 26.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012127527625872		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.12012127527625872 | validation: 0.15734960190674527]
	TIME [epoch: 26.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11883753053685844		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.11883753053685844 | validation: 0.14464591894175058]
	TIME [epoch: 26.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12155359918869219		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.12155359918869219 | validation: 0.16333736604757987]
	TIME [epoch: 26.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11915891898758173		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.11915891898758173 | validation: 0.14151726765931563]
	TIME [epoch: 26.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1252198410737795		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.1252198410737795 | validation: 0.17140890354254298]
	TIME [epoch: 26.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12224574303871406		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.12224574303871406 | validation: 0.1422581459982895]
	TIME [epoch: 26.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286701444469955		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.12286701444469955 | validation: 0.14705806534244567]
	TIME [epoch: 26.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12203090938806167		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.12203090938806167 | validation: 0.14661373116673304]
	TIME [epoch: 26.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11549456415136006		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.11549456415136006 | validation: 0.1433462089789895]
	TIME [epoch: 26.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11667593700621152		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.11667593700621152 | validation: 0.15859997101298848]
	TIME [epoch: 26.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12013488121167211		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.12013488121167211 | validation: 0.1410306470630128]
	TIME [epoch: 26.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11699277590723227		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.11699277590723227 | validation: 0.1797308003152291]
	TIME [epoch: 26.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1239688370697811		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.1239688370697811 | validation: 0.14288191145909057]
	TIME [epoch: 26.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12456555196835271		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.12456555196835271 | validation: 0.17069845400631134]
	TIME [epoch: 26.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12211963644733713		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.12211963644733713 | validation: 0.1516596273713735]
	TIME [epoch: 26.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100843963483464		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.12100843963483464 | validation: 0.15903123743231362]
	TIME [epoch: 26.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11907469901761054		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.11907469901761054 | validation: 0.16195839506604146]
	TIME [epoch: 26.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946016057450454		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.11946016057450454 | validation: 0.1608183691571757]
	TIME [epoch: 26.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11911960614761803		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.11911960614761803 | validation: 0.1374589270109703]
	TIME [epoch: 26.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11873839992036953		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.11873839992036953 | validation: 0.18019209452439752]
	TIME [epoch: 26.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12228746319543574		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.12228746319543574 | validation: 0.13971017093767207]
	TIME [epoch: 26.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12177577710145049		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.12177577710145049 | validation: 0.17304373241126111]
	TIME [epoch: 26.5 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12358442237354812		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.12358442237354812 | validation: 0.14946615002943417]
	TIME [epoch: 26.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1170268935690246		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.1170268935690246 | validation: 0.16289064667373454]
	TIME [epoch: 26.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1228992753739862		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.1228992753739862 | validation: 0.16170330376569297]
	TIME [epoch: 26.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1150652187689044		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.1150652187689044 | validation: 0.14989114584521063]
	TIME [epoch: 26.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1170934316280303		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.1170934316280303 | validation: 0.14861248515556888]
	TIME [epoch: 26.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11774089335900957		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.11774089335900957 | validation: 0.1390134568060268]
	TIME [epoch: 26.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783825127584571		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.11783825127584571 | validation: 0.16777749794618366]
	TIME [epoch: 26.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11953469879289838		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.11953469879289838 | validation: 0.14797211591462842]
	TIME [epoch: 26.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12359161243341209		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.12359161243341209 | validation: 0.14453029667790449]
	TIME [epoch: 26.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492327816038973		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.11492327816038973 | validation: 0.15488710029642136]
	TIME [epoch: 26.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1159500474417864		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.1159500474417864 | validation: 0.14920324969888038]
	TIME [epoch: 26.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12172877201917906		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.12172877201917906 | validation: 0.16289883455485457]
	TIME [epoch: 26.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678457808794417		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.11678457808794417 | validation: 0.14511099552435894]
	TIME [epoch: 26.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12513746012601393		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.12513746012601393 | validation: 0.1605230767391605]
	TIME [epoch: 26.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11699071409843281		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.11699071409843281 | validation: 0.13898715112970036]
	TIME [epoch: 26.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431504962433935		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.12431504962433935 | validation: 0.16057449870404486]
	TIME [epoch: 26.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11929423193450912		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.11929423193450912 | validation: 0.14836777501492068]
	TIME [epoch: 26.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684786479436023		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.11684786479436023 | validation: 0.1698146240985439]
	TIME [epoch: 26.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11788164710435779		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.11788164710435779 | validation: 0.16460045070081297]
	TIME [epoch: 26.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1148861316985785		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.1148861316985785 | validation: 0.13857329162356544]
	TIME [epoch: 26.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11485345484090037		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.11485345484090037 | validation: 0.1482445839637184]
	TIME [epoch: 26.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11685027681637136		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.11685027681637136 | validation: 0.1716486170931677]
	TIME [epoch: 26.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11622790100492685		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.11622790100492685 | validation: 0.14321937730124581]
	TIME [epoch: 26.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11790438191451973		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.11790438191451973 | validation: 0.1549127166930917]
	TIME [epoch: 26.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11671805935274268		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.11671805935274268 | validation: 0.14269118065875142]
	TIME [epoch: 26.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11361539418814254		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.11361539418814254 | validation: 0.15890856816782786]
	TIME [epoch: 26.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11995564678751214		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.11995564678751214 | validation: 0.12663221635439878]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1147.pth
	Model improved!!!
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12995098270978425		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.12995098270978425 | validation: 0.16841418989522328]
	TIME [epoch: 26.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11926753087316533		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.11926753087316533 | validation: 0.14879565060497751]
	TIME [epoch: 26.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11674614292212829		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.11674614292212829 | validation: 0.1342321938385378]
	TIME [epoch: 26.5 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11804934078162602		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.11804934078162602 | validation: 0.14915095054288463]
	TIME [epoch: 26.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153846070688445		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.1153846070688445 | validation: 0.15453223862326304]
	TIME [epoch: 26.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739061963649267		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.11739061963649267 | validation: 0.14597952286892446]
	TIME [epoch: 26.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11540673005978637		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.11540673005978637 | validation: 0.15166715820726054]
	TIME [epoch: 26.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11351305581695577		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.11351305581695577 | validation: 0.1397667062787161]
	TIME [epoch: 26.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11444140764821022		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.11444140764821022 | validation: 0.13926917614662182]
	TIME [epoch: 26.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12071146235382907		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.12071146235382907 | validation: 0.14890370546317838]
	TIME [epoch: 26.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11406377022842489		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.11406377022842489 | validation: 0.14165729854917072]
	TIME [epoch: 26.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11366526699535008		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.11366526699535008 | validation: 0.16741733109602822]
	TIME [epoch: 26.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11332598319501852		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.11332598319501852 | validation: 0.15455238368642082]
	TIME [epoch: 26.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11497317861932885		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.11497317861932885 | validation: 0.1412588662186973]
	TIME [epoch: 26.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739750649275862		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.11739750649275862 | validation: 0.13228818368323933]
	TIME [epoch: 26.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11187839106117604		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.11187839106117604 | validation: 0.12762933091646259]
	TIME [epoch: 26.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11514351545695124		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.11514351545695124 | validation: 0.15167767772328455]
	TIME [epoch: 26.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564209968331338		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.11564209968331338 | validation: 0.14017075634122675]
	TIME [epoch: 26.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11355766132098488		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.11355766132098488 | validation: 0.17036778494556218]
	TIME [epoch: 26.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1178580845875203		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.1178580845875203 | validation: 0.13079794044501517]
	TIME [epoch: 26.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297524275573967		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.1297524275573967 | validation: 0.16990732393781585]
	TIME [epoch: 26.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11545835048000377		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.11545835048000377 | validation: 0.14336557386132928]
	TIME [epoch: 26.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11800288722248203		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.11800288722248203 | validation: 0.13755179773612544]
	TIME [epoch: 26.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684280754711157		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.11684280754711157 | validation: 0.15713840126667722]
	TIME [epoch: 26.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11025760087685836		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.11025760087685836 | validation: 0.16549081141790456]
	TIME [epoch: 26.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11514904604192616		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.11514904604192616 | validation: 0.15135532595911566]
	TIME [epoch: 26.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539370415209398		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.11539370415209398 | validation: 0.16021605054519142]
	TIME [epoch: 26.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11402824621955251		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.11402824621955251 | validation: 0.14944972178132873]
	TIME [epoch: 26.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11183000538030019		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.11183000538030019 | validation: 0.15680266169818247]
	TIME [epoch: 26.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11372687478699427		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.11372687478699427 | validation: 0.16556247522968293]
	TIME [epoch: 26.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1166232046491173		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.1166232046491173 | validation: 0.1380647058582217]
	TIME [epoch: 26.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11762887638718052		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.11762887638718052 | validation: 0.16184082139819325]
	TIME [epoch: 26.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539210903337813		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.11539210903337813 | validation: 0.1433265921143035]
	TIME [epoch: 26.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149219320344315		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.1149219320344315 | validation: 0.15658203712894064]
	TIME [epoch: 26.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410113888588334		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.11410113888588334 | validation: 0.14881829775780808]
	TIME [epoch: 26.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112244645266396		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.1112244645266396 | validation: 0.151753832783041]
	TIME [epoch: 26.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11423496183102326		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.11423496183102326 | validation: 0.12951813972530699]
	TIME [epoch: 26.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11504095419033872		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.11504095419033872 | validation: 0.142389782265826]
	TIME [epoch: 26.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11380295891917236		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.11380295891917236 | validation: 0.1693352326404848]
	TIME [epoch: 26.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12020078156061147		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.12020078156061147 | validation: 0.14673019803739007]
	TIME [epoch: 26.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11620623325506146		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.11620623325506146 | validation: 0.1671307491741004]
	TIME [epoch: 26.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.114556059894786		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.114556059894786 | validation: 0.13512412626682]
	TIME [epoch: 26.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11353525368917833		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.11353525368917833 | validation: 0.13812575693381943]
	TIME [epoch: 26.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1135614282251434		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.1135614282251434 | validation: 0.14333977539896334]
	TIME [epoch: 26.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11203152999597403		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.11203152999597403 | validation: 0.15164987320946227]
	TIME [epoch: 26.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11535677051720518		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.11535677051720518 | validation: 0.14408571250398353]
	TIME [epoch: 26.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11441888912821874		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.11441888912821874 | validation: 0.14138967948147038]
	TIME [epoch: 26.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11476357560892346		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.11476357560892346 | validation: 0.14213678103371807]
	TIME [epoch: 26.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11341285130392968		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.11341285130392968 | validation: 0.148453917828246]
	TIME [epoch: 26.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11348515794592132		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.11348515794592132 | validation: 0.15193169305614246]
	TIME [epoch: 26.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11374589587103265		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.11374589587103265 | validation: 0.1521461269170383]
	TIME [epoch: 26.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136172974927899		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.11136172974927899 | validation: 0.1553115424081544]
	TIME [epoch: 26.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466424074367851		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.11466424074367851 | validation: 0.15273088293413334]
	TIME [epoch: 26.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11054630755669251		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.11054630755669251 | validation: 0.13472338797946604]
	TIME [epoch: 26.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1124289782462486		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.1124289782462486 | validation: 0.16891582567318336]
	TIME [epoch: 26.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11789996993195714		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.11789996993195714 | validation: 0.1321282132454047]
	TIME [epoch: 26.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12222903178365979		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.12222903178365979 | validation: 0.14635209427038343]
	TIME [epoch: 26.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11290220980116378		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.11290220980116378 | validation: 0.13795035985350804]
	TIME [epoch: 26.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1150237453403558		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.1150237453403558 | validation: 0.13661090895822045]
	TIME [epoch: 26.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219210931363048		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.11219210931363048 | validation: 0.15460168625664245]
	TIME [epoch: 26.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11243584371527941		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.11243584371527941 | validation: 0.15486296195468935]
	TIME [epoch: 26.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11429626036544122		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.11429626036544122 | validation: 0.15045895701840628]
	TIME [epoch: 26.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11662158403766902		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.11662158403766902 | validation: 0.15225810494970157]
	TIME [epoch: 26.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304733009785728		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.11304733009785728 | validation: 0.1278995017529016]
	TIME [epoch: 26.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11289378154284126		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.11289378154284126 | validation: 0.16404569992174464]
	TIME [epoch: 26.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11401980025480263		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.11401980025480263 | validation: 0.14991207740479862]
	TIME [epoch: 26.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11372225150759381		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.11372225150759381 | validation: 0.14867025810774784]
	TIME [epoch: 26.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10975585260037708		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.10975585260037708 | validation: 0.166923183695108]
	TIME [epoch: 26.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492854785817867		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.11492854785817867 | validation: 0.14180902114045493]
	TIME [epoch: 26.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712482583603899		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.11712482583603899 | validation: 0.1488660711403476]
	TIME [epoch: 26.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126108979144554		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.1126108979144554 | validation: 0.15213340670468875]
	TIME [epoch: 26.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11393927882967889		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.11393927882967889 | validation: 0.1367859736856504]
	TIME [epoch: 26.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11174423832379969		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.11174423832379969 | validation: 0.1629865514858454]
	TIME [epoch: 26.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630345292771027		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.11630345292771027 | validation: 0.1368023983413228]
	TIME [epoch: 26.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11222933064911342		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.11222933064911342 | validation: 0.15493902366939505]
	TIME [epoch: 26.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11387715264197965		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.11387715264197965 | validation: 0.14271213783323947]
	TIME [epoch: 26.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895694717703976		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.10895694717703976 | validation: 0.1450385971420449]
	TIME [epoch: 26.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11242600393006182		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.11242600393006182 | validation: 0.15112911412378346]
	TIME [epoch: 26.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1121865178542966		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.1121865178542966 | validation: 0.13115641511534584]
	TIME [epoch: 26.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11247011830828693		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.11247011830828693 | validation: 0.15454879682061776]
	TIME [epoch: 26.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11117196043148506		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.11117196043148506 | validation: 0.15775160806641797]
	TIME [epoch: 26.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967169956676356		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.10967169956676356 | validation: 0.13577085705741934]
	TIME [epoch: 26.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11828750100935519		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.11828750100935519 | validation: 0.1497189803139953]
	TIME [epoch: 26.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10615469181778242		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.10615469181778242 | validation: 0.15421914151837646]
	TIME [epoch: 26.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10557655944185491		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.10557655944185491 | validation: 0.1408924470959933]
	TIME [epoch: 26.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11537658403727907		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.11537658403727907 | validation: 0.13338535325166187]
	TIME [epoch: 26.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1138234250035497		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.1138234250035497 | validation: 0.16907949713298273]
	TIME [epoch: 26.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116854560806744		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.11116854560806744 | validation: 0.13956172778584972]
	TIME [epoch: 26.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192046853682357		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.11192046853682357 | validation: 0.1399405537776404]
	TIME [epoch: 26.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532962751305296		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.10532962751305296 | validation: 0.13241190047042384]
	TIME [epoch: 26.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10962923709936105		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.10962923709936105 | validation: 0.16174840686866535]
	TIME [epoch: 26.5 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11597877090541958		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.11597877090541958 | validation: 0.14638241637495183]
	TIME [epoch: 26.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11231129937524319		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.11231129937524319 | validation: 0.17500458733898958]
	TIME [epoch: 26.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10856890365031287		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.10856890365031287 | validation: 0.12567993056929508]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1241.pth
	Model improved!!!
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11442223691928206		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.11442223691928206 | validation: 0.13338177575247415]
	TIME [epoch: 26.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10716514943055344		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.10716514943055344 | validation: 0.15360175883885263]
	TIME [epoch: 26.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10957759361178844		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.10957759361178844 | validation: 0.13155862359336884]
	TIME [epoch: 26.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10661713627371736		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.10661713627371736 | validation: 0.14884624016682377]
	TIME [epoch: 26.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060888401757107		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.11060888401757107 | validation: 0.15273815856727346]
	TIME [epoch: 26.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626512123450915		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.10626512123450915 | validation: 0.14420119639821477]
	TIME [epoch: 26.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10843426395527457		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.10843426395527457 | validation: 0.1378204138759001]
	TIME [epoch: 26.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10618944032677384		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.10618944032677384 | validation: 0.13639640959645014]
	TIME [epoch: 26.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813891550154126		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.10813891550154126 | validation: 0.14996467652627787]
	TIME [epoch: 26.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10854494344209535		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.10854494344209535 | validation: 0.14636933736323007]
	TIME [epoch: 26.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545095071908904		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.10545095071908904 | validation: 0.14862656727716717]
	TIME [epoch: 26.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11025366661345949		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.11025366661345949 | validation: 0.13492921542292832]
	TIME [epoch: 26.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11053371788116462		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.11053371788116462 | validation: 0.1428607418616677]
	TIME [epoch: 26.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921967250263925		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.10921967250263925 | validation: 0.13648542004637415]
	TIME [epoch: 26.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11443114994381738		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.11443114994381738 | validation: 0.15221280367990841]
	TIME [epoch: 26.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10978710782903087		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.10978710782903087 | validation: 0.14208531034971258]
	TIME [epoch: 26.5 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10996865990021418		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.10996865990021418 | validation: 0.1366677972809322]
	TIME [epoch: 26.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10912937092634312		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.10912937092634312 | validation: 0.16416078060138684]
	TIME [epoch: 26.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11106209642590137		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.11106209642590137 | validation: 0.13631262660919627]
	TIME [epoch: 26.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11148546902537003		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.11148546902537003 | validation: 0.15240256465836746]
	TIME [epoch: 26.6 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167781256449447		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.11167781256449447 | validation: 0.15326321028591125]
	TIME [epoch: 26.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10958811957586065		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.10958811957586065 | validation: 0.13350242903766946]
	TIME [epoch: 26.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11095248550209778		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.11095248550209778 | validation: 0.13854819558140977]
	TIME [epoch: 26.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176673483814849		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.11176673483814849 | validation: 0.13713046394707798]
	TIME [epoch: 26.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1048917494648594		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.1048917494648594 | validation: 0.1395764067470378]
	TIME [epoch: 26.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110903513621425		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.1110903513621425 | validation: 0.1356189725201226]
	TIME [epoch: 26.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10612123896744137		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.10612123896744137 | validation: 0.1439026505060502]
	TIME [epoch: 26.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10926492362090522		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.10926492362090522 | validation: 0.17837230361671902]
	TIME [epoch: 26.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11543424635408812		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.11543424635408812 | validation: 0.12525214983745842]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1270.pth
	Model improved!!!
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11607503324541364		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.11607503324541364 | validation: 0.1375641204755949]
	TIME [epoch: 26.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10843362667442068		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.10843362667442068 | validation: 0.15387859878583862]
	TIME [epoch: 26.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10992976849365878		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.10992976849365878 | validation: 0.14151323603524307]
	TIME [epoch: 26.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10877454138686671		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.10877454138686671 | validation: 0.12997877288822698]
	TIME [epoch: 26.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10741052923181128		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.10741052923181128 | validation: 0.14520254866317095]
	TIME [epoch: 26.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10846840860804868		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.10846840860804868 | validation: 0.14100340661494878]
	TIME [epoch: 26.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10779015866502889		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.10779015866502889 | validation: 0.15150946568987084]
	TIME [epoch: 26.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002510234198834		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.11002510234198834 | validation: 0.13594475716217902]
	TIME [epoch: 26.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10332593320178358		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.10332593320178358 | validation: 0.13900403318041862]
	TIME [epoch: 26.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10729687952858374		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.10729687952858374 | validation: 0.1400671243009184]
	TIME [epoch: 26.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111574689319137		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.1111574689319137 | validation: 0.15466296853293443]
	TIME [epoch: 26.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11095629848000617		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.11095629848000617 | validation: 0.1385486339377035]
	TIME [epoch: 26.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10779409152875419		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.10779409152875419 | validation: 0.1468012276942368]
	TIME [epoch: 26.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732110875455769		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.10732110875455769 | validation: 0.1582756242281445]
	TIME [epoch: 26.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041530621371498		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.1041530621371498 | validation: 0.13703052725348872]
	TIME [epoch: 26.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655034847282624		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.10655034847282624 | validation: 0.13060950192909945]
	TIME [epoch: 26.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10838348249822634		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.10838348249822634 | validation: 0.15036851966132378]
	TIME [epoch: 26.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929743367218137		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.10929743367218137 | validation: 0.1447589741241828]
	TIME [epoch: 26.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11433624844484352		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.11433624844484352 | validation: 0.14707758649468103]
	TIME [epoch: 26.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10934692190270419		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.10934692190270419 | validation: 0.13979509893109074]
	TIME [epoch: 26.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10451146649644483		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.10451146649644483 | validation: 0.14321132144730148]
	TIME [epoch: 26.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691923916402454		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.10691923916402454 | validation: 0.14513782116057103]
	TIME [epoch: 26.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022840799489274		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.11022840799489274 | validation: 0.12903329455568424]
	TIME [epoch: 26.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002762885955107		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.11002762885955107 | validation: 0.14099632991137018]
	TIME [epoch: 26.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10919712161492266		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.10919712161492266 | validation: 0.1705165713509348]
	TIME [epoch: 26.6 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606566889252139		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.10606566889252139 | validation: 0.13172501904448183]
	TIME [epoch: 26.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944412298140589		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.10944412298140589 | validation: 0.1300035091755731]
	TIME [epoch: 26.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551096999610465		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.10551096999610465 | validation: 0.15497871111128098]
	TIME [epoch: 26.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064330634187394		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.1064330634187394 | validation: 0.15143289113535174]
	TIME [epoch: 26.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11079109421685825		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.11079109421685825 | validation: 0.13533521353909794]
	TIME [epoch: 26.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10676794497349257		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.10676794497349257 | validation: 0.13449021613312426]
	TIME [epoch: 26.6 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10983485348307997		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.10983485348307997 | validation: 0.13974789913323857]
	TIME [epoch: 26.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007806332632626		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.11007806332632626 | validation: 0.1181289520074599]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1303.pth
	Model improved!!!
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1069899045156793		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.1069899045156793 | validation: 0.13673060083338895]
	TIME [epoch: 26.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10710405511910637		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.10710405511910637 | validation: 0.14701702847512885]
	TIME [epoch: 26.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626839432730005		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.10626839432730005 | validation: 0.13931111957898584]
	TIME [epoch: 26.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566615631152687		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.10566615631152687 | validation: 0.1324813672044356]
	TIME [epoch: 26.5 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1073648209606906		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.1073648209606906 | validation: 0.15475209560651237]
	TIME [epoch: 26.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10679677099887919		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.10679677099887919 | validation: 0.14781777035728227]
	TIME [epoch: 26.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465362265060903		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.10465362265060903 | validation: 0.1452707100574689]
	TIME [epoch: 26.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10586660519045206		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.10586660519045206 | validation: 0.1453310601674152]
	TIME [epoch: 26.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10577741743415839		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.10577741743415839 | validation: 0.13161977104943395]
	TIME [epoch: 26.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566342730038013		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.10566342730038013 | validation: 0.15252582971558526]
	TIME [epoch: 26.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1068614070307863		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.1068614070307863 | validation: 0.13042086101339598]
	TIME [epoch: 26.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10292052278181663		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.10292052278181663 | validation: 0.14080962389104684]
	TIME [epoch: 26.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10904805017667482		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.10904805017667482 | validation: 0.1294785668608362]
	TIME [epoch: 26.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722947102064374		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.10722947102064374 | validation: 0.1491381324543719]
	TIME [epoch: 26.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10371679628163438		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.10371679628163438 | validation: 0.12272005116138862]
	TIME [epoch: 26.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116079412794293		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.11116079412794293 | validation: 0.1355835075947843]
	TIME [epoch: 26.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10381263148445272		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.10381263148445272 | validation: 0.14410168223706904]
	TIME [epoch: 26.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039531238785181		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.1039531238785181 | validation: 0.12712493039842362]
	TIME [epoch: 26.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10614916861440951		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.10614916861440951 | validation: 0.16195513469721898]
	TIME [epoch: 26.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120900166238233		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.11120900166238233 | validation: 0.1337759006578347]
	TIME [epoch: 26.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932890001924606		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.10932890001924606 | validation: 0.14092986593555798]
	TIME [epoch: 26.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10247162233112421		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.10247162233112421 | validation: 0.1424229639702114]
	TIME [epoch: 26.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10666074443101917		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.10666074443101917 | validation: 0.1327609613906389]
	TIME [epoch: 26.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041369904387024		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.1041369904387024 | validation: 0.14288970986400024]
	TIME [epoch: 26.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10567341743200943		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.10567341743200943 | validation: 0.1546214564397831]
	TIME [epoch: 26.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10618830138577359		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.10618830138577359 | validation: 0.14271085299880015]
	TIME [epoch: 26.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584610608556086		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.10584610608556086 | validation: 0.13778541855229737]
	TIME [epoch: 26.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10651384727513495		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.10651384727513495 | validation: 0.1423011791520926]
	TIME [epoch: 26.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10649195449816702		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.10649195449816702 | validation: 0.14218847258593806]
	TIME [epoch: 26.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1079690872307785		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.1079690872307785 | validation: 0.13358115069563367]
	TIME [epoch: 26.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10528152314696834		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.10528152314696834 | validation: 0.1551821145381807]
	TIME [epoch: 26.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10490755893084726		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.10490755893084726 | validation: 0.14611021327351006]
	TIME [epoch: 26.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10642266570379977		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.10642266570379977 | validation: 0.1400394292205458]
	TIME [epoch: 26.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10370046529344362		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.10370046529344362 | validation: 0.1351401421379946]
	TIME [epoch: 26.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10041174555917533		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.10041174555917533 | validation: 0.12963395810520859]
	TIME [epoch: 26.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503421585661211		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.10503421585661211 | validation: 0.14127581845759127]
	TIME [epoch: 26.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10123208891603737		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.10123208891603737 | validation: 0.14264444090814843]
	TIME [epoch: 26.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10106545755353011		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.10106545755353011 | validation: 0.1472718139560146]
	TIME [epoch: 26.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558697615409197		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.10558697615409197 | validation: 0.13889504326116328]
	TIME [epoch: 26.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10207393753039914		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.10207393753039914 | validation: 0.14141295008121882]
	TIME [epoch: 26.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10280971389697438		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.10280971389697438 | validation: 0.15567149078037287]
	TIME [epoch: 26.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10423106049570749		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.10423106049570749 | validation: 0.13067167917948358]
	TIME [epoch: 26.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10292484975212428		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.10292484975212428 | validation: 0.15493726710360237]
	TIME [epoch: 26.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09963814194322408		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.09963814194322408 | validation: 0.14789141851928286]
	TIME [epoch: 26.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10393580277538486		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.10393580277538486 | validation: 0.12572540344346797]
	TIME [epoch: 26.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545644586872463		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.10545644586872463 | validation: 0.14510750193813227]
	TIME [epoch: 26.5 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10624231875997592		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.10624231875997592 | validation: 0.15886343867371663]
	TIME [epoch: 26.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220630125514785		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.10220630125514785 | validation: 0.14754086646925096]
	TIME [epoch: 26.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737344302929185		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.10737344302929185 | validation: 0.141715281204394]
	TIME [epoch: 26.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330202220285845		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.10330202220285845 | validation: 0.15247871177493114]
	TIME [epoch: 26.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10117686850592172		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.10117686850592172 | validation: 0.1333446118757972]
	TIME [epoch: 26.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101834444515229		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.101834444515229 | validation: 0.13848764102925823]
	TIME [epoch: 26.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10406092339054798		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.10406092339054798 | validation: 0.14084460994698178]
	TIME [epoch: 26.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10621381677740792		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.10621381677740792 | validation: 0.1454072896391906]
	TIME [epoch: 26.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10520335295388325		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.10520335295388325 | validation: 0.13510456342183405]
	TIME [epoch: 26.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050479947310861		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.1050479947310861 | validation: 0.1329113062964877]
	TIME [epoch: 26.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000412028638515		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.11000412028638515 | validation: 0.1394512767373857]
	TIME [epoch: 26.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10208607999829372		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.10208607999829372 | validation: 0.15842912451844413]
	TIME [epoch: 26.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10315066374354558		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.10315066374354558 | validation: 0.12531016357424335]
	TIME [epoch: 26.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10303232727074317		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.10303232727074317 | validation: 0.1333130562909849]
	TIME [epoch: 26.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10276494917367478		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.10276494917367478 | validation: 0.1347157899565976]
	TIME [epoch: 26.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182244897530553		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.10182244897530553 | validation: 0.15244107306429788]
	TIME [epoch: 26.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244435997104591		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.10244435997104591 | validation: 0.1380245581661224]
	TIME [epoch: 26.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10556513738570127		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.10556513738570127 | validation: 0.13573192982150184]
	TIME [epoch: 26.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189923375804026		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.10189923375804026 | validation: 0.14375159475603505]
	TIME [epoch: 26.5 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1008746395585748		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.1008746395585748 | validation: 0.13127335928236508]
	TIME [epoch: 26.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162958876842045		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.10162958876842045 | validation: 0.13321792601546606]
	TIME [epoch: 26.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034830912906796		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.1034830912906796 | validation: 0.1542651615709544]
	TIME [epoch: 26.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1049960491577728		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.1049960491577728 | validation: 0.12785213055708639]
	TIME [epoch: 26.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108381550490668		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.10108381550490668 | validation: 0.13054251138901735]
	TIME [epoch: 26.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10343827630343096		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.10343827630343096 | validation: 0.15202318655466787]
	TIME [epoch: 26.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1068227043889495		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.1068227043889495 | validation: 0.13670648815291872]
	TIME [epoch: 26.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10473450855693792		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.10473450855693792 | validation: 0.12842117808348968]
	TIME [epoch: 26.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192360042021925		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.10192360042021925 | validation: 0.13981569431233615]
	TIME [epoch: 26.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10620601799710681		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.10620601799710681 | validation: 0.12735181649426344]
	TIME [epoch: 26.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10313802442471368		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.10313802442471368 | validation: 0.13711620450987239]
	TIME [epoch: 26.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154695883507608		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.10154695883507608 | validation: 0.1353945763320583]
	TIME [epoch: 26.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039036817344282		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.1039036817344282 | validation: 0.1462639718277073]
	TIME [epoch: 26.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404966539292865		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.10404966539292865 | validation: 0.13429886610482009]
	TIME [epoch: 26.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015538151908487		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.10015538151908487 | validation: 0.146628533697585]
	TIME [epoch: 26.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994274558081214		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.0994274558081214 | validation: 0.13885301540380907]
	TIME [epoch: 26.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256304641097647		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.10256304641097647 | validation: 0.15915207583194607]
	TIME [epoch: 26.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1024889884654831		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.1024889884654831 | validation: 0.12967199051406733]
	TIME [epoch: 26.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10332824749229028		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.10332824749229028 | validation: 0.14066509896343155]
	TIME [epoch: 26.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158624042929194		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.10158624042929194 | validation: 0.14228468033320904]
	TIME [epoch: 26.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410039390090992		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.10410039390090992 | validation: 0.12709939474795698]
	TIME [epoch: 26.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10598133698119283		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.10598133698119283 | validation: 0.14980631585258972]
	TIME [epoch: 26.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10311218466066645		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.10311218466066645 | validation: 0.13389630793063967]
	TIME [epoch: 26.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004898040297325		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.1004898040297325 | validation: 0.13436104447716904]
	TIME [epoch: 26.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10041451564850816		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.10041451564850816 | validation: 0.1366653557379572]
	TIME [epoch: 26.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404382276618108		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.10404382276618108 | validation: 0.13506184118735184]
	TIME [epoch: 26.5 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10101816406229265		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.10101816406229265 | validation: 0.14131068775435537]
	TIME [epoch: 26.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985004527008076		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.0985004527008076 | validation: 0.12870422808014068]
	TIME [epoch: 26.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10403169600698377		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.10403169600698377 | validation: 0.1385582365696352]
	TIME [epoch: 26.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163767818369493		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.10163767818369493 | validation: 0.13307602490638049]
	TIME [epoch: 26.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162680914526394		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.10162680914526394 | validation: 0.1328128344968851]
	TIME [epoch: 26.5 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10204317471734271		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.10204317471734271 | validation: 0.1396374744162141]
	TIME [epoch: 26.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10442949821141874		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.10442949821141874 | validation: 0.13827544369986913]
	TIME [epoch: 26.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09975664117740593		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.09975664117740593 | validation: 0.1438436158804063]
	TIME [epoch: 26.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132373373272512		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.10132373373272512 | validation: 0.14281422841391853]
	TIME [epoch: 26.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10599232436052365		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.10599232436052365 | validation: 0.1430641337378364]
	TIME [epoch: 26.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_153004/states/model_phi1_4c_v_mmd1_1404.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 20271.006 seconds.
