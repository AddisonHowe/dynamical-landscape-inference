Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1973852000

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.1789766157820045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1789766157820045 | validation: 4.776745982564349]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.200275687555581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.200275687555581 | validation: 6.195612201594684]
	TIME [epoch: 1.41 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.354541018571024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.354541018571024 | validation: 5.517722607248227]
	TIME [epoch: 1.37 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.688944693818439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.688944693818439 | validation: 5.847653239078237]
	TIME [epoch: 1.37 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.424450065706606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.424450065706606 | validation: 5.684861222805125]
	TIME [epoch: 1.37 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.238614529477336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.238614529477336 | validation: 5.0959966830653975]
	TIME [epoch: 1.37 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.940475330962381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.940475330962381 | validation: 4.7276766825563135]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.878031457622107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.878031457622107 | validation: 4.771274025413443]
	TIME [epoch: 1.38 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.862275684859246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.862275684859246 | validation: 4.679560164269708]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.784472934023731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.784472934023731 | validation: 4.731511466969766]
	TIME [epoch: 1.38 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.743356445590914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.743356445590914 | validation: 4.815561200221606]
	TIME [epoch: 1.38 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7039192138572705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7039192138572705 | validation: 4.738564464545274]
	TIME [epoch: 1.38 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6622449913016055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6622449913016055 | validation: 4.700276220655348]
	TIME [epoch: 1.38 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.616169141654002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.616169141654002 | validation: 4.619447429987739]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.577558597245573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.577558597245573 | validation: 4.598460845608048]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.533328829317695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.533328829317695 | validation: 4.539334513528085]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4929433314830325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4929433314830325 | validation: 4.581960891662315]
	TIME [epoch: 1.38 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.459308182909391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.459308182909391 | validation: 4.465826860975533]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.487160200064799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.487160200064799 | validation: 4.851895687211]
	TIME [epoch: 1.38 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6047624235724856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6047624235724856 | validation: 4.332832099403525]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342337808407899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342337808407899 | validation: 4.360985320238915]
	TIME [epoch: 1.38 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2948165980257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2948165980257 | validation: 4.507709969728715]
	TIME [epoch: 1.38 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.29352565438333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.29352565438333 | validation: 4.270631599673071]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.323737399485682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323737399485682 | validation: 4.683667684683482]
	TIME [epoch: 1.37 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3844395654335475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3844395654335475 | validation: 4.208303316386085]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.158134158991361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.158134158991361 | validation: 4.1940731780714655]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.149579667239038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.149579667239038 | validation: 4.47949253080441]
	TIME [epoch: 1.38 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.186795878454931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.186795878454931 | validation: 4.091116921707614]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.113216257267299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113216257267299 | validation: 4.3020817008104535]
	TIME [epoch: 1.37 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.068528135315907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.068528135315907 | validation: 4.098208026602352]
	TIME [epoch: 1.37 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005834127656546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.005834127656546 | validation: 4.189173370705189]
	TIME [epoch: 1.37 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.962192066970788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.962192066970788 | validation: 3.9933277960548303]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9345877956701334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9345877956701334 | validation: 4.28373119255486]
	TIME [epoch: 1.38 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9650695871189967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9650695871189967 | validation: 3.8616592204223794]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.959075509525683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.959075509525683 | validation: 3.9254320522089814]
	TIME [epoch: 1.38 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.949240288736113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.949240288736113 | validation: 3.6392079637305943]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6340548319952637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6340548319952637 | validation: 3.7983899573610973]
	TIME [epoch: 1.38 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8660285809363577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8660285809363577 | validation: 3.7031016019996303]
	TIME [epoch: 1.38 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6955176221019856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6955176221019856 | validation: 3.3598218322671087]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4957431955517997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4957431955517997 | validation: 3.3523469699832362]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.301962187781659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.301962187781659 | validation: 2.54297126402696]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.677405789301458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.677405789301458 | validation: 2.2755459395961326]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5053738049496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5053738049496 | validation: 2.5458088713274143]
	TIME [epoch: 1.38 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2992266981972582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2992266981972582 | validation: 2.0412630708431303]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9563143277644162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9563143277644162 | validation: 1.9913371134450681]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7627030728885689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7627030728885689 | validation: 1.6067808462152606]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4547385134157513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4547385134157513 | validation: 1.4339624561330542]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3735303320874652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3735303320874652 | validation: 1.5421434300579158]
	TIME [epoch: 1.38 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3914500324135544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3914500324135544 | validation: 1.5583083788669718]
	TIME [epoch: 1.38 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5593066893436105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5593066893436105 | validation: 1.4223342499691938]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2279757668662972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2279757668662972 | validation: 1.1456660752682912]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0464691986280275		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.0464691986280275 | validation: 1.0403163450409785]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929949310293823		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9929949310293823 | validation: 1.0515874961559928]
	TIME [epoch: 1.38 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9672909547996335		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.9672909547996335 | validation: 1.0752494606299414]
	TIME [epoch: 1.38 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0035313520593714		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0035313520593714 | validation: 1.1904722145360207]
	TIME [epoch: 1.38 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1099463728328294		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.1099463728328294 | validation: 0.9815031674095418]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919957094576082		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.919957094576082 | validation: 0.9421041824345976]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825867619618513		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.8825867619618513 | validation: 0.9413303765537994]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857872271703292		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.857872271703292 | validation: 0.9002337924036881]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8665034230525777		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.8665034230525777 | validation: 0.9273030302108144]
	TIME [epoch: 1.38 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860057264786679		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.860057264786679 | validation: 0.9707523300592323]
	TIME [epoch: 1.38 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.942812741674885		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.942812741674885 | validation: 0.9458728926397839]
	TIME [epoch: 1.38 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8534964222628862		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.8534964222628862 | validation: 0.8707751408605807]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802867002624754		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.802867002624754 | validation: 0.8215425955267492]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786997231876114		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.786997231876114 | validation: 0.8416481068956666]
	TIME [epoch: 1.37 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662203257608016		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.7662203257608016 | validation: 0.8031127688668729]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621492776356752		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.7621492776356752 | validation: 0.8323228210676463]
	TIME [epoch: 1.38 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600342820327893		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.7600342820327893 | validation: 0.7973417479749085]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7579263705605436		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7579263705605436 | validation: 0.8794443349517538]
	TIME [epoch: 1.38 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739905008213817		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.7739905008213817 | validation: 0.7679489144587945]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773027816042849		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.7773027816042849 | validation: 0.9037623172082322]
	TIME [epoch: 1.38 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855164784954538		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.7855164784954538 | validation: 0.7943025397007378]
	TIME [epoch: 1.38 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968445540125221		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.7968445540125221 | validation: 0.94150617068712]
	TIME [epoch: 1.38 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653049404205655		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8653049404205655 | validation: 1.0087956250639956]
	TIME [epoch: 1.38 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828641567992577		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8828641567992577 | validation: 0.9761565205718601]
	TIME [epoch: 1.38 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9995217577206343		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9995217577206343 | validation: 0.9603986411449459]
	TIME [epoch: 1.38 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810962256351515		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.810962256351515 | validation: 0.8587753661549539]
	TIME [epoch: 1.38 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696793490441292		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.7696793490441292 | validation: 0.8057112416783454]
	TIME [epoch: 1.38 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772188444170303		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.7772188444170303 | validation: 0.8213454341909433]
	TIME [epoch: 1.38 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350990345654281		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7350990345654281 | validation: 0.7747335686161148]
	TIME [epoch: 1.38 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729071320571315		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.729071320571315 | validation: 0.7871967257897794]
	TIME [epoch: 1.38 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719504720967132		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.719504720967132 | validation: 0.8021938602895416]
	TIME [epoch: 1.38 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728844332271789		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.728844332271789 | validation: 0.8088087962110951]
	TIME [epoch: 1.38 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722827567887638		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.722827567887638 | validation: 0.7656186395549022]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332078930351646		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7332078930351646 | validation: 0.9192790693069628]
	TIME [epoch: 1.38 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844613469984231		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.7844613469984231 | validation: 0.8056352010912455]
	TIME [epoch: 1.38 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376402702993067		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8376402702993067 | validation: 0.9449633991237679]
	TIME [epoch: 1.38 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049745554123393		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8049745554123393 | validation: 0.809950605027845]
	TIME [epoch: 1.38 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720628547615597		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.720628547615597 | validation: 0.7615103884505642]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7482426189638709		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7482426189638709 | validation: 1.0820928468682187]
	TIME [epoch: 1.38 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518910252238183		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8518910252238183 | validation: 0.9171404052135872]
	TIME [epoch: 1.38 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9592626900774466		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9592626900774466 | validation: 0.8395746780230049]
	TIME [epoch: 1.38 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144235255621922		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7144235255621922 | validation: 0.8438293527201672]
	TIME [epoch: 1.38 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72092755316144		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.72092755316144 | validation: 0.7589476899290782]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740704278417456		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.740704278417456 | validation: 0.8574148368531976]
	TIME [epoch: 1.38 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719907774165541		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.719907774165541 | validation: 0.7668742106062978]
	TIME [epoch: 1.38 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052002999353546		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.7052002999353546 | validation: 0.794217895935678]
	TIME [epoch: 1.38 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945584099156362		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.6945584099156362 | validation: 0.7800835920984945]
	TIME [epoch: 1.38 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940975315698927		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.6940975315698927 | validation: 0.8148464362458747]
	TIME [epoch: 1.38 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022875979182416		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.7022875979182416 | validation: 0.8557770054692342]
	TIME [epoch: 1.38 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966757475980353		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7966757475980353 | validation: 1.0720156383407848]
	TIME [epoch: 1.38 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539408331806312		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.9539408331806312 | validation: 0.7726583549766919]
	TIME [epoch: 1.37 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720906832468096		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.720906832468096 | validation: 0.7899539711325402]
	TIME [epoch: 1.38 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782168329721884		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.6782168329721884 | validation: 0.787716525790558]
	TIME [epoch: 1.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.689860871550288		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.689860871550288 | validation: 0.7459983154463478]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707353016287755		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.707353016287755 | validation: 0.8521354695927734]
	TIME [epoch: 1.37 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257630485836054		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7257630485836054 | validation: 0.7451998773336606]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343837593153089		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.7343837593153089 | validation: 0.9650824245939209]
	TIME [epoch: 1.38 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748051715505886		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.7748051715505886 | validation: 0.7198849331752011]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249251171754169		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.7249251171754169 | validation: 0.9835777830337419]
	TIME [epoch: 1.38 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563787940629897		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7563787940629897 | validation: 0.8215745719988565]
	TIME [epoch: 1.38 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079298445132157		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8079298445132157 | validation: 0.9033753773586239]
	TIME [epoch: 1.38 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078258765942271		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7078258765942271 | validation: 0.7871986302089087]
	TIME [epoch: 1.38 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714210558892506		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.6714210558892506 | validation: 0.7434986547996871]
	TIME [epoch: 1.39 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722059555174645		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.6722059555174645 | validation: 0.7970246000811161]
	TIME [epoch: 1.38 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731334338450604		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.6731334338450604 | validation: 0.7014756282607083]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651550810764791		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.6651550810764791 | validation: 0.8588963854303091]
	TIME [epoch: 1.37 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679718138392189		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.679718138392189 | validation: 0.7121601334841365]
	TIME [epoch: 1.37 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652366058281334		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7652366058281334 | validation: 1.163305514492185]
	TIME [epoch: 1.37 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077692547856927		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.9077692547856927 | validation: 0.7357801321622142]
	TIME [epoch: 1.37 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657415504807052		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.657415504807052 | validation: 0.7359311387643332]
	TIME [epoch: 1.37 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305989290296918		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7305989290296918 | validation: 0.934809916638595]
	TIME [epoch: 1.37 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934722927784773		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7934722927784773 | validation: 0.831812390389737]
	TIME [epoch: 1.38 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934831032347182		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6934831032347182 | validation: 0.7257187921416528]
	TIME [epoch: 1.37 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964873275046657		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.6964873275046657 | validation: 0.9044314181074249]
	TIME [epoch: 1.37 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943890806288555		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.6943890806288555 | validation: 0.7216056803184788]
	TIME [epoch: 1.37 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547321287966906		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.6547321287966906 | validation: 0.7391438861759809]
	TIME [epoch: 1.37 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446797302615054		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6446797302615054 | validation: 0.7584503159071749]
	TIME [epoch: 1.37 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440184146737781		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6440184146737781 | validation: 0.7047890820741447]
	TIME [epoch: 1.38 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658385883203944		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6658385883203944 | validation: 0.8628019722830218]
	TIME [epoch: 1.37 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367430215733488		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.7367430215733488 | validation: 0.7953886680436009]
	TIME [epoch: 1.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786531552060796		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.786531552060796 | validation: 0.8886745354121409]
	TIME [epoch: 1.37 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740910218811702		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.740910218811702 | validation: 0.7790840080984062]
	TIME [epoch: 1.37 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486497027717388		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6486497027717388 | validation: 0.7090828951268237]
	TIME [epoch: 1.37 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424235443813575		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6424235443813575 | validation: 0.78404177851573]
	TIME [epoch: 1.38 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477685454472147		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.6477685454472147 | validation: 0.6751072804217817]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426426123264127		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6426426123264127 | validation: 0.8591332518514702]
	TIME [epoch: 1.38 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685859390502685		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.685859390502685 | validation: 0.6852284067795612]
	TIME [epoch: 1.38 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033125861630475		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7033125861630475 | validation: 0.9485862577813358]
	TIME [epoch: 1.38 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710908844696777		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.710908844696777 | validation: 0.69379335593689]
	TIME [epoch: 1.38 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664686497408437		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.664686497408437 | validation: 0.852349466423921]
	TIME [epoch: 1.38 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709995248027242		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6709995248027242 | validation: 0.7549444011699062]
	TIME [epoch: 1.38 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550327659405605		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6550327659405605 | validation: 0.7201498457108116]
	TIME [epoch: 1.37 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508412546642618		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6508412546642618 | validation: 0.7929858729724188]
	TIME [epoch: 1.38 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673672447084661		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.673672447084661 | validation: 0.7020951045055149]
	TIME [epoch: 1.38 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6845131144244365		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6845131144244365 | validation: 0.8273401880461514]
	TIME [epoch: 1.38 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806625327188047		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.6806625327188047 | validation: 0.6917353399527811]
	TIME [epoch: 1.38 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462524001109626		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6462524001109626 | validation: 0.7363477896260058]
	TIME [epoch: 1.38 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631063543010436		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.631063543010436 | validation: 0.714612773050538]
	TIME [epoch: 1.38 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198299729546956		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6198299729546956 | validation: 0.7266924253199458]
	TIME [epoch: 1.38 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6185475176240496		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6185475176240496 | validation: 0.7145167389115372]
	TIME [epoch: 1.38 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351040914823289		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6351040914823289 | validation: 0.8152032553944635]
	TIME [epoch: 1.38 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625071412738456		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6625071412738456 | validation: 0.707955540911056]
	TIME [epoch: 1.37 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061761245806895		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7061761245806895 | validation: 0.8930614391994447]
	TIME [epoch: 1.38 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920834675556862		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6920834675556862 | validation: 0.6824734879957007]
	TIME [epoch: 1.38 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6153528043785831		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6153528043785831 | validation: 0.6708050360001819]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5939071959069558		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.5939071959069558 | validation: 0.6980622707066128]
	TIME [epoch: 1.38 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5943923298071216		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.5943923298071216 | validation: 0.6222834443221243]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045857553949062		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6045857553949062 | validation: 0.8514643264880498]
	TIME [epoch: 1.37 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453523474672558		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6453523474672558 | validation: 0.651299290660567]
	TIME [epoch: 1.37 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642830872365681		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6642830872365681 | validation: 0.8216999008443026]
	TIME [epoch: 1.37 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6126741886549809		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.6126741886549809 | validation: 0.6111349690213872]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755093341856042		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.5755093341856042 | validation: 0.6655921287866181]
	TIME [epoch: 1.38 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612209833691547		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5612209833691547 | validation: 0.6886150682197312]
	TIME [epoch: 1.38 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.55894882737411		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.55894882737411 | validation: 0.6609611576416453]
	TIME [epoch: 1.38 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670566764292886		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.5670566764292886 | validation: 0.8217136377124223]
	TIME [epoch: 1.38 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864707897275032		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6864707897275032 | validation: 0.9588732566131624]
	TIME [epoch: 1.38 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548714174888319		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7548714174888319 | validation: 0.6548973379667764]
	TIME [epoch: 1.38 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5742779422978899		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.5742779422978899 | validation: 0.5847741292089484]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499729286332752		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5499729286332752 | validation: 0.6343472803970925]
	TIME [epoch: 1.38 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.529519885787898		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.529519885787898 | validation: 0.6331093801961224]
	TIME [epoch: 1.38 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442182926887752		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5442182926887752 | validation: 0.7148666583186288]
	TIME [epoch: 1.38 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263854749136849		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6263854749136849 | validation: 0.8689854479007786]
	TIME [epoch: 1.38 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182912551670111		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7182912551670111 | validation: 0.947186502930959]
	TIME [epoch: 1.39 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403962280807817		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6403962280807817 | validation: 0.9319685118277802]
	TIME [epoch: 1.38 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6068263957135424		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6068263957135424 | validation: 0.6482901841159961]
	TIME [epoch: 1.38 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5239888654949744		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.5239888654949744 | validation: 0.6062376967483013]
	TIME [epoch: 1.38 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5671556160945213		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5671556160945213 | validation: 0.7510820647099674]
	TIME [epoch: 1.38 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5993640135572389		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.5993640135572389 | validation: 0.7933194021989505]
	TIME [epoch: 1.38 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.573368996124709		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.573368996124709 | validation: 0.5878326502279573]
	TIME [epoch: 1.38 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49637720610952096		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.49637720610952096 | validation: 0.5412318982406895]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5047394631646804		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.5047394631646804 | validation: 0.5889299793314842]
	TIME [epoch: 1.38 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48804672133258153		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.48804672133258153 | validation: 0.6424337430286802]
	TIME [epoch: 1.38 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5079377040016414		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.5079377040016414 | validation: 0.6361709946114277]
	TIME [epoch: 1.38 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.573927679236321		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.573927679236321 | validation: 0.6440699583555233]
	TIME [epoch: 1.38 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5202842709033634		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5202842709033634 | validation: 0.6240763989817477]
	TIME [epoch: 1.38 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183770572366783		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.5183770572366783 | validation: 0.5497420986510152]
	TIME [epoch: 1.38 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45517701867285965		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.45517701867285965 | validation: 0.5742180382655491]
	TIME [epoch: 1.38 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45751438644898995		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.45751438644898995 | validation: 0.5593920695311593]
	TIME [epoch: 1.38 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4484177965755015		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.4484177965755015 | validation: 0.5604986245690264]
	TIME [epoch: 1.38 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44167224487327544		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.44167224487327544 | validation: 0.6247685392712938]
	TIME [epoch: 1.38 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410879625543684		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5410879625543684 | validation: 0.9284554095670414]
	TIME [epoch: 1.38 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726051718204133		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.726051718204133 | validation: 0.8156020315746556]
	TIME [epoch: 1.38 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732815976181763		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6732815976181763 | validation: 0.6460042946639659]
	TIME [epoch: 1.38 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5178137117841785		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.5178137117841785 | validation: 0.6163520324106758]
	TIME [epoch: 1.38 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401443501918733		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.5401443501918733 | validation: 0.6292932757765359]
	TIME [epoch: 1.38 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531948056116145		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.5531948056116145 | validation: 0.6224573279783667]
	TIME [epoch: 1.38 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44170251700187535		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.44170251700187535 | validation: 0.5713107146469307]
	TIME [epoch: 1.38 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4559173361016534		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.4559173361016534 | validation: 0.5362176457848998]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4875180082594416		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.4875180082594416 | validation: 0.5206606649736185]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41985629794552154		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.41985629794552154 | validation: 0.5295267972872877]
	TIME [epoch: 180 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41094354515067166		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.41094354515067166 | validation: 0.5528520057064118]
	TIME [epoch: 2.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4068235505177738		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.4068235505177738 | validation: 0.5135289831024293]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4159948558077988		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.4159948558077988 | validation: 0.5924551268439223]
	TIME [epoch: 2.72 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4671601138645816		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.4671601138645816 | validation: 0.5388465480597088]
	TIME [epoch: 2.73 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40593710014317796		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.40593710014317796 | validation: 0.5048868765745524]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39353582642938006		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.39353582642938006 | validation: 0.49717932480817223]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3985899937106309		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.3985899937106309 | validation: 0.5947423732467316]
	TIME [epoch: 2.72 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45334711153784396		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.45334711153784396 | validation: 0.5346181769860324]
	TIME [epoch: 2.73 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963780413343864		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.3963780413343864 | validation: 0.4908074569684214]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3898419343689007		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.3898419343689007 | validation: 0.4704554280244666]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3814433124019918		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.3814433124019918 | validation: 0.6073465913822144]
	TIME [epoch: 2.73 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42085366221175036		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.42085366221175036 | validation: 0.4835737208247823]
	TIME [epoch: 2.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37212402251621685		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.37212402251621685 | validation: 0.47869142981115714]
	TIME [epoch: 2.73 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36754066478481423		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.36754066478481423 | validation: 0.47346964251070944]
	TIME [epoch: 2.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35406711435650834		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.35406711435650834 | validation: 0.5295532452569439]
	TIME [epoch: 2.74 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3776280098140289		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.3776280098140289 | validation: 0.4833973456109541]
	TIME [epoch: 2.73 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3769641521014729		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.3769641521014729 | validation: 0.5774202441874002]
	TIME [epoch: 2.73 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4306262321989209		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4306262321989209 | validation: 0.45467158008820646]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281421701393753		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.3281421701393753 | validation: 0.4615162022493152]
	TIME [epoch: 2.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.334765129068608		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.334765129068608 | validation: 0.5227564187152203]
	TIME [epoch: 2.73 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3804591942333476		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.3804591942333476 | validation: 0.4562074315236766]
	TIME [epoch: 2.73 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3252440667765015		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.3252440667765015 | validation: 0.47109309813963895]
	TIME [epoch: 2.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31255924306825683		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.31255924306825683 | validation: 0.4264928816177876]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031339817170551		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.3031339817170551 | validation: 0.509728644098444]
	TIME [epoch: 2.71 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34500105043159013		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.34500105043159013 | validation: 0.434433799777904]
	TIME [epoch: 2.71 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3074211430577755		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.3074211430577755 | validation: 0.4809251750792882]
	TIME [epoch: 2.71 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3524776018782665		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.3524776018782665 | validation: 0.48380448871934567]
	TIME [epoch: 2.71 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33499612752768154		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.33499612752768154 | validation: 0.5088469906023131]
	TIME [epoch: 2.71 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485654641050101		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.3485654641050101 | validation: 0.42219072479748426]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27107485826131006		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.27107485826131006 | validation: 0.43266788954845214]
	TIME [epoch: 2.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944611593614525		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.2944611593614525 | validation: 0.48532850691149865]
	TIME [epoch: 2.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36473880433522127		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.36473880433522127 | validation: 0.40255037018796785]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25688442542538165		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.25688442542538165 | validation: 0.40963037910623684]
	TIME [epoch: 2.74 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2472026207124625		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.2472026207124625 | validation: 0.39907501744103907]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.259184403089196		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.259184403089196 | validation: 0.43307694985338485]
	TIME [epoch: 2.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864723191587131		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.2864723191587131 | validation: 0.6015281633279645]
	TIME [epoch: 2.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44397737921299163		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.44397737921299163 | validation: 0.50372956596356]
	TIME [epoch: 2.73 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784769476169372		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.2784769476169372 | validation: 0.5815643860228582]
	TIME [epoch: 2.73 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4264821067643181		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4264821067643181 | validation: 0.5703512438621138]
	TIME [epoch: 2.74 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055957918284742		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.4055957918284742 | validation: 0.5727007161971013]
	TIME [epoch: 2.73 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36502996465670046		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.36502996465670046 | validation: 0.4483831906519371]
	TIME [epoch: 2.73 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290408111079913		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.3290408111079913 | validation: 0.35647615523834725]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2510812482563632		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.2510812482563632 | validation: 0.3940769066698017]
	TIME [epoch: 2.73 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24034203640938498		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.24034203640938498 | validation: 0.38231432500530627]
	TIME [epoch: 2.74 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22819590078469654		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.22819590078469654 | validation: 0.3509011492405391]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21736232934871463		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.21736232934871463 | validation: 0.33114973912916834]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217715930932775		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.217715930932775 | validation: 0.37573153387092983]
	TIME [epoch: 2.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22098156027822036		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.22098156027822036 | validation: 0.3501063966269349]
	TIME [epoch: 2.74 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21269224072936532		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.21269224072936532 | validation: 0.41783162143166774]
	TIME [epoch: 2.74 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23759997554440854		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.23759997554440854 | validation: 0.48933448637277066]
	TIME [epoch: 2.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471798116263186		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.3471798116263186 | validation: 0.3887075250352383]
	TIME [epoch: 2.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2190019301292118		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2190019301292118 | validation: 0.3991385105284083]
	TIME [epoch: 2.74 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22354928333403457		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.22354928333403457 | validation: 0.49960490589276274]
	TIME [epoch: 2.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456947210284588		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3456947210284588 | validation: 0.346259772570528]
	TIME [epoch: 2.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19564702506511744		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.19564702506511744 | validation: 0.425405100462445]
	TIME [epoch: 2.73 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2547346706809918		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.2547346706809918 | validation: 0.48973569146683]
	TIME [epoch: 2.73 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36785940513754073		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.36785940513754073 | validation: 0.36592964754538015]
	TIME [epoch: 2.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20689623820001524		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.20689623820001524 | validation: 0.5223068730539862]
	TIME [epoch: 2.73 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3377959205882354		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.3377959205882354 | validation: 0.5179296427950755]
	TIME [epoch: 2.73 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30897677503916354		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.30897677503916354 | validation: 0.4505054732860931]
	TIME [epoch: 2.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265242278567403		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.265242278567403 | validation: 0.38587027011186853]
	TIME [epoch: 2.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2212010129203318		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.2212010129203318 | validation: 0.31052845887729125]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18734640800140775		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.18734640800140775 | validation: 0.3413757418628904]
	TIME [epoch: 2.74 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18574653803249316		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.18574653803249316 | validation: 0.3501277812827871]
	TIME [epoch: 2.74 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18435395159431736		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.18435395159431736 | validation: 0.3096669584233751]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17907723541294446		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.17907723541294446 | validation: 0.35657335984788335]
	TIME [epoch: 2.73 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749878753433349		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.1749878753433349 | validation: 0.32760160805169747]
	TIME [epoch: 2.73 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16644811300386994		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.16644811300386994 | validation: 0.38405969459706363]
	TIME [epoch: 2.73 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1818072554669297		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.1818072554669297 | validation: 0.3391475931420371]
	TIME [epoch: 2.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20111235554214196		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.20111235554214196 | validation: 0.4286009887412062]
	TIME [epoch: 2.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19312350649672141		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.19312350649672141 | validation: 0.4541212074064025]
	TIME [epoch: 2.73 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24546628905028134		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.24546628905028134 | validation: 0.6307020070892083]
	TIME [epoch: 2.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47835852525507777		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.47835852525507777 | validation: 0.5730360015637934]
	TIME [epoch: 2.73 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355032769634523		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.355032769634523 | validation: 0.39824215467088964]
	TIME [epoch: 2.73 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21204109994369613		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.21204109994369613 | validation: 0.3158716726952316]
	TIME [epoch: 2.73 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1691314506698951		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.1691314506698951 | validation: 0.30278136506818903]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15764663354051997		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.15764663354051997 | validation: 0.34357429094251835]
	TIME [epoch: 2.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16217275059331648		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.16217275059331648 | validation: 0.3148792535089264]
	TIME [epoch: 2.73 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1739647757934592		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.1739647757934592 | validation: 0.37313108010105694]
	TIME [epoch: 2.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17907094903936965		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.17907094903936965 | validation: 0.42113813913013476]
	TIME [epoch: 2.74 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24357901000361465		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.24357901000361465 | validation: 0.33976226570301726]
	TIME [epoch: 2.73 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16826756648580268		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.16826756648580268 | validation: 0.38528615207120354]
	TIME [epoch: 2.73 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17962259440347794		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.17962259440347794 | validation: 0.31619076751275443]
	TIME [epoch: 2.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205572458458336		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.205572458458336 | validation: 0.37806088545323624]
	TIME [epoch: 2.73 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16887023968206769		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.16887023968206769 | validation: 0.4159047725046927]
	TIME [epoch: 2.73 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2178965653575731		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.2178965653575731 | validation: 0.589702417802659]
	TIME [epoch: 2.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37446408692267313		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.37446408692267313 | validation: 0.5324632832310681]
	TIME [epoch: 2.73 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23451299413563817		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.23451299413563817 | validation: 0.39191564687135416]
	TIME [epoch: 2.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896267087502539		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.1896267087502539 | validation: 0.38367658095478474]
	TIME [epoch: 2.74 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1853090669629608		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.1853090669629608 | validation: 0.3467577380479789]
	TIME [epoch: 2.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15643865867769635		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.15643865867769635 | validation: 0.3220690490237544]
	TIME [epoch: 2.74 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14707289147609978		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.14707289147609978 | validation: 0.35151762075095805]
	TIME [epoch: 2.74 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15981785943720675		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.15981785943720675 | validation: 0.3900249091611241]
	TIME [epoch: 2.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15865199200248456		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.15865199200248456 | validation: 0.3229816181824583]
	TIME [epoch: 2.73 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13683906833595433		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.13683906833595433 | validation: 0.35667633736442217]
	TIME [epoch: 2.73 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14852727248511902		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.14852727248511902 | validation: 0.32615841785750166]
	TIME [epoch: 2.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15559336133537355		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.15559336133537355 | validation: 0.4122143839235859]
	TIME [epoch: 2.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1549659858442285		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.1549659858442285 | validation: 0.3012229497150282]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411784111791044		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.1411784111791044 | validation: 0.35807873621112396]
	TIME [epoch: 2.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13594336905020943		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.13594336905020943 | validation: 0.3284214521213395]
	TIME [epoch: 2.71 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14790496007772092		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.14790496007772092 | validation: 0.42435945313358037]
	TIME [epoch: 2.71 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136391958044013		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.2136391958044013 | validation: 0.6537385021936988]
	TIME [epoch: 2.71 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920505266788517		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.3920505266788517 | validation: 0.5062029491058253]
	TIME [epoch: 2.71 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23962610706997547		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.23962610706997547 | validation: 0.47760207481655126]
	TIME [epoch: 2.71 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25517709623228596		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.25517709623228596 | validation: 0.374219319987867]
	TIME [epoch: 2.71 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21108014252494833		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.21108014252494833 | validation: 0.36423137376108133]
	TIME [epoch: 2.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14130501956219113		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.14130501956219113 | validation: 0.3434462565814178]
	TIME [epoch: 2.71 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14734827067214493		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.14734827067214493 | validation: 0.30866528434664453]
	TIME [epoch: 2.71 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723543370831834		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.13723543370831834 | validation: 0.3102162623238295]
	TIME [epoch: 2.71 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272171578310012		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.12272171578310012 | validation: 0.3525295846220615]
	TIME [epoch: 2.71 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267588722222404		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.1267588722222404 | validation: 0.3126807512260529]
	TIME [epoch: 2.71 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12533635180735822		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.12533635180735822 | validation: 0.32026726880243295]
	TIME [epoch: 2.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12005482939362938		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.12005482939362938 | validation: 0.347299962646682]
	TIME [epoch: 2.71 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12541819214375832		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.12541819214375832 | validation: 0.33120682932684975]
	TIME [epoch: 2.71 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15151141890362255		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.15151141890362255 | validation: 0.46373614030967714]
	TIME [epoch: 2.71 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23186083307484776		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.23186083307484776 | validation: 0.41585114683575963]
	TIME [epoch: 2.71 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18045341066569254		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.18045341066569254 | validation: 0.31744764487083593]
	TIME [epoch: 2.71 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12947306052422522		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.12947306052422522 | validation: 0.3177880301733684]
	TIME [epoch: 2.71 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961571538252503		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.10961571538252503 | validation: 0.32633813641170256]
	TIME [epoch: 2.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564826987420293		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.11564826987420293 | validation: 0.3257231540274563]
	TIME [epoch: 2.71 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126691973085897		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.126691973085897 | validation: 0.33004399826771413]
	TIME [epoch: 2.71 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13913494926122336		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.13913494926122336 | validation: 0.5439750208081726]
	TIME [epoch: 2.71 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018321435340331		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.2018321435340331 | validation: 0.6845273638592788]
	TIME [epoch: 2.71 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47334058219171843		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.47334058219171843 | validation: 0.3907183806518784]
	TIME [epoch: 2.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14493247481339502		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.14493247481339502 | validation: 0.31102056706392067]
	TIME [epoch: 2.71 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24810901796257817		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.24810901796257817 | validation: 0.30530073223633425]
	TIME [epoch: 2.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167138829310512		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.1167138829310512 | validation: 0.37955480061507113]
	TIME [epoch: 2.71 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16315545987192523		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.16315545987192523 | validation: 0.39348300214183585]
	TIME [epoch: 2.71 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321551072953862		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.1321551072953862 | validation: 0.32376780362604735]
	TIME [epoch: 2.71 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11940212796721779		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.11940212796721779 | validation: 0.31873157825483833]
	TIME [epoch: 2.71 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12507979539335048		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.12507979539335048 | validation: 0.366535725819701]
	TIME [epoch: 2.71 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124372122364222		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.11124372122364222 | validation: 0.3355669425374688]
	TIME [epoch: 2.71 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10958786315272914		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.10958786315272914 | validation: 0.32711386607411347]
	TIME [epoch: 2.71 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11332191516880791		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.11332191516880791 | validation: 0.3204378189927831]
	TIME [epoch: 2.71 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12364748361641205		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.12364748361641205 | validation: 0.4102961022667665]
	TIME [epoch: 2.71 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13023524760125152		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.13023524760125152 | validation: 0.33658045731535874]
	TIME [epoch: 2.71 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13866858134015783		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.13866858134015783 | validation: 0.412344158857507]
	TIME [epoch: 2.71 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14658872662425756		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.14658872662425756 | validation: 0.3584860358208495]
	TIME [epoch: 2.71 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16060396718220027		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.16060396718220027 | validation: 0.4684776457835511]
	TIME [epoch: 2.72 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553567689365413		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.1553567689365413 | validation: 0.38068912165275315]
	TIME [epoch: 2.71 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11449129270353794		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.11449129270353794 | validation: 0.3109631614790817]
	TIME [epoch: 2.71 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10891612482378836		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.10891612482378836 | validation: 0.3354904815561157]
	TIME [epoch: 2.71 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1119787057777846		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.1119787057777846 | validation: 0.3551749462374653]
	TIME [epoch: 2.71 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12076602230967305		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.12076602230967305 | validation: 0.34025791854388243]
	TIME [epoch: 2.71 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13063300319148727		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.13063300319148727 | validation: 0.3732657577279941]
	TIME [epoch: 2.71 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14461978650624016		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.14461978650624016 | validation: 0.3781552639603907]
	TIME [epoch: 2.71 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14824251701258628		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.14824251701258628 | validation: 0.39854928813462986]
	TIME [epoch: 2.71 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15181391211265025		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.15181391211265025 | validation: 0.32775178736133576]
	TIME [epoch: 2.72 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12382490465730656		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.12382490465730656 | validation: 0.31485806170827996]
	TIME [epoch: 2.71 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679011297187931		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.09679011297187931 | validation: 0.3436657725283568]
	TIME [epoch: 2.71 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000464212685317		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.1000464212685317 | validation: 0.2989912793463338]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1092709931340178		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.1092709931340178 | validation: 0.38916190134142314]
	TIME [epoch: 2.73 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132212030687628		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.1132212030687628 | validation: 0.32359706179180103]
	TIME [epoch: 2.74 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12018494417676756		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.12018494417676756 | validation: 0.42325394468045874]
	TIME [epoch: 2.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13636035559924783		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.13636035559924783 | validation: 0.33703548780962767]
	TIME [epoch: 2.73 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11305489227060765		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.11305489227060765 | validation: 0.2837143282127909]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015011151343901		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1015011151343901 | validation: 0.30838754242733846]
	TIME [epoch: 2.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10200880002844472		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.10200880002844472 | validation: 0.3698863611433386]
	TIME [epoch: 2.73 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13343549857952336		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.13343549857952336 | validation: 0.3548994660054204]
	TIME [epoch: 2.73 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142214217518629		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.142214217518629 | validation: 0.4259333332665053]
	TIME [epoch: 2.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15266697493200282		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.15266697493200282 | validation: 0.33403388490086505]
	TIME [epoch: 2.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12424445669856678		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.12424445669856678 | validation: 0.3352731385676159]
	TIME [epoch: 2.73 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365086154855135		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.09365086154855135 | validation: 0.2879936080010661]
	TIME [epoch: 2.74 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681072119629106		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.08681072119629106 | validation: 0.3258051940320099]
	TIME [epoch: 2.73 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808527848431078		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.0808527848431078 | validation: 0.32875808765373166]
	TIME [epoch: 2.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08352807567241044		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.08352807567241044 | validation: 0.3103624421208182]
	TIME [epoch: 2.73 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0853626859927962		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.0853626859927962 | validation: 0.3588369900075943]
	TIME [epoch: 2.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635965388135357		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.10635965388135357 | validation: 0.42090282158006814]
	TIME [epoch: 2.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18204056571067553		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.18204056571067553 | validation: 0.5029908281480852]
	TIME [epoch: 2.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21617435255453246		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.21617435255453246 | validation: 0.37059647149737557]
	TIME [epoch: 2.73 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1092018388562751		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.1092018388562751 | validation: 0.29661567376893433]
	TIME [epoch: 2.74 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09480690262735579		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.09480690262735579 | validation: 0.3578711012446087]
	TIME [epoch: 2.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282397474886157		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.10282397474886157 | validation: 0.3478917135160282]
	TIME [epoch: 2.73 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11594599738800987		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.11594599738800987 | validation: 0.42980551855491156]
	TIME [epoch: 2.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12401137350054323		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.12401137350054323 | validation: 0.3564508406015246]
	TIME [epoch: 2.74 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12325910829669762		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.12325910829669762 | validation: 0.26949859875972443]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11812862579539907		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.11812862579539907 | validation: 0.3042369836651685]
	TIME [epoch: 2.71 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09250231006745878		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.09250231006745878 | validation: 0.3210696343611019]
	TIME [epoch: 2.72 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08815310647467126		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.08815310647467126 | validation: 0.2955291829913664]
	TIME [epoch: 2.7 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07925593905866056		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.07925593905866056 | validation: 0.30974217718627367]
	TIME [epoch: 2.71 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08029613831780683		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.08029613831780683 | validation: 0.2920555219681562]
	TIME [epoch: 2.71 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839853151373398		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.07839853151373398 | validation: 0.31295894146885417]
	TIME [epoch: 2.71 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034691476164735		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.08034691476164735 | validation: 0.286862880977667]
	TIME [epoch: 2.71 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247624639625826		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.09247624639625826 | validation: 0.41872627874033147]
	TIME [epoch: 2.71 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313727218147307		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1313727218147307 | validation: 0.38870921145751586]
	TIME [epoch: 2.71 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1837401925204548		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1837401925204548 | validation: 0.43537265160017524]
	TIME [epoch: 2.71 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17166712123789907		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.17166712123789907 | validation: 0.3006894728657912]
	TIME [epoch: 2.71 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513960099238824		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.07513960099238824 | validation: 0.33655330041887127]
	TIME [epoch: 2.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912586976784714		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.08912586976784714 | validation: 0.2903077314831631]
	TIME [epoch: 2.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09086803195898564		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.09086803195898564 | validation: 0.2927349313878768]
	TIME [epoch: 2.71 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07501197353701955		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.07501197353701955 | validation: 0.3148423616704592]
	TIME [epoch: 2.71 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07192986008240071		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.07192986008240071 | validation: 0.2932097478387084]
	TIME [epoch: 2.71 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07195800588393941		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.07195800588393941 | validation: 0.29574756290611465]
	TIME [epoch: 2.71 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07518301020122574		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.07518301020122574 | validation: 0.2812730310851192]
	TIME [epoch: 2.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727630705447364		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.07727630705447364 | validation: 0.3501850203083008]
	TIME [epoch: 2.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08281959259448982		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.08281959259448982 | validation: 0.2879801938484858]
	TIME [epoch: 2.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764599402860707		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.09764599402860707 | validation: 0.4667601328761542]
	TIME [epoch: 2.71 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14605289644896688		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.14605289644896688 | validation: 0.41517667216293747]
	TIME [epoch: 2.71 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19462808177852453		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.19462808177852453 | validation: 0.27673105224482875]
	TIME [epoch: 2.71 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1277212081593882		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.1277212081593882 | validation: 0.30811469922534596]
	TIME [epoch: 2.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07389262891243567		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.07389262891243567 | validation: 0.2901424348629648]
	TIME [epoch: 2.73 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106465819139246		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.07106465819139246 | validation: 0.2970482218762201]
	TIME [epoch: 2.73 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08274937037060465		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.08274937037060465 | validation: 0.41357054709923863]
	TIME [epoch: 2.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19705703409281014		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.19705703409281014 | validation: 0.3179264701951476]
	TIME [epoch: 2.74 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725955433515967		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.08725955433515967 | validation: 0.26280072773777363]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07774110991291178		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.07774110991291178 | validation: 0.2901218119123525]
	TIME [epoch: 2.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07304366521816293		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.07304366521816293 | validation: 0.2984516122768035]
	TIME [epoch: 2.73 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0713313816654229		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.0713313816654229 | validation: 0.40579026882199437]
	TIME [epoch: 2.72 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09436441879527216		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.09436441879527216 | validation: 0.28443397618880323]
	TIME [epoch: 2.72 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09689293971678203		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.09689293971678203 | validation: 0.3177351634989662]
	TIME [epoch: 2.72 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983714509956478		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.0983714509956478 | validation: 0.2734862814923672]
	TIME [epoch: 2.72 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140765988316128		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.09140765988316128 | validation: 0.31812272371031136]
	TIME [epoch: 2.72 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08185339231251451		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.08185339231251451 | validation: 0.2849685375144887]
	TIME [epoch: 2.72 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075941805159896		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.08075941805159896 | validation: 0.31005397721595246]
	TIME [epoch: 2.72 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631258736435408		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.07631258736435408 | validation: 0.33066340353122525]
	TIME [epoch: 2.72 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127344074798428		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.127344074798428 | validation: 0.30220505704301825]
	TIME [epoch: 2.72 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08222732628220668		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.08222732628220668 | validation: 0.2742663769721867]
	TIME [epoch: 2.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07032622194824706		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.07032622194824706 | validation: 0.30015140670396745]
	TIME [epoch: 2.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07627891030060124		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.07627891030060124 | validation: 0.2960821419038921]
	TIME [epoch: 2.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09240215230226671		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.09240215230226671 | validation: 0.34185683297337655]
	TIME [epoch: 2.73 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13816209294885848		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.13816209294885848 | validation: 0.3918246556190823]
	TIME [epoch: 2.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721068144595741		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.08721068144595741 | validation: 0.2862697428344605]
	TIME [epoch: 2.72 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019099603428275		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.1019099603428275 | validation: 0.2958071411480719]
	TIME [epoch: 2.72 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06717345582370891		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.06717345582370891 | validation: 0.35581733593547693]
	TIME [epoch: 2.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07509370781138584		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.07509370781138584 | validation: 0.2677680824883956]
	TIME [epoch: 2.73 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08061433227803874		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.08061433227803874 | validation: 0.3081745589019684]
	TIME [epoch: 2.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940872545115765		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.07940872545115765 | validation: 0.27981003736925864]
	TIME [epoch: 2.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417212657845537		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.07417212657845537 | validation: 0.31520904766699975]
	TIME [epoch: 2.72 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655534228627713		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.0655534228627713 | validation: 0.27614530145450183]
	TIME [epoch: 2.72 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06093001329915002		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.06093001329915002 | validation: 0.24287072514502334]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058662948730821535		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.058662948730821535 | validation: 0.28413689756538363]
	TIME [epoch: 2.72 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534303155603371		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.05534303155603371 | validation: 0.25849649023438936]
	TIME [epoch: 2.72 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317121212182492		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.06317121212182492 | validation: 0.27955355688751043]
	TIME [epoch: 2.73 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08687856931581607		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.08687856931581607 | validation: 0.4252464722763616]
	TIME [epoch: 2.73 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17370606252499834		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.17370606252499834 | validation: 0.3452600115408707]
	TIME [epoch: 2.73 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15227983254319502		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.15227983254319502 | validation: 0.4036957827718771]
	TIME [epoch: 2.73 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191853422078349		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.08191853422078349 | validation: 0.2528194388703476]
	TIME [epoch: 2.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07666753310844646		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.07666753310844646 | validation: 0.2716414665544186]
	TIME [epoch: 2.73 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06545704529087008		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.06545704529087008 | validation: 0.2814172284468617]
	TIME [epoch: 2.74 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597000022617372		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.05597000022617372 | validation: 0.24524484677369907]
	TIME [epoch: 2.74 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06163404642694856		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.06163404642694856 | validation: 0.280392277048945]
	TIME [epoch: 2.73 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057761436967369635		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.057761436967369635 | validation: 0.2503943031967985]
	TIME [epoch: 2.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055357488367689466		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.055357488367689466 | validation: 0.25078186935482827]
	TIME [epoch: 2.73 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05508602378418509		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.05508602378418509 | validation: 0.23274242738822967]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059787027972735406		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.059787027972735406 | validation: 0.36383349098117984]
	TIME [epoch: 2.73 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0796063029497473		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.0796063029497473 | validation: 0.2778012491393918]
	TIME [epoch: 2.73 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11204404621627999		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.11204404621627999 | validation: 0.3541750433378984]
	TIME [epoch: 2.73 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981099283289792		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.14981099283289792 | validation: 0.30719989910443496]
	TIME [epoch: 2.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11238896844096953		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.11238896844096953 | validation: 0.2785212590419645]
	TIME [epoch: 2.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05393447844301738		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.05393447844301738 | validation: 0.2439187666378103]
	TIME [epoch: 2.73 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06585883407302447		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.06585883407302447 | validation: 0.27312993259540636]
	TIME [epoch: 2.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691438268025467		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.0691438268025467 | validation: 0.26783116847285476]
	TIME [epoch: 2.73 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452071497134823		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.06452071497134823 | validation: 0.2665209011486068]
	TIME [epoch: 2.73 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05802926241096314		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.05802926241096314 | validation: 0.23550377187874638]
	TIME [epoch: 2.72 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05386502154928681		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.05386502154928681 | validation: 0.23665745214413667]
	TIME [epoch: 2.73 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051673737821197004		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.051673737821197004 | validation: 0.25778481806549497]
	TIME [epoch: 2.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208308422712883		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.05208308422712883 | validation: 0.22222450922415704]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804687910762631		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.05804687910762631 | validation: 0.3491125958314374]
	TIME [epoch: 2.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17672354489568576		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.17672354489568576 | validation: 0.3545431170039547]
	TIME [epoch: 2.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14041207442619708		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.14041207442619708 | validation: 0.2872013983698439]
	TIME [epoch: 2.72 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07988598152756007		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.07988598152756007 | validation: 0.2803086379343134]
	TIME [epoch: 2.74 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08309663498835505		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.08309663498835505 | validation: 0.25653369558380584]
	TIME [epoch: 2.73 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05221032540617143		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.05221032540617143 | validation: 0.25052206342531347]
	TIME [epoch: 2.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06246581603892683		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.06246581603892683 | validation: 0.2875832773710341]
	TIME [epoch: 2.73 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060230780372559724		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.060230780372559724 | validation: 0.24227609644823]
	TIME [epoch: 2.74 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05275387853295549		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.05275387853295549 | validation: 0.23576670108064032]
	TIME [epoch: 2.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622062302552418		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.05622062302552418 | validation: 0.2582810503476077]
	TIME [epoch: 2.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06438266373295606		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.06438266373295606 | validation: 0.2698469902370215]
	TIME [epoch: 2.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889806457984565		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.06889806457984565 | validation: 0.2659345844089817]
	TIME [epoch: 2.73 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532234598691431		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.10532234598691431 | validation: 0.39064763111511647]
	TIME [epoch: 2.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09478236665710459		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.09478236665710459 | validation: 0.2881040943554229]
	TIME [epoch: 2.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05625238622061019		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.05625238622061019 | validation: 0.1977406819423872]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05535757816217851		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.05535757816217851 | validation: 0.22325315280777902]
	TIME [epoch: 2.72 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054973700655912054		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.054973700655912054 | validation: 0.23002598576679115]
	TIME [epoch: 2.72 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05438469040035107		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.05438469040035107 | validation: 0.203596193807711]
	TIME [epoch: 2.73 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048502829393793016		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.048502829393793016 | validation: 0.20628682386787808]
	TIME [epoch: 2.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04989855512465554		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.04989855512465554 | validation: 0.2345583288887906]
	TIME [epoch: 2.73 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05339020955763195		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.05339020955763195 | validation: 0.20313613292922794]
	TIME [epoch: 2.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05753671179232293		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.05753671179232293 | validation: 0.25897696429563277]
	TIME [epoch: 2.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969201656789733		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.0969201656789733 | validation: 0.41714376182414936]
	TIME [epoch: 2.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2538911477227163		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.2538911477227163 | validation: 0.33718059471195433]
	TIME [epoch: 2.73 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746923853580076		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.0746923853580076 | validation: 0.27507364257474326]
	TIME [epoch: 2.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13534604630402725		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.13534604630402725 | validation: 0.2645694195166956]
	TIME [epoch: 2.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07729418693756845		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.07729418693756845 | validation: 0.29379632557272445]
	TIME [epoch: 2.73 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696543283120732		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.0696543283120732 | validation: 0.2338626583881525]
	TIME [epoch: 2.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05553236152064443		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.05553236152064443 | validation: 0.22619472031803675]
	TIME [epoch: 2.73 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04947280480519936		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.04947280480519936 | validation: 0.2536256609260101]
	TIME [epoch: 2.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05148538610458132		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.05148538610458132 | validation: 0.2317558318487948]
	TIME [epoch: 2.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0541167903101492		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.0541167903101492 | validation: 0.22396696686835604]
	TIME [epoch: 2.73 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046548664606314516		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.046548664606314516 | validation: 0.2202928691318886]
	TIME [epoch: 2.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046221286496560605		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.046221286496560605 | validation: 0.23416285074815946]
	TIME [epoch: 2.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576501505919138		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.04576501505919138 | validation: 0.2056861927056821]
	TIME [epoch: 2.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046407204594081486		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.046407204594081486 | validation: 0.21151501729687727]
	TIME [epoch: 2.73 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051214765571085154		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.051214765571085154 | validation: 0.20766965150777802]
	TIME [epoch: 2.72 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755627657068219		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.04755627657068219 | validation: 0.18703424503268948]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357621552645755		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.05357621552645755 | validation: 0.2743160491567328]
	TIME [epoch: 2.72 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059948204862308146		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.059948204862308146 | validation: 0.21419502867001108]
	TIME [epoch: 2.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157736184158597		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.07157736184158597 | validation: 0.28981577050256496]
	TIME [epoch: 2.73 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10377866019024484		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.10377866019024484 | validation: 0.3230530241630274]
	TIME [epoch: 2.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18252915645430917		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.18252915645430917 | validation: 0.27164497546894834]
	TIME [epoch: 182 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052323872796755135		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.052323872796755135 | validation: 0.19717537897061502]
	TIME [epoch: 5.85 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435108477183298		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.07435108477183298 | validation: 0.2515559070502242]
	TIME [epoch: 5.85 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06297610085890346		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.06297610085890346 | validation: 0.273070745609959]
	TIME [epoch: 5.86 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04877888495508124		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.04877888495508124 | validation: 0.21432567405477718]
	TIME [epoch: 5.84 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053332200492576975		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.053332200492576975 | validation: 0.25475796047417476]
	TIME [epoch: 5.85 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07285603127973157		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.07285603127973157 | validation: 0.27507872981297476]
	TIME [epoch: 5.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058770472555107534		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.058770472555107534 | validation: 0.23174765826760746]
	TIME [epoch: 5.84 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838485092351332		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.05838485092351332 | validation: 0.22132604820446788]
	TIME [epoch: 5.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490412873177176		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.04490412873177176 | validation: 0.2182690494346313]
	TIME [epoch: 5.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04492823490166991		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.04492823490166991 | validation: 0.2076062750021969]
	TIME [epoch: 5.84 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047847777888286494		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.047847777888286494 | validation: 0.22388732081478518]
	TIME [epoch: 5.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05740636792834857		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.05740636792834857 | validation: 0.2564293570296573]
	TIME [epoch: 5.85 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487121783173846		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.05487121783173846 | validation: 0.22477339631475957]
	TIME [epoch: 5.84 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678578690919132		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.05678578690919132 | validation: 0.2100372139789035]
	TIME [epoch: 5.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048125749950598984		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.048125749950598984 | validation: 0.1973371685503475]
	TIME [epoch: 5.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04993078241889107		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.04993078241889107 | validation: 0.2514335831432747]
	TIME [epoch: 5.85 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042038420958704525		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.042038420958704525 | validation: 0.19504135577489892]
	TIME [epoch: 5.84 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03992523747942208		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.03992523747942208 | validation: 0.17448576089802073]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042925516475708206		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.042925516475708206 | validation: 0.1774720297190262]
	TIME [epoch: 5.83 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613761854766131		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.04613761854766131 | validation: 0.24623921861933346]
	TIME [epoch: 5.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795491713431882		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.0795491713431882 | validation: 0.3121799629881246]
	TIME [epoch: 5.85 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15971712171189062		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.15971712171189062 | validation: 0.3337749421599462]
	TIME [epoch: 5.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06312716718177992		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.06312716718177992 | validation: 0.1929851110691773]
	TIME [epoch: 5.85 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08928603203233283		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.08928603203233283 | validation: 0.25160596094246196]
	TIME [epoch: 5.86 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715315362488326		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.07715315362488326 | validation: 0.2919337195001301]
	TIME [epoch: 5.85 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913346612132659		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.05913346612132659 | validation: 0.223321641559145]
	TIME [epoch: 5.86 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05300426564963243		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.05300426564963243 | validation: 0.2013260284222688]
	TIME [epoch: 5.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046040524872638		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.04046040524872638 | validation: 0.22638491962114565]
	TIME [epoch: 5.85 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043860872268251805		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.043860872268251805 | validation: 0.20078977566413567]
	TIME [epoch: 5.84 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04157468623913229		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.04157468623913229 | validation: 0.19666935868853616]
	TIME [epoch: 5.84 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03947880469112196		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.03947880469112196 | validation: 0.22404809548804994]
	TIME [epoch: 5.83 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04112961730952757		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.04112961730952757 | validation: 0.20906781179940084]
	TIME [epoch: 5.85 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040834040024568835		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.040834040024568835 | validation: 0.21157465252170968]
	TIME [epoch: 5.85 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054874579662011876		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.054874579662011876 | validation: 0.2676287999067336]
	TIME [epoch: 5.85 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056127754776910246		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.056127754776910246 | validation: 0.21951343614833121]
	TIME [epoch: 5.84 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058924164576483154		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.058924164576483154 | validation: 0.20811074827472728]
	TIME [epoch: 5.85 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04239493587442675		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.04239493587442675 | validation: 0.18036721879054576]
	TIME [epoch: 5.84 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047433775082877265		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.047433775082877265 | validation: 0.26480273387964076]
	TIME [epoch: 5.85 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04479540960491768		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.04479540960491768 | validation: 0.3242887561907253]
	TIME [epoch: 5.84 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16998898208222663		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16998898208222663 | validation: 0.38046184467280647]
	TIME [epoch: 5.85 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0802519037889048		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.0802519037889048 | validation: 0.2055959661434923]
	TIME [epoch: 5.84 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08037326409824438		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.08037326409824438 | validation: 0.2335902149355308]
	TIME [epoch: 5.85 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08224965487171172		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.08224965487171172 | validation: 0.2638871389624849]
	TIME [epoch: 5.84 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060288217509512826		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.060288217509512826 | validation: 0.21526115214996602]
	TIME [epoch: 5.85 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0482417595060608		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.0482417595060608 | validation: 0.2068903763608142]
	TIME [epoch: 5.84 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514855065153019		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.04514855065153019 | validation: 0.2164049081162701]
	TIME [epoch: 5.85 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03805628183116977		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.03805628183116977 | validation: 0.1930599164119071]
	TIME [epoch: 5.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03751705942525936		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.03751705942525936 | validation: 0.20940877783430834]
	TIME [epoch: 5.85 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03894030340633421		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.03894030340633421 | validation: 0.20402244138679326]
	TIME [epoch: 5.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036474625049645625		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.036474625049645625 | validation: 0.18962253827448594]
	TIME [epoch: 5.85 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545617446080388		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.03545617446080388 | validation: 0.16641503039172623]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608922098453081		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.03608922098453081 | validation: 0.19409969294544413]
	TIME [epoch: 5.85 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564276480456021		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.03564276480456021 | validation: 0.16532621223396513]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680355447982515		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.03680355447982515 | validation: 0.18554846012535184]
	TIME [epoch: 5.84 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0370583423552365		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.0370583423552365 | validation: 0.16635462143573893]
	TIME [epoch: 5.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04193712676662898		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.04193712676662898 | validation: 0.20464623979357122]
	TIME [epoch: 5.85 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06492939601791883		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.06492939601791883 | validation: 0.34391008897065]
	TIME [epoch: 5.85 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08392779118039449		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.08392779118039449 | validation: 0.24787412893998118]
	TIME [epoch: 5.84 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728521673164581		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.0728521673164581 | validation: 0.17890932986085295]
	TIME [epoch: 5.85 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037763665227535034		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.037763665227535034 | validation: 0.18677494674323658]
	TIME [epoch: 5.84 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03817570653778295		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.03817570653778295 | validation: 0.3346844614699064]
	TIME [epoch: 5.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05097356805227704		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.05097356805227704 | validation: 0.24215036374545804]
	TIME [epoch: 5.84 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04245033429218137		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.04245033429218137 | validation: 0.18329506205440624]
	TIME [epoch: 5.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034990522514262115		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.034990522514262115 | validation: 0.17131801075998954]
	TIME [epoch: 5.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852697365112315		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.03852697365112315 | validation: 0.18362247016499228]
	TIME [epoch: 5.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034853500882332376		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.034853500882332376 | validation: 0.22147637619363802]
	TIME [epoch: 5.85 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03810331971756598		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.03810331971756598 | validation: 0.21769646793513733]
	TIME [epoch: 5.85 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0565415302778219		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.0565415302778219 | validation: 0.20126533485005746]
	TIME [epoch: 5.85 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955626375379508		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.0955626375379508 | validation: 0.24447109067577766]
	TIME [epoch: 5.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072038059095194		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.1072038059095194 | validation: 0.2675222714831497]
	TIME [epoch: 5.84 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055981959228904865		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.055981959228904865 | validation: 0.22504925215344201]
	TIME [epoch: 5.84 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053630419560501255		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.053630419560501255 | validation: 0.22533601514244384]
	TIME [epoch: 5.84 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054032473255457736		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.054032473255457736 | validation: 0.2064204572186256]
	TIME [epoch: 5.84 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04134927717943421		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.04134927717943421 | validation: 0.16792838421644185]
	TIME [epoch: 5.85 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04653719831232186		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.04653719831232186 | validation: 0.1747825466399264]
	TIME [epoch: 5.84 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034535686277475296		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.034535686277475296 | validation: 0.20241198432836224]
	TIME [epoch: 5.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032166557321947835		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.032166557321947835 | validation: 0.15929556000162207]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035866140472796036		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.035866140472796036 | validation: 0.19539348462487904]
	TIME [epoch: 5.83 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03320635935944198		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.03320635935944198 | validation: 0.16509916013972706]
	TIME [epoch: 5.85 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038637469447441085		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.038637469447441085 | validation: 0.25573041433828364]
	TIME [epoch: 5.85 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04374271213210011		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.04374271213210011 | validation: 0.16435287386643305]
	TIME [epoch: 5.84 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036004301612322265		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.036004301612322265 | validation: 0.17377545980912024]
	TIME [epoch: 5.85 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973279000647279		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.03973279000647279 | validation: 0.1810904663504394]
	TIME [epoch: 5.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03944416834721373		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.03944416834721373 | validation: 0.19291180019373447]
	TIME [epoch: 5.85 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531478793292752		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.05531478793292752 | validation: 0.2461356650102114]
	TIME [epoch: 5.85 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06768282473630348		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.06768282473630348 | validation: 0.1771911798193957]
	TIME [epoch: 5.85 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461356976796444		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.05461356976796444 | validation: 0.19017845022725968]
	TIME [epoch: 5.85 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031474505897152456		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.031474505897152456 | validation: 0.16705995135589613]
	TIME [epoch: 5.84 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037312208031799236		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.037312208031799236 | validation: 0.1919091086585489]
	TIME [epoch: 5.84 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053536948084444605		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.053536948084444605 | validation: 0.24678064283296336]
	TIME [epoch: 5.84 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743990397538144		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.04743990397538144 | validation: 0.20276662128079648]
	TIME [epoch: 5.84 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039994894157736625		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.039994894157736625 | validation: 0.16576108407785198]
	TIME [epoch: 5.84 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03435437719478422		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.03435437719478422 | validation: 0.17842542267079134]
	TIME [epoch: 5.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033872696191419256		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.033872696191419256 | validation: 0.1769073750996482]
	TIME [epoch: 5.84 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03382044087147233		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.03382044087147233 | validation: 0.1663385225466649]
	TIME [epoch: 5.84 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030845897954383		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.03030845897954383 | validation: 0.25981565698732345]
	TIME [epoch: 5.85 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037305030532218		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.037305030532218 | validation: 0.1505460310513308]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792147397629531		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.03792147397629531 | validation: 0.16963031673649095]
	TIME [epoch: 5.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035372181573715425		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.035372181573715425 | validation: 0.16295352283732886]
	TIME [epoch: 5.85 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037249205804828614		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.037249205804828614 | validation: 0.14837032607773]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03375175674200388		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.03375175674200388 | validation: 0.17623349460153426]
	TIME [epoch: 5.84 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04015813023520713		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.04015813023520713 | validation: 0.2349661666290814]
	TIME [epoch: 5.85 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244459913681037		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.09244459913681037 | validation: 0.35562406324084145]
	TIME [epoch: 5.86 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08596770798028679		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.08596770798028679 | validation: 0.2702114504103791]
	TIME [epoch: 5.85 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056387060213285656		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.056387060213285656 | validation: 0.17984113133378338]
	TIME [epoch: 5.86 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344058619325303		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.04344058619325303 | validation: 0.1535535533713253]
	TIME [epoch: 5.85 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088366148953751		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.05088366148953751 | validation: 0.19298110928357387]
	TIME [epoch: 5.84 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043273009968742386		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.043273009968742386 | validation: 0.18792697467762304]
	TIME [epoch: 5.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03086398330451989		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.03086398330451989 | validation: 0.1782896891946737]
	TIME [epoch: 5.84 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029952314254865647		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.029952314254865647 | validation: 0.1549171724200858]
	TIME [epoch: 5.84 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027207455726219267		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.027207455726219267 | validation: 0.25761025289049666]
	TIME [epoch: 5.83 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13465062400294214		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.13465062400294214 | validation: 0.24608932456897317]
	TIME [epoch: 5.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058996145040503566		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.058996145040503566 | validation: 0.26561709092965285]
	TIME [epoch: 5.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08412150014340508		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.08412150014340508 | validation: 0.16404214281254764]
	TIME [epoch: 5.84 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03332320568153101		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.03332320568153101 | validation: 0.1851845496710972]
	TIME [epoch: 5.84 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04036219353095619		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.04036219353095619 | validation: 0.16724559836584582]
	TIME [epoch: 5.84 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03119700433556972		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.03119700433556972 | validation: 0.17389504854807947]
	TIME [epoch: 5.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083751367279706		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.04083751367279706 | validation: 0.17867773782717666]
	TIME [epoch: 5.85 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037230124828596105		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.037230124828596105 | validation: 0.17449711413301017]
	TIME [epoch: 5.85 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029489331405547343		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.029489331405547343 | validation: 0.17322674662954096]
	TIME [epoch: 5.84 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029249992333428808		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.029249992333428808 | validation: 0.16700207744527573]
	TIME [epoch: 5.84 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029803923955195648		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.029803923955195648 | validation: 0.1648436349005092]
	TIME [epoch: 5.85 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03100349589217247		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.03100349589217247 | validation: 0.16211573609522958]
	TIME [epoch: 5.85 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03402515903827224		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.03402515903827224 | validation: 0.19034139046404389]
	TIME [epoch: 5.84 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030557111755381925		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.030557111755381925 | validation: 0.16932279873286285]
	TIME [epoch: 5.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029006534686813988		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.029006534686813988 | validation: 0.13688286445374695]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03262693479718126		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.03262693479718126 | validation: 0.1723025475970964]
	TIME [epoch: 5.84 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03012214335532686		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.03012214335532686 | validation: 0.13662329404373072]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330879991261428		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.04330879991261428 | validation: 0.20515775799111566]
	TIME [epoch: 5.83 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04582784630587121		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.04582784630587121 | validation: 0.1931843408996882]
	TIME [epoch: 5.84 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06707827457827033		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.06707827457827033 | validation: 0.19679014663919625]
	TIME [epoch: 5.83 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035574285834428884		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.035574285834428884 | validation: 0.1670550891257857]
	TIME [epoch: 5.84 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028693197675500187		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.028693197675500187 | validation: 0.15724833744940742]
	TIME [epoch: 5.84 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02807505009619398		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.02807505009619398 | validation: 0.13927977039689324]
	TIME [epoch: 5.83 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03795623043981503		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.03795623043981503 | validation: 0.1736401280569648]
	TIME [epoch: 5.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873291639497161		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.04873291639497161 | validation: 0.22494248206452552]
	TIME [epoch: 5.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178874176171		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.04178874176171 | validation: 0.1375771445407051]
	TIME [epoch: 5.84 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05175067902377376		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.05175067902377376 | validation: 0.21324321751830044]
	TIME [epoch: 5.84 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06822183176427146		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06822183176427146 | validation: 0.49827324640679244]
	TIME [epoch: 5.85 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13207398294821707		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.13207398294821707 | validation: 0.4361397627177322]
	TIME [epoch: 5.84 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08190899376013992		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08190899376013992 | validation: 0.22342682116420978]
	TIME [epoch: 5.85 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043593370560171874		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.043593370560171874 | validation: 0.15618491921255045]
	TIME [epoch: 5.84 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388252437530851		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.0388252437530851 | validation: 0.14953200948802886]
	TIME [epoch: 5.85 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035926239544842246		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.035926239544842246 | validation: 0.1515896991674417]
	TIME [epoch: 5.85 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017244391375632		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.03017244391375632 | validation: 0.17292367224061522]
	TIME [epoch: 5.85 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029024017920850697		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.029024017920850697 | validation: 0.15946402307574314]
	TIME [epoch: 5.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029061243012394673		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.029061243012394673 | validation: 0.14572052105177913]
	TIME [epoch: 5.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393504436166056		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.03393504436166056 | validation: 0.14020794109879017]
	TIME [epoch: 5.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032373972410511966		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.032373972410511966 | validation: 0.16680938356781044]
	TIME [epoch: 5.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041563410519553035		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.041563410519553035 | validation: 0.2097300849777903]
	TIME [epoch: 5.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03465903053022151		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.03465903053022151 | validation: 0.17916742069385866]
	TIME [epoch: 5.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028606649085766333		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.028606649085766333 | validation: 0.15076726725922265]
	TIME [epoch: 5.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02952264058138586		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.02952264058138586 | validation: 0.13144143816473894]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03081473363441045		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.03081473363441045 | validation: 0.1561368815770913]
	TIME [epoch: 5.86 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029348609420463863		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.029348609420463863 | validation: 0.17324795150552644]
	TIME [epoch: 5.85 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047874768279560595		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.047874768279560595 | validation: 0.18853911985588392]
	TIME [epoch: 5.86 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598688562912019		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.0598688562912019 | validation: 0.2024102607591777]
	TIME [epoch: 5.83 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764920450716267		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.03764920450716267 | validation: 0.1537449351514817]
	TIME [epoch: 5.82 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569883028411609		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.04569883028411609 | validation: 0.16429751695774442]
	TIME [epoch: 5.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0397726196458073		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0397726196458073 | validation: 0.20228411373193325]
	TIME [epoch: 5.83 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035596839338039414		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.035596839338039414 | validation: 0.155543391121084]
	TIME [epoch: 5.83 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03067683899402882		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.03067683899402882 | validation: 0.14285759264320483]
	TIME [epoch: 5.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028022248385176553		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.028022248385176553 | validation: 0.15954408347200252]
	TIME [epoch: 5.85 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029083506852738816		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.029083506852738816 | validation: 0.1663887553765723]
	TIME [epoch: 5.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043632473014645186		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.043632473014645186 | validation: 0.21621107182956756]
	TIME [epoch: 5.85 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583876024777755		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.05583876024777755 | validation: 0.19672723249492394]
	TIME [epoch: 5.84 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558039027060605		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.05558039027060605 | validation: 0.1743539480590939]
	TIME [epoch: 5.84 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030885740428325283		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.030885740428325283 | validation: 0.15907568840497677]
	TIME [epoch: 5.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05046868763973755		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.05046868763973755 | validation: 0.1756879160577349]
	TIME [epoch: 5.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03731663859784769		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.03731663859784769 | validation: 0.17659843696497504]
	TIME [epoch: 5.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03005567045281985		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.03005567045281985 | validation: 0.1528657107603786]
	TIME [epoch: 5.83 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030114484424411368		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.030114484424411368 | validation: 0.12283945023736052]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02774048111485727		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.02774048111485727 | validation: 0.14212933541319325]
	TIME [epoch: 5.85 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029587817041636405		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.029587817041636405 | validation: 0.14091235210961472]
	TIME [epoch: 5.86 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026148408297256422		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.026148408297256422 | validation: 0.148060216038024]
	TIME [epoch: 5.85 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024977031162195847		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.024977031162195847 | validation: 0.14856018124414191]
	TIME [epoch: 5.86 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0251438516758952		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.0251438516758952 | validation: 0.14119554182736013]
	TIME [epoch: 5.86 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026888395088118716		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.026888395088118716 | validation: 0.24202429252232982]
	TIME [epoch: 5.85 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0847650691887563		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0847650691887563 | validation: 0.2358950164306579]
	TIME [epoch: 5.86 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04966245699144707		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.04966245699144707 | validation: 0.21164539090167495]
	TIME [epoch: 5.85 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02950117671259582		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.02950117671259582 | validation: 0.18209831634804413]
	TIME [epoch: 5.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03366001619748863		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.03366001619748863 | validation: 0.15347905869891357]
	TIME [epoch: 5.84 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030045565566855288		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.030045565566855288 | validation: 0.1602969295687142]
	TIME [epoch: 5.84 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02773209885644433		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.02773209885644433 | validation: 0.19055581123270116]
	TIME [epoch: 5.84 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04243733138969464		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.04243733138969464 | validation: 0.17631203466570555]
	TIME [epoch: 5.84 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030133942796822247		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.030133942796822247 | validation: 0.15454265281821555]
	TIME [epoch: 5.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03135680401550511		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.03135680401550511 | validation: 0.16971256046063019]
	TIME [epoch: 5.83 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04034829821398656		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.04034829821398656 | validation: 0.1475449240695282]
	TIME [epoch: 5.84 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025726047482194758		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.025726047482194758 | validation: 0.14242501357491247]
	TIME [epoch: 5.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036008906672789144		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.036008906672789144 | validation: 0.17480210169993682]
	TIME [epoch: 5.83 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03925993609720962		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.03925993609720962 | validation: 0.15109473537537546]
	TIME [epoch: 5.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025115082771843673		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.025115082771843673 | validation: 0.13600698711068346]
	TIME [epoch: 5.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02938307331323757		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.02938307331323757 | validation: 0.14267569491037724]
	TIME [epoch: 5.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024442772830510533		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.024442772830510533 | validation: 0.12718623354495937]
	TIME [epoch: 5.84 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027063376086737954		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.027063376086737954 | validation: 0.15145637182172852]
	TIME [epoch: 5.85 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02486855502867616		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.02486855502867616 | validation: 0.1535821749506003]
	TIME [epoch: 5.84 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415289553256742		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.02415289553256742 | validation: 0.14829115615264116]
	TIME [epoch: 5.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030778806625250822		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.030778806625250822 | validation: 0.15398479511702387]
	TIME [epoch: 5.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852448450468745		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.03852448450468745 | validation: 0.18478342115919597]
	TIME [epoch: 5.83 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056650689212148275		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.056650689212148275 | validation: 0.18394984942389503]
	TIME [epoch: 5.86 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04091809498372429		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.04091809498372429 | validation: 0.21538631024837304]
	TIME [epoch: 5.85 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04189103029552899		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.04189103029552899 | validation: 0.15086088700441946]
	TIME [epoch: 5.85 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02800533741931072		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.02800533741931072 | validation: 0.14042155845967338]
	TIME [epoch: 5.85 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034461390175807835		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.034461390175807835 | validation: 0.1589600593086688]
	TIME [epoch: 5.85 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03420227439479017		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.03420227439479017 | validation: 0.14430169701337434]
	TIME [epoch: 5.86 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026741093561220754		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.026741093561220754 | validation: 0.1597999759218615]
	TIME [epoch: 5.85 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02500731499430475		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.02500731499430475 | validation: 0.1346371861981182]
	TIME [epoch: 5.87 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025024840916314923		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.025024840916314923 | validation: 0.13304364139621236]
	TIME [epoch: 5.86 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415625878949802		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.02415625878949802 | validation: 0.13353117484712312]
	TIME [epoch: 5.86 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027411842729256765		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.027411842729256765 | validation: 0.13760614618681427]
	TIME [epoch: 5.86 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024242417046448344		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.024242417046448344 | validation: 0.1348908874574066]
	TIME [epoch: 5.87 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025430651178702815		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.025430651178702815 | validation: 0.1392286623451561]
	TIME [epoch: 5.86 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026044181845548228		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.026044181845548228 | validation: 0.19286031879731788]
	TIME [epoch: 5.87 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0616015018735991		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.0616015018735991 | validation: 0.186268907259045]
	TIME [epoch: 5.87 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0298885534844492		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.0298885534844492 | validation: 0.15155078629169993]
	TIME [epoch: 5.87 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027826532221519016		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.027826532221519016 | validation: 0.1672950231316595]
	TIME [epoch: 5.86 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02847007158502815		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.02847007158502815 | validation: 0.1454023796512944]
	TIME [epoch: 5.86 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028295634596882478		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.028295634596882478 | validation: 0.15366850794918638]
	TIME [epoch: 5.86 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023474705124560936		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.023474705124560936 | validation: 0.24189632245606224]
	TIME [epoch: 5.86 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655426741098629		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06655426741098629 | validation: 0.1822836880999691]
	TIME [epoch: 5.86 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0412393799473719		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.0412393799473719 | validation: 0.18197392488439545]
	TIME [epoch: 5.87 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053442667983813995		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.053442667983813995 | validation: 0.1686049189010418]
	TIME [epoch: 5.86 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916011006753387		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.03916011006753387 | validation: 0.14695344808072314]
	TIME [epoch: 5.86 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02312410076798147		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.02312410076798147 | validation: 0.1598164359392404]
	TIME [epoch: 5.86 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028341380287401092		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.028341380287401092 | validation: 0.1347773710836551]
	TIME [epoch: 5.86 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027161765737649057		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.027161765737649057 | validation: 0.13624741183584022]
	TIME [epoch: 5.86 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02183332137762904		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.02183332137762904 | validation: 0.1410030304289565]
	TIME [epoch: 5.86 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023993088020560726		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.023993088020560726 | validation: 0.12781480124013786]
	TIME [epoch: 5.86 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020309131474893823		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.020309131474893823 | validation: 0.12781383899492757]
	TIME [epoch: 5.86 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0237583261910258		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.0237583261910258 | validation: 0.1345956757976424]
	TIME [epoch: 5.86 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022820921940679847		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.022820921940679847 | validation: 0.12932179932060503]
	TIME [epoch: 5.85 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02462553354015931		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.02462553354015931 | validation: 0.24231339001109387]
	TIME [epoch: 5.86 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03978452108963389		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.03978452108963389 | validation: 0.15736081885061703]
	TIME [epoch: 5.87 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027880236114507594		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.027880236114507594 | validation: 0.13590979380431706]
	TIME [epoch: 5.86 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027489568454650302		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.027489568454650302 | validation: 0.12479774244034217]
	TIME [epoch: 5.87 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030366319370239696		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.030366319370239696 | validation: 0.15681970586271704]
	TIME [epoch: 5.87 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030429710978313873		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.030429710978313873 | validation: 0.15250212049130318]
	TIME [epoch: 5.87 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540219245462073		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.02540219245462073 | validation: 0.13156565074267618]
	TIME [epoch: 5.88 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03250925628240308		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.03250925628240308 | validation: 0.14434505633036157]
	TIME [epoch: 5.86 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02728364490283882		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.02728364490283882 | validation: 0.12989101571100142]
	TIME [epoch: 5.88 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024219749412181645		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.024219749412181645 | validation: 0.12479314453371189]
	TIME [epoch: 5.87 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02365529815830624		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.02365529815830624 | validation: 0.28770128123725086]
	TIME [epoch: 5.87 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04217159747151887		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04217159747151887 | validation: 0.1990643821621089]
	TIME [epoch: 5.87 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03628950811977771		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.03628950811977771 | validation: 0.13095678575833544]
	TIME [epoch: 5.86 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025309229146292286		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.025309229146292286 | validation: 0.12739885512953628]
	TIME [epoch: 5.86 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030004857323898012		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.030004857323898012 | validation: 0.13919163805661153]
	TIME [epoch: 5.86 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03069197154123314		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.03069197154123314 | validation: 0.1606434578062472]
	TIME [epoch: 5.86 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029265246976497686		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.029265246976497686 | validation: 0.1339007670146489]
	TIME [epoch: 5.86 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02684632430978668		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.02684632430978668 | validation: 0.12974673116264854]
	TIME [epoch: 5.85 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02139060779706119		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.02139060779706119 | validation: 0.12550712836446362]
	TIME [epoch: 5.86 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02207597112903004		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.02207597112903004 | validation: 0.12609338012341859]
	TIME [epoch: 5.87 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024064481010270113		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.024064481010270113 | validation: 0.12498231941620969]
	TIME [epoch: 5.86 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02314708542874242		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.02314708542874242 | validation: 0.12366652027848593]
	TIME [epoch: 5.86 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748320790455972		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.03748320790455972 | validation: 0.15276546027072122]
	TIME [epoch: 5.85 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027260512736321758		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.027260512736321758 | validation: 0.1299595761402065]
	TIME [epoch: 5.81 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648154439556379		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.02648154439556379 | validation: 0.13250782264862054]
	TIME [epoch: 5.85 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026432497169560587		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.026432497169560587 | validation: 0.13790482333162166]
	TIME [epoch: 5.86 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026297701866151765		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.026297701866151765 | validation: 0.1453561127443875]
	TIME [epoch: 5.85 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0266680907941044		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0266680907941044 | validation: 0.11661437585644761]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023879415179292166		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.023879415179292166 | validation: 0.12134623735157751]
	TIME [epoch: 5.84 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024506691131840546		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.024506691131840546 | validation: 0.13706640056990713]
	TIME [epoch: 5.85 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02583309510972499		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.02583309510972499 | validation: 0.1333208040629963]
	TIME [epoch: 5.85 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027182710594522153		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.027182710594522153 | validation: 0.13014868574231528]
	TIME [epoch: 5.84 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02493527071195383		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.02493527071195383 | validation: 0.1450072518661758]
	TIME [epoch: 5.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02518339077501473		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.02518339077501473 | validation: 0.14756012421831596]
	TIME [epoch: 5.84 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023517804915457692		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.023517804915457692 | validation: 0.13206731340041675]
	TIME [epoch: 5.84 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02280221328636747		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.02280221328636747 | validation: 0.14118491121023302]
	TIME [epoch: 5.84 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02208016337622476		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.02208016337622476 | validation: 0.13675082036784186]
	TIME [epoch: 5.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927276327061342		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.03927276327061342 | validation: 0.15411643438346512]
	TIME [epoch: 5.83 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03941999118849629		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.03941999118849629 | validation: 0.14492405330533095]
	TIME [epoch: 5.84 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02997721083497229		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.02997721083497229 | validation: 0.13214970289345881]
	TIME [epoch: 5.85 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023464961330142503		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.023464961330142503 | validation: 0.12426138352329714]
	TIME [epoch: 5.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178486375652499		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.03178486375652499 | validation: 0.13558787604076017]
	TIME [epoch: 5.84 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03170565500472856		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.03170565500472856 | validation: 0.15776130690352552]
	TIME [epoch: 5.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02664316006457687		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.02664316006457687 | validation: 0.16440635540706935]
	TIME [epoch: 5.84 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03029167496761238		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.03029167496761238 | validation: 0.13678344097894568]
	TIME [epoch: 5.84 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021905374551387073		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.021905374551387073 | validation: 0.12753171557834425]
	TIME [epoch: 5.85 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023474235350179617		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.023474235350179617 | validation: 0.14074804396923798]
	TIME [epoch: 5.85 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028913401543375867		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.028913401543375867 | validation: 0.15562203236638772]
	TIME [epoch: 5.84 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022456985518190097		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.022456985518190097 | validation: 0.11916815264381345]
	TIME [epoch: 5.84 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03262976113491255		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.03262976113491255 | validation: 0.13347986262567726]
	TIME [epoch: 5.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030246119290343796		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.030246119290343796 | validation: 0.14180874561147402]
	TIME [epoch: 5.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02187121056079696		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.02187121056079696 | validation: 0.1419260144252355]
	TIME [epoch: 5.84 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02343666569772009		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.02343666569772009 | validation: 0.12440211693493566]
	TIME [epoch: 5.84 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021241357884550602		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.021241357884550602 | validation: 0.12435582831317793]
	TIME [epoch: 5.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02257847177156159		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.02257847177156159 | validation: 0.12527896687577025]
	TIME [epoch: 5.84 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024367015854810256		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.024367015854810256 | validation: 0.1267273287189894]
	TIME [epoch: 5.84 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02111497114112364		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.02111497114112364 | validation: 0.11595217910563156]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023295053037675127		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.023295053037675127 | validation: 0.12205928360174645]
	TIME [epoch: 5.85 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027952766103849964		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.027952766103849964 | validation: 0.12922097489304646]
	TIME [epoch: 5.87 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025626822516187562		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.025626822516187562 | validation: 0.14667410752980942]
	TIME [epoch: 5.86 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023095971405071154		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.023095971405071154 | validation: 0.11698111126493616]
	TIME [epoch: 5.88 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02383507539294283		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.02383507539294283 | validation: 0.14240019209419716]
	TIME [epoch: 5.87 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02169721512617658		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.02169721512617658 | validation: 0.12142153504039506]
	TIME [epoch: 5.87 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0214061882066074		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.0214061882066074 | validation: 0.12413217646793717]
	TIME [epoch: 5.86 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024716248485719353		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.024716248485719353 | validation: 0.14031307988822395]
	TIME [epoch: 5.87 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929280524922409		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.03929280524922409 | validation: 0.14882820065975408]
	TIME [epoch: 5.88 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02574300116485327		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.02574300116485327 | validation: 0.13977171097861904]
	TIME [epoch: 5.88 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02142057677670935		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.02142057677670935 | validation: 0.11746066225209482]
	TIME [epoch: 5.86 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024191473330093388		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.024191473330093388 | validation: 0.11654378431443818]
	TIME [epoch: 5.86 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022315775192893322		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.022315775192893322 | validation: 0.1207765930892329]
	TIME [epoch: 5.86 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022985682964783677		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.022985682964783677 | validation: 0.11334087952672177]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_803.pth
	Model improved!!!
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020531507841166644		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.020531507841166644 | validation: 0.16212000372978583]
	TIME [epoch: 5.85 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0237757786435321		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.0237757786435321 | validation: 0.11544231606285651]
	TIME [epoch: 5.86 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026319055134917405		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.026319055134917405 | validation: 0.11696736220463554]
	TIME [epoch: 5.85 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02165059831082432		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.02165059831082432 | validation: 0.13650514922107965]
	TIME [epoch: 5.87 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020315148454196366		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.020315148454196366 | validation: 0.11309211624701843]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243466106692271		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.02243466106692271 | validation: 0.13134283968518018]
	TIME [epoch: 5.86 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02122151492354383		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.02122151492354383 | validation: 0.11464453008727282]
	TIME [epoch: 5.86 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021582540562412093		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.021582540562412093 | validation: 0.14734957182413946]
	TIME [epoch: 5.85 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03767193373732399		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.03767193373732399 | validation: 0.17432083820423097]
	TIME [epoch: 5.85 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712484038210209		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.03712484038210209 | validation: 0.15667720644809255]
	TIME [epoch: 5.87 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04135407184138717		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.04135407184138717 | validation: 0.1484871920086826]
	TIME [epoch: 5.86 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027119189534542815		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.027119189534542815 | validation: 0.1510304411575254]
	TIME [epoch: 5.86 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026309221792457076		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.026309221792457076 | validation: 0.3834027946141412]
	TIME [epoch: 5.86 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881251285312368		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05881251285312368 | validation: 0.39767025926403904]
	TIME [epoch: 5.86 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707788698457769		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.05707788698457769 | validation: 0.27670244780037984]
	TIME [epoch: 5.86 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04388370255889203		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.04388370255889203 | validation: 0.17287671246721298]
	TIME [epoch: 5.87 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02467001874417225		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.02467001874417225 | validation: 0.13174201886239412]
	TIME [epoch: 5.86 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021197904766858375		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.021197904766858375 | validation: 0.12411581949259705]
	TIME [epoch: 5.86 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02184248976689026		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.02184248976689026 | validation: 0.13092288262404872]
	TIME [epoch: 5.86 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02479931940366764		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.02479931940366764 | validation: 0.12650207045738152]
	TIME [epoch: 5.86 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02105992605041096		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.02105992605041096 | validation: 0.12209585382527931]
	TIME [epoch: 5.85 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01762668994538416		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.01762668994538416 | validation: 0.12039144654314143]
	TIME [epoch: 5.86 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017834576511076106		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.017834576511076106 | validation: 0.1301925694972816]
	TIME [epoch: 5.86 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019787586860563776		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.019787586860563776 | validation: 0.1342476391220117]
	TIME [epoch: 5.86 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018121752713805755		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.018121752713805755 | validation: 0.12356169781912381]
	TIME [epoch: 5.86 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02077822643590294		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.02077822643590294 | validation: 0.13064013041726646]
	TIME [epoch: 5.86 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002468528726838		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.02002468528726838 | validation: 0.11595003856641789]
	TIME [epoch: 5.87 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02054827299796747		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.02054827299796747 | validation: 0.11988543001614921]
	TIME [epoch: 5.87 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020905568709940616		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.020905568709940616 | validation: 0.11878941583458479]
	TIME [epoch: 5.87 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019468117361931065		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.019468117361931065 | validation: 0.11771430555769738]
	TIME [epoch: 5.86 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020435941888836306		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.020435941888836306 | validation: 0.11897426152652271]
	TIME [epoch: 5.86 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021425114828404045		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.021425114828404045 | validation: 0.10386540937349734]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020205883814879203		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.020205883814879203 | validation: 0.09955888975357777]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01947994963015952		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.01947994963015952 | validation: 0.10983910392720427]
	TIME [epoch: 5.84 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019867615300926716		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.019867615300926716 | validation: 0.11668668198630094]
	TIME [epoch: 5.85 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020874131295461656		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.020874131295461656 | validation: 0.11619484937876497]
	TIME [epoch: 5.86 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223049742579187		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.0223049742579187 | validation: 0.1071468513757611]
	TIME [epoch: 5.86 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02705957106830204		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.02705957106830204 | validation: 0.16586312427209146]
	TIME [epoch: 5.86 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03411441704121548		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.03411441704121548 | validation: 0.1212638888920862]
	TIME [epoch: 5.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028302602799856923		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.028302602799856923 | validation: 0.12650924367275437]
	TIME [epoch: 5.85 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020314630282729534		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.020314630282729534 | validation: 0.12152800774007085]
	TIME [epoch: 5.85 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028911708932923403		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.028911708932923403 | validation: 0.11812238594200464]
	TIME [epoch: 5.86 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024651666244097856		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.024651666244097856 | validation: 0.12127385448304262]
	TIME [epoch: 5.85 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0188322109601072		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.0188322109601072 | validation: 0.12121192797058633]
	TIME [epoch: 5.85 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206604495819907		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.0206604495819907 | validation: 0.11852061546587467]
	TIME [epoch: 5.86 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019518715161604305		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.019518715161604305 | validation: 0.10952940073195042]
	TIME [epoch: 5.86 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021169045756642087		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.021169045756642087 | validation: 0.10549820751616795]
	TIME [epoch: 5.86 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02228371632102325		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.02228371632102325 | validation: 0.11227675578795987]
	TIME [epoch: 5.87 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01788243846628044		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.01788243846628044 | validation: 0.10347271716912448]
	TIME [epoch: 5.87 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018198837037099114		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.018198837037099114 | validation: 0.11149340060385154]
	TIME [epoch: 5.87 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01977566404094266		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.01977566404094266 | validation: 0.12133749933113794]
	TIME [epoch: 5.87 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021375351803392596		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.021375351803392596 | validation: 0.09779006432380039]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019771949264390517		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.019771949264390517 | validation: 0.1331188184774144]
	TIME [epoch: 5.85 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02244781284801399		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.02244781284801399 | validation: 0.11681651597183565]
	TIME [epoch: 5.86 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027405666573040853		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.027405666573040853 | validation: 0.14101875412924075]
	TIME [epoch: 5.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023034865744625554		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.023034865744625554 | validation: 0.11060582139034009]
	TIME [epoch: 5.87 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019445905096580177		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.019445905096580177 | validation: 0.11656139512962693]
	TIME [epoch: 5.86 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022989742389200235		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.022989742389200235 | validation: 0.1281353721171355]
	TIME [epoch: 5.87 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029902059973833026		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.029902059973833026 | validation: 0.11752480091326674]
	TIME [epoch: 5.85 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020957075224140358		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.020957075224140358 | validation: 0.10818795324761671]
	TIME [epoch: 5.86 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021251553782782377		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.021251553782782377 | validation: 0.12124628401720755]
	TIME [epoch: 5.85 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023434389115643956		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.023434389115643956 | validation: 0.12332892284836457]
	TIME [epoch: 5.85 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022004658211191998		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.022004658211191998 | validation: 0.13407836923439456]
	TIME [epoch: 5.85 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020737094324869122		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.020737094324869122 | validation: 0.13226771635742784]
	TIME [epoch: 5.85 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018912233495247274		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.018912233495247274 | validation: 0.11646138794440733]
	TIME [epoch: 5.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017685252840820107		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.017685252840820107 | validation: 0.12020913972270429]
	TIME [epoch: 5.86 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020806972514131147		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.020806972514131147 | validation: 0.1335353268994286]
	TIME [epoch: 5.86 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03173099672387714		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.03173099672387714 | validation: 0.13994245921762624]
	TIME [epoch: 5.85 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551432978273706		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03551432978273706 | validation: 0.13174747266689862]
	TIME [epoch: 5.86 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020619905957293263		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.020619905957293263 | validation: 0.12308320711343275]
	TIME [epoch: 5.85 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022568478447957584		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.022568478447957584 | validation: 0.13891946770768954]
	TIME [epoch: 5.86 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02126376857791441		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.02126376857791441 | validation: 0.10404968580345444]
	TIME [epoch: 5.85 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024089879668945883		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.024089879668945883 | validation: 0.1209266481177776]
	TIME [epoch: 5.85 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019790325655739512		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.019790325655739512 | validation: 0.2893710232038341]
	TIME [epoch: 5.85 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676752411572935		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.04676752411572935 | validation: 0.3346048506486536]
	TIME [epoch: 5.85 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142132670103037		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04142132670103037 | validation: 0.2510481946284672]
	TIME [epoch: 5.85 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03174282782082669		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03174282782082669 | validation: 0.17139990235759892]
	TIME [epoch: 5.85 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02230699961740906		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.02230699961740906 | validation: 0.12209456115680424]
	TIME [epoch: 5.85 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023247156935754815		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.023247156935754815 | validation: 0.11577908604031634]
	TIME [epoch: 5.84 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01858568611480078		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.01858568611480078 | validation: 0.10472250553462553]
	TIME [epoch: 5.85 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023714339702400788		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.023714339702400788 | validation: 0.10940780855116597]
	TIME [epoch: 5.86 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019347535748244337		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.019347535748244337 | validation: 0.11872461723227223]
	TIME [epoch: 5.86 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017992030950878785		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.017992030950878785 | validation: 0.12292512780070242]
	TIME [epoch: 5.86 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019450486407859784		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.019450486407859784 | validation: 0.11440881200958274]
	TIME [epoch: 5.86 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0194082386632967		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.0194082386632967 | validation: 0.1226314531696736]
	TIME [epoch: 5.86 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018496768269681472		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.018496768269681472 | validation: 0.105676052585997]
	TIME [epoch: 5.86 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018148708222356556		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.018148708222356556 | validation: 0.11047595685778036]
	TIME [epoch: 5.86 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01832109897907298		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.01832109897907298 | validation: 0.14664799883289711]
	TIME [epoch: 5.84 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03923657566727941		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03923657566727941 | validation: 0.12120708503081115]
	TIME [epoch: 5.85 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023105941671496658		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.023105941671496658 | validation: 0.1340213677213686]
	TIME [epoch: 5.85 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01840254494326093		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.01840254494326093 | validation: 0.12238752673717573]
	TIME [epoch: 5.85 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020074973623605493		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.020074973623605493 | validation: 0.1318137417724469]
	TIME [epoch: 5.85 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961154443363558		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.01961154443363558 | validation: 0.12566743123172694]
	TIME [epoch: 5.86 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019490821744737832		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.019490821744737832 | validation: 0.11623118092647614]
	TIME [epoch: 5.85 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02050961025847329		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.02050961025847329 | validation: 0.11756832132867863]
	TIME [epoch: 5.85 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017660319017131033		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.017660319017131033 | validation: 0.15376352783432595]
	TIME [epoch: 5.85 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943791973994377		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03943791973994377 | validation: 0.1371664432116495]
	TIME [epoch: 5.86 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021157011457212015		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.021157011457212015 | validation: 0.13702582221844947]
	TIME [epoch: 5.82 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02691151738404125		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.02691151738404125 | validation: 0.11948450235829466]
	TIME [epoch: 5.82 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017193231067207798		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.017193231067207798 | validation: 0.114949798737308]
	TIME [epoch: 5.82 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018880739263996187		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.018880739263996187 | validation: 0.11498639902296687]
	TIME [epoch: 5.82 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018493613384855244		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.018493613384855244 | validation: 0.12139745645787017]
	TIME [epoch: 5.82 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018297630769495678		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.018297630769495678 | validation: 0.10606710473512276]
	TIME [epoch: 5.82 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018523520037731007		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.018523520037731007 | validation: 0.11665447217592076]
	TIME [epoch: 5.82 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017022411416677957		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.017022411416677957 | validation: 0.10384528422675698]
	TIME [epoch: 5.82 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0187838311383443		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.0187838311383443 | validation: 0.11529209574904238]
	TIME [epoch: 5.82 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020626212802297535		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.020626212802297535 | validation: 0.12487661180759631]
	TIME [epoch: 5.82 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03098103444720078		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.03098103444720078 | validation: 0.1399349125861574]
	TIME [epoch: 5.88 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02221492357929944		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.02221492357929944 | validation: 0.12154286868780906]
	TIME [epoch: 5.81 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021366202435613272		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.021366202435613272 | validation: 0.11414747904516714]
	TIME [epoch: 5.81 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024174619776984273		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.024174619776984273 | validation: 0.11302504599381816]
	TIME [epoch: 5.81 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02391656376308662		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.02391656376308662 | validation: 0.11094159242606763]
	TIME [epoch: 5.81 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019238796536879477		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.019238796536879477 | validation: 0.11435264949582212]
	TIME [epoch: 5.81 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017365671845153453		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.017365671845153453 | validation: 0.12395696144710477]
	TIME [epoch: 5.81 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019129567556800328		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.019129567556800328 | validation: 0.11791088278710071]
	TIME [epoch: 5.81 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018331817776479353		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.018331817776479353 | validation: 0.11077333008303195]
	TIME [epoch: 5.81 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01689791163857248		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.01689791163857248 | validation: 0.1464332539040154]
	TIME [epoch: 5.81 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02198661746043933		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.02198661746043933 | validation: 0.11628300246581227]
	TIME [epoch: 5.81 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020756735880297025		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.020756735880297025 | validation: 0.1466812576312133]
	TIME [epoch: 5.81 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02991370542084907		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.02991370542084907 | validation: 0.15190017442457698]
	TIME [epoch: 5.82 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02237327931414961		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.02237327931414961 | validation: 0.13500196218153682]
	TIME [epoch: 5.81 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02192321254897319		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.02192321254897319 | validation: 0.11924425945263001]
	TIME [epoch: 5.82 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018535107894000317		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.018535107894000317 | validation: 0.12054850242195084]
	TIME [epoch: 5.81 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02079115193236442		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.02079115193236442 | validation: 0.10596629648076035]
	TIME [epoch: 5.81 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01582597512835855		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.01582597512835855 | validation: 0.1059584000178524]
	TIME [epoch: 5.81 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021443460095792916		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.021443460095792916 | validation: 0.11486132449851721]
	TIME [epoch: 5.81 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01752304975170927		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.01752304975170927 | validation: 0.10882252417722751]
	TIME [epoch: 5.81 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018597183661196812		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.018597183661196812 | validation: 0.10139247316708605]
	TIME [epoch: 5.81 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019306136167965712		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.019306136167965712 | validation: 0.1175702422324898]
	TIME [epoch: 5.81 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019086327696810564		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.019086327696810564 | validation: 0.11624244655623883]
	TIME [epoch: 5.81 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019064743273963247		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.019064743273963247 | validation: 0.11317013025506727]
	TIME [epoch: 5.81 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019509902602576602		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.019509902602576602 | validation: 0.10115499094835945]
	TIME [epoch: 5.82 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0197488102514602		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0197488102514602 | validation: 0.10720712591712417]
	TIME [epoch: 5.81 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018100408099057917		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.018100408099057917 | validation: 0.11096897001762161]
	TIME [epoch: 5.81 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019072291265202206		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.019072291265202206 | validation: 0.12536049677060088]
	TIME [epoch: 5.82 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019248974899106572		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.019248974899106572 | validation: 0.11917458908061401]
	TIME [epoch: 5.81 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01853696087614319		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.01853696087614319 | validation: 0.10917746763727071]
	TIME [epoch: 5.81 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018592292357816648		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.018592292357816648 | validation: 0.113287786510412]
	TIME [epoch: 5.81 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018110903587954898		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.018110903587954898 | validation: 0.10423524259311732]
	TIME [epoch: 5.81 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01830755595732864		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.01830755595732864 | validation: 0.29565558032285333]
	TIME [epoch: 5.81 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04114096356874663		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.04114096356874663 | validation: 0.27698648581381774]
	TIME [epoch: 5.82 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033408839011233725		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.033408839011233725 | validation: 0.1799951565996815]
	TIME [epoch: 5.82 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025091151072798884		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.025091151072798884 | validation: 0.11143780837978833]
	TIME [epoch: 5.81 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019880271532559928		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.019880271532559928 | validation: 0.11103605299408033]
	TIME [epoch: 5.81 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021588266802828472		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.021588266802828472 | validation: 0.1067958603104436]
	TIME [epoch: 5.82 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02103766321830605		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.02103766321830605 | validation: 0.11134539838465607]
	TIME [epoch: 5.82 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020428567354949842		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.020428567354949842 | validation: 0.11183002517800732]
	TIME [epoch: 5.83 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01850415306986925		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.01850415306986925 | validation: 0.10075576184627072]
	TIME [epoch: 5.82 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0182446669408186		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0182446669408186 | validation: 0.11005104447577058]
	TIME [epoch: 5.82 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017575269369515322		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.017575269369515322 | validation: 0.11672746406562556]
	TIME [epoch: 5.81 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01716357120662928		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.01716357120662928 | validation: 0.10381907173425316]
	TIME [epoch: 5.85 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01579694517671129		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.01579694517671129 | validation: 0.09711198142578867]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01667319613336763		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.01667319613336763 | validation: 0.13304633177900885]
	TIME [epoch: 5.84 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026805016476446904		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.026805016476446904 | validation: 0.11189976544926875]
	TIME [epoch: 5.85 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016849587306536885		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.016849587306536885 | validation: 0.11801526462924256]
	TIME [epoch: 5.83 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019850638378668		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.019850638378668 | validation: 0.11131963138241663]
	TIME [epoch: 5.84 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017037267691234363		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.017037267691234363 | validation: 0.11636390079380532]
	TIME [epoch: 5.82 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016811081220755048		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.016811081220755048 | validation: 0.13453352769678345]
	TIME [epoch: 5.85 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01776345770886791		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.01776345770886791 | validation: 0.11333240747727058]
	TIME [epoch: 5.85 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016565440670273095		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.016565440670273095 | validation: 0.11136316095214221]
	TIME [epoch: 5.85 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01573229814072579		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.01573229814072579 | validation: 0.10644034271450585]
	TIME [epoch: 5.85 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01671991894182429		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.01671991894182429 | validation: 0.1008073828702266]
	TIME [epoch: 5.85 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017764106079648064		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.017764106079648064 | validation: 0.11185045210518987]
	TIME [epoch: 5.85 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967121060893418		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.01967121060893418 | validation: 0.11170348143737863]
	TIME [epoch: 5.86 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01673212541518562		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.01673212541518562 | validation: 0.10468475429325533]
	TIME [epoch: 5.85 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017211649069947356		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.017211649069947356 | validation: 0.10370446256640382]
	TIME [epoch: 5.85 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016994989536025344		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.016994989536025344 | validation: 0.09322430839564326]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017577044303741546		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.017577044303741546 | validation: 0.10974883832577811]
	TIME [epoch: 5.83 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01712737808008311		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.01712737808008311 | validation: 0.1008498362012754]
	TIME [epoch: 5.84 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01717898357893918		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.01717898357893918 | validation: 0.10075091280084823]
	TIME [epoch: 5.85 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015350805710288878		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.015350805710288878 | validation: 0.11298261070252019]
	TIME [epoch: 5.85 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017648078809356392		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.017648078809356392 | validation: 0.09891864531397197]
	TIME [epoch: 5.85 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01839821788087844		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.01839821788087844 | validation: 0.11261841378987025]
	TIME [epoch: 5.85 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01881164125878976		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.01881164125878976 | validation: 0.10001971300361741]
	TIME [epoch: 5.84 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021380134778226614		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.021380134778226614 | validation: 0.12308877061393882]
	TIME [epoch: 5.84 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016735829056869152		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.016735829056869152 | validation: 0.10475966120902057]
	TIME [epoch: 5.84 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020236749475140096		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.020236749475140096 | validation: 0.10165924286573645]
	TIME [epoch: 5.84 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02055103225820381		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.02055103225820381 | validation: 0.11059338169060651]
	TIME [epoch: 5.84 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023555608254114093		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.023555608254114093 | validation: 0.11764556359277717]
	TIME [epoch: 5.84 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520615863017139		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.02520615863017139 | validation: 0.12852569854049292]
	TIME [epoch: 5.84 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020548870342447394		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.020548870342447394 | validation: 0.1233540827192619]
	TIME [epoch: 5.83 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01633285966922222		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.01633285966922222 | validation: 0.11750473028866562]
	TIME [epoch: 5.84 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019477933484041605		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.019477933484041605 | validation: 0.10823599034880056]
	TIME [epoch: 5.84 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018867061575980098		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.018867061575980098 | validation: 0.11123715562755532]
	TIME [epoch: 5.84 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01738562364483992		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.01738562364483992 | validation: 0.19336267180281502]
	TIME [epoch: 5.84 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07473647082340198		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.07473647082340198 | validation: 0.2151064634882235]
	TIME [epoch: 5.85 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928987173668732		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.07928987173668732 | validation: 0.13100973574007568]
	TIME [epoch: 5.85 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021835797570981478		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.021835797570981478 | validation: 0.12104284114667037]
	TIME [epoch: 5.84 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023638158197719274		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.023638158197719274 | validation: 0.12341254251023259]
	TIME [epoch: 5.85 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03408768460812372		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.03408768460812372 | validation: 0.1338267550107681]
	TIME [epoch: 5.85 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396300230532285		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.03396300230532285 | validation: 0.11964869501659443]
	TIME [epoch: 5.86 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02014366725864747		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.02014366725864747 | validation: 0.12446255111548098]
	TIME [epoch: 5.85 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01812905180843942		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.01812905180843942 | validation: 0.1166977276411718]
	TIME [epoch: 5.86 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017918289940905736		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.017918289940905736 | validation: 0.12220784562649095]
	TIME [epoch: 5.83 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016677424440692982		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.016677424440692982 | validation: 0.10874419794066376]
	TIME [epoch: 5.85 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017293682720957684		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.017293682720957684 | validation: 0.12222038749121376]
	TIME [epoch: 5.84 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0169835174711665		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0169835174711665 | validation: 0.12861641625192125]
	TIME [epoch: 5.86 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01630119886135935		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.01630119886135935 | validation: 0.1266293760267547]
	TIME [epoch: 202 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01831292050332243		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.01831292050332243 | validation: 0.10877457935300523]
	TIME [epoch: 12.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01747219711690444		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.01747219711690444 | validation: 0.11520042875573018]
	TIME [epoch: 12.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017085657420927044		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.017085657420927044 | validation: 0.11754432665422057]
	TIME [epoch: 12.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01715066933217326		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.01715066933217326 | validation: 0.12069576188697245]
	TIME [epoch: 12.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015158927563763835		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.015158927563763835 | validation: 0.10982369812643458]
	TIME [epoch: 12.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016320844183393902		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.016320844183393902 | validation: 0.10916892947157666]
	TIME [epoch: 12.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015694603702228756		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.015694603702228756 | validation: 0.11226906307998188]
	TIME [epoch: 12.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016238734240670075		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.016238734240670075 | validation: 0.11544744707172785]
	TIME [epoch: 12.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015249057992333122		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.015249057992333122 | validation: 0.11199256506890452]
	TIME [epoch: 12.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018149827010296354		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.018149827010296354 | validation: 0.101768391493943]
	TIME [epoch: 12.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834388580175232		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.01834388580175232 | validation: 0.11552975148882681]
	TIME [epoch: 12.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018108290969121622		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.018108290969121622 | validation: 0.11374679416241262]
	TIME [epoch: 12.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018646641814281325		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.018646641814281325 | validation: 0.10606460948203007]
	TIME [epoch: 12.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016868477969936384		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.016868477969936384 | validation: 0.12926765664968826]
	TIME [epoch: 12.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017070030544865368		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.017070030544865368 | validation: 0.11899525561847879]
	TIME [epoch: 12.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017658550842152634		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.017658550842152634 | validation: 0.10758616452700953]
	TIME [epoch: 12.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017761536617486508		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.017761536617486508 | validation: 0.11183135755723948]
	TIME [epoch: 12.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016886363669622362		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.016886363669622362 | validation: 0.11780149191530281]
	TIME [epoch: 12.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017658136837378772		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.017658136837378772 | validation: 0.10945860053491292]
	TIME [epoch: 12.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017617478776354		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.017617478776354 | validation: 0.10815721812896198]
	TIME [epoch: 12.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01773722071985462		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.01773722071985462 | validation: 0.10974514685627056]
	TIME [epoch: 12.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01824365197335978		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.01824365197335978 | validation: 0.10938855766371627]
	TIME [epoch: 12.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015959691477348232		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.015959691477348232 | validation: 0.12036292201939935]
	TIME [epoch: 12.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015270372452318355		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.015270372452318355 | validation: 0.10577782381539001]
	TIME [epoch: 12.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01610892001338196		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.01610892001338196 | validation: 0.10352103122919401]
	TIME [epoch: 12.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01588211079313594		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.01588211079313594 | validation: 0.10832742029394625]
	TIME [epoch: 12.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015208821682263242		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.015208821682263242 | validation: 0.10037691158144187]
	TIME [epoch: 12.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01630228832893658		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.01630228832893658 | validation: 0.11120553805295064]
	TIME [epoch: 12.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015797268589464272		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.015797268589464272 | validation: 0.10321025724595945]
	TIME [epoch: 12.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01646704760916969		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.01646704760916969 | validation: 0.09427489175222774]
	TIME [epoch: 12.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016626959645894102		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.016626959645894102 | validation: 0.09915146102006589]
	TIME [epoch: 12.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017409215123946366		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.017409215123946366 | validation: 0.11030211922010286]
	TIME [epoch: 12.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017151322826070608		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.017151322826070608 | validation: 0.10551201946907468]
	TIME [epoch: 12.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01791796012274016		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.01791796012274016 | validation: 0.12127086186324552]
	TIME [epoch: 12.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018583803139137937		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.018583803139137937 | validation: 0.10409681699168934]
	TIME [epoch: 12.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017273820595220254		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.017273820595220254 | validation: 0.10465053217900269]
	TIME [epoch: 12.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017299421351120482		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.017299421351120482 | validation: 0.10939516322734366]
	TIME [epoch: 12.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01686484640103143		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.01686484640103143 | validation: 0.11595854995694017]
	TIME [epoch: 12.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014468593050375575		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.014468593050375575 | validation: 0.10777625302587411]
	TIME [epoch: 12.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01677577988816788		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.01677577988816788 | validation: 0.10389568985623446]
	TIME [epoch: 12.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01781861802259652		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.01781861802259652 | validation: 0.11464988488899107]
	TIME [epoch: 12.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0178240000911674		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.0178240000911674 | validation: 0.10204256991826247]
	TIME [epoch: 12.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018911626488771222		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.018911626488771222 | validation: 0.1036879412723558]
	TIME [epoch: 12.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016701343330738885		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.016701343330738885 | validation: 0.11548032762029502]
	TIME [epoch: 12.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01641212551984013		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.01641212551984013 | validation: 0.11695083886504638]
	TIME [epoch: 12.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017792490800860605		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.017792490800860605 | validation: 0.10909256986654917]
	TIME [epoch: 12.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017338835583847207		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.017338835583847207 | validation: 0.10501075092012353]
	TIME [epoch: 12.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01626980035801834		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.01626980035801834 | validation: 0.0948571446315768]
	TIME [epoch: 12.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017510222427041696		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.017510222427041696 | validation: 0.09853498389956983]
	TIME [epoch: 12.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01631255898613536		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.01631255898613536 | validation: 0.10589610749173342]
	TIME [epoch: 12.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01750186119974023		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.01750186119974023 | validation: 0.09718937662234894]
	TIME [epoch: 12.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016263695818409262		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.016263695818409262 | validation: 0.1121439413217408]
	TIME [epoch: 12.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019804234480847532		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.019804234480847532 | validation: 0.10279708403528627]
	TIME [epoch: 12.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01615293128052449		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.01615293128052449 | validation: 0.12081972620746284]
	TIME [epoch: 12.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018594270182040115		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.018594270182040115 | validation: 0.11638195391454298]
	TIME [epoch: 12.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016731615831793605		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.016731615831793605 | validation: 0.11361039419489778]
	TIME [epoch: 12.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015456679310893959		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.015456679310893959 | validation: 0.10865736578040615]
	TIME [epoch: 12.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016450945967397726		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.016450945967397726 | validation: 0.11156486309271729]
	TIME [epoch: 12.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01678207161244808		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.01678207161244808 | validation: 0.10792572750566892]
	TIME [epoch: 12.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017464928567577937		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.017464928567577937 | validation: 0.09666783191061351]
	TIME [epoch: 12.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016423195340248307		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.016423195340248307 | validation: 0.11351482909422508]
	TIME [epoch: 12.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017154853491211017		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.017154853491211017 | validation: 0.10294835348097925]
	TIME [epoch: 12.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01639383058846624		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.01639383058846624 | validation: 0.1033566867954203]
	TIME [epoch: 12.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01816571522841766		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.01816571522841766 | validation: 0.10804183856447128]
	TIME [epoch: 12.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016194543352374823		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.016194543352374823 | validation: 0.09454413386587225]
	TIME [epoch: 12.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01532092975286503		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.01532092975286503 | validation: 0.10916092202749557]
	TIME [epoch: 12.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015397978635151566		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.015397978635151566 | validation: 0.10720362601385065]
	TIME [epoch: 12.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01766671180432079		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.01766671180432079 | validation: 0.09847366619618088]
	TIME [epoch: 12.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018473903153041593		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.018473903153041593 | validation: 0.11924828261968555]
	TIME [epoch: 12.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017791603875213537		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.017791603875213537 | validation: 0.09529287269287157]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241125_132639/states/model_phi1_4b_v_mmd1_1071.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5699.330 seconds.
