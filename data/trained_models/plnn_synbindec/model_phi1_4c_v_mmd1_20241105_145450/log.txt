Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1749961598

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.307030248687004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.307030248687004 | validation: 5.572110390057203]
	TIME [epoch: 159 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.443523565150423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.443523565150423 | validation: 4.322500492070733]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.265610683158845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.265610683158845 | validation: 5.863826718930532]
	TIME [epoch: 2.72 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.640816867167753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.640816867167753 | validation: 4.013046379098744]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000531212539921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.000531212539921 | validation: 3.9512590143834063]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.898078186310109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.898078186310109 | validation: 3.6710376289694895]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.60823461898795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.60823461898795 | validation: 3.7534012685194607]
	TIME [epoch: 2.74 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6282116193655947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6282116193655947 | validation: 3.698647211899558]
	TIME [epoch: 2.74 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6372199063986614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6372199063986614 | validation: 3.7009524603813446]
	TIME [epoch: 2.74 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5955132961576353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5955132961576353 | validation: 3.5649361940426503]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.482962764602048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.482962764602048 | validation: 3.5797085021025907]
	TIME [epoch: 2.74 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4591109693707187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4591109693707187 | validation: 3.578199750556464]
	TIME [epoch: 2.73 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.456173585774055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.456173585774055 | validation: 3.549966519156734]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4826842128942817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4826842128942817 | validation: 3.6562533244937856]
	TIME [epoch: 2.72 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5342455465334686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5342455465334686 | validation: 3.5523637060023088]
	TIME [epoch: 2.71 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5044538286540945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5044538286540945 | validation: 3.6034872029920297]
	TIME [epoch: 2.72 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4772732225264598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4772732225264598 | validation: 3.4946793692901097]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4229034614176754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4229034614176754 | validation: 3.5006058723598708]
	TIME [epoch: 2.74 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3830807109637524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3830807109637524 | validation: 3.472592665924612]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.364418080871677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.364418080871677 | validation: 3.611709432510745]
	TIME [epoch: 2.74 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.422489208564741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.422489208564741 | validation: 3.543818333768858]
	TIME [epoch: 2.72 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5210264108929676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5210264108929676 | validation: 3.511582358295978]
	TIME [epoch: 2.72 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3963110396532636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3963110396532636 | validation: 3.41329772746702]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2735660940321845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2735660940321845 | validation: 3.4042731664822288]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.296223237561762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296223237561762 | validation: 4.165205037464411]
	TIME [epoch: 2.74 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.811481659357357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.811481659357357 | validation: 3.6513768352053972]
	TIME [epoch: 2.74 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.624448784836863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.624448784836863 | validation: 3.5476493079461586]
	TIME [epoch: 2.74 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.536019003605716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.536019003605716 | validation: 3.3720204517299495]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2748654606747625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2748654606747625 | validation: 3.4419984294756105]
	TIME [epoch: 2.72 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.293262469637832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.293262469637832 | validation: 3.3325156307310184]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2131667754397335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2131667754397335 | validation: 3.3330268375498906]
	TIME [epoch: 2.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.226518003340795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.226518003340795 | validation: 3.2786676691394607]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1720400694621764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1720400694621764 | validation: 3.322925800913651]
	TIME [epoch: 2.72 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1755072365379498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1755072365379498 | validation: 3.2937605902252827]
	TIME [epoch: 2.72 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.246398603680932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.246398603680932 | validation: 3.46087358390605]
	TIME [epoch: 2.72 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2880011611567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2880011611567 | validation: 3.2510975198186545]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2120995978225944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2120995978225944 | validation: 3.2951368058912283]
	TIME [epoch: 2.72 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1320863051100005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1320863051100005 | validation: 3.2043976869998683]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1526207766337087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1526207766337087 | validation: 3.482999376285038]
	TIME [epoch: 2.72 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2601224491643506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2601224491643506 | validation: 3.297420088435688]
	TIME [epoch: 2.72 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.297722499785298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.297722499785298 | validation: 3.1809345874461856]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1131898682493024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1131898682493024 | validation: 3.2247252129068134]
	TIME [epoch: 2.72 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0758160975640982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0758160975640982 | validation: 3.154574028200093]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1070319727043034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1070319727043034 | validation: 3.4290558782742693]
	TIME [epoch: 2.72 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2088835872023327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2088835872023327 | validation: 3.1617560109929137]
	TIME [epoch: 2.72 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.157468784775874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157468784775874 | validation: 3.087720725730411]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.019932084806237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.019932084806237 | validation: 3.1967755521574754]
	TIME [epoch: 2.72 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0407198830880895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0407198830880895 | validation: 3.1000712468709732]
	TIME [epoch: 2.72 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0713399891809514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0713399891809514 | validation: 3.290943375255884]
	TIME [epoch: 2.72 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1153835359657682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1153835359657682 | validation: 3.0625732671060035]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.015492414821287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.015492414821287 | validation: 3.0378996847998]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9275274363167334		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.9275274363167334 | validation: 3.007236838580093]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.901709899368902		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.901709899368902 | validation: 2.9531111736362323]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.888836892132124		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.888836892132124 | validation: 3.4414304787679812]
	TIME [epoch: 2.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2107165674212075		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.2107165674212075 | validation: 3.382428236184883]
	TIME [epoch: 2.73 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3376733773885836		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.3376733773885836 | validation: 3.2820484218273522]
	TIME [epoch: 2.72 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.250606504765209		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.250606504765209 | validation: 3.1123410713685]
	TIME [epoch: 2.72 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0665025717890435		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.0665025717890435 | validation: 3.053866624000658]
	TIME [epoch: 2.72 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.930386137642992		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.930386137642992 | validation: 3.0149069584690658]
	TIME [epoch: 2.72 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9374133589638673		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.9374133589638673 | validation: 2.9804855237867307]
	TIME [epoch: 2.72 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8877636376179443		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.8877636376179443 | validation: 2.943185630108996]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.853394371675463		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.853394371675463 | validation: 2.923405167634979]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8381814608826574		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.8381814608826574 | validation: 2.9336416591951253]
	TIME [epoch: 2.72 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8136683221002445		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8136683221002445 | validation: 2.918237299793752]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8685222074408343		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.8685222074408343 | validation: 3.472571168050702]
	TIME [epoch: 2.72 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.24316588241923		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.24316588241923 | validation: 2.945940571151129]
	TIME [epoch: 2.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9317995182394907		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.9317995182394907 | validation: 2.901172038391522]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8547639869141004		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.8547639869141004 | validation: 2.8719641614216194]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.79628972173349		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.79628972173349 | validation: 2.765386245904976]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7106012225128575		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.7106012225128575 | validation: 2.6811403837729824]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6306619256932184		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.6306619256932184 | validation: 2.6157175522760783]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.47750783092928		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.47750783092928 | validation: 2.2454403999122725]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1096349169594517		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.1096349169594517 | validation: 1.7756020703308353]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8733247672759534		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.8733247672759534 | validation: 3.303342731886761]
	TIME [epoch: 2.74 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7814362600054143		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.7814362600054143 | validation: 2.9852849668279244]
	TIME [epoch: 2.74 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.29889599361724		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.29889599361724 | validation: 2.0351904470434423]
	TIME [epoch: 2.73 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9988346723342312		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.9988346723342312 | validation: 1.5195344166746994]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5933352930712406		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.5933352930712406 | validation: 1.5884968992115733]
	TIME [epoch: 2.74 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7064829590952781		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.7064829590952781 | validation: 1.340900618796384]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4369335541792412		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.4369335541792412 | validation: 1.2317247626684877]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2921910875136662		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.2921910875136662 | validation: 1.069041425602903]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156916692829246		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.1156916692829246 | validation: 0.9618415447436859]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0248103358514393		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.0248103358514393 | validation: 0.9690325036096388]
	TIME [epoch: 2.72 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247735026336375		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.0247735026336375 | validation: 1.056174295489719]
	TIME [epoch: 2.73 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066178064607338		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.066178064607338 | validation: 1.1824531707869865]
	TIME [epoch: 2.73 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3931657756818643		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.3931657756818643 | validation: 1.2931724043299901]
	TIME [epoch: 2.73 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307683052060184		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.307683052060184 | validation: 0.9620264157964468]
	TIME [epoch: 2.74 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0425152247482166		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0425152247482166 | validation: 0.9841282588424545]
	TIME [epoch: 2.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1434979206266858		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.1434979206266858 | validation: 0.8751442731861158]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9738383273880495		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9738383273880495 | validation: 0.8355892280094199]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556202732605038		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9556202732605038 | validation: 0.7936703449907201]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9361960521984247		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9361960521984247 | validation: 0.7966556442765729]
	TIME [epoch: 2.71 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9229407905563107		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9229407905563107 | validation: 0.8165395312152631]
	TIME [epoch: 2.72 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930835035438422		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.930835035438422 | validation: 0.8685800000917858]
	TIME [epoch: 2.72 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722729941123427		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.9722729941123427 | validation: 0.8638028045422331]
	TIME [epoch: 2.72 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0164981713273982		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.0164981713273982 | validation: 0.8306449907765668]
	TIME [epoch: 2.72 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951935891442993		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.951935891442993 | validation: 0.7962531364226528]
	TIME [epoch: 2.72 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9294782808597338		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9294782808597338 | validation: 0.7959820994039883]
	TIME [epoch: 2.72 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320838053711729		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9320838053711729 | validation: 0.8052800196579988]
	TIME [epoch: 2.75 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614032657389898		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9614032657389898 | validation: 0.8295052090305792]
	TIME [epoch: 2.72 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9694388833583735		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.9694388833583735 | validation: 0.75604198609998]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996161677314598		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8996161677314598 | validation: 0.7616336538078354]
	TIME [epoch: 2.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027617119384767		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9027617119384767 | validation: 0.7729681893725959]
	TIME [epoch: 2.74 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916747641554831		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.916747641554831 | validation: 0.8382122174611919]
	TIME [epoch: 2.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9954838849707898		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9954838849707898 | validation: 0.8834547917316613]
	TIME [epoch: 2.73 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0356943398353144		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.0356943398353144 | validation: 0.760734379364624]
	TIME [epoch: 2.73 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028293870330015		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9028293870330015 | validation: 0.7460459821554015]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971985491002006		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8971985491002006 | validation: 0.7946600818772085]
	TIME [epoch: 2.73 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436136218905756		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9436136218905756 | validation: 0.8414089592175223]
	TIME [epoch: 2.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.997458115978992		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.997458115978992 | validation: 0.9485583330889643]
	TIME [epoch: 2.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0895172517320308		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.0895172517320308 | validation: 0.7431281422245246]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8932467945601702		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8932467945601702 | validation: 0.7663543605028814]
	TIME [epoch: 2.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904794641366243		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.904794641366243 | validation: 0.8329003864374512]
	TIME [epoch: 2.73 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9764120703659281		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9764120703659281 | validation: 0.7945583883497749]
	TIME [epoch: 2.73 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930378910843557		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.930378910843557 | validation: 0.8126498029191643]
	TIME [epoch: 2.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386658746593043		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9386658746593043 | validation: 0.8411907122715085]
	TIME [epoch: 2.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0013514925816887		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.0013514925816887 | validation: 0.824953125272716]
	TIME [epoch: 2.73 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9571437576850137		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9571437576850137 | validation: 0.7560329861991799]
	TIME [epoch: 2.73 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981374335522417		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8981374335522417 | validation: 0.7619000441888668]
	TIME [epoch: 2.73 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161684139657998		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.9161684139657998 | validation: 0.790452247842655]
	TIME [epoch: 2.73 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9304262532044463		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.9304262532044463 | validation: 0.7957649999379113]
	TIME [epoch: 2.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367851932467582		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9367851932467582 | validation: 0.7498437678895806]
	TIME [epoch: 2.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874724979819025		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8874724979819025 | validation: 0.794675860588853]
	TIME [epoch: 2.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940362485938247		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.940362485938247 | validation: 0.7591663449528765]
	TIME [epoch: 2.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984715644521205		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8984715644521205 | validation: 0.7374030368638516]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810445995668837		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8810445995668837 | validation: 0.7502783564190788]
	TIME [epoch: 2.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719136365136302		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8719136365136302 | validation: 0.7592831055352551]
	TIME [epoch: 2.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996748140916454		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8996748140916454 | validation: 0.7640438183095418]
	TIME [epoch: 2.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840443532624432		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8840443532624432 | validation: 0.7825633765706327]
	TIME [epoch: 2.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238136162881918		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.9238136162881918 | validation: 0.8153167431285621]
	TIME [epoch: 2.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975367798455355		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9975367798455355 | validation: 0.8639178919514464]
	TIME [epoch: 2.72 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0405384796908113		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.0405384796908113 | validation: 0.8054186067675378]
	TIME [epoch: 2.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472694275461259		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9472694275461259 | validation: 0.7734521813924107]
	TIME [epoch: 2.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092980337628754		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9092980337628754 | validation: 0.7479578050072893]
	TIME [epoch: 2.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115504152068346		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9115504152068346 | validation: 0.8620106531708456]
	TIME [epoch: 2.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186550799306915		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.0186550799306915 | validation: 0.8163558638590115]
	TIME [epoch: 2.73 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9950197028429432		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.9950197028429432 | validation: 0.7502953954540618]
	TIME [epoch: 2.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726073184653005		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8726073184653005 | validation: 0.7419758154854804]
	TIME [epoch: 2.73 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919310528798223		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8919310528798223 | validation: 0.7459592101683054]
	TIME [epoch: 2.73 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884455305135235		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.884455305135235 | validation: 0.7396623615443807]
	TIME [epoch: 2.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748298936499408		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8748298936499408 | validation: 0.7351534405102073]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676554159472925		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8676554159472925 | validation: 0.7503677200393839]
	TIME [epoch: 2.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.893955719744267		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.893955719744267 | validation: 0.7613045273858466]
	TIME [epoch: 2.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9241417401182609		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.9241417401182609 | validation: 0.780560890595877]
	TIME [epoch: 2.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122441156406572		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.9122441156406572 | validation: 0.7484458697368687]
	TIME [epoch: 2.73 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961420833547997		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8961420833547997 | validation: 0.7853140318473635]
	TIME [epoch: 2.74 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380157815093534		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.9380157815093534 | validation: 0.7729836592640691]
	TIME [epoch: 2.74 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258314024516855		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.9258314024516855 | validation: 0.791654378283461]
	TIME [epoch: 2.73 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9469543728805132		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9469543728805132 | validation: 0.7409399651843851]
	TIME [epoch: 2.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617966618940796		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8617966618940796 | validation: 0.8183174724548827]
	TIME [epoch: 2.73 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9775561558882216		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.9775561558882216 | validation: 0.8388054752142193]
	TIME [epoch: 2.73 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0270246515372912		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.0270246515372912 | validation: 0.7298281383482544]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840016345273509		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8840016345273509 | validation: 0.7936283163688409]
	TIME [epoch: 2.73 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459882598921567		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9459882598921567 | validation: 0.7838361692727007]
	TIME [epoch: 2.73 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457473117716441		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.9457473117716441 | validation: 0.7202122600866769]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730896184974205		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8730896184974205 | validation: 0.7727089048017233]
	TIME [epoch: 2.73 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9310927234368993		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.9310927234368993 | validation: 0.7431067227181346]
	TIME [epoch: 2.73 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832922605949735		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8832922605949735 | validation: 0.7179200255136096]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623680202599546		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8623680202599546 | validation: 0.7437352930559933]
	TIME [epoch: 2.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748778019890757		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8748778019890757 | validation: 0.7739985690338407]
	TIME [epoch: 2.72 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023030505164675		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.9023030505164675 | validation: 0.757773857937671]
	TIME [epoch: 2.73 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841116315898606		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8841116315898606 | validation: 0.7253492798000563]
	TIME [epoch: 2.73 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686431101948614		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8686431101948614 | validation: 0.7461668381873395]
	TIME [epoch: 2.73 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765022961376311		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8765022961376311 | validation: 0.7298204384017464]
	TIME [epoch: 2.73 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782771486535765		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8782771486535765 | validation: 0.7304157251111365]
	TIME [epoch: 2.73 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612682662247026		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8612682662247026 | validation: 0.7487662974814826]
	TIME [epoch: 2.73 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087720423489387		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.9087720423489387 | validation: 0.7564182158146742]
	TIME [epoch: 2.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9128528587493159		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9128528587493159 | validation: 0.7385068279677034]
	TIME [epoch: 2.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882390297991152		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.882390297991152 | validation: 0.7137477968803089]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468415421158659		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8468415421158659 | validation: 0.7396436806421574]
	TIME [epoch: 2.72 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754750387495722		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8754750387495722 | validation: 0.7983725952949423]
	TIME [epoch: 2.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9720506716779284		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9720506716779284 | validation: 0.8608699700583324]
	TIME [epoch: 2.72 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0625456118021765		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0625456118021765 | validation: 0.8081885280331242]
	TIME [epoch: 2.72 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9097504062235072		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9097504062235072 | validation: 0.7841113561027573]
	TIME [epoch: 2.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90668355274187		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.90668355274187 | validation: 0.7699578532808405]
	TIME [epoch: 2.72 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.94759535985864		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.94759535985864 | validation: 0.7391534631978773]
	TIME [epoch: 2.72 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526624356695711		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8526624356695711 | validation: 0.7515130638607483]
	TIME [epoch: 2.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733494580826187		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8733494580826187 | validation: 0.7376901621341505]
	TIME [epoch: 2.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8483017694719339		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8483017694719339 | validation: 0.7363223912637992]
	TIME [epoch: 2.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8398763689144044		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8398763689144044 | validation: 0.7228433289982674]
	TIME [epoch: 2.72 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337479972115212		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8337479972115212 | validation: 0.7501065653989856]
	TIME [epoch: 2.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473382167567118		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8473382167567118 | validation: 0.7694089514245714]
	TIME [epoch: 2.72 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8849746502481206		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8849746502481206 | validation: 0.7604047232133722]
	TIME [epoch: 2.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831569423701359		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8831569423701359 | validation: 0.7338352639831416]
	TIME [epoch: 2.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246501188586128		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8246501188586128 | validation: 0.7070110319398932]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105762793497385		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8105762793497385 | validation: 0.7252695244899532]
	TIME [epoch: 2.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059890927648123		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8059890927648123 | validation: 0.7406661657584878]
	TIME [epoch: 2.73 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403653494068479		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8403653494068479 | validation: 0.8753223902737594]
	TIME [epoch: 2.72 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9599557805050836		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9599557805050836 | validation: 0.774922373687671]
	TIME [epoch: 2.73 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973088568705789		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8973088568705789 | validation: 0.7056610558538448]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151954153215565		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8151954153215565 | validation: 0.7203712217192815]
	TIME [epoch: 2.73 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431148070101775		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.8431148070101775 | validation: 0.6936230410418333]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876901058577578		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7876901058577578 | validation: 0.7098936722138909]
	TIME [epoch: 2.73 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869095214014253		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7869095214014253 | validation: 0.7509868756532758]
	TIME [epoch: 2.73 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002466434707608		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8002466434707608 | validation: 0.7876394408877191]
	TIME [epoch: 2.72 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262614131561115		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9262614131561115 | validation: 0.8573813138610554]
	TIME [epoch: 2.73 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899884283669862		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.899884283669862 | validation: 0.6871931011956811]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883146908270444		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7883146908270444 | validation: 0.679334321470228]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011977731428296		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8011977731428296 | validation: 0.6899561820566597]
	TIME [epoch: 2.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876799320505704		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7876799320505704 | validation: 0.7280598197408304]
	TIME [epoch: 2.73 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150821914666138		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8150821914666138 | validation: 0.701191759607316]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982271337342246		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7982271337342246 | validation: 0.6906372552768207]
	TIME [epoch: 5.92 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807661009701006		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7807661009701006 | validation: 0.6649807002971647]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718122873225229		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7718122873225229 | validation: 0.7156058236759805]
	TIME [epoch: 5.9 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767944390981648		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7767944390981648 | validation: 0.6804049271320738]
	TIME [epoch: 5.89 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803563651639948		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7803563651639948 | validation: 0.7813511493080285]
	TIME [epoch: 5.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815393623045023		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.815393623045023 | validation: 0.7319445390810912]
	TIME [epoch: 5.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460305213464397		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8460305213464397 | validation: 0.6793767271637963]
	TIME [epoch: 5.89 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488825904155314		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7488825904155314 | validation: 0.6515198939903439]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222776275877427		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7222776275877427 | validation: 0.6497527044534234]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275319708931389		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7275319708931389 | validation: 0.7571968562211604]
	TIME [epoch: 5.91 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830817315477424		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7830817315477424 | validation: 0.8271381161911727]
	TIME [epoch: 5.91 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172748534935216		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.9172748534935216 | validation: 0.6810414177734937]
	TIME [epoch: 5.91 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074866257441892		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.8074866257441892 | validation: 0.6758776716459582]
	TIME [epoch: 5.91 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717774445043034		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7717774445043034 | validation: 0.6489352695792943]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192708133366458		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7192708133366458 | validation: 0.6553577445580983]
	TIME [epoch: 5.89 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704617057750657		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.704617057750657 | validation: 0.647174360626589]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69358076415177		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.69358076415177 | validation: 0.6546046398184544]
	TIME [epoch: 5.88 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450376646945835		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7450376646945835 | validation: 0.8678789505425009]
	TIME [epoch: 5.87 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9620757709159017		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9620757709159017 | validation: 0.6773976752596695]
	TIME [epoch: 5.89 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8219686612136371		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.8219686612136371 | validation: 0.6082283490279178]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181608171570716		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7181608171570716 | validation: 0.6773028710585937]
	TIME [epoch: 5.9 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749429914645969		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.749429914645969 | validation: 0.6468168025545548]
	TIME [epoch: 5.88 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268646217785628		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7268646217785628 | validation: 0.6778004143959789]
	TIME [epoch: 5.89 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196227181628245		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7196227181628245 | validation: 0.6221254123466878]
	TIME [epoch: 5.86 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062605407589648		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7062605407589648 | validation: 0.6629632460988033]
	TIME [epoch: 5.88 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209860770697847		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7209860770697847 | validation: 0.7480570229223901]
	TIME [epoch: 5.87 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511547257443979		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7511547257443979 | validation: 0.6841818274037763]
	TIME [epoch: 5.89 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586438592903016		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7586438592903016 | validation: 0.6591254038880218]
	TIME [epoch: 5.89 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931607063594138		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6931607063594138 | validation: 0.6285252613440502]
	TIME [epoch: 5.89 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687769045168343		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.687769045168343 | validation: 0.6907670089399609]
	TIME [epoch: 5.88 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310827412345212		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7310827412345212 | validation: 0.687987024168718]
	TIME [epoch: 5.89 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79896273769099		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.79896273769099 | validation: 0.7504844383189175]
	TIME [epoch: 5.89 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200959103464114		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8200959103464114 | validation: 0.6386505493482693]
	TIME [epoch: 5.89 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733050995231604		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.733050995231604 | validation: 0.6422590334280676]
	TIME [epoch: 5.89 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992066845923105		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6992066845923105 | validation: 0.6457367976235272]
	TIME [epoch: 5.9 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870440548661583		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6870440548661583 | validation: 0.6150323369906282]
	TIME [epoch: 5.89 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686357173615858		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6686357173615858 | validation: 0.6402562830661549]
	TIME [epoch: 5.9 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6664284876817214		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6664284876817214 | validation: 0.6161725448992437]
	TIME [epoch: 5.89 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597919208209774		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6597919208209774 | validation: 0.6426868894923884]
	TIME [epoch: 5.88 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708727283153141		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6708727283153141 | validation: 0.6292327332516691]
	TIME [epoch: 5.89 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084155922054657		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7084155922054657 | validation: 0.777295197297304]
	TIME [epoch: 5.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868163120408218		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7868163120408218 | validation: 0.6679421215820279]
	TIME [epoch: 5.9 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867488935605266		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7867488935605266 | validation: 0.6852429547861831]
	TIME [epoch: 5.89 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472572184696042		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.7472572184696042 | validation: 0.6153956365308263]
	TIME [epoch: 5.89 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786917960369055		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6786917960369055 | validation: 0.5994023076238969]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437259566531021		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6437259566531021 | validation: 0.6251030577465535]
	TIME [epoch: 5.9 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388169696490714		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6388169696490714 | validation: 0.5911910943451795]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656053033054923		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.656053033054923 | validation: 0.7247160641403527]
	TIME [epoch: 5.92 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434405937934498		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7434405937934498 | validation: 0.6690258571606096]
	TIME [epoch: 5.92 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820521921501502		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7820521921501502 | validation: 0.6359034245379714]
	TIME [epoch: 5.92 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703423622884047		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6703423622884047 | validation: 0.6102966650282027]
	TIME [epoch: 5.9 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509286759251418		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6509286759251418 | validation: 0.5840412248669404]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6513200315689727		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6513200315689727 | validation: 0.6742974494030023]
	TIME [epoch: 5.88 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732810661950694		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6732810661950694 | validation: 0.6310774279150171]
	TIME [epoch: 5.89 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881910389299251		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6881910389299251 | validation: 0.69047362256557]
	TIME [epoch: 5.88 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6688592724018076		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6688592724018076 | validation: 0.5912687780704017]
	TIME [epoch: 5.88 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6445770330133993		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6445770330133993 | validation: 0.6138444074458231]
	TIME [epoch: 5.92 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297556666864237		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6297556666864237 | validation: 0.6062380360203626]
	TIME [epoch: 5.92 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6638903493145409		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6638903493145409 | validation: 0.698018961927669]
	TIME [epoch: 5.92 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200729405028105		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7200729405028105 | validation: 0.599581867980064]
	TIME [epoch: 5.91 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728040413335862		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6728040413335862 | validation: 0.601061772123415]
	TIME [epoch: 5.91 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288853729808677		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6288853729808677 | validation: 0.5626676426773497]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316531221651148		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6316531221651148 | validation: 0.6574781600287949]
	TIME [epoch: 5.88 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631287729002687		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6631287729002687 | validation: 0.6290683187066062]
	TIME [epoch: 5.87 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.70195090598148		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.70195090598148 | validation: 0.6322686406767976]
	TIME [epoch: 5.88 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497555543730613		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6497555543730613 | validation: 0.5528841441234812]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250928783869221		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6250928783869221 | validation: 0.6450131349553507]
	TIME [epoch: 5.88 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625282878880486		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6625282878880486 | validation: 0.6272565272174089]
	TIME [epoch: 5.88 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409748609949505		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7409748609949505 | validation: 0.6188268609873137]
	TIME [epoch: 5.89 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435419287312706		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6435419287312706 | validation: 0.5567040812072007]
	TIME [epoch: 5.87 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078886627034426		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6078886627034426 | validation: 0.5548177519891965]
	TIME [epoch: 5.88 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5903667488677448		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5903667488677448 | validation: 0.6046755808492922]
	TIME [epoch: 5.87 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.60962166793884		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.60962166793884 | validation: 0.5755255104901708]
	TIME [epoch: 5.89 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356648431892099		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6356648431892099 | validation: 0.679808405995709]
	TIME [epoch: 5.88 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648948748240651		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6648948748240651 | validation: 0.5723061534054]
	TIME [epoch: 5.87 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6593301683569653		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6593301683569653 | validation: 0.6137924849949066]
	TIME [epoch: 5.88 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069323881545249		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6069323881545249 | validation: 0.6112546516248284]
	TIME [epoch: 5.88 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288864651758789		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6288864651758789 | validation: 0.7136058424489765]
	TIME [epoch: 5.87 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816270817723694		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6816270817723694 | validation: 0.5463531648617261]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6041136132715935		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6041136132715935 | validation: 0.5616035513155408]
	TIME [epoch: 5.9 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5903494111220756		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5903494111220756 | validation: 0.5684625408736975]
	TIME [epoch: 5.91 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6074434131399649		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6074434131399649 | validation: 0.6035320102827684]
	TIME [epoch: 5.9 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6066421931767546		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6066421931767546 | validation: 0.5508879986373357]
	TIME [epoch: 5.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.606712982623277		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.606712982623277 | validation: 0.6015026752821528]
	TIME [epoch: 5.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6050011070550234		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6050011070550234 | validation: 0.5379403120471542]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632053009644355		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.632053009644355 | validation: 0.6196938321545932]
	TIME [epoch: 5.88 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273216223250867		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6273216223250867 | validation: 0.5397255099077869]
	TIME [epoch: 5.89 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160759660765718		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6160759660765718 | validation: 0.5728452083651691]
	TIME [epoch: 5.89 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5796096081620682		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5796096081620682 | validation: 0.5392975869733604]
	TIME [epoch: 5.9 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593935022191253		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5593935022191253 | validation: 0.5521656553149095]
	TIME [epoch: 5.91 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635557480304101		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5635557480304101 | validation: 0.5837468323291569]
	TIME [epoch: 5.9 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5812247398035322		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5812247398035322 | validation: 0.5685655175050838]
	TIME [epoch: 5.91 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988421916010986		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.5988421916010986 | validation: 0.5817651693102665]
	TIME [epoch: 5.92 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5691202903472419		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5691202903472419 | validation: 0.5346044871363994]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5550379666451974		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5550379666451974 | validation: 0.6034724374861784]
	TIME [epoch: 5.91 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694534948762929		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5694534948762929 | validation: 0.5304958729303767]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6134331744410965		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6134331744410965 | validation: 0.7230313448298978]
	TIME [epoch: 5.91 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497179765628801		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7497179765628801 | validation: 0.5492430789200181]
	TIME [epoch: 5.92 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244754957608153		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6244754957608153 | validation: 0.5224231741302153]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401755910272832		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5401755910272832 | validation: 0.573929093684707]
	TIME [epoch: 5.91 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617524389510039		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5617524389510039 | validation: 0.5218851462592593]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899406748985794		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5899406748985794 | validation: 0.5964365472727661]
	TIME [epoch: 5.91 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5945487676947017		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5945487676947017 | validation: 0.515837872167874]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5924153516457289		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.5924153516457289 | validation: 0.5588098967175631]
	TIME [epoch: 5.91 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5545038836465875		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5545038836465875 | validation: 0.5298077645223461]
	TIME [epoch: 5.91 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5378954018203019		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.5378954018203019 | validation: 0.4954657731655925]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5427148442788924		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5427148442788924 | validation: 0.5482266405941426]
	TIME [epoch: 5.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5552835448930415		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.5552835448930415 | validation: 0.4929942239797823]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5407064574628397		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5407064574628397 | validation: 0.5274540673047418]
	TIME [epoch: 5.91 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5257081142202437		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.5257081142202437 | validation: 0.4757470128381087]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5221062694528346		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.5221062694528346 | validation: 0.5766842081297917]
	TIME [epoch: 5.91 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432890498704191		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5432890498704191 | validation: 0.5265948233417833]
	TIME [epoch: 5.92 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5952375054489534		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5952375054489534 | validation: 0.5623726520759997]
	TIME [epoch: 5.91 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388518453444221		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.5388518453444221 | validation: 0.48118514499411075]
	TIME [epoch: 5.9 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5312777492317045		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.5312777492317045 | validation: 0.5997847078478172]
	TIME [epoch: 5.91 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810811067681906		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.5810811067681906 | validation: 0.5092343324788235]
	TIME [epoch: 5.91 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964465569406227		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.5964465569406227 | validation: 0.5319642789262667]
	TIME [epoch: 5.91 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5268275570802357		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.5268275570802357 | validation: 0.43747807279928197]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49648814925157986		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.49648814925157986 | validation: 0.5084630809335728]
	TIME [epoch: 5.91 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4975067318199263		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4975067318199263 | validation: 0.5189185579737646]
	TIME [epoch: 5.91 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252182646162094		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.5252182646162094 | validation: 0.518316796304813]
	TIME [epoch: 5.92 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5219533277586377		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.5219533277586377 | validation: 0.49705689246263535]
	TIME [epoch: 5.92 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500161707106076		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.500161707106076 | validation: 0.47586527983482213]
	TIME [epoch: 5.91 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4821386038627999		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.4821386038627999 | validation: 0.4891693564295443]
	TIME [epoch: 5.92 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4829665076730226		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.4829665076730226 | validation: 0.4460223206262739]
	TIME [epoch: 5.92 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48230419135040276		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.48230419135040276 | validation: 0.5648366548006015]
	TIME [epoch: 5.91 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415286702920409		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.5415286702920409 | validation: 0.5088946877866624]
	TIME [epoch: 5.92 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059237709482492		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6059237709482492 | validation: 0.5810894448244748]
	TIME [epoch: 5.93 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5387314504915535		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5387314504915535 | validation: 0.4754146405127487]
	TIME [epoch: 5.92 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5267259850618224		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.5267259850618224 | validation: 0.51061228347033]
	TIME [epoch: 5.9 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4911879659790509		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.4911879659790509 | validation: 0.4640688908733129]
	TIME [epoch: 5.92 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47788134159290707		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.47788134159290707 | validation: 0.45746152631337644]
	TIME [epoch: 5.91 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4777596592636279		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4777596592636279 | validation: 0.4885424410650355]
	TIME [epoch: 5.91 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47943077862178485		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.47943077862178485 | validation: 0.45486829208591795]
	TIME [epoch: 5.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5101674140013128		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5101674140013128 | validation: 0.5104580031308545]
	TIME [epoch: 5.91 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086543617103836		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5086543617103836 | validation: 0.4200609585447095]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48988396466051953		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.48988396466051953 | validation: 0.4790597860210657]
	TIME [epoch: 5.92 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619958051042565		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.4619958051042565 | validation: 0.4357111320250517]
	TIME [epoch: 5.91 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45646512093219554		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.45646512093219554 | validation: 0.48357088408459803]
	TIME [epoch: 5.91 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47305317759658916		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.47305317759658916 | validation: 0.48617622196842203]
	TIME [epoch: 5.92 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.513170963643324		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.513170963643324 | validation: 0.4581325947169245]
	TIME [epoch: 5.91 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44883993109176734		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.44883993109176734 | validation: 0.4130531657271972]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4348184813758428		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.4348184813758428 | validation: 0.4843047349128088]
	TIME [epoch: 5.91 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46378675658076624		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.46378675658076624 | validation: 0.541935255869351]
	TIME [epoch: 5.93 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187712453001275		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6187712453001275 | validation: 0.5257058127215251]
	TIME [epoch: 5.91 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211765441647973		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5211765441647973 | validation: 0.4244974395057879]
	TIME [epoch: 5.93 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4921786436459549		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.4921786436459549 | validation: 0.45364995409389774]
	TIME [epoch: 5.92 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4280436239133401		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.4280436239133401 | validation: 0.4255764025561815]
	TIME [epoch: 5.92 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4250587047834209		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.4250587047834209 | validation: 0.41208723598369246]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41842609671989317		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.41842609671989317 | validation: 0.4124026734958272]
	TIME [epoch: 5.92 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101790718541131		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.4101790718541131 | validation: 0.4199606387424739]
	TIME [epoch: 5.9 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4104408330390059		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.4104408330390059 | validation: 0.4318693007943784]
	TIME [epoch: 5.93 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4282383634748163		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4282383634748163 | validation: 0.4220510720613286]
	TIME [epoch: 5.93 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43262173865581743		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.43262173865581743 | validation: 0.42096842052907396]
	TIME [epoch: 5.89 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43618708530280276		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.43618708530280276 | validation: 0.3973034022692752]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44095181775329095		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.44095181775329095 | validation: 0.49250311336774744]
	TIME [epoch: 5.9 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4543121576116739		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.4543121576116739 | validation: 0.4596725222559728]
	TIME [epoch: 5.91 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608211766618549		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.5608211766618549 | validation: 0.6039052356824749]
	TIME [epoch: 5.92 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5214221805623772		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5214221805623772 | validation: 0.37052514010753596]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4002906388068535		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.4002906388068535 | validation: 0.366168154951668]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4024460045078483		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.4024460045078483 | validation: 0.4605797830862386]
	TIME [epoch: 5.88 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40913664783836035		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.40913664783836035 | validation: 0.3752083319711754]
	TIME [epoch: 5.89 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3800823030704254		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.3800823030704254 | validation: 0.3912860243620159]
	TIME [epoch: 5.91 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3697398726119027		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.3697398726119027 | validation: 0.3636974007911557]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3700331445758277		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.3700331445758277 | validation: 0.39199758935106943]
	TIME [epoch: 5.9 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37077049277153884		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.37077049277153884 | validation: 0.3976282461107545]
	TIME [epoch: 5.9 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42628287483293237		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.42628287483293237 | validation: 0.5938843153631691]
	TIME [epoch: 5.92 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5897222794500003		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.5897222794500003 | validation: 0.40468335663010446]
	TIME [epoch: 5.91 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48648820922370734		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.48648820922370734 | validation: 0.4238183608000671]
	TIME [epoch: 5.91 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3827603146153809		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.3827603146153809 | validation: 0.403794529595284]
	TIME [epoch: 5.91 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41172203908426935		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.41172203908426935 | validation: 0.34966636301175147]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40335927242445024		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.40335927242445024 | validation: 0.44123742113641884]
	TIME [epoch: 5.9 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40213210269779764		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.40213210269779764 | validation: 0.33967650616261746]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42480712488470956		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.42480712488470956 | validation: 0.45623250601905296]
	TIME [epoch: 5.9 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3993002185669651		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3993002185669651 | validation: 0.3719352626319194]
	TIME [epoch: 5.97 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39697185305368027		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.39697185305368027 | validation: 0.38906384288566365]
	TIME [epoch: 5.9 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3545395784946543		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.3545395784946543 | validation: 0.3264698930016015]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3349772823385942		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.3349772823385942 | validation: 0.34801937210099676]
	TIME [epoch: 5.89 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33289531360620345		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.33289531360620345 | validation: 0.32834207115922026]
	TIME [epoch: 5.89 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3502181620179497		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.3502181620179497 | validation: 0.5049106073027325]
	TIME [epoch: 5.9 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42519725580043405		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.42519725580043405 | validation: 0.46650582276134644]
	TIME [epoch: 5.9 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857887399615365		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5857887399615365 | validation: 0.42962781381950255]
	TIME [epoch: 5.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991108146640596		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3991108146640596 | validation: 0.32795851754040384]
	TIME [epoch: 5.91 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3249820375050932		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.3249820375050932 | validation: 0.33367962316256716]
	TIME [epoch: 5.91 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3322601085949665		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3322601085949665 | validation: 0.37315441945106637]
	TIME [epoch: 5.9 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3373579598574549		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3373579598574549 | validation: 0.2974029915372408]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3408507149743325		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3408507149743325 | validation: 0.38946523118168486]
	TIME [epoch: 5.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3498859452663092		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.3498859452663092 | validation: 0.33615119328841925]
	TIME [epoch: 5.91 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4011106809062652		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.4011106809062652 | validation: 0.4479337204768099]
	TIME [epoch: 5.91 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332447681487303		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4332447681487303 | validation: 0.35357252272533835]
	TIME [epoch: 5.89 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39647218305730536		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.39647218305730536 | validation: 0.3461719484964378]
	TIME [epoch: 5.9 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34272805353184504		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.34272805353184504 | validation: 0.3618234823809548]
	TIME [epoch: 5.89 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3321398239097104		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.3321398239097104 | validation: 0.3502408412662245]
	TIME [epoch: 5.9 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136756330001796		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.4136756330001796 | validation: 0.45176973030141165]
	TIME [epoch: 5.88 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37471832394076066		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.37471832394076066 | validation: 0.30972191561879087]
	TIME [epoch: 5.9 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587644352180082		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.3587644352180082 | validation: 0.35991343669798]
	TIME [epoch: 5.87 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3057355571229065		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.3057355571229065 | validation: 0.28019075408608796]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32039066360318574		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.32039066360318574 | validation: 0.5304081796178587]
	TIME [epoch: 5.91 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494475721191668		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.4494475721191668 | validation: 0.35121022722614903]
	TIME [epoch: 5.9 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4388539459503198		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4388539459503198 | validation: 0.34485830439565446]
	TIME [epoch: 5.9 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31229729807507395		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.31229729807507395 | validation: 0.2753663931536223]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848060735172149		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2848060735172149 | validation: 0.2955984846050474]
	TIME [epoch: 5.89 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27802913855799133		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.27802913855799133 | validation: 0.31332429617509056]
	TIME [epoch: 5.89 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28561348128103975		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.28561348128103975 | validation: 0.3059862810674157]
	TIME [epoch: 5.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33421014970918494		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.33421014970918494 | validation: 0.6242070816213933]
	TIME [epoch: 5.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157727400200222		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5157727400200222 | validation: 0.29363757524889467]
	TIME [epoch: 5.89 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31525853153195527		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.31525853153195527 | validation: 0.2939526712519535]
	TIME [epoch: 5.89 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33565965559796995		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.33565965559796995 | validation: 0.6291568568928331]
	TIME [epoch: 5.89 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154834886107617		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5154834886107617 | validation: 0.29624466296748597]
	TIME [epoch: 5.9 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32822438170790585		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.32822438170790585 | validation: 0.31159857525124074]
	TIME [epoch: 5.89 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069317369233209		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.3069317369233209 | validation: 0.3699632390637893]
	TIME [epoch: 5.89 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3329607027926219		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3329607027926219 | validation: 0.27664608520239037]
	TIME [epoch: 5.89 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2932466607821497		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.2932466607821497 | validation: 0.27441977001940393]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660126491357804		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2660126491357804 | validation: 0.2871304988285739]
	TIME [epoch: 5.89 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2657234081968567		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2657234081968567 | validation: 0.25924990651978946]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2786034515571862		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2786034515571862 | validation: 0.36707635918448794]
	TIME [epoch: 5.89 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3126700936373518		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3126700936373518 | validation: 0.3236843734302277]
	TIME [epoch: 5.9 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3747278874158813		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3747278874158813 | validation: 0.3375221823079894]
	TIME [epoch: 5.9 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29955494482738715		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.29955494482738715 | validation: 0.2564244577828366]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26783313320714347		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.26783313320714347 | validation: 0.42734737229680975]
	TIME [epoch: 5.91 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3547510669662809		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.3547510669662809 | validation: 0.4492269362844432]
	TIME [epoch: 5.91 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544114333990048		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.544114333990048 | validation: 0.3726765305443531]
	TIME [epoch: 5.88 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3236284358026127		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3236284358026127 | validation: 0.2422791133723969]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27178403564218895		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.27178403564218895 | validation: 0.38176536955192913]
	TIME [epoch: 5.91 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3134283216291502		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3134283216291502 | validation: 0.2853379441378791]
	TIME [epoch: 5.91 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31244234306280944		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.31244234306280944 | validation: 0.3300372410808769]
	TIME [epoch: 5.91 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29253083678719344		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.29253083678719344 | validation: 0.2441401359319758]
	TIME [epoch: 5.91 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25545746521299983		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.25545746521299983 | validation: 0.3255343896473235]
	TIME [epoch: 5.9 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944710725324519		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.2944710725324519 | validation: 0.2904860644953944]
	TIME [epoch: 5.91 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578251291877831		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.3578251291877831 | validation: 0.446231481347221]
	TIME [epoch: 5.9 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37887855532904413		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.37887855532904413 | validation: 0.2758558870069207]
	TIME [epoch: 5.91 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33493814668555866		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.33493814668555866 | validation: 0.3388168895009258]
	TIME [epoch: 5.91 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792980841117231		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.2792980841117231 | validation: 0.24064926902300257]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27957322889348724		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.27957322889348724 | validation: 0.30973558168236154]
	TIME [epoch: 5.9 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612332448706029		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.2612332448706029 | validation: 0.2557028021125688]
	TIME [epoch: 5.9 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.256169194112653		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.256169194112653 | validation: 0.3350386613790681]
	TIME [epoch: 5.91 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2737366057788272		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.2737366057788272 | validation: 0.2847105544596325]
	TIME [epoch: 5.89 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2933554268877616		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.2933554268877616 | validation: 0.3424725350638047]
	TIME [epoch: 5.9 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28019575985867806		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.28019575985867806 | validation: 0.23347198933491714]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765280075599857		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.2765280075599857 | validation: 0.4233529962309095]
	TIME [epoch: 5.89 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3325279104130124		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.3325279104130124 | validation: 0.31666771943805205]
	TIME [epoch: 5.89 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975195842462793		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.3975195842462793 | validation: 0.3324554480560167]
	TIME [epoch: 5.91 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28996463379156895		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.28996463379156895 | validation: 0.23793142089336575]
	TIME [epoch: 5.89 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23893370030705008		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.23893370030705008 | validation: 0.26718310568845177]
	TIME [epoch: 5.91 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23513679237208374		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.23513679237208374 | validation: 0.2216621399054215]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23395174231054383		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.23395174231054383 | validation: 0.30414500586354715]
	TIME [epoch: 5.92 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23534215833055466		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.23534215833055466 | validation: 0.24305445517383203]
	TIME [epoch: 5.9 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968473805901299		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.2968473805901299 | validation: 0.6215204900375499]
	TIME [epoch: 5.89 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4815681872008666		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.4815681872008666 | validation: 0.22503262980385486]
	TIME [epoch: 5.89 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2544604812075011		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2544604812075011 | validation: 0.24991476010370905]
	TIME [epoch: 5.89 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22135268293785992		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.22135268293785992 | validation: 0.24800661583230205]
	TIME [epoch: 5.88 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22588788997441328		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.22588788997441328 | validation: 0.27007391031734057]
	TIME [epoch: 5.9 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252325687539913		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.252325687539913 | validation: 0.2947004069666275]
	TIME [epoch: 5.89 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2973981656424258		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2973981656424258 | validation: 0.3957359019651119]
	TIME [epoch: 5.89 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266592991014748		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.3266592991014748 | validation: 0.2578340440403355]
	TIME [epoch: 5.89 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29926313869705623		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.29926313869705623 | validation: 0.3815732989362407]
	TIME [epoch: 5.88 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.302880779262		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.302880779262 | validation: 0.25941264896644195]
	TIME [epoch: 5.89 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32656316212083264		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.32656316212083264 | validation: 0.3704516253490591]
	TIME [epoch: 5.88 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2772947929860584		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.2772947929860584 | validation: 0.2080921959518885]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23583416630554374		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.23583416630554374 | validation: 0.28300412440641465]
	TIME [epoch: 5.89 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23745420333391237		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.23745420333391237 | validation: 0.236665731386159]
	TIME [epoch: 5.89 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526449787267134		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.2526449787267134 | validation: 0.2840105632107532]
	TIME [epoch: 5.9 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2703768246592902		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2703768246592902 | validation: 0.24092467891267]
	TIME [epoch: 5.88 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2202316103714314		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2202316103714314 | validation: 0.21560315001641916]
	TIME [epoch: 5.9 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2090560717720344		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2090560717720344 | validation: 0.26681507185930003]
	TIME [epoch: 5.88 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217562273351067		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.217562273351067 | validation: 0.26009110850210865]
	TIME [epoch: 5.89 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29391885420943		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.29391885420943 | validation: 0.5965158514506097]
	TIME [epoch: 5.89 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.451836293646134		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.451836293646134 | validation: 0.2441995276508409]
	TIME [epoch: 5.88 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28892814106233367		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.28892814106233367 | validation: 0.2752477044632123]
	TIME [epoch: 5.88 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22920540093563346		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.22920540093563346 | validation: 0.21523189031807224]
	TIME [epoch: 5.89 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2342246381069799		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2342246381069799 | validation: 0.3019761357425643]
	TIME [epoch: 5.88 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2315274326018991		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.2315274326018991 | validation: 0.23155911970162113]
	TIME [epoch: 5.89 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28365624171427667		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.28365624171427667 | validation: 0.45233459069514836]
	TIME [epoch: 5.89 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33303895837068254		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.33303895837068254 | validation: 0.20869159056226164]
	TIME [epoch: 5.88 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568148095990763		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2568148095990763 | validation: 0.29244568847438385]
	TIME [epoch: 5.9 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.226923444949007		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.226923444949007 | validation: 0.21495809171582422]
	TIME [epoch: 5.89 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22285435553917532		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.22285435553917532 | validation: 0.23190428325306778]
	TIME [epoch: 5.89 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21042492402241714		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.21042492402241714 | validation: 0.2324194714297784]
	TIME [epoch: 5.9 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21308727251679976		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.21308727251679976 | validation: 0.23309926849362084]
	TIME [epoch: 5.89 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2192181829825215		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2192181829825215 | validation: 0.29397925009400444]
	TIME [epoch: 5.88 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24193764516737973		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.24193764516737973 | validation: 0.25459319090123633]
	TIME [epoch: 5.89 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145566350530436		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.3145566350530436 | validation: 0.47830577175977]
	TIME [epoch: 5.91 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34289515133571813		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.34289515133571813 | validation: 0.2194241611511191]
	TIME [epoch: 5.91 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27171223376169823		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.27171223376169823 | validation: 0.3056545139199813]
	TIME [epoch: 5.9 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22978243426019357		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.22978243426019357 | validation: 0.20595307096962712]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21660613638798162		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.21660613638798162 | validation: 0.3030499507582196]
	TIME [epoch: 5.89 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2233765483933234		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.2233765483933234 | validation: 0.19882248599495025]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23018060404512178		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.23018060404512178 | validation: 0.3619642415769027]
	TIME [epoch: 5.94 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25677710133265613		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.25677710133265613 | validation: 0.20984158720587415]
	TIME [epoch: 5.88 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25392756925605825		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.25392756925605825 | validation: 0.3463844459146581]
	TIME [epoch: 5.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25544077539531174		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.25544077539531174 | validation: 0.21061134718856167]
	TIME [epoch: 5.89 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24004756953390313		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.24004756953390313 | validation: 0.29128675198908943]
	TIME [epoch: 5.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22749270221269707		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.22749270221269707 | validation: 0.20256918109323566]
	TIME [epoch: 5.9 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21572530263812645		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.21572530263812645 | validation: 0.3157160220219939]
	TIME [epoch: 5.9 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22616471914155742		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.22616471914155742 | validation: 0.20775322339353536]
	TIME [epoch: 5.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25116739866161014		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.25116739866161014 | validation: 0.34325501026212707]
	TIME [epoch: 5.89 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2485045411580097		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2485045411580097 | validation: 0.1803407539823796]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2164592385183053		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.2164592385183053 | validation: 0.32330339237645805]
	TIME [epoch: 5.89 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2374919410252991		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2374919410252991 | validation: 0.2194980013893998]
	TIME [epoch: 5.9 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591599710443594		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2591599710443594 | validation: 0.3443768457512697]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516655713086218		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.2516655713086218 | validation: 0.1805787746790189]
	TIME [epoch: 12.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21977378198322875		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.21977378198322875 | validation: 0.2885256680782986]
	TIME [epoch: 12.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21075685117156334		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.21075685117156334 | validation: 0.1813285890500144]
	TIME [epoch: 12.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2040154642923084		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.2040154642923084 | validation: 0.27366960543726515]
	TIME [epoch: 12.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21680258004764605		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.21680258004764605 | validation: 0.2359877133811363]
	TIME [epoch: 12.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2447971547060849		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.2447971547060849 | validation: 0.29270213744115153]
	TIME [epoch: 12.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24271868445305078		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.24271868445305078 | validation: 0.1922669349632817]
	TIME [epoch: 12.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22782020097742944		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.22782020097742944 | validation: 0.30182379816570193]
	TIME [epoch: 12.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21561669001911246		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.21561669001911246 | validation: 0.2027728139561405]
	TIME [epoch: 12.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25275487175692696		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.25275487175692696 | validation: 0.4536442909210379]
	TIME [epoch: 12.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200863422678906		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.3200863422678906 | validation: 0.20982325063203922]
	TIME [epoch: 12.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2424997084866901		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.2424997084866901 | validation: 0.3318353937801652]
	TIME [epoch: 12.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23817285120539247		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.23817285120539247 | validation: 0.18763921530279584]
	TIME [epoch: 12.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21973748767363382		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.21973748767363382 | validation: 0.2343019413265644]
	TIME [epoch: 12.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1967321341752842		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.1967321341752842 | validation: 0.18874531450706156]
	TIME [epoch: 12.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19490884712505646		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.19490884712505646 | validation: 0.21525573841351503]
	TIME [epoch: 12.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20195820254001873		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.20195820254001873 | validation: 0.2037828657513591]
	TIME [epoch: 12.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18838723215416336		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.18838723215416336 | validation: 0.243947982385025]
	TIME [epoch: 12.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18992607129996356		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.18992607129996356 | validation: 0.18337630351586764]
	TIME [epoch: 12.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21756205283977673		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.21756205283977673 | validation: 0.4085100291463215]
	TIME [epoch: 12.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27928236244119176		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.27928236244119176 | validation: 0.21722445734018053]
	TIME [epoch: 12.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29172028585251736		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.29172028585251736 | validation: 0.3229175987716645]
	TIME [epoch: 12.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2172827585285309		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.2172827585285309 | validation: 0.16266623481336664]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18366100659055865		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.18366100659055865 | validation: 0.26023269517310965]
	TIME [epoch: 12.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1917503180695199		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.1917503180695199 | validation: 0.1749920355478608]
	TIME [epoch: 12.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19816796006743642		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.19816796006743642 | validation: 0.3559205625384954]
	TIME [epoch: 12.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23575973517548765		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.23575973517548765 | validation: 0.20748225242124052]
	TIME [epoch: 12.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574969042449671		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.2574969042449671 | validation: 0.35266460249713094]
	TIME [epoch: 12.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24544058770946897		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.24544058770946897 | validation: 0.1938223378689688]
	TIME [epoch: 12.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21073007953757703		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.21073007953757703 | validation: 0.2418985352514858]
	TIME [epoch: 12.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1985298843872057		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1985298843872057 | validation: 0.2084004836224266]
	TIME [epoch: 12.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20293928976479975		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.20293928976479975 | validation: 0.25489280249467194]
	TIME [epoch: 12.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2142260654643259		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.2142260654643259 | validation: 0.16336874357196554]
	TIME [epoch: 12.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18488333814010763		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.18488333814010763 | validation: 0.2618664361373322]
	TIME [epoch: 12.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18346129018380775		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.18346129018380775 | validation: 0.18440906521188422]
	TIME [epoch: 12.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22340850127841935		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.22340850127841935 | validation: 0.39052027362169056]
	TIME [epoch: 12.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687056299587917		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.2687056299587917 | validation: 0.18884965494757167]
	TIME [epoch: 12.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23736440100231743		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.23736440100231743 | validation: 0.2779648532298113]
	TIME [epoch: 12.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19694081909058128		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.19694081909058128 | validation: 0.18310360408078555]
	TIME [epoch: 12.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21160917594471987		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.21160917594471987 | validation: 0.2688535551877051]
	TIME [epoch: 12.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2033115731459686		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.2033115731459686 | validation: 0.17444605123337975]
	TIME [epoch: 12.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19452572663182352		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.19452572663182352 | validation: 0.30922400975634046]
	TIME [epoch: 12.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21362791616141943		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.21362791616141943 | validation: 0.17866467227332417]
	TIME [epoch: 12.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2094222935740561		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.2094222935740561 | validation: 0.3169682440703328]
	TIME [epoch: 12.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2149991470840294		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.2149991470840294 | validation: 0.19412073886542158]
	TIME [epoch: 12.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20355673873706098		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.20355673873706098 | validation: 0.25596958825892385]
	TIME [epoch: 12.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18580946720343047		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.18580946720343047 | validation: 0.15804801785106776]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19108265529974894		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.19108265529974894 | validation: 0.3288036209699867]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23051402884012145		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.23051402884012145 | validation: 0.19051030726732418]
	TIME [epoch: 12.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22758035642234603		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.22758035642234603 | validation: 0.23737473492352504]
	TIME [epoch: 12.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1802154156911053		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1802154156911053 | validation: 0.1545361156567634]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16821973666677387		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.16821973666677387 | validation: 0.23850334582350438]
	TIME [epoch: 12.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17619826645557154		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.17619826645557154 | validation: 0.1740403437565907]
	TIME [epoch: 12.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20369298885371492		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.20369298885371492 | validation: 0.43508292723378394]
	TIME [epoch: 12.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925567891689776		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.2925567891689776 | validation: 0.1858993031203711]
	TIME [epoch: 12.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20298419484439176		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.20298419484439176 | validation: 0.22514821781995062]
	TIME [epoch: 12.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17568050632481266		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.17568050632481266 | validation: 0.18570551161631682]
	TIME [epoch: 12.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726562322590353		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1726562322590353 | validation: 0.23015976680147576]
	TIME [epoch: 12.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19046147784831507		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.19046147784831507 | validation: 0.2028497101542917]
	TIME [epoch: 12.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19206401775335147		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.19206401775335147 | validation: 0.19020139279219794]
	TIME [epoch: 12.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17623687219467976		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.17623687219467976 | validation: 0.16266871903911165]
	TIME [epoch: 12.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15873879400994179		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.15873879400994179 | validation: 0.19879813603536753]
	TIME [epoch: 12.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614695660091874		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1614695660091874 | validation: 0.17806244423225115]
	TIME [epoch: 12.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727918711922103		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.1727918711922103 | validation: 0.3605085771336082]
	TIME [epoch: 12.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2413152864767986		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.2413152864767986 | validation: 0.2575184800262667]
	TIME [epoch: 12.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3352664308159979		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.3352664308159979 | validation: 0.38173800115961565]
	TIME [epoch: 12.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25117699888549294		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.25117699888549294 | validation: 0.15899682441646934]
	TIME [epoch: 12.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1733616002181893		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1733616002181893 | validation: 0.17588403440485326]
	TIME [epoch: 12.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15450242457492613		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.15450242457492613 | validation: 0.14963012397752673]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15367940242846223		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.15367940242846223 | validation: 0.20406909913091506]
	TIME [epoch: 12.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15121479480813907		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.15121479480813907 | validation: 0.16364289576477764]
	TIME [epoch: 12.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18503568129962653		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.18503568129962653 | validation: 0.3600035741353569]
	TIME [epoch: 12.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22805337146959137		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.22805337146959137 | validation: 0.17234267707588377]
	TIME [epoch: 12.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2243762206645484		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.2243762206645484 | validation: 0.35031360247111243]
	TIME [epoch: 12.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22421869549757495		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.22421869549757495 | validation: 0.16491101765066538]
	TIME [epoch: 12.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18158356339637544		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.18158356339637544 | validation: 0.24894537246535908]
	TIME [epoch: 12.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1753438714701757		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1753438714701757 | validation: 0.15375041055995436]
	TIME [epoch: 12.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734832916155999		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.1734832916155999 | validation: 0.2125650319830209]
	TIME [epoch: 12.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1647736128172079		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1647736128172079 | validation: 0.15879226996614754]
	TIME [epoch: 12.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16261769351384558		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.16261769351384558 | validation: 0.25090245951185786]
	TIME [epoch: 12.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19365123551532357		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.19365123551532357 | validation: 0.18931784911969843]
	TIME [epoch: 12.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19567717706167187		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.19567717706167187 | validation: 0.20884262553742028]
	TIME [epoch: 12.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17388113628888247		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.17388113628888247 | validation: 0.16223899205267633]
	TIME [epoch: 12.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16021434439786802		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.16021434439786802 | validation: 0.23243967622373463]
	TIME [epoch: 12.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637950805670541		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1637950805670541 | validation: 0.16923415237310802]
	TIME [epoch: 12.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.204988754912787		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.204988754912787 | validation: 0.42384697964158846]
	TIME [epoch: 12.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27217395303640113		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.27217395303640113 | validation: 0.1631697595493372]
	TIME [epoch: 12.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824083983224367		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1824083983224367 | validation: 0.2609495951050616]
	TIME [epoch: 12.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17460956648037454		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.17460956648037454 | validation: 0.1482490314907897]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16113432175698938		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.16113432175698938 | validation: 0.21826530285126755]
	TIME [epoch: 12.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606251562120918		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.1606251562120918 | validation: 0.16181429220871843]
	TIME [epoch: 12.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680545575530735		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.1680545575530735 | validation: 0.23793754196974437]
	TIME [epoch: 12.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17739737788393609		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.17739737788393609 | validation: 0.1554197145426763]
	TIME [epoch: 12.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18088334656152683		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.18088334656152683 | validation: 0.2935498650944016]
	TIME [epoch: 12.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20644322311350283		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.20644322311350283 | validation: 0.16507201247012487]
	TIME [epoch: 12.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22103381759276097		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.22103381759276097 | validation: 0.25613993000665164]
	TIME [epoch: 12.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16725753112334815		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.16725753112334815 | validation: 0.13819080583350166]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15313599275054732		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.15313599275054732 | validation: 0.224838807450354]
	TIME [epoch: 12.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15976802308965027		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.15976802308965027 | validation: 0.17527100091848907]
	TIME [epoch: 12.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19019291353113651		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.19019291353113651 | validation: 0.3386669654708198]
	TIME [epoch: 12.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22145046056208473		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.22145046056208473 | validation: 0.1548396631143775]
	TIME [epoch: 12.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17541108282699658		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.17541108282699658 | validation: 0.2645678862400012]
	TIME [epoch: 12.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750336095312321		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1750336095312321 | validation: 0.14293095033728465]
	TIME [epoch: 12.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16640954462123822		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.16640954462123822 | validation: 0.20870469636100106]
	TIME [epoch: 12.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15860350614352572		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.15860350614352572 | validation: 0.14390377972353216]
	TIME [epoch: 12.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15026409563286233		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.15026409563286233 | validation: 0.26786307082723354]
	TIME [epoch: 12.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17198876398713714		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.17198876398713714 | validation: 0.15190445613080106]
	TIME [epoch: 12.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18195067578204935		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.18195067578204935 | validation: 0.3074208403784249]
	TIME [epoch: 12.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20251776279946945		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.20251776279946945 | validation: 0.17940148218210228]
	TIME [epoch: 12.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028274173907195		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.2028274173907195 | validation: 0.2679784029381449]
	TIME [epoch: 12.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19415400134428343		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.19415400134428343 | validation: 0.14303288306392567]
	TIME [epoch: 12.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163847409496382		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.163847409496382 | validation: 0.20630389357309287]
	TIME [epoch: 12.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14413730786589668		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.14413730786589668 | validation: 0.15151950358646382]
	TIME [epoch: 12.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15155779112396903		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.15155779112396903 | validation: 0.1863531137607696]
	TIME [epoch: 12.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519875369399445		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.1519875369399445 | validation: 0.16078360660718244]
	TIME [epoch: 12.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148188738511102		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.148188738511102 | validation: 0.14427897978264423]
	TIME [epoch: 12.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14728969698729708		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.14728969698729708 | validation: 0.22638452332390868]
	TIME [epoch: 12.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15942590510770274		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.15942590510770274 | validation: 0.1491260383485262]
	TIME [epoch: 12.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18004732134180423		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.18004732134180423 | validation: 0.4239949704805348]
	TIME [epoch: 12.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27270530219840977		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.27270530219840977 | validation: 0.15263675001607996]
	TIME [epoch: 12.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1817082893669558		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1817082893669558 | validation: 0.21897133788856182]
	TIME [epoch: 12.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14618703203904063		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.14618703203904063 | validation: 0.13642288653357926]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14330254941868392		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.14330254941868392 | validation: 0.1978612341417747]
	TIME [epoch: 12.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14685084862770306		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.14685084862770306 | validation: 0.13171054866633322]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1472181717533448		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.1472181717533448 | validation: 0.23810456450440018]
	TIME [epoch: 12.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1705246690902492		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.1705246690902492 | validation: 0.153765110005028]
	TIME [epoch: 12.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1877156258375827		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.1877156258375827 | validation: 0.37658419895713474]
	TIME [epoch: 12.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23570638503674032		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.23570638503674032 | validation: 0.1462759368694176]
	TIME [epoch: 12.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15260902290208048		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15260902290208048 | validation: 0.1802562846024355]
	TIME [epoch: 12.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14345054255497242		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.14345054255497242 | validation: 0.14576362198491793]
	TIME [epoch: 12.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462173236149176		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.1462173236149176 | validation: 0.22120605922268877]
	TIME [epoch: 12.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15925636788195324		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.15925636788195324 | validation: 0.1363511004856317]
	TIME [epoch: 12.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622240936544992		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.1622240936544992 | validation: 0.2044566226090824]
	TIME [epoch: 12.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538886410707559		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.1538886410707559 | validation: 0.13514117688936528]
	TIME [epoch: 12.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15840686776700566		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.15840686776700566 | validation: 0.3114219255513955]
	TIME [epoch: 12.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19745646059494718		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.19745646059494718 | validation: 0.18557492919035218]
	TIME [epoch: 12.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208284268828073		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.2208284268828073 | validation: 0.26265333569172816]
	TIME [epoch: 12.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756809510321811		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.1756809510321811 | validation: 0.1341207482408775]
	TIME [epoch: 12.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797514598223937		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.13797514598223937 | validation: 0.15062239629584398]
	TIME [epoch: 12.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13240481046577368		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.13240481046577368 | validation: 0.13346387271002227]
	TIME [epoch: 12.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13513068467010544		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.13513068467010544 | validation: 0.17409783938357182]
	TIME [epoch: 12.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356716624336849		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.1356716624336849 | validation: 0.13950205746320118]
	TIME [epoch: 12.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14610976179923854		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.14610976179923854 | validation: 0.24735863668948854]
	TIME [epoch: 12.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16570183214465728		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.16570183214465728 | validation: 0.1601579894775029]
	TIME [epoch: 12.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028685095133294		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.2028685095133294 | validation: 0.35320030967145666]
	TIME [epoch: 12.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22733503742697025		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.22733503742697025 | validation: 0.1519205845723799]
	TIME [epoch: 12.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800779596654935		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.1800779596654935 | validation: 0.1746387412126684]
	TIME [epoch: 12.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13512049004564922		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.13512049004564922 | validation: 0.1382302658979726]
	TIME [epoch: 12.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129810761918173		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.129810761918173 | validation: 0.1318438179131358]
	TIME [epoch: 12.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12693179202900237		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.12693179202900237 | validation: 0.21809390026530817]
	TIME [epoch: 12.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508094314745876		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.1508094314745876 | validation: 0.14838149467605405]
	TIME [epoch: 12.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19589578056086418		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.19589578056086418 | validation: 0.29607340806297316]
	TIME [epoch: 12.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814267909323462		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.18814267909323462 | validation: 0.12839542044887756]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15453537121906474		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.15453537121906474 | validation: 0.20405365322662206]
	TIME [epoch: 12.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14515265625928028		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.14515265625928028 | validation: 0.12877177420699693]
	TIME [epoch: 12.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13695277138451953		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.13695277138451953 | validation: 0.21365302465796787]
	TIME [epoch: 12.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467145242936778		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.1467145242936778 | validation: 0.1417627693305053]
	TIME [epoch: 12.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18992699426809906		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.18992699426809906 | validation: 0.35610215965375397]
	TIME [epoch: 12.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21979445380637785		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.21979445380637785 | validation: 0.1372732915847872]
	TIME [epoch: 12.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14440473415808025		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.14440473415808025 | validation: 0.15625102662888604]
	TIME [epoch: 12.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12818686473616747		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.12818686473616747 | validation: 0.15135853027174503]
	TIME [epoch: 12.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12739475474764755		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.12739475474764755 | validation: 0.1279193043655836]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327647307316891		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.12327647307316891 | validation: 0.14898616930699013]
	TIME [epoch: 12.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12604357664759333		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.12604357664759333 | validation: 0.13081163232465887]
	TIME [epoch: 12.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13394664890466268		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.13394664890466268 | validation: 0.22994772659938134]
	TIME [epoch: 12.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15447652355906594		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.15447652355906594 | validation: 0.16850148508757495]
	TIME [epoch: 12.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1796182693623948		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.1796182693623948 | validation: 0.2860961668918596]
	TIME [epoch: 12.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1729591396005536		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.1729591396005536 | validation: 0.18536573138923906]
	TIME [epoch: 12.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22724027270578476		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.22724027270578476 | validation: 0.27060461187218027]
	TIME [epoch: 12.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18282839567551773		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.18282839567551773 | validation: 0.13545870500645774]
	TIME [epoch: 12.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1423329010725998		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.1423329010725998 | validation: 0.19700896089175435]
	TIME [epoch: 12.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1391224801857559		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.1391224801857559 | validation: 0.12474415259280087]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1406387910809511		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.1406387910809511 | validation: 0.18638701670935015]
	TIME [epoch: 12.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13261935207715847		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.13261935207715847 | validation: 0.12241805590339538]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474865942725182		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.12474865942725182 | validation: 0.14871110152815908]
	TIME [epoch: 12.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12598335918923537		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.12598335918923537 | validation: 0.12228107828667115]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12777964716515364		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.12777964716515364 | validation: 0.22024002571095216]
	TIME [epoch: 12.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517444466107894		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.1517444466107894 | validation: 0.15826730358517138]
	TIME [epoch: 12.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1981572238091771		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.1981572238091771 | validation: 0.39547865296985396]
	TIME [epoch: 12.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24404473936603407		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.24404473936603407 | validation: 0.11881519971203125]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12588925850718338		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.12588925850718338 | validation: 0.13258156844155003]
	TIME [epoch: 12.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15281570522283341		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.15281570522283341 | validation: 0.3174988026479623]
	TIME [epoch: 12.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19941550692213558		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.19941550692213558 | validation: 0.13153347608619131]
	TIME [epoch: 12.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15794304952555493		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.15794304952555493 | validation: 0.1675164474704484]
	TIME [epoch: 12.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255320223627441		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.1255320223627441 | validation: 0.18157525361323473]
	TIME [epoch: 12.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1354194789429093		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.1354194789429093 | validation: 0.1594562349106757]
	TIME [epoch: 12.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1559680826664188		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.1559680826664188 | validation: 0.23186036662095041]
	TIME [epoch: 12.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15332770746557323		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.15332770746557323 | validation: 0.11427028510997493]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14254925464809087		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.14254925464809087 | validation: 0.2340501415029025]
	TIME [epoch: 12.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15483195931046512		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.15483195931046512 | validation: 0.13465736562936828]
	TIME [epoch: 12.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14870313200840246		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.14870313200840246 | validation: 0.15722575964210417]
	TIME [epoch: 12.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12784410793656034		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.12784410793656034 | validation: 0.11813742745631597]
	TIME [epoch: 12.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12947039711973807		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.12947039711973807 | validation: 0.17648504354368263]
	TIME [epoch: 12.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12774361980377552		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.12774361980377552 | validation: 0.11788482346698569]
	TIME [epoch: 12.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1370435646801746		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.1370435646801746 | validation: 0.19967015783625677]
	TIME [epoch: 12.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1417542549791635		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.1417542549791635 | validation: 0.1441839827825028]
	TIME [epoch: 12.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1805736038089767		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1805736038089767 | validation: 0.33160512162710953]
	TIME [epoch: 12.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18598315980730937		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.18598315980730937 | validation: 0.1257269449215824]
	TIME [epoch: 12.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14196245910644423		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.14196245910644423 | validation: 0.15142546752830724]
	TIME [epoch: 12.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11995866305625086		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.11995866305625086 | validation: 0.11874609732453792]
	TIME [epoch: 12.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11530374924234998		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.11530374924234998 | validation: 0.1572430947560962]
	TIME [epoch: 12.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12257971694641175		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.12257971694641175 | validation: 0.12126323475400741]
	TIME [epoch: 12.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.121632930941404		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.121632930941404 | validation: 0.20816646349807436]
	TIME [epoch: 12.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14222165799832562		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.14222165799832562 | validation: 0.14353037365975818]
	TIME [epoch: 12.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17145458034277078		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.17145458034277078 | validation: 0.3364382961394393]
	TIME [epoch: 12.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20656363525756852		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.20656363525756852 | validation: 0.12740636510996792]
	TIME [epoch: 12.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15512156567595628		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.15512156567595628 | validation: 0.19024945029822368]
	TIME [epoch: 12.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308408469091495		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.1308408469091495 | validation: 0.11912946640372342]
	TIME [epoch: 12.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11966342219481212		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.11966342219481212 | validation: 0.14202653564742557]
	TIME [epoch: 12.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11981415094015192		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.11981415094015192 | validation: 0.13625134056020624]
	TIME [epoch: 12.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12008001626952391		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.12008001626952391 | validation: 0.11616883625616806]
	TIME [epoch: 12.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12038510154093764		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.12038510154093764 | validation: 0.13062069968259596]
	TIME [epoch: 12.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601113240315579		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.11601113240315579 | validation: 0.15212428949143356]
	TIME [epoch: 12.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12498712739563046		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.12498712739563046 | validation: 0.12461922843103386]
	TIME [epoch: 12.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997086152524275		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.11997086152524275 | validation: 0.20749892778046305]
	TIME [epoch: 12.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379378542355659		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.1379378542355659 | validation: 0.1698004555444843]
	TIME [epoch: 12.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21267040146085564		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.21267040146085564 | validation: 0.3962068564866122]
	TIME [epoch: 12.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24399736510021722		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.24399736510021722 | validation: 0.11251203773251549]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12227615776260203		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.12227615776260203 | validation: 0.12295029688780855]
	TIME [epoch: 12.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12642128555691046		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.12642128555691046 | validation: 0.19863982761653998]
	TIME [epoch: 12.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411027505825195		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.1411027505825195 | validation: 0.126454885261002]
	TIME [epoch: 12.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524062447432204		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.1524062447432204 | validation: 0.24200847444103363]
	TIME [epoch: 12.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15025102359693404		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.15025102359693404 | validation: 0.12153070300239809]
	TIME [epoch: 12.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12924484824739926		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.12924484824739926 | validation: 0.13537611875920455]
	TIME [epoch: 12.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11789271980794727		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.11789271980794727 | validation: 0.15023058935361744]
	TIME [epoch: 12.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1218467311263684		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.1218467311263684 | validation: 0.12677330847393603]
	TIME [epoch: 12.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13109784785150974		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.13109784785150974 | validation: 0.22008786457223084]
	TIME [epoch: 12.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13578556895932864		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.13578556895932864 | validation: 0.12369144895238363]
	TIME [epoch: 12.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13404328255532322		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.13404328255532322 | validation: 0.2102441290140157]
	TIME [epoch: 12.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14335385859901245		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.14335385859901245 | validation: 0.1398039588717055]
	TIME [epoch: 12.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17604873894212958		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.17604873894212958 | validation: 0.22478428726517363]
	TIME [epoch: 12.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14931158063446423		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.14931158063446423 | validation: 0.11738243487277411]
	TIME [epoch: 12.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220749288821499		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.12220749288821499 | validation: 0.13062739535930495]
	TIME [epoch: 12.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11409564099832638		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.11409564099832638 | validation: 0.1151788853477807]
	TIME [epoch: 12.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1163418559882774		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.1163418559882774 | validation: 0.16727987589977392]
	TIME [epoch: 12.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315635775739958		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.12315635775739958 | validation: 0.12633194930454147]
	TIME [epoch: 12.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14003734288651468		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.14003734288651468 | validation: 0.21721422379755428]
	TIME [epoch: 12.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13760741029244206		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.13760741029244206 | validation: 0.11639047123727973]
	TIME [epoch: 12.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13328493121508278		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.13328493121508278 | validation: 0.17522685026856577]
	TIME [epoch: 12.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13337796831422996		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.13337796831422996 | validation: 0.1206602764496491]
	TIME [epoch: 12.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.141691683231824		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.141691683231824 | validation: 0.19815485619101578]
	TIME [epoch: 12.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13318653850145493		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.13318653850145493 | validation: 0.11451364855774754]
	TIME [epoch: 12.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254819608072322		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.1254819608072322 | validation: 0.18738778937735615]
	TIME [epoch: 12.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178198802454988		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.12178198802454988 | validation: 0.11267249300582155]
	TIME [epoch: 12.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13477077498423945		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.13477077498423945 | validation: 0.24125660551849437]
	TIME [epoch: 12.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14959829524054005		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.14959829524054005 | validation: 0.1374152168569344]
	TIME [epoch: 12.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15694787765925927		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.15694787765925927 | validation: 0.21340363278780297]
	TIME [epoch: 12.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14282176787077963		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.14282176787077963 | validation: 0.1163004024479196]
	TIME [epoch: 12.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13546753057111727		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.13546753057111727 | validation: 0.15845053500778036]
	TIME [epoch: 12.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12116394293432613		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.12116394293432613 | validation: 0.12268761972394998]
	TIME [epoch: 12.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007397342356581		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.11007397342356581 | validation: 0.10696761995939452]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12341076168667064		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.12341076168667064 | validation: 0.24117747926180633]
	TIME [epoch: 12.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464472405166013		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.1464472405166013 | validation: 0.13272901107393875]
	TIME [epoch: 12.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15807658792372253		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.15807658792372253 | validation: 0.21941973563810777]
	TIME [epoch: 12.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13500222621457844		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.13500222621457844 | validation: 0.11905386054722489]
	TIME [epoch: 12.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11360479810266137		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.11360479810266137 | validation: 0.1162760756727269]
	TIME [epoch: 12.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11333430873950537		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.11333430873950537 | validation: 0.13279561956776578]
	TIME [epoch: 12.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11365446908632057		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.11365446908632057 | validation: 0.11158493158488887]
	TIME [epoch: 12.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11499564691030965		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.11499564691030965 | validation: 0.17829677758706178]
	TIME [epoch: 12.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12185898762240326		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.12185898762240326 | validation: 0.1429571165702829]
	TIME [epoch: 12.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17133356615392528		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.17133356615392528 | validation: 0.3010323917128911]
	TIME [epoch: 12.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16994842509853883		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.16994842509853883 | validation: 0.11472254346806834]
	TIME [epoch: 12.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626567157769142		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.12626567157769142 | validation: 0.14801899163599055]
	TIME [epoch: 12.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11052825451203564		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.11052825451203564 | validation: 0.13216011230779642]
	TIME [epoch: 12.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11108968430922475		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.11108968430922475 | validation: 0.10799193718185429]
	TIME [epoch: 12.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260550428996757		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.1260550428996757 | validation: 0.22766779050238953]
	TIME [epoch: 12.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435943117682653		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.1435943117682653 | validation: 0.11159002835154644]
	TIME [epoch: 12.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12895996952076905		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.12895996952076905 | validation: 0.19130189980578224]
	TIME [epoch: 12.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12950467558014497		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.12950467558014497 | validation: 0.10858682508284301]
	TIME [epoch: 12.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345433020048568		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.1345433020048568 | validation: 0.24333328088259976]
	TIME [epoch: 12.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15218638694163147		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.15218638694163147 | validation: 0.10794737484715332]
	TIME [epoch: 12.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913607136604842		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.11913607136604842 | validation: 0.15603382514362604]
	TIME [epoch: 12.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1150333932320399		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.1150333932320399 | validation: 0.10450811956839896]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11400169795526327		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.11400169795526327 | validation: 0.13029435313160317]
	TIME [epoch: 12.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11237584697815992		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.11237584697815992 | validation: 0.11355341500124871]
	TIME [epoch: 12.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10676277370089049		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.10676277370089049 | validation: 0.12308431670402156]
	TIME [epoch: 12.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11151374763675649		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.11151374763675649 | validation: 0.11772123888980512]
	TIME [epoch: 12.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11592256555163967		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.11592256555163967 | validation: 0.14308645435772288]
	TIME [epoch: 12.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11462681250331917		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.11462681250331917 | validation: 0.11419467829484622]
	TIME [epoch: 12.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13464089320851255		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.13464089320851255 | validation: 0.2624348507981799]
	TIME [epoch: 12.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15756325058326787		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.15756325058326787 | validation: 0.10989736948898093]
	TIME [epoch: 12.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14277709821642393		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.14277709821642393 | validation: 0.19089329886645376]
	TIME [epoch: 12.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12267886447518629		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.12267886447518629 | validation: 0.10322334466206816]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12486575815201917		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.12486575815201917 | validation: 0.2356352080550675]
	TIME [epoch: 12.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14870054666118432		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.14870054666118432 | validation: 0.11595529289822432]
	TIME [epoch: 12.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12294006148112135		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.12294006148112135 | validation: 0.12987779153692636]
	TIME [epoch: 12.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10642897762093075		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.10642897762093075 | validation: 0.11906802472702574]
	TIME [epoch: 12.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11190575928932738		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.11190575928932738 | validation: 0.10915614568850014]
	TIME [epoch: 12.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1129397589222426		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.1129397589222426 | validation: 0.25787611470503186]
	TIME [epoch: 12.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16396930594420592		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.16396930594420592 | validation: 0.1173551145678605]
	TIME [epoch: 12.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14294993579285228		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.14294993579285228 | validation: 0.15501078751108416]
	TIME [epoch: 12.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11465588714939305		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.11465588714939305 | validation: 0.11939193963336554]
	TIME [epoch: 12.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1175137564306524		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.1175137564306524 | validation: 0.12335428094069767]
	TIME [epoch: 12.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10610697013145433		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.10610697013145433 | validation: 0.11045953817378247]
	TIME [epoch: 12.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136210404446545		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.11136210404446545 | validation: 0.12365211337897704]
	TIME [epoch: 12.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11053927672692088		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.11053927672692088 | validation: 0.11322497693670407]
	TIME [epoch: 12.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10572927665194151		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.10572927665194151 | validation: 0.11507434078256569]
	TIME [epoch: 12.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862227706399566		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.10862227706399566 | validation: 0.11085933611256048]
	TIME [epoch: 12.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356183781694749		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.10356183781694749 | validation: 0.10438158757149335]
	TIME [epoch: 12.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10419696295025711		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.10419696295025711 | validation: 0.14584452659501643]
	TIME [epoch: 12.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11809666777066571		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.11809666777066571 | validation: 0.11832430422489255]
	TIME [epoch: 12.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12690278113630585		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.12690278113630585 | validation: 0.287501140240824]
	TIME [epoch: 12.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17542040930857689		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.17542040930857689 | validation: 0.13930529182717816]
	TIME [epoch: 12.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17211143414921887		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.17211143414921887 | validation: 0.16887787788030703]
	TIME [epoch: 12.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12171233845094395		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.12171233845094395 | validation: 0.10931980305602257]
	TIME [epoch: 12.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1043226355295127		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.1043226355295127 | validation: 0.10494334570203906]
	TIME [epoch: 12.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10490681337351765		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.10490681337351765 | validation: 0.13543318033483634]
	TIME [epoch: 12.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10830997301169354		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.10830997301169354 | validation: 0.10520058936154496]
	TIME [epoch: 12.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11150769684173689		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.11150769684173689 | validation: 0.11935728257747535]
	TIME [epoch: 12.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1053086165326103		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1053086165326103 | validation: 0.14417795167027886]
	TIME [epoch: 12.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11381435973047571		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.11381435973047571 | validation: 0.11538871669163481]
	TIME [epoch: 12.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13551417682590822		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.13551417682590822 | validation: 0.2874076871071866]
	TIME [epoch: 12.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16972332571608345		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.16972332571608345 | validation: 0.10418348178102849]
	TIME [epoch: 12.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860493464095545		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.11860493464095545 | validation: 0.1369383782582554]
	TIME [epoch: 12.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732060722059464		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.10732060722059464 | validation: 0.10519224947277549]
	TIME [epoch: 12.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910922770540445		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.10910922770540445 | validation: 0.155471074716223]
	TIME [epoch: 12.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11104457191818895		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.11104457191818895 | validation: 0.11230205254424189]
	TIME [epoch: 12.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10467335146572612		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.10467335146572612 | validation: 0.106505142030831]
	TIME [epoch: 12.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743523110358033		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.10743523110358033 | validation: 0.20901077185701855]
	TIME [epoch: 12.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14000078437527472		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.14000078437527472 | validation: 0.10709736465561848]
	TIME [epoch: 12.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302083112684384		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.1302083112684384 | validation: 0.229435435353066]
	TIME [epoch: 12.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14612493884666528		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.14612493884666528 | validation: 0.09526331315702208]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147559741663614		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.1147559741663614 | validation: 0.10401458516275637]
	TIME [epoch: 12.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346143606559437		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.10346143606559437 | validation: 0.15714679472175497]
	TIME [epoch: 12.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131384644101439		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.1131384644101439 | validation: 0.10091461525162085]
	TIME [epoch: 12.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964656190326533		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.10964656190326533 | validation: 0.1403803498619449]
	TIME [epoch: 12.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10952830236530399		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.10952830236530399 | validation: 0.11259145389707093]
	TIME [epoch: 12.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783524781238697		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.11783524781238697 | validation: 0.1935352643764021]
	TIME [epoch: 12.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12491720323301359		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.12491720323301359 | validation: 0.09339339969622809]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11285913195543447		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.11285913195543447 | validation: 0.11793273119771315]
	TIME [epoch: 12.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161177799171284		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.10161177799171284 | validation: 0.10322222353683325]
	TIME [epoch: 12.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10074041094212444		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.10074041094212444 | validation: 0.13650604130578753]
	TIME [epoch: 12.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10917571405099309		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.10917571405099309 | validation: 0.09302131426751631]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12310054091765274		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.12310054091765274 | validation: 0.22466389779457985]
	TIME [epoch: 12.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430812746657229		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.1430812746657229 | validation: 0.09912474148666911]
	TIME [epoch: 12.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12454304168037311		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.12454304168037311 | validation: 0.15031288402251908]
	TIME [epoch: 12.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1100177825613402		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.1100177825613402 | validation: 0.10350392187104984]
	TIME [epoch: 12.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042392017032039		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.1042392017032039 | validation: 0.1146300645665393]
	TIME [epoch: 12.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10349811411145059		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.10349811411145059 | validation: 0.121775272330206]
	TIME [epoch: 12.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10044501722530683		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10044501722530683 | validation: 0.10423705235597645]
	TIME [epoch: 12.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1053835687027242		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.1053835687027242 | validation: 0.15395527596470882]
	TIME [epoch: 12.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11685065237890677		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.11685065237890677 | validation: 0.11624068049827625]
	TIME [epoch: 12.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13062007229900235		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.13062007229900235 | validation: 0.24032516565282852]
	TIME [epoch: 12.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14310902171732998		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.14310902171732998 | validation: 0.09533246087452758]
	TIME [epoch: 12.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932156807475776		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.10932156807475776 | validation: 0.11245038336302926]
	TIME [epoch: 12.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10512585381701799		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.10512585381701799 | validation: 0.16871229227124143]
	TIME [epoch: 12.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11668920136550102		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.11668920136550102 | validation: 0.09919441333994841]
	TIME [epoch: 12.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758329187408274		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.11758329187408274 | validation: 0.16176430527228094]
	TIME [epoch: 12.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532539668098483		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.11532539668098483 | validation: 0.09140312518056558]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10855842956812377		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.10855842956812377 | validation: 0.12281532392077642]
	TIME [epoch: 12.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10208613227996541		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.10208613227996541 | validation: 0.0983080958730146]
	TIME [epoch: 12.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042406612057416		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.1042406612057416 | validation: 0.182409583235112]
	TIME [epoch: 12.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126286327351617		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.126286327351617 | validation: 0.11243055592233792]
	TIME [epoch: 12.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270573803618916		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.1270573803618916 | validation: 0.16008557263885825]
	TIME [epoch: 12.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11249055926912954		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.11249055926912954 | validation: 0.08900842732132803]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10943478033706744		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.10943478033706744 | validation: 0.12646453239984662]
	TIME [epoch: 12.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10491231040438347		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.10491231040438347 | validation: 0.09685041769666497]
	TIME [epoch: 12.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977034094596436		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.0977034094596436 | validation: 0.11496179064157777]
	TIME [epoch: 12.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10099332799043137		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.10099332799043137 | validation: 0.10636008958920105]
	TIME [epoch: 12.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09904053933697371		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.09904053933697371 | validation: 0.11857156140929787]
	TIME [epoch: 12.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10026934381942067		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.10026934381942067 | validation: 0.10291603171285027]
	TIME [epoch: 12.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09780681272476603		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.09780681272476603 | validation: 0.10046899342720379]
	TIME [epoch: 12.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10180320295380253		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.10180320295380253 | validation: 0.17688065948442816]
	TIME [epoch: 12.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12525388073767232		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.12525388073767232 | validation: 0.1191588547575935]
	TIME [epoch: 12.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15761833364905992		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.15761833364905992 | validation: 0.28117375781899895]
	TIME [epoch: 12.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637563636975352		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.1637563636975352 | validation: 0.1081388893456627]
	TIME [epoch: 12.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10482084058479724		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.10482084058479724 | validation: 0.1048970412488413]
	TIME [epoch: 12.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12659083921302108		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.12659083921302108 | validation: 0.1651610768805552]
	TIME [epoch: 12.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519477176830442		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.11519477176830442 | validation: 0.10410471320846892]
	TIME [epoch: 12.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102102063514865		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.10102102063514865 | validation: 0.09462165224849271]
	TIME [epoch: 12.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10644256458295726		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.10644256458295726 | validation: 0.1716879877389731]
	TIME [epoch: 12.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11408724947883592		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.11408724947883592 | validation: 0.10450160632395507]
	TIME [epoch: 12.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10208561313344396		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.10208561313344396 | validation: 0.11100296284634004]
	TIME [epoch: 12.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908194882409824		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.09908194882409824 | validation: 0.1094261791193679]
	TIME [epoch: 12.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09930009094926785		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.09930009094926785 | validation: 0.14519725650859278]
	TIME [epoch: 12.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967190248370362		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.10967190248370362 | validation: 0.10555544024114144]
	TIME [epoch: 12.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764367517036119		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.11764367517036119 | validation: 0.14251423302306127]
	TIME [epoch: 12.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10664922987959585		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.10664922987959585 | validation: 0.09592731268067506]
	TIME [epoch: 12.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055306810245458		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.10055306810245458 | validation: 0.0950955641669787]
	TIME [epoch: 12.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070145497733314		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.10070145497733314 | validation: 0.1525805521498351]
	TIME [epoch: 12.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10919418177630293		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.10919418177630293 | validation: 0.09153353736989246]
	TIME [epoch: 12.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10141750292922709		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.10141750292922709 | validation: 0.14067564961888743]
	TIME [epoch: 12.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10528746246766253		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.10528746246766253 | validation: 0.10780572309112098]
	TIME [epoch: 12.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10581860373012265		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.10581860373012265 | validation: 0.12620006892594623]
	TIME [epoch: 12.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131681401990524		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.10131681401990524 | validation: 0.09642152527500979]
	TIME [epoch: 12.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10382056028703864		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.10382056028703864 | validation: 0.14214673538157727]
	TIME [epoch: 12.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10927114319431827		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.10927114319431827 | validation: 0.10590940510884202]
	TIME [epoch: 12.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12140802474428344		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.12140802474428344 | validation: 0.2163761857551303]
	TIME [epoch: 12.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14021215952422794		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.14021215952422794 | validation: 0.09334708714604097]
	TIME [epoch: 12.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11042335803954612		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.11042335803954612 | validation: 0.09689936868221272]
	TIME [epoch: 12.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10462623620602125		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.10462623620602125 | validation: 0.17907291090059752]
	TIME [epoch: 12.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1215976775511839		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.1215976775511839 | validation: 0.1054681120619859]
	TIME [epoch: 12.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189275040046709		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.1189275040046709 | validation: 0.13217925597225882]
	TIME [epoch: 12.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092652482061594		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.10092652482061594 | validation: 0.10622019746431802]
	TIME [epoch: 12.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09289191056344245		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.09289191056344245 | validation: 0.09494969513468134]
	TIME [epoch: 12.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298041481786119		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.10298041481786119 | validation: 0.17975907509757516]
	TIME [epoch: 12.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11879771151266802		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.11879771151266802 | validation: 0.09706949860219703]
	TIME [epoch: 12.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875981406398313		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.10875981406398313 | validation: 0.10396750675255484]
	TIME [epoch: 12.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09520097624740714		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.09520097624740714 | validation: 0.1158702206511991]
	TIME [epoch: 12.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781383895750187		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.09781383895750187 | validation: 0.09383320674331405]
	TIME [epoch: 12.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11215603254227098		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.11215603254227098 | validation: 0.14701354963567562]
	TIME [epoch: 12.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11058591040177927		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.11058591040177927 | validation: 0.09650756618089085]
	TIME [epoch: 12.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09913721524140189		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.09913721524140189 | validation: 0.10614193425411861]
	TIME [epoch: 12.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09370062103185539		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.09370062103185539 | validation: 0.10535939933300563]
	TIME [epoch: 12.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977932806401572		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.0977932806401572 | validation: 0.10845468214911654]
	TIME [epoch: 12.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09612054827276408		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.09612054827276408 | validation: 0.12274031091318514]
	TIME [epoch: 12.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968266044043539		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.09968266044043539 | validation: 0.10063596026480802]
	TIME [epoch: 12.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11843516551496876		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.11843516551496876 | validation: 0.23756798724591688]
	TIME [epoch: 12.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140183861403126		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.140183861403126 | validation: 0.09172199261105475]
	TIME [epoch: 12.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10654243100784377		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.10654243100784377 | validation: 0.12756720034665192]
	TIME [epoch: 12.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10353683402782221		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.10353683402782221 | validation: 0.10256792936896582]
	TIME [epoch: 12.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584686765306083		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.09584686765306083 | validation: 0.10114203601396926]
	TIME [epoch: 12.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09451598088711383		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.09451598088711383 | validation: 0.11036734946262615]
	TIME [epoch: 12.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09333966579328845		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.09333966579328845 | validation: 0.09179279062983622]
	TIME [epoch: 12.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09901498811306952		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.09901498811306952 | validation: 0.12168134554868558]
	TIME [epoch: 12.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09784661672747255		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.09784661672747255 | validation: 0.09022421393172686]
	TIME [epoch: 12.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10442497191977804		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.10442497191977804 | validation: 0.1751187135869163]
	TIME [epoch: 12.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11503498950730613		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.11503498950730613 | validation: 0.09506045096269865]
	TIME [epoch: 12.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742443568954206		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.10742443568954206 | validation: 0.1279618622906611]
	TIME [epoch: 12.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282348268198593		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.10282348268198593 | validation: 0.08576831517312106]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09828980247040946		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.09828980247040946 | validation: 0.1084412453922212]
	TIME [epoch: 12.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09617090792088628		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.09617090792088628 | validation: 0.10355484752805333]
	TIME [epoch: 12.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304137098846291		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.09304137098846291 | validation: 0.1074082351820202]
	TIME [epoch: 12.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10000151657096011		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.10000151657096011 | validation: 0.10224154816497857]
	TIME [epoch: 12.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09100395695236262		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.09100395695236262 | validation: 0.11342114838262812]
	TIME [epoch: 12.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09300451048854712		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.09300451048854712 | validation: 0.10562500218541662]
	TIME [epoch: 12.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997236236876963		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0997236236876963 | validation: 0.11706388546059401]
	TIME [epoch: 12.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508163178428923		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.09508163178428923 | validation: 0.09180846996650696]
	TIME [epoch: 12.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12206162608429204		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.12206162608429204 | validation: 0.23710974510677918]
	TIME [epoch: 12.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14272223525565197		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.14272223525565197 | validation: 0.09488070828604617]
	TIME [epoch: 12.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494836895067152		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.11494836895067152 | validation: 0.10006034979857387]
	TIME [epoch: 12.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09581537451309943		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.09581537451309943 | validation: 0.12829918385194264]
	TIME [epoch: 12.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09800369988359119		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.09800369988359119 | validation: 0.0897997927465526]
	TIME [epoch: 12.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585323399022793		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.10585323399022793 | validation: 0.14493452421830857]
	TIME [epoch: 12.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152035292337103		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.10152035292337103 | validation: 0.09425698420984224]
	TIME [epoch: 12.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618983769548452		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.09618983769548452 | validation: 0.08991964010056677]
	TIME [epoch: 12.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507408980593873		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.09507408980593873 | validation: 0.10849113618482271]
	TIME [epoch: 12.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09019547287479374		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.09019547287479374 | validation: 0.08995188867496111]
	TIME [epoch: 12.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09314580530350616		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.09314580530350616 | validation: 0.09227442804802782]
	TIME [epoch: 12.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671496174347785		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.09671496174347785 | validation: 0.10465710121722958]
	TIME [epoch: 12.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222789144584985		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.09222789144584985 | validation: 0.09820789647478234]
	TIME [epoch: 12.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270901481339434		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.09270901481339434 | validation: 0.137308923753202]
	TIME [epoch: 12.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10972561600518942		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.10972561600518942 | validation: 0.1140126164442462]
	TIME [epoch: 12.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487610072767572		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.1487610072767572 | validation: 0.14058014674418726]
	TIME [epoch: 12.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09906564172220751		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.09906564172220751 | validation: 0.10105640287051726]
	TIME [epoch: 12.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09278870431506478		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.09278870431506478 | validation: 0.09089003326553736]
	TIME [epoch: 12.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022668475150217		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.1022668475150217 | validation: 0.13437872086858943]
	TIME [epoch: 12.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10327574013149869		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.10327574013149869 | validation: 0.09189429706289742]
	TIME [epoch: 12.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09409276621465036		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.09409276621465036 | validation: 0.09349679539464438]
	TIME [epoch: 12.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195581150705656		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.09195581150705656 | validation: 0.11119996246296221]
	TIME [epoch: 12.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09482930682604648		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.09482930682604648 | validation: 0.08705983174358263]
	TIME [epoch: 12.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10205700036186506		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.10205700036186506 | validation: 0.14079840813176467]
	TIME [epoch: 12.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613118698777864		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.10613118698777864 | validation: 0.0826252628088736]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031051777849287		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.10031051777849287 | validation: 0.10679147819318094]
	TIME [epoch: 12.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381834738459383		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.09381834738459383 | validation: 0.12778981883939797]
	TIME [epoch: 12.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09589541468435751		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.09589541468435751 | validation: 0.08485272212664088]
	TIME [epoch: 12.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09979204679344285		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.09979204679344285 | validation: 0.12986663259271075]
	TIME [epoch: 12.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0996675653679859		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.0996675653679859 | validation: 0.09697248197632226]
	TIME [epoch: 12.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11220653579659008		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.11220653579659008 | validation: 0.13298253264279516]
	TIME [epoch: 12.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09775669300811657		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.09775669300811657 | validation: 0.10078647829861127]
	TIME [epoch: 12.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473231006122726		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.09473231006122726 | validation: 0.08958694408386125]
	TIME [epoch: 12.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473313002005032		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.09473313002005032 | validation: 0.1003088461342757]
	TIME [epoch: 12.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09367881319234363		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.09367881319234363 | validation: 0.09525585042969337]
	TIME [epoch: 12.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09025095698849862		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.09025095698849862 | validation: 0.10916952426401441]
	TIME [epoch: 12.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294834789745732		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.09294834789745732 | validation: 0.08933932593691707]
	TIME [epoch: 12.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346624157217223		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.10346624157217223 | validation: 0.1979345075115212]
	TIME [epoch: 12.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12482574389229228		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.12482574389229228 | validation: 0.0889167475652414]
	TIME [epoch: 12.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779025009312609		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.09779025009312609 | validation: 0.09340091407243618]
	TIME [epoch: 12.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185216992481605		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.09185216992481605 | validation: 0.12925891360133296]
	TIME [epoch: 12.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09860611589885948		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.09860611589885948 | validation: 0.08781534668860375]
	TIME [epoch: 12.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212939843714714		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.11212939843714714 | validation: 0.1389577549250579]
	TIME [epoch: 12.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304904929613791		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.10304904929613791 | validation: 0.1042310792252158]
	TIME [epoch: 12.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056794524280914		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.09056794524280914 | validation: 0.08795357378788171]
	TIME [epoch: 12.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11236712564420223		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.11236712564420223 | validation: 0.12216370403409649]
	TIME [epoch: 12.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260927294603832		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.10260927294603832 | validation: 0.11846913781471567]
	TIME [epoch: 12.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084705005838428		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.09084705005838428 | validation: 0.09270052449031754]
	TIME [epoch: 12.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09252672779157665		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.09252672779157665 | validation: 0.11326586933303538]
	TIME [epoch: 12.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263498460305805		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.09263498460305805 | validation: 0.09213589224907999]
	TIME [epoch: 12.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942836637943359		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0942836637943359 | validation: 0.10214933754713103]
	TIME [epoch: 12.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09029497916497349		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.09029497916497349 | validation: 0.09415562521634145]
	TIME [epoch: 12.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996302886381853		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.08996302886381853 | validation: 0.125533352249407]
	TIME [epoch: 12.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973465527468116		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0973465527468116 | validation: 0.08979814605061186]
	TIME [epoch: 12.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144330178555112		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.11144330178555112 | validation: 0.10384286773029901]
	TIME [epoch: 12.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08765462718431373		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.08765462718431373 | validation: 0.08216360546072701]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864067012335994		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.08864067012335994 | validation: 0.09090689874906435]
	TIME [epoch: 12.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921101864222494		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.0921101864222494 | validation: 0.16541853746885743]
	TIME [epoch: 12.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10595216382079393		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.10595216382079393 | validation: 0.08774752123225267]
	TIME [epoch: 12.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817879093197624		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.10817879093197624 | validation: 0.16196150161773734]
	TIME [epoch: 12.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11664166275908397		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.11664166275908397 | validation: 0.09133729629218279]
	TIME [epoch: 12.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08558397693797254		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.08558397693797254 | validation: 0.08536811389856315]
	TIME [epoch: 12.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0926303595957424		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.0926303595957424 | validation: 0.10594826066065591]
	TIME [epoch: 12.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0904037529188432		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.0904037529188432 | validation: 0.08639203472504385]
	TIME [epoch: 12.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339896639845012		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.09339896639845012 | validation: 0.09647695226472867]
	TIME [epoch: 12.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090202681618745		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.090202681618745 | validation: 0.08957892861597205]
	TIME [epoch: 12.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909229296846384		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.08909229296846384 | validation: 0.0916954946675742]
	TIME [epoch: 12.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294513881725813		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.09294513881725813 | validation: 0.08537225076480309]
	TIME [epoch: 12.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09144315162534589		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.09144315162534589 | validation: 0.14609058933568833]
	TIME [epoch: 12.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147926973214988		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.10147926973214988 | validation: 0.08451043829907395]
	TIME [epoch: 12.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982767545553226		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0982767545553226 | validation: 0.10440907163003121]
	TIME [epoch: 12.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918148543978844		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.0918148543978844 | validation: 0.12063556395507068]
	TIME [epoch: 193 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09198470825291599		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.09198470825291599 | validation: 0.07901065614497765]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09371308543214442		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.09371308543214442 | validation: 0.15015789495528017]
	TIME [epoch: 25.9 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10454066885999465		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.10454066885999465 | validation: 0.08307790157135611]
	TIME [epoch: 25.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690325619903419		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.09690325619903419 | validation: 0.09473436016945969]
	TIME [epoch: 25.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710187289306771		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.08710187289306771 | validation: 0.13129901420211745]
	TIME [epoch: 25.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618209827222252		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.09618209827222252 | validation: 0.09101506097676863]
	TIME [epoch: 25.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10649689986817988		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.10649689986817988 | validation: 0.12068141450091702]
	TIME [epoch: 25.9 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09285282147379756		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.09285282147379756 | validation: 0.08453401660596527]
	TIME [epoch: 25.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0908966036485711		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.0908966036485711 | validation: 0.09798562133806338]
	TIME [epoch: 25.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08495501913136363		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.08495501913136363 | validation: 0.10130530453197295]
	TIME [epoch: 25.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08988814269110684		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.08988814269110684 | validation: 0.09370959234664013]
	TIME [epoch: 25.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504458021996527		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.09504458021996527 | validation: 0.12615377855686494]
	TIME [epoch: 25.9 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354701818252835		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.09354701818252835 | validation: 0.08227819707150397]
	TIME [epoch: 26 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372611400732445		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.09372611400732445 | validation: 0.12107385140259641]
	TIME [epoch: 25.9 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966664469182144		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.0966664469182144 | validation: 0.0821121539345811]
	TIME [epoch: 25.9 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09246401756494434		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.09246401756494434 | validation: 0.14623670199381836]
	TIME [epoch: 25.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702770966364472		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.10702770966364472 | validation: 0.0828522910770334]
	TIME [epoch: 25.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576495412335581		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.09576495412335581 | validation: 0.14371361488639073]
	TIME [epoch: 25.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1001876608424033		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.1001876608424033 | validation: 0.07911162783063097]
	TIME [epoch: 25.9 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08719241698636047		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.08719241698636047 | validation: 0.0808975165187717]
	TIME [epoch: 25.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09230297117584098		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.09230297117584098 | validation: 0.1265698093248788]
	TIME [epoch: 26 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09624821069731475		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.09624821069731475 | validation: 0.11591013688516934]
	TIME [epoch: 25.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08775758566538955		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.08775758566538955 | validation: 0.08606464478298065]
	TIME [epoch: 25.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08800411207089369		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.08800411207089369 | validation: 0.09024094154873859]
	TIME [epoch: 25.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816850717134553		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.08816850717134553 | validation: 0.09136013113619386]
	TIME [epoch: 25.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09017108672411286		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.09017108672411286 | validation: 0.08108414788337699]
	TIME [epoch: 25.9 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09202462268521144		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.09202462268521144 | validation: 0.10793407543922662]
	TIME [epoch: 26 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08874468317047401		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.08874468317047401 | validation: 0.07406683377611076]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1029.pth
	Model improved!!!
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372819843164237		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.09372819843164237 | validation: 0.10733253393467512]
	TIME [epoch: 25.9 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09389157908504757		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.09389157908504757 | validation: 0.08489682522586584]
	TIME [epoch: 25.9 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937741497320636		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.0937741497320636 | validation: 0.1295603308957267]
	TIME [epoch: 25.9 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09328906495385046		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.09328906495385046 | validation: 0.08458638528621179]
	TIME [epoch: 25.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09277475600029615		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.09277475600029615 | validation: 0.1324851661248231]
	TIME [epoch: 25.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09437061081067512		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.09437061081067512 | validation: 0.08770063924281771]
	TIME [epoch: 25.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925727466558073		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0925727466558073 | validation: 0.1252670149604961]
	TIME [epoch: 26 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936600989516122		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.0936600989516122 | validation: 0.08160799637939914]
	TIME [epoch: 25.9 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870898746845804		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.0870898746845804 | validation: 0.08875889047339164]
	TIME [epoch: 25.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711736959201151		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.08711736959201151 | validation: 0.09030496674233701]
	TIME [epoch: 25.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861190064103706		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.08861190064103706 | validation: 0.09140070272575927]
	TIME [epoch: 25.9 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08883506835581333		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.08883506835581333 | validation: 0.11763285883882571]
	TIME [epoch: 25.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09192882364288618		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.09192882364288618 | validation: 0.08041643834469592]
	TIME [epoch: 25.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10296197344787796		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.10296197344787796 | validation: 0.11932572728881558]
	TIME [epoch: 25.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219257966751748		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.09219257966751748 | validation: 0.08401573343370114]
	TIME [epoch: 25.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09332682213345414		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.09332682213345414 | validation: 0.12075572084749792]
	TIME [epoch: 25.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003943457891682		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.09003943457891682 | validation: 0.09834689561924947]
	TIME [epoch: 25.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526294974526356		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.08526294974526356 | validation: 0.09726490978076595]
	TIME [epoch: 25.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08756464630835634		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.08756464630835634 | validation: 0.09078219655778637]
	TIME [epoch: 25.9 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08974950154059996		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.08974950154059996 | validation: 0.07635374489807525]
	TIME [epoch: 25.9 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861225365582283		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.0861225365582283 | validation: 0.12763040644734575]
	TIME [epoch: 25.9 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0933480297877139		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.0933480297877139 | validation: 0.0861321149354117]
	TIME [epoch: 26 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10930740606630206		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.10930740606630206 | validation: 0.1262311807620132]
	TIME [epoch: 25.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668228768693535		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.08668228768693535 | validation: 0.10323229029788737]
	TIME [epoch: 25.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08548509306037211		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.08548509306037211 | validation: 0.08313180100158964]
	TIME [epoch: 25.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10496417112450863		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.10496417112450863 | validation: 0.11457751036911319]
	TIME [epoch: 25.9 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950496874850202		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.0950496874850202 | validation: 0.09782125413983085]
	TIME [epoch: 25.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08485256263927048		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.08485256263927048 | validation: 0.08739510828906748]
	TIME [epoch: 25.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870502715034508		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.0870502715034508 | validation: 0.10533830204966609]
	TIME [epoch: 25.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08836614221868541		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.08836614221868541 | validation: 0.0903200048972652]
	TIME [epoch: 25.9 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08760055615141717		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.08760055615141717 | validation: 0.08696592825704935]
	TIME [epoch: 25.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08536441485466625		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.08536441485466625 | validation: 0.10530716016352869]
	TIME [epoch: 25.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08801187460192644		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.08801187460192644 | validation: 0.08318163310880096]
	TIME [epoch: 25.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0864761941125148		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.0864761941125148 | validation: 0.10413828990979797]
	TIME [epoch: 25.9 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084590510735026		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.084590510735026 | validation: 0.08218034767910509]
	TIME [epoch: 25.9 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08621114879147258		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.08621114879147258 | validation: 0.1120278938028732]
	TIME [epoch: 25.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09109893672056762		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.09109893672056762 | validation: 0.07969674034136685]
	TIME [epoch: 26 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172687305908099		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.09172687305908099 | validation: 0.09131076738388438]
	TIME [epoch: 25.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08722570432474795		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.08722570432474795 | validation: 0.08298449272277163]
	TIME [epoch: 26 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08605947146749916		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.08605947146749916 | validation: 0.14869071612918544]
	TIME [epoch: 25.9 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10720004446818085		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.10720004446818085 | validation: 0.07324046557807354]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1070.pth
	Model improved!!!
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386282653742972		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.09386282653742972 | validation: 0.08596185382525216]
	TIME [epoch: 25.9 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779240638947093		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.08779240638947093 | validation: 0.13157994870161877]
	TIME [epoch: 25.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09078460272691351		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.09078460272691351 | validation: 0.08463840563418674]
	TIME [epoch: 25.9 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08691350993251439		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.08691350993251439 | validation: 0.08439344259648035]
	TIME [epoch: 25.9 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08353072025620582		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.08353072025620582 | validation: 0.08030118173563885]
	TIME [epoch: 25.9 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848722485492211		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.0848722485492211 | validation: 0.08473957150275435]
	TIME [epoch: 25.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08575432716073199		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.08575432716073199 | validation: 0.09092482661247364]
	TIME [epoch: 25.9 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455190422909029		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.08455190422909029 | validation: 0.09260227087632406]
	TIME [epoch: 26 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08287367926425414		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.08287367926425414 | validation: 0.08320725395142076]
	TIME [epoch: 26 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08709495723602981		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.08709495723602981 | validation: 0.09948959769190022]
	TIME [epoch: 25.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0866060709589302		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.0866060709589302 | validation: 0.08363251755119586]
	TIME [epoch: 25.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08460349561668572		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.08460349561668572 | validation: 0.10700425752106067]
	TIME [epoch: 25.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08727025967160057		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.08727025967160057 | validation: 0.08918496471806221]
	TIME [epoch: 25.9 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10947722765473993		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.10947722765473993 | validation: 0.13941333228855163]
	TIME [epoch: 25.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09912643422721837		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.09912643422721837 | validation: 0.09354527797934631]
	TIME [epoch: 25.9 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08135935538423697		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.08135935538423697 | validation: 0.07716658727158845]
	TIME [epoch: 26 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09114114433547373		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.09114114433547373 | validation: 0.09959537167393134]
	TIME [epoch: 25.9 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08745353511257495		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.08745353511257495 | validation: 0.08132240637469826]
	TIME [epoch: 25.9 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697528649408275		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.08697528649408275 | validation: 0.08960667196092312]
	TIME [epoch: 25.9 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08478623130701518		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.08478623130701518 | validation: 0.0775654302131839]
	TIME [epoch: 25.9 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08746755361906683		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.08746755361906683 | validation: 0.11828454338837907]
	TIME [epoch: 25.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09260138623851759		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.09260138623851759 | validation: 0.08283351446938753]
	TIME [epoch: 25.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09686337009160574		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.09686337009160574 | validation: 0.138731570535254]
	TIME [epoch: 25.9 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943499434254328		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.0943499434254328 | validation: 0.08576229950432451]
	TIME [epoch: 25.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08368773108564025		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.08368773108564025 | validation: 0.08595246520739437]
	TIME [epoch: 25.9 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826149164621662		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.08826149164621662 | validation: 0.09318758541474526]
	TIME [epoch: 25.9 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376005983208178		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.08376005983208178 | validation: 0.10199613232104668]
	TIME [epoch: 25.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08740023016518177		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.08740023016518177 | validation: 0.0824981023292063]
	TIME [epoch: 25.9 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913950900160423		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.0913950900160423 | validation: 0.11916961210696826]
	TIME [epoch: 25.9 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840724349214468		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.0840724349214468 | validation: 0.08315867709496365]
	TIME [epoch: 25.9 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592276477477757		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.08592276477477757 | validation: 0.0942795967408131]
	TIME [epoch: 25.9 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115969601177525		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.08115969601177525 | validation: 0.09541889097163872]
	TIME [epoch: 25.9 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08370341547963026		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.08370341547963026 | validation: 0.11212754047610679]
	TIME [epoch: 25.9 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0923093130505711		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.0923093130505711 | validation: 0.08040361081325792]
	TIME [epoch: 25.9 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829004374840987		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.0829004374840987 | validation: 0.09390870456022765]
	TIME [epoch: 25.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826153528884369		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.0826153528884369 | validation: 0.10885583130685866]
	TIME [epoch: 25.9 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09529570936632466		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.09529570936632466 | validation: 0.07421534418433172]
	TIME [epoch: 25.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09008731868668594		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.09008731868668594 | validation: 0.13169450488994744]
	TIME [epoch: 25.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09780571196055689		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.09780571196055689 | validation: 0.0862383756401651]
	TIME [epoch: 25.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620059481073364		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.08620059481073364 | validation: 0.1205876386298221]
	TIME [epoch: 25.9 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0851344264211517		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.0851344264211517 | validation: 0.07531707757620196]
	TIME [epoch: 25.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841408920313457		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.0841408920313457 | validation: 0.07899051229148922]
	TIME [epoch: 25.9 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587920378070889		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.08587920378070889 | validation: 0.09765643658192898]
	TIME [epoch: 25.9 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08240899337186097		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.08240899337186097 | validation: 0.10194240969228607]
	TIME [epoch: 25.9 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08339203634761592		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.08339203634761592 | validation: 0.07428429792690523]
	TIME [epoch: 25.9 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470199695631492		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.08470199695631492 | validation: 0.13106192062651809]
	TIME [epoch: 25.9 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09155766676699138		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.09155766676699138 | validation: 0.06961629489250142]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1117.pth
	Model improved!!!
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08669567717839162		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.08669567717839162 | validation: 0.08232558049056406]
	TIME [epoch: 25.9 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08517409322672968		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.08517409322672968 | validation: 0.10362247346850978]
	TIME [epoch: 25.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08439513784935057		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.08439513784935057 | validation: 0.07999252041134902]
	TIME [epoch: 25.8 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247789812797767		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.09247789812797767 | validation: 0.09174202585836244]
	TIME [epoch: 25.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436000549406178		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.08436000549406178 | validation: 0.12926267735406816]
	TIME [epoch: 25.8 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09190557013435706		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.09190557013435706 | validation: 0.08291064934765613]
	TIME [epoch: 25.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08917355007667667		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.08917355007667667 | validation: 0.08899422204680782]
	TIME [epoch: 25.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824883031387471		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.0824883031387471 | validation: 0.11373942836405507]
	TIME [epoch: 25.9 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09460481312662461		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.09460481312662461 | validation: 0.07844354489075225]
	TIME [epoch: 25.9 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08857451439691527		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.08857451439691527 | validation: 0.07326391236823014]
	TIME [epoch: 26 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08416057508327113		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.08416057508327113 | validation: 0.10980803072662204]
	TIME [epoch: 25.9 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08420254417805684		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.08420254417805684 | validation: 0.08399810400146711]
	TIME [epoch: 26 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128375619805599		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.08128375619805599 | validation: 0.09251781312309389]
	TIME [epoch: 25.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841399911032418		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.0841399911032418 | validation: 0.07731861412460128]
	TIME [epoch: 25.9 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852390198402771		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.0852390198402771 | validation: 0.10805271401130155]
	TIME [epoch: 25.9 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09255233735410698		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.09255233735410698 | validation: 0.07067248293117818]
	TIME [epoch: 25.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08343943234688206		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.08343943234688206 | validation: 0.08785029546119057]
	TIME [epoch: 25.9 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040109801625572		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.08040109801625572 | validation: 0.1069466307436242]
	TIME [epoch: 25.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08676684255012251		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.08676684255012251 | validation: 0.07355617592798498]
	TIME [epoch: 25.9 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08458520195489495		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.08458520195489495 | validation: 0.09192899691098316]
	TIME [epoch: 25.9 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08451449942866258		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.08451449942866258 | validation: 0.09889558323427992]
	TIME [epoch: 25.9 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08194581184158949		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.08194581184158949 | validation: 0.07789644544922446]
	TIME [epoch: 25.9 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0814916597459752		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.0814916597459752 | validation: 0.08160024620586731]
	TIME [epoch: 25.9 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08131755329644925		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.08131755329644925 | validation: 0.1085874637273948]
	TIME [epoch: 25.9 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08469748096487546		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.08469748096487546 | validation: 0.08000473949395412]
	TIME [epoch: 25.9 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08648823322426605		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.08648823322426605 | validation: 0.0857705113888671]
	TIME [epoch: 25.9 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08317451588791158		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.08317451588791158 | validation: 0.10186369577930823]
	TIME [epoch: 25.9 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08650482207730761		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.08650482207730761 | validation: 0.0729504464516512]
	TIME [epoch: 25.9 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08973630729244592		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.08973630729244592 | validation: 0.09133864097052422]
	TIME [epoch: 25.9 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08381101948638911		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.08381101948638911 | validation: 0.10309693303160912]
	TIME [epoch: 25.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269635036986811		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.08269635036986811 | validation: 0.07585211427032484]
	TIME [epoch: 25.9 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815474283847876		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.0815474283847876 | validation: 0.07348027311833623]
	TIME [epoch: 25.9 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08530245484234004		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.08530245484234004 | validation: 0.08300225259901561]
	TIME [epoch: 25.9 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08293151167129033		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.08293151167129033 | validation: 0.07364215504817424]
	TIME [epoch: 25.9 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08654966735852203		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.08654966735852203 | validation: 0.1287739075658438]
	TIME [epoch: 25.9 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0967090143852941		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.0967090143852941 | validation: 0.0786364542603985]
	TIME [epoch: 25.9 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08715587390454772		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.08715587390454772 | validation: 0.0817625364451361]
	TIME [epoch: 25.9 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08496869627913484		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.08496869627913484 | validation: 0.09705291132959011]
	TIME [epoch: 25.9 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07856441231226796		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.07856441231226796 | validation: 0.09594062163498948]
	TIME [epoch: 25.8 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08404778735303733		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.08404778735303733 | validation: 0.07966311384549472]
	TIME [epoch: 25.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936595413571743		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.0936595413571743 | validation: 0.10288032373704376]
	TIME [epoch: 25.8 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843307008309671		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.0843307008309671 | validation: 0.08126232423517883]
	TIME [epoch: 25.9 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08398359030141034		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.08398359030141034 | validation: 0.07808213163625051]
	TIME [epoch: 25.9 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795750573331609		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.0795750573331609 | validation: 0.07466560436052128]
	TIME [epoch: 25.9 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329770253855422		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.08329770253855422 | validation: 0.08574445187665143]
	TIME [epoch: 25.9 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08126851239011376		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.08126851239011376 | validation: 0.07945212175246108]
	TIME [epoch: 25.9 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826687565145096		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.0826687565145096 | validation: 0.07039630766049103]
	TIME [epoch: 25.8 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07996240555960092		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.07996240555960092 | validation: 0.0821357532318025]
	TIME [epoch: 25.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777728450074796		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.0777728450074796 | validation: 0.07651323121868592]
	TIME [epoch: 25.8 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07997776893781881		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.07997776893781881 | validation: 0.09557313676590992]
	TIME [epoch: 25.8 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139811921451635		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.08139811921451635 | validation: 0.07785255608898523]
	TIME [epoch: 25.9 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08224098548403415		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.08224098548403415 | validation: 0.07821521994961188]
	TIME [epoch: 25.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08347021321874518		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.08347021321874518 | validation: 0.10703568002885291]
	TIME [epoch: 25.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08415863143621742		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.08415863143621742 | validation: 0.06966360982003482]
	TIME [epoch: 25.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08421949402618321		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.08421949402618321 | validation: 0.08932095468178002]
	TIME [epoch: 25.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08279990810943717		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.08279990810943717 | validation: 0.07509677587663575]
	TIME [epoch: 25.9 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08187859444210689		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.08187859444210689 | validation: 0.13447823630411287]
	TIME [epoch: 25.9 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507970495147454		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.09507970495147454 | validation: 0.07881722145199328]
	TIME [epoch: 25.9 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130263086269654		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.08130263086269654 | validation: 0.07889993305043902]
	TIME [epoch: 25.9 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08240677821072684		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.08240677821072684 | validation: 0.11021763653875812]
	TIME [epoch: 25.8 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466859437020151		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.08466859437020151 | validation: 0.0758967771746057]
	TIME [epoch: 25.9 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08239273201950686		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.08239273201950686 | validation: 0.11821364660510834]
	TIME [epoch: 25.9 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08830137296062247		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.08830137296062247 | validation: 0.076336875826757]
	TIME [epoch: 25.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08073565923107878		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.08073565923107878 | validation: 0.07849410195040069]
	TIME [epoch: 25.9 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838119264208034		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.0838119264208034 | validation: 0.10271056989257249]
	TIME [epoch: 25.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08529506192085666		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.08529506192085666 | validation: 0.08253343843884532]
	TIME [epoch: 25.9 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07700016736661051		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.07700016736661051 | validation: 0.07033974536560748]
	TIME [epoch: 25.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939260426215981		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.07939260426215981 | validation: 0.07803364154498851]
	TIME [epoch: 25.8 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08134295577084309		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.08134295577084309 | validation: 0.11263003521006776]
	TIME [epoch: 25.9 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344363501274014		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.08344363501274014 | validation: 0.07297367233025294]
	TIME [epoch: 25.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08123616637688141		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.08123616637688141 | validation: 0.07536520452055054]
	TIME [epoch: 25.9 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07802057600029881		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.07802057600029881 | validation: 0.09988890708453924]
	TIME [epoch: 25.9 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08032649894471149		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.08032649894471149 | validation: 0.07094811226439433]
	TIME [epoch: 25.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08791128732858618		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.08791128732858618 | validation: 0.08887156367343617]
	TIME [epoch: 25.9 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08244462472575513		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.08244462472575513 | validation: 0.070613186892281]
	TIME [epoch: 25.9 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808182173146519		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.0808182173146519 | validation: 0.0788132417346496]
	TIME [epoch: 25.9 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014366860546904		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.08014366860546904 | validation: 0.08444743799371864]
	TIME [epoch: 25.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07794878370453674		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.07794878370453674 | validation: 0.08053529866768844]
	TIME [epoch: 25.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0803348221527516		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.0803348221527516 | validation: 0.07824907736089773]
	TIME [epoch: 25.8 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07884026630687269		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.07884026630687269 | validation: 0.07535729279203474]
	TIME [epoch: 25.9 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08456474149320382		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.08456474149320382 | validation: 0.1052400175808963]
	TIME [epoch: 25.9 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125748136479659		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.08125748136479659 | validation: 0.07121328178216696]
	TIME [epoch: 25.9 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493442813826262		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.08493442813826262 | validation: 0.09394578927556495]
	TIME [epoch: 25.8 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285939704545807		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.08285939704545807 | validation: 0.07746206025321117]
	TIME [epoch: 25.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965639082892974		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.07965639082892974 | validation: 0.07394484369394073]
	TIME [epoch: 25.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835727737047279		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.0835727737047279 | validation: 0.09895688337638074]
	TIME [epoch: 25.8 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242071266168882		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.08242071266168882 | validation: 0.08009606232182083]
	TIME [epoch: 25.8 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133167195676859		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.08133167195676859 | validation: 0.079703695489236]
	TIME [epoch: 25.8 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325714454710897		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.08325714454710897 | validation: 0.09045113202943703]
	TIME [epoch: 25.9 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07871461594517162		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.07871461594517162 | validation: 0.0794244377607485]
	TIME [epoch: 25.8 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928708387180784		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.07928708387180784 | validation: 0.07309338850970666]
	TIME [epoch: 25.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855636245107864		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.07855636245107864 | validation: 0.08927468441813807]
	TIME [epoch: 25.9 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034706744392381		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.08034706744392381 | validation: 0.08663403718890526]
	TIME [epoch: 25.9 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752215650065537		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.0752215650065537 | validation: 0.07384486227745128]
	TIME [epoch: 25.9 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08357476135345197		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.08357476135345197 | validation: 0.12653977996160778]
	TIME [epoch: 25.8 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0899411192559265		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.0899411192559265 | validation: 0.07800894816161717]
	TIME [epoch: 25.8 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08094176253419219		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.08094176253419219 | validation: 0.07202711460693778]
	TIME [epoch: 25.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08318666070766886		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.08318666070766886 | validation: 0.11979219365303383]
	TIME [epoch: 25.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09176518990902452		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.09176518990902452 | validation: 0.07296049998722545]
	TIME [epoch: 25.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07834176546188039		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.07834176546188039 | validation: 0.07345791272664233]
	TIME [epoch: 25.8 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07973282511575405		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.07973282511575405 | validation: 0.09092584349650225]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_145450/states/model_phi1_4c_v_mmd1_1218.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 15041.501 seconds.
