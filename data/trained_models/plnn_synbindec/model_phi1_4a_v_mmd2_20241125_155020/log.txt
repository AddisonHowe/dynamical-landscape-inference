Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 643749455

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5806946316738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5806946316738 | validation: 3.884440928541597]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8312327622556506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8312327622556506 | validation: 4.036993944096529]
	TIME [epoch: 0.786 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.122968526911409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.122968526911409 | validation: 2.356826646196045]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0063504822274907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0063504822274907 | validation: 3.6210906311175712]
	TIME [epoch: 0.699 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7748319578700547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7748319578700547 | validation: 1.4852554349016374]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.727258232668837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.727258232668837 | validation: 1.6807444048577656]
	TIME [epoch: 0.7 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2893754048014494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2893754048014494 | validation: 1.8753167650655085]
	TIME [epoch: 0.699 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.092507117956275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.092507117956275 | validation: 1.7329269909912022]
	TIME [epoch: 0.697 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9328569113826897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9328569113826897 | validation: 1.4019344873519486]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.860610920168008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.860610920168008 | validation: 1.774630546904145]
	TIME [epoch: 0.698 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9578631025126516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9578631025126516 | validation: 1.9465511441111127]
	TIME [epoch: 0.697 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2197304923070806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2197304923070806 | validation: 1.566065852317858]
	TIME [epoch: 0.698 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7319221629406496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7319221629406496 | validation: 1.5590856627783918]
	TIME [epoch: 0.698 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6793165431198178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6793165431198178 | validation: 1.5091514183611254]
	TIME [epoch: 0.696 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6526370213009034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6526370213009034 | validation: 1.3809156862380483]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6226961633427317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6226961633427317 | validation: 1.4058331027896693]
	TIME [epoch: 0.7 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.57025989516369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.57025989516369 | validation: 1.341848819251271]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5379868639972767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5379868639972767 | validation: 1.262770642366017]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5041524102776322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5041524102776322 | validation: 1.2398308979115424]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4788108006883396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4788108006883396 | validation: 1.3060294544858497]
	TIME [epoch: 0.701 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4754857952769656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4754857952769656 | validation: 1.2207329787006662]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5061753173090473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5061753173090473 | validation: 1.3341681201596964]
	TIME [epoch: 0.699 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4957175054115426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4957175054115426 | validation: 1.185033651655567]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.449774918640751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.449774918640751 | validation: 1.1395849160821052]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3905018704337113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3905018704337113 | validation: 1.1139626064533856]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3564673794574555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3564673794574555 | validation: 1.0677147744529982]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3298674431945618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3298674431945618 | validation: 1.0852547198515987]
	TIME [epoch: 0.7 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3044363415229592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3044363415229592 | validation: 1.084700463059034]
	TIME [epoch: 0.699 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2976288833320728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2976288833320728 | validation: 1.0481609452121594]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3242763467017415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3242763467017415 | validation: 1.2477352151156822]
	TIME [epoch: 0.698 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3631884430391759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3631884430391759 | validation: 0.9893878260224183]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3521109387977537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3521109387977537 | validation: 1.0366403068342185]
	TIME [epoch: 0.698 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2175010524435537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2175010524435537 | validation: 0.9529069586363589]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17081865615576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17081865615576 | validation: 0.8383703681813728]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17241211804137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17241211804137 | validation: 1.2090652689291121]
	TIME [epoch: 0.701 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2473247824297453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2473247824297453 | validation: 0.6839070451493714]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2568740969750387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2568740969750387 | validation: 0.9396157525591997]
	TIME [epoch: 0.699 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123130400082358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.123130400082358 | validation: 0.9713622998456192]
	TIME [epoch: 0.699 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114525366711096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.114525366711096 | validation: 0.8306173321481677]
	TIME [epoch: 0.696 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136083589249502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.136083589249502 | validation: 1.0618476550050535]
	TIME [epoch: 0.696 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1431074938621812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1431074938621812 | validation: 0.9002018178984532]
	TIME [epoch: 0.693 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1153513268714883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1153513268714883 | validation: 0.8876361227346252]
	TIME [epoch: 0.696 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152229292978476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.152229292978476 | validation: 1.1809579039729725]
	TIME [epoch: 0.699 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1685687085338021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1685687085338021 | validation: 0.691468434655887]
	TIME [epoch: 0.697 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2179885295371955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2179885295371955 | validation: 0.7551321523134584]
	TIME [epoch: 0.693 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.018424617201211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.018424617201211 | validation: 1.180914927178139]
	TIME [epoch: 0.694 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101941350273736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1101941350273736 | validation: 0.6530498281801687]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474631231755292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1474631231755292 | validation: 0.6833384709727013]
	TIME [epoch: 0.701 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0262288362318894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0262288362318894 | validation: 1.1299081641521969]
	TIME [epoch: 0.697 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053047780750982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.053047780750982 | validation: 0.6788287596984937]
	TIME [epoch: 0.697 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980573017710012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9980573017710012 | validation: 0.7186640238322481]
	TIME [epoch: 0.698 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.991092507841677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.991092507841677 | validation: 0.9658095623196253]
	TIME [epoch: 0.699 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944600634312042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9944600634312042 | validation: 0.8257557514848677]
	TIME [epoch: 0.696 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9718034699629639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9718034699629639 | validation: 0.7674091145953427]
	TIME [epoch: 0.697 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9885069018218533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9885069018218533 | validation: 0.9335200069509062]
	TIME [epoch: 0.697 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9950310140748267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9950310140748267 | validation: 0.7821097400131117]
	TIME [epoch: 0.698 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0423641557961159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0423641557961159 | validation: 0.8662116276562015]
	TIME [epoch: 0.697 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9713202806576706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9713202806576706 | validation: 0.8506352254452205]
	TIME [epoch: 0.695 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9588950282722002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9588950282722002 | validation: 0.7487992984147556]
	TIME [epoch: 0.694 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388598321940071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9388598321940071 | validation: 0.9247228520760287]
	TIME [epoch: 0.696 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440733842688533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9440733842688533 | validation: 0.6573231256302212]
	TIME [epoch: 0.697 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9607068127114794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9607068127114794 | validation: 1.0064451322470334]
	TIME [epoch: 0.695 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9686805337292902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9686805337292902 | validation: 0.6047943854292319]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9562281801797198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9562281801797198 | validation: 0.9542028484841083]
	TIME [epoch: 0.7 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470404781134246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9470404781134246 | validation: 0.6521427541490828]
	TIME [epoch: 0.698 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206605596409113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0206605596409113 | validation: 0.927277827001277]
	TIME [epoch: 0.696 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9827332464167091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9827332464167091 | validation: 0.7959163566942968]
	TIME [epoch: 0.694 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9923777403089387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9923777403089387 | validation: 0.7972772122057523]
	TIME [epoch: 0.695 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9069759029540723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9069759029540723 | validation: 0.8072406026848447]
	TIME [epoch: 0.697 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923375138621932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923375138621932 | validation: 0.6979570111350828]
	TIME [epoch: 0.7 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935484635081804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8935484635081804 | validation: 0.9645446901643322]
	TIME [epoch: 0.698 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269050724058221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9269050724058221 | validation: 0.6035986148429766]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0066622661028952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0066622661028952 | validation: 1.0650726816380365]
	TIME [epoch: 0.699 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722115862005012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9722115862005012 | validation: 0.6600031056000497]
	TIME [epoch: 0.699 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945345465693178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945345465693178 | validation: 0.7859029317408324]
	TIME [epoch: 0.694 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9260864909529605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9260864909529605 | validation: 0.932438345909326]
	TIME [epoch: 0.694 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9641575117533114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9641575117533114 | validation: 0.7846257585354812]
	TIME [epoch: 0.694 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9175541688838571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9175541688838571 | validation: 0.7502956893865075]
	TIME [epoch: 0.696 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123835988629029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9123835988629029 | validation: 0.785287239597928]
	TIME [epoch: 0.695 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926551265063114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926551265063114 | validation: 0.7639858428326591]
	TIME [epoch: 0.696 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991748994382229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991748994382229 | validation: 0.7889216725077555]
	TIME [epoch: 0.697 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991417924265914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991417924265914 | validation: 0.7211268695962272]
	TIME [epoch: 0.698 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9316028356940801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9316028356940801 | validation: 0.9516567700600533]
	TIME [epoch: 0.699 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480844042794755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9480844042794755 | validation: 0.6071098076188891]
	TIME [epoch: 0.696 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0398881507708877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0398881507708877 | validation: 0.9402489140309399]
	TIME [epoch: 0.696 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166585124551376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9166585124551376 | validation: 0.6982557458174672]
	TIME [epoch: 0.697 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953661407696575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953661407696575 | validation: 0.8464432821047905]
	TIME [epoch: 0.698 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086627062270168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9086627062270168 | validation: 0.766212228331457]
	TIME [epoch: 0.698 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9129207752500353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9129207752500353 | validation: 0.8292905990645956]
	TIME [epoch: 0.697 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9414402110429576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9414402110429576 | validation: 0.8439124568694258]
	TIME [epoch: 0.696 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915039064998758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.915039064998758 | validation: 0.7196277973791247]
	TIME [epoch: 0.694 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9208217734972383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9208217734972383 | validation: 0.9481769990004981]
	TIME [epoch: 0.697 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221111179572017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9221111179572017 | validation: 0.587661088515258]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779349927784954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9779349927784954 | validation: 1.0635763692282347]
	TIME [epoch: 0.7 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9699380665717782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9699380665717782 | validation: 0.6798887334800412]
	TIME [epoch: 0.695 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840157797418278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8840157797418278 | validation: 0.8141574928973832]
	TIME [epoch: 0.697 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737628891368131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737628891368131 | validation: 0.7150742732844552]
	TIME [epoch: 0.694 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8662236705510007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8662236705510007 | validation: 0.8057223980241999]
	TIME [epoch: 0.692 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578756587011506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8578756587011506 | validation: 0.6811619437976821]
	TIME [epoch: 0.691 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767617472153961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767617472153961 | validation: 0.90780782381032]
	TIME [epoch: 0.696 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918674632035735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918674632035735 | validation: 0.6984227396975637]
	TIME [epoch: 0.696 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267420888355988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9267420888355988 | validation: 1.0149599071987208]
	TIME [epoch: 0.694 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9772348894369799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9772348894369799 | validation: 0.9566991060136264]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944620187784918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9944620187784918 | validation: 0.6534041747120928]
	TIME [epoch: 0.694 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156879657490086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.156879657490086 | validation: 0.8134299588839774]
	TIME [epoch: 0.696 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885794580723578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885794580723578 | validation: 1.0048100499916541]
	TIME [epoch: 0.693 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9905246463574102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9905246463574102 | validation: 0.6757980650355844]
	TIME [epoch: 0.694 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0726779560007373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0726779560007373 | validation: 0.8211612412807234]
	TIME [epoch: 0.693 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730676771202923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730676771202923 | validation: 0.9108220199018553]
	TIME [epoch: 0.693 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9620669030321918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9620669030321918 | validation: 0.687511721270419]
	TIME [epoch: 0.695 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9154803059069174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9154803059069174 | validation: 0.7669633248675372]
	TIME [epoch: 0.695 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478246149181727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8478246149181727 | validation: 0.8159077795198928]
	TIME [epoch: 0.694 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8783398150555636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8783398150555636 | validation: 0.7684192033133486]
	TIME [epoch: 0.694 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888725207429852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888725207429852 | validation: 0.7449663373683526]
	TIME [epoch: 0.696 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590089238517297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8590089238517297 | validation: 0.9344984012222514]
	TIME [epoch: 0.695 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913511661181046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8913511661181046 | validation: 0.6418192790987026]
	TIME [epoch: 0.693 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332671799660941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9332671799660941 | validation: 1.0036899862689566]
	TIME [epoch: 0.692 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262942705953231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9262942705953231 | validation: 0.6496546704699244]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731998213274612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8731998213274612 | validation: 0.7959668877119346]
	TIME [epoch: 0.697 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551348899447799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551348899447799 | validation: 0.7227724555418652]
	TIME [epoch: 0.693 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8716752743511105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8716752743511105 | validation: 0.8658680061535446]
	TIME [epoch: 0.692 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090781200855865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9090781200855865 | validation: 0.7940057768320008]
	TIME [epoch: 0.691 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785458183871613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9785458183871613 | validation: 0.8204863642925873]
	TIME [epoch: 0.696 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775644270463395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775644270463395 | validation: 0.7614987311087251]
	TIME [epoch: 0.696 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588682955812017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588682955812017 | validation: 0.7367264134958642]
	TIME [epoch: 0.695 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355274824396144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8355274824396144 | validation: 0.8229617061878284]
	TIME [epoch: 0.696 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663897175792431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663897175792431 | validation: 0.6780184819045707]
	TIME [epoch: 0.691 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908289157919069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908289157919069 | validation: 1.1281854229441055]
	TIME [epoch: 0.698 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9771032519932432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9771032519932432 | validation: 0.5852018758331763]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9129554004193878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9129554004193878 | validation: 0.8905765047698229]
	TIME [epoch: 0.7 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278683880806828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278683880806828 | validation: 0.7902494305934411]
	TIME [epoch: 0.697 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0364270884338516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0364270884338516 | validation: 0.7858934740548564]
	TIME [epoch: 0.697 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520222044806457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520222044806457 | validation: 0.7131154667052122]
	TIME [epoch: 0.695 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377558703392515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377558703392515 | validation: 0.8116924819808304]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8463493671315664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8463493671315664 | validation: 0.7407742842543364]
	TIME [epoch: 0.692 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8535799882842733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8535799882842733 | validation: 0.9119274903110948]
	TIME [epoch: 0.694 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892028361432232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8892028361432232 | validation: 0.7032983012944571]
	TIME [epoch: 0.696 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889563677587753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889563677587753 | validation: 0.930956150182486]
	TIME [epoch: 0.695 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891243183932402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891243183932402 | validation: 0.6464524412372351]
	TIME [epoch: 0.707 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372718175656075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372718175656075 | validation: 0.7779220123945149]
	TIME [epoch: 0.693 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304693080471514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304693080471514 | validation: 0.6706770031915126]
	TIME [epoch: 0.695 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953440115516369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953440115516369 | validation: 1.0483945561855883]
	TIME [epoch: 0.698 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0562291051002048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0562291051002048 | validation: 0.6598106724778166]
	TIME [epoch: 0.695 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.039464141659769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.039464141659769 | validation: 0.7278503615752239]
	TIME [epoch: 0.693 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334854526052007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334854526052007 | validation: 0.9432625888460809]
	TIME [epoch: 0.692 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537452512439518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9537452512439518 | validation: 0.6879165792249571]
	TIME [epoch: 0.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957399591190713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.957399591190713 | validation: 0.7716022384666601]
	TIME [epoch: 0.696 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236225632596827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8236225632596827 | validation: 0.8379679309391274]
	TIME [epoch: 0.694 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8819355943805042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8819355943805042 | validation: 0.6910261538337881]
	TIME [epoch: 0.695 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8954202786964853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8954202786964853 | validation: 0.7689921061277003]
	TIME [epoch: 0.695 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330685165702891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8330685165702891 | validation: 0.7468410070168479]
	TIME [epoch: 0.696 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230894731302855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8230894731302855 | validation: 0.7226947878095378]
	TIME [epoch: 0.695 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399994824907233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8399994824907233 | validation: 0.7398818810456291]
	TIME [epoch: 0.697 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8530085837994982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8530085837994982 | validation: 1.056005772699189]
	TIME [epoch: 0.696 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9473530976137045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9473530976137045 | validation: 0.654582008744478]
	TIME [epoch: 0.694 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190500642295081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9190500642295081 | validation: 0.9030385756160402]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585832756840501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8585832756840501 | validation: 0.6289652717659732]
	TIME [epoch: 0.695 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502311972291683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502311972291683 | validation: 0.7615001653400546]
	TIME [epoch: 0.695 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865743792145853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.865743792145853 | validation: 0.8340616529892002]
	TIME [epoch: 0.693 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9219021635682293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9219021635682293 | validation: 0.797649114610966]
	TIME [epoch: 0.698 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616580375088294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8616580375088294 | validation: 0.6605881229826212]
	TIME [epoch: 0.696 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691696485620932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8691696485620932 | validation: 0.8602585236779178]
	TIME [epoch: 0.696 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667986674854461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667986674854461 | validation: 0.5959451670787426]
	TIME [epoch: 0.693 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012823836875583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012823836875583 | validation: 0.9219891635359503]
	TIME [epoch: 0.695 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759058253737604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759058253737604 | validation: 0.6233864982058074]
	TIME [epoch: 0.697 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840993772262153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.840993772262153 | validation: 0.7938865321796957]
	TIME [epoch: 0.695 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8171672200392611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8171672200392611 | validation: 0.6310453212725711]
	TIME [epoch: 0.692 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159618959626039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8159618959626039 | validation: 0.7597339096654121]
	TIME [epoch: 0.693 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121920707343719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121920707343719 | validation: 0.713751728878581]
	TIME [epoch: 0.696 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8618517898279174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8618517898279174 | validation: 0.9797696067440406]
	TIME [epoch: 0.697 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0764655862987056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0764655862987056 | validation: 1.0401366295391392]
	TIME [epoch: 0.695 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0607565451108778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0607565451108778 | validation: 0.6896880630727282]
	TIME [epoch: 0.693 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146284005591673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146284005591673 | validation: 0.8801945711738363]
	TIME [epoch: 0.694 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9477908292452187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9477908292452187 | validation: 0.7705915703043236]
	TIME [epoch: 0.696 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9078950106249267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9078950106249267 | validation: 0.7154924465111283]
	TIME [epoch: 0.7 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978124146931902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978124146931902 | validation: 0.7599781989006757]
	TIME [epoch: 0.701 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839977930408037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839977930408037 | validation: 0.6512010753124444]
	TIME [epoch: 0.701 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704274321567678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8704274321567678 | validation: 0.8130996496561915]
	TIME [epoch: 0.702 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827887265077506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.827887265077506 | validation: 0.613981489188804]
	TIME [epoch: 0.7 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268409005543823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268409005543823 | validation: 0.9768371754203642]
	TIME [epoch: 0.702 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817897136903288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8817897136903288 | validation: 0.595665826406449]
	TIME [epoch: 0.702 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908759503547197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908759503547197 | validation: 0.855029664540063]
	TIME [epoch: 0.701 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8395804248088322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8395804248088322 | validation: 0.6493749904163941]
	TIME [epoch: 0.7 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090794824814718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090794824814718 | validation: 0.7586268713711052]
	TIME [epoch: 0.7 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350636803688309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8350636803688309 | validation: 0.6768077171462848]
	TIME [epoch: 0.702 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909991091271852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.909991091271852 | validation: 0.8144074888467604]
	TIME [epoch: 0.7 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813722206185273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813722206185273 | validation: 0.73624175627408]
	TIME [epoch: 0.695 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752103502073874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752103502073874 | validation: 0.6806632157004842]
	TIME [epoch: 0.695 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149341523184489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149341523184489 | validation: 0.7877288233313693]
	TIME [epoch: 0.697 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090969082480317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090969082480317 | validation: 0.5983984517840327]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8441328905543989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8441328905543989 | validation: 0.8977949582851932]
	TIME [epoch: 0.693 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741407236733945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741407236733945 | validation: 0.5794885013931186]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669337283713408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8669337283713408 | validation: 0.809184219465614]
	TIME [epoch: 0.698 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842496030834869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.842496030834869 | validation: 0.7259871873598011]
	TIME [epoch: 0.695 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877765818098726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.877765818098726 | validation: 0.7735029744446962]
	TIME [epoch: 0.693 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574345244597598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8574345244597598 | validation: 0.7866287040026148]
	TIME [epoch: 0.692 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599167889147732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599167889147732 | validation: 0.6986935308479927]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183187104391254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183187104391254 | validation: 0.7152385638444033]
	TIME [epoch: 0.698 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136376090387563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136376090387563 | validation: 0.7270847305480972]
	TIME [epoch: 0.693 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814835861823085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814835861823085 | validation: 0.6832122443190646]
	TIME [epoch: 0.697 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831919653088296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831919653088296 | validation: 0.8065190793002901]
	TIME [epoch: 175 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645852671129426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645852671129426 | validation: 0.7164993861317086]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038452825427894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038452825427894 | validation: 0.7447537262944494]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235985366395163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235985366395163 | validation: 0.7508415230162041]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106174139903383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8106174139903383 | validation: 0.6587245670770757]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133961086018812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8133961086018812 | validation: 0.9408285822532042]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659453735503931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659453735503931 | validation: 0.6115860019970502]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513167378158221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8513167378158221 | validation: 0.8421920612774594]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326558517112528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326558517112528 | validation: 0.5906652801053032]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202942167229194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202942167229194 | validation: 0.7711837177877681]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684887026426784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8684887026426784 | validation: 0.7834401863084132]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661285267802945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9661285267802945 | validation: 0.7354010867666887]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228206342585156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228206342585156 | validation: 0.7140532446684871]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919715178044533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7919715178044533 | validation: 0.6249977892018499]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7948228857089958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7948228857089958 | validation: 0.823845121499097]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152468026128352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152468026128352 | validation: 0.5865420169992762]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748363295623206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8748363295623206 | validation: 0.9067439289299944]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915396501333684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915396501333684 | validation: 0.5989553328318069]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054695636710406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9054695636710406 | validation: 0.7140339818312649]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007398513111581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007398513111581 | validation: 0.8163684462363344]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210095042213044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8210095042213044 | validation: 0.6699380183348129]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202318664090983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202318664090983 | validation: 0.7785467513492411]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365266872623565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365266872623565 | validation: 0.7343597772856594]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824071385306452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.824071385306452 | validation: 0.6530868339169079]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8660887477985392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8660887477985392 | validation: 0.753887013055787]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8263257483109544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8263257483109544 | validation: 0.6632446380123638]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152382770057304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152382770057304 | validation: 0.7286675199052699]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973928716569354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973928716569354 | validation: 0.6910340068757465]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806205459961424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806205459961424 | validation: 0.6919797289749932]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148378056149437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8148378056149437 | validation: 0.8282297130727149]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670161438573345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670161438573345 | validation: 0.7255927648447731]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424178153508584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424178153508584 | validation: 0.7388287258951074]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852093216702056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852093216702056 | validation: 0.740824732237045]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089789293580413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089789293580413 | validation: 0.6055395383588239]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033373807951208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033373807951208 | validation: 0.7384583152882238]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024887766261861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8024887766261861 | validation: 0.6080470760562492]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80691273464343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.80691273464343 | validation: 0.7490846218475007]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807714360245665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807714360245665 | validation: 0.6249212006467539]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350487326826973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8350487326826973 | validation: 0.7592908772229242]
	TIME [epoch: 1.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140258709810959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8140258709810959 | validation: 0.6838000645950532]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.818291870420099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.818291870420099 | validation: 0.7156807414162694]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076779219650058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076779219650058 | validation: 0.8542292329855985]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.851470151655844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.851470151655844 | validation: 0.6674336300804556]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379943491089659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379943491089659 | validation: 0.8669004788953996]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8175247951457835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8175247951457835 | validation: 0.6197830161713835]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764763074839963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764763074839963 | validation: 0.6785789331680667]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570067568685097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570067568685097 | validation: 0.6314448199547057]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673447719018539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673447719018539 | validation: 0.7054208971403506]
	TIME [epoch: 1.37 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887147009247026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887147009247026 | validation: 0.6612463731709104]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005514160293993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005514160293993 | validation: 0.8238937132749591]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023549644307796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9023549644307796 | validation: 0.7428708981753039]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646939352726285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646939352726285 | validation: 0.682570621697566]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747274504831795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747274504831795 | validation: 0.711454023598652]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761525423622142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761525423622142 | validation: 0.5958841499169217]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814125225262535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814125225262535 | validation: 0.8239106079529199]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016810611156981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016810611156981 | validation: 0.5813253070453516]
	TIME [epoch: 1.37 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299762054755868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8299762054755868 | validation: 0.7974529021733565]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911961698187867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911961698187867 | validation: 0.5971533393715904]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713724084933847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713724084933847 | validation: 0.6850179712419099]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776843459771147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776843459771147 | validation: 0.760192864811727]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8570918799872016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8570918799872016 | validation: 0.7938023143938482]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906272592030871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.906272592030871 | validation: 0.7809442355443281]
	TIME [epoch: 1.37 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723724541451157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723724541451157 | validation: 0.6746017566240133]
	TIME [epoch: 1.37 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554381640295325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554381640295325 | validation: 0.6046348210024163]
	TIME [epoch: 1.37 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503073044952572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503073044952572 | validation: 0.716263519182026]
	TIME [epoch: 1.37 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632778727492706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7632778727492706 | validation: 0.6163384707363577]
	TIME [epoch: 1.37 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591624896194135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591624896194135 | validation: 0.7203800178764116]
	TIME [epoch: 1.37 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592473640154997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592473640154997 | validation: 0.6328053628810113]
	TIME [epoch: 1.37 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648425380315567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648425380315567 | validation: 0.7077058262015907]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8111058477068939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8111058477068939 | validation: 0.8336056143155806]
	TIME [epoch: 1.37 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197466695274648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9197466695274648 | validation: 0.6879598848837024]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9475836166988046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9475836166988046 | validation: 0.6700209023128125]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502564476911997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502564476911997 | validation: 0.6950815130035901]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656647029328056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656647029328056 | validation: 0.5898807698690856]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8303994140890782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8303994140890782 | validation: 0.7523876994782192]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106801733368691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8106801733368691 | validation: 0.6873967335562311]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789281271840197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.789281271840197 | validation: 0.6531390709904166]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906985120149481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906985120149481 | validation: 0.8270635862462283]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980939758052737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980939758052737 | validation: 0.6019801214894682]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755801421091231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755801421091231 | validation: 0.6503885731715258]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399043090260823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399043090260823 | validation: 0.6187693017729745]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378288710817316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378288710817316 | validation: 0.6314843429122925]
	TIME [epoch: 1.37 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7416678229095226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7416678229095226 | validation: 0.6335797935846283]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544655787053297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544655787053297 | validation: 0.6992501199905692]
	TIME [epoch: 1.42 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444266075707532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444266075707532 | validation: 0.854784368991999]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9601088412103232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9601088412103232 | validation: 0.7629827912188868]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717819291766179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8717819291766179 | validation: 0.6143997370343728]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438984735560007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438984735560007 | validation: 0.6800974722053035]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415452152513993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7415452152513993 | validation: 0.6187755038146651]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841176190303288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841176190303288 | validation: 0.7163715183783899]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699190205543553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7699190205543553 | validation: 0.6167305401623615]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660769083850523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660769083850523 | validation: 0.6371569604845193]
	TIME [epoch: 1.37 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461049765322793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461049765322793 | validation: 0.7414855504429619]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661046696702632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661046696702632 | validation: 0.6495411139780636]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058772080616753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8058772080616753 | validation: 0.8291650583693309]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000652206549365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8000652206549365 | validation: 0.6232684308874843]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338895734730942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338895734730942 | validation: 0.611154817191792]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368718911916411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7368718911916411 | validation: 0.6929624231784788]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645691700832775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645691700832775 | validation: 0.59190350305573]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377368745848398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377368745848398 | validation: 0.7346380417772963]
	TIME [epoch: 1.37 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926651599719747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7926651599719747 | validation: 0.6670197086771446]
	TIME [epoch: 1.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720185127295156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720185127295156 | validation: 0.6321760062335758]
	TIME [epoch: 1.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558525316928257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7558525316928257 | validation: 0.7297605105320314]
	TIME [epoch: 1.37 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550510987584835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7550510987584835 | validation: 0.630241944532547]
	TIME [epoch: 1.37 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470853880563066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470853880563066 | validation: 0.6913062473896759]
	TIME [epoch: 1.37 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491419092647554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491419092647554 | validation: 0.6596533856925111]
	TIME [epoch: 1.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557977395712163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557977395712163 | validation: 0.631716666857924]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800306077426897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800306077426897 | validation: 0.6841754005459528]
	TIME [epoch: 1.37 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7755970064503087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7755970064503087 | validation: 0.6219886144425614]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854434102239126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854434102239126 | validation: 0.6664426294009754]
	TIME [epoch: 1.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548540612192671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548540612192671 | validation: 0.6358259684057919]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461313826240712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461313826240712 | validation: 0.6380118445935454]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534848810640602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7534848810640602 | validation: 0.7086238715569211]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771339331721084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771339331721084 | validation: 0.5976132562641866]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730418322684846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7730418322684846 | validation: 0.7483354067350357]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605372757465928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605372757465928 | validation: 0.6195268917820187]
	TIME [epoch: 1.37 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284359255611057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284359255611057 | validation: 0.6076347744584176]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238192489186039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7238192489186039 | validation: 0.6830822099009426]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515236485968751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7515236485968751 | validation: 0.582064315960096]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021451867869885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021451867869885 | validation: 0.6696895888202145]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690719180494167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690719180494167 | validation: 0.6442927369110286]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545848943406811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545848943406811 | validation: 0.6303225190057081]
	TIME [epoch: 1.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461889017440422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461889017440422 | validation: 0.7164866849366276]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488781572847998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7488781572847998 | validation: 0.6270884047531557]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401642625618249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7401642625618249 | validation: 0.6479451425892676]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730275897782605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730275897782605 | validation: 0.6397894779061222]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733895704987003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733895704987003 | validation: 0.6207406685770119]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473198635337849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473198635337849 | validation: 0.669475940370811]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431923292004669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7431923292004669 | validation: 0.6230093983732756]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445365345660078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445365345660078 | validation: 0.6512136403067439]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436481830298591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7436481830298591 | validation: 0.6453773022507598]
	TIME [epoch: 1.37 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393342461670263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393342461670263 | validation: 0.5964633179932614]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328924131842919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7328924131842919 | validation: 0.7450188692310257]
	TIME [epoch: 1.37 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450520199469451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450520199469451 | validation: 0.5825495346557004]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406360540470119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406360540470119 | validation: 0.6992631487143603]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346093146777972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346093146777972 | validation: 0.6015519226946033]
	TIME [epoch: 1.37 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143485000841667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143485000841667 | validation: 0.5809740275180693]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7330520365061773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7330520365061773 | validation: 0.6874634733051104]
	TIME [epoch: 1.37 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764579944536598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764579944536598 | validation: 0.604277052712546]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949177205563114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7949177205563114 | validation: 0.6309364560866089]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179753065209613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179753065209613 | validation: 0.6685766037245049]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194082385400773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7194082385400773 | validation: 0.5574975170783697]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145401574835265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145401574835265 | validation: 0.6827556819265655]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076976898453949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7076976898453949 | validation: 0.5841729954372112]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987195080157134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987195080157134 | validation: 0.5886007868124665]
	TIME [epoch: 1.37 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196674540614214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196674540614214 | validation: 0.6684516419572928]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491880773754571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491880773754571 | validation: 0.5847940760014276]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841313898826675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841313898826675 | validation: 0.6272043748461136]
	TIME [epoch: 1.37 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077657287724088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7077657287724088 | validation: 0.6270324054744723]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910454205472901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910454205472901 | validation: 0.540293396168093]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882933376692205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882933376692205 | validation: 0.6740537797621052]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991077409221387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6991077409221387 | validation: 0.5732853331141845]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032028422693608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7032028422693608 | validation: 0.6677683052316504]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259033224548128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259033224548128 | validation: 0.6302169186430611]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350780741213783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350780741213783 | validation: 0.5895561746974046]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490359818256229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7490359818256229 | validation: 0.6265594617991125]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080029097851286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7080029097851286 | validation: 0.5703760915790445]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783770507237281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6783770507237281 | validation: 0.5606357319263973]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734743261511156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734743261511156 | validation: 0.631235916075068]
	TIME [epoch: 1.37 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745491416430826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6745491416430826 | validation: 0.520038286045018]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980694194015592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6980694194015592 | validation: 0.7147636319729628]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131549996769736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7131549996769736 | validation: 0.5955882795444886]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182424172899201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182424172899201 | validation: 0.5767533052544749]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138607821286892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138607821286892 | validation: 0.6206518769223092]
	TIME [epoch: 1.37 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047272664683786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047272664683786 | validation: 0.5222152909079131]
	TIME [epoch: 1.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761260533004125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761260533004125 | validation: 0.5499953645794139]
	TIME [epoch: 1.37 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514250004693642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6514250004693642 | validation: 0.606665567535901]
	TIME [epoch: 1.37 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540289886130672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6540289886130672 | validation: 0.5163322767678812]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645005225066868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.645005225066868 | validation: 0.6310869735334749]
	TIME [epoch: 1.37 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611675787174826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6611675787174826 | validation: 0.47981012101830967]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6801464444909874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6801464444909874 | validation: 0.7490757942417519]
	TIME [epoch: 1.37 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022483042149341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022483042149341 | validation: 0.6023561408725376]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7271830863352079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7271830863352079 | validation: 0.5436540042300428]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037552905532747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037552905532747 | validation: 0.6084884283019945]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608638677688388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608638677688388 | validation: 0.5720285021266854]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355057642417306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6355057642417306 | validation: 0.5147620018572304]
	TIME [epoch: 1.37 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332899468969623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332899468969623 | validation: 0.550356589446264]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.62547457761122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.62547457761122 | validation: 0.5247024479738557]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194427181487782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6194427181487782 | validation: 0.523798541779993]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6210405615433604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6210405615433604 | validation: 0.5756912292617021]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471935477878293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6471935477878293 | validation: 0.6333673177376421]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765645386488378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765645386488378 | validation: 0.64642942736183]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920946699015704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7920946699015704 | validation: 0.5388354588119239]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324449801050726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6324449801050726 | validation: 0.5308729628476095]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113812114155818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6113812114155818 | validation: 0.5333140315444407]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6128629901801456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128629901801456 | validation: 0.5097817289155068]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6257937782963297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6257937782963297 | validation: 0.6118986027604718]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307947194361525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6307947194361525 | validation: 0.4851590300960057]
	TIME [epoch: 1.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591068450503791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6591068450503791 | validation: 0.7407554487372909]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019917096665023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7019917096665023 | validation: 0.47425702573488365]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441604470177694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441604470177694 | validation: 0.4982695161180264]
	TIME [epoch: 1.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130603441421345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6130603441421345 | validation: 0.563478024869902]
	TIME [epoch: 1.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.611609238231853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.611609238231853 | validation: 0.44333177743855795]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327486488010421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6327486488010421 | validation: 0.5762387323709549]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351271663089801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351271663089801 | validation: 0.5020892417170492]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640888453658721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.640888453658721 | validation: 0.5480575262100281]
	TIME [epoch: 1.37 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258879179226817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6258879179226817 | validation: 0.5432226183997027]
	TIME [epoch: 1.37 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291407345028887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6291407345028887 | validation: 0.5157398055378242]
	TIME [epoch: 1.37 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129578952007675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6129578952007675 | validation: 0.5440271217355034]
	TIME [epoch: 1.37 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6010494956966524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6010494956966524 | validation: 0.48527474214577415]
	TIME [epoch: 1.37 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957620695220109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5957620695220109 | validation: 0.5660385445325516]
	TIME [epoch: 1.37 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6093116869960102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6093116869960102 | validation: 0.4937719336872226]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338717003919069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6338717003919069 | validation: 0.6228821751710667]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617859394115182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617859394115182 | validation: 0.5173767299843867]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372385076586552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6372385076586552 | validation: 0.4360603459909824]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073446788971273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6073446788971273 | validation: 0.6086355022338679]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6055930858661234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6055930858661234 | validation: 0.4131261907044399]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585324542549546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585324542549546 | validation: 0.5907243732508916]
	TIME [epoch: 1.37 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888454388929601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5888454388929601 | validation: 0.4547517464981999]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574489398654843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574489398654843 | validation: 0.5203357773484123]
	TIME [epoch: 1.37 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659156315062129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5659156315062129 | validation: 0.46925322420540355]
	TIME [epoch: 1.37 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5518529097307744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5518529097307744 | validation: 0.45557156769088963]
	TIME [epoch: 1.37 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.555899285758293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.555899285758293 | validation: 0.562080263884887]
	TIME [epoch: 1.37 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864318157211113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864318157211113 | validation: 0.8622188620045439]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9510578778698812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9510578778698812 | validation: 0.39249022973776576]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6004833203289403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6004833203289403 | validation: 0.6231062758495534]
	TIME [epoch: 1.37 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406501547525925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6406501547525925 | validation: 0.5170486317094716]
	TIME [epoch: 1.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341801882729322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6341801882729322 | validation: 0.46480677869061965]
	TIME [epoch: 1.37 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786438771066835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786438771066835 | validation: 0.49015383498295695]
	TIME [epoch: 1.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593273743484616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5593273743484616 | validation: 0.4754797679977447]
	TIME [epoch: 1.37 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5488004965394936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5488004965394936 | validation: 0.5015880974302521]
	TIME [epoch: 1.37 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5485429518299627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5485429518299627 | validation: 0.4071642864266154]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452934296772519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5452934296772519 | validation: 0.6468513077702629]
	TIME [epoch: 1.37 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5818353206454352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5818353206454352 | validation: 0.4216124636837603]
	TIME [epoch: 1.37 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923302144368414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923302144368414 | validation: 0.7111313829326645]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212857699618318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6212857699618318 | validation: 0.48594686020741806]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320480315639767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5320480315639767 | validation: 0.41997843809151497]
	TIME [epoch: 1.37 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5686440301652645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5686440301652645 | validation: 0.5202988246773576]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5527720731431566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5527720731431566 | validation: 0.47991006718056]
	TIME [epoch: 1.37 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410177231732057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5410177231732057 | validation: 0.4245222310185772]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5801386974335673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5801386974335673 | validation: 0.5666510759719513]
	TIME [epoch: 1.37 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308944790207671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6308944790207671 | validation: 0.49297472141919335]
	TIME [epoch: 1.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029659795935052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029659795935052 | validation: 0.4400275236945193]
	TIME [epoch: 1.37 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5361549603930059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5361549603930059 | validation: 0.4977821586360735]
	TIME [epoch: 1.37 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203437047487673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5203437047487673 | validation: 0.4334921441052375]
	TIME [epoch: 1.37 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076281898874736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5076281898874736 | validation: 0.42811841139478485]
	TIME [epoch: 1.37 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094878632567781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5094878632567781 | validation: 0.47030513110685845]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217625353866804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5217625353866804 | validation: 0.5724848286959151]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610024935647769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6610024935647769 | validation: 0.6211531631755449]
	TIME [epoch: 1.37 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532247662692737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532247662692737 | validation: 0.48905276753992644]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375178754576003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5375178754576003 | validation: 0.4300775445021735]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117472267523716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117472267523716 | validation: 0.48334377424096914]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179224258892492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179224258892492 | validation: 0.4429358501171704]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49913895714710266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49913895714710266 | validation: 0.4550917406940549]
	TIME [epoch: 1.37 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4778139567061139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4778139567061139 | validation: 0.36647189323083196]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4701162229763691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4701162229763691 | validation: 0.5832631751860726]
	TIME [epoch: 1.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5180671128851719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5180671128851719 | validation: 0.43634459867128705]
	TIME [epoch: 1.37 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983586723489936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983586723489936 | validation: 0.7879028914948427]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277758373802736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6277758373802736 | validation: 0.6039690007600401]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307840871364241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6307840871364241 | validation: 0.4148595587570591]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635850731911223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5635850731911223 | validation: 0.4506209347520409]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314399752866653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5314399752866653 | validation: 0.4929650125514933]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106411387071603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5106411387071603 | validation: 0.48400814183924257]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4997263020041548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4997263020041548 | validation: 0.43136294341837955]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756980928269273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4756980928269273 | validation: 0.42391293705389566]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4645749957924409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4645749957924409 | validation: 0.4302151394004181]
	TIME [epoch: 1.37 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4480602028270728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4480602028270728 | validation: 0.3807351252661854]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.449321634440838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.449321634440838 | validation: 0.5557507825279613]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5403966187508641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5403966187508641 | validation: 0.8307018693464561]
	TIME [epoch: 1.37 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9695418918708834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9695418918708834 | validation: 0.32254232672985295]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5532174489838851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5532174489838851 | validation: 0.6624904202867564]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5468257601491444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5468257601491444 | validation: 0.4493325607310992]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5756570507543947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5756570507543947 | validation: 0.43193284344644217]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4458336483229205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4458336483229205 | validation: 0.4784847845681246]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44586732401695744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44586732401695744 | validation: 0.35573679371887146]
	TIME [epoch: 1.37 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44553170515272783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44553170515272783 | validation: 0.5205878722667157]
	TIME [epoch: 1.37 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44537796349897846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44537796349897846 | validation: 0.280924086129723]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48808234169300463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48808234169300463 | validation: 0.7165584969621274]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5922731858349086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5922731858349086 | validation: 0.4153766948020825]
	TIME [epoch: 1.37 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5002263918358008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5002263918358008 | validation: 0.4158958002952578]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5228398565975645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5228398565975645 | validation: 0.514445517051207]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656385121271072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4656385121271072 | validation: 0.39077725832307986]
	TIME [epoch: 1.37 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4150313288523815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4150313288523815 | validation: 0.3567232589922633]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4089642190306397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4089642190306397 | validation: 0.47322263569971457]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40568349513540575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40568349513540575 | validation: 0.30356062968419506]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496244942961695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4496244942961695 | validation: 0.838560739710998]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6304678817144774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6304678817144774 | validation: 0.6171459839758519]
	TIME [epoch: 1.37 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773787180064815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7773787180064815 | validation: 0.24774125226151292]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284422645176924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284422645176924 | validation: 0.6187432210129984]
	TIME [epoch: 1.37 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4939068697985485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4939068697985485 | validation: 0.49091992837592946]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4528607594950554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4528607594950554 | validation: 0.371513294425819]
	TIME [epoch: 1.37 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43019147678553643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43019147678553643 | validation: 0.3896316744674555]
	TIME [epoch: 1.37 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.397511345009368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.397511345009368 | validation: 0.42700861692270886]
	TIME [epoch: 1.37 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38135816965288627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38135816965288627 | validation: 0.34174710365229066]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3811512158909268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3811512158909268 | validation: 0.4273515238433162]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871247813354482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871247813354482 | validation: 0.3419087310513256]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567683737526353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5567683737526353 | validation: 0.5231079025746327]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5265316230241208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5265316230241208 | validation: 0.4949326767055836]
	TIME [epoch: 1.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5123167122169653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5123167122169653 | validation: 0.354669085772652]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114408000568819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5114408000568819 | validation: 0.5244563748212142]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42992601954274706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42992601954274706 | validation: 0.4312998465561763]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3919219685443639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3919219685443639 | validation: 0.3530846929528612]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.403919957731264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.403919957731264 | validation: 0.4729910673885572]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953636959046719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3953636959046719 | validation: 0.32920785030550476]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37687261479600975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37687261479600975 | validation: 0.4363700393559067]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39796331014087644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39796331014087644 | validation: 0.4400904948050734]
	TIME [epoch: 1.37 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5213129766512407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5213129766512407 | validation: 0.34268507465527787]
	TIME [epoch: 1.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46551152974577625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46551152974577625 | validation: 0.5270509502368992]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41544934752894236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41544934752894236 | validation: 0.3105705370190337]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4108760414448757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4108760414448757 | validation: 0.5099115533717872]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36928633002522804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36928633002522804 | validation: 0.32357646488847025]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37464265427826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37464265427826 | validation: 0.3875843095692879]
	TIME [epoch: 2.71 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3647082488320669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3647082488320669 | validation: 0.37095863496492615]
	TIME [epoch: 2.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864153922215282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3864153922215282 | validation: 0.42215347083153615]
	TIME [epoch: 2.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40715131821817907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40715131821817907 | validation: 0.41904031450207857]
	TIME [epoch: 2.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4620525406789196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4620525406789196 | validation: 0.4184260523480725]
	TIME [epoch: 2.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3700234438124393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3700234438124393 | validation: 0.31176563251839395]
	TIME [epoch: 2.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3341955328240616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3341955328240616 | validation: 0.41629116402065947]
	TIME [epoch: 2.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30326391230045235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30326391230045235 | validation: 0.23411055936339562]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3564148688044674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3564148688044674 | validation: 0.7964882303875491]
	TIME [epoch: 2.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651194578669102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5651194578669102 | validation: 0.2839883257120289]
	TIME [epoch: 2.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227682746435607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227682746435607 | validation: 0.36340490241119827]
	TIME [epoch: 2.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195404213516356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3195404213516356 | validation: 0.4348700064367836]
	TIME [epoch: 2.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4085383247363881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4085383247363881 | validation: 0.41287017360815775]
	TIME [epoch: 2.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4965731617152349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4965731617152349 | validation: 0.3892836362003839]
	TIME [epoch: 2.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31183108327677606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31183108327677606 | validation: 0.3135994274650442]
	TIME [epoch: 2.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639328671494701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2639328671494701 | validation: 0.339878582546129]
	TIME [epoch: 2.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25977430713991534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25977430713991534 | validation: 0.2609232444065412]
	TIME [epoch: 2.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3679749825355007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3679749825355007 | validation: 0.6310347866041695]
	TIME [epoch: 2.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048338879718514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048338879718514 | validation: 0.29407796644484663]
	TIME [epoch: 2.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35125986708930684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35125986708930684 | validation: 0.307381108291605]
	TIME [epoch: 2.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24801821527139317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24801821527139317 | validation: 0.3230873189828855]
	TIME [epoch: 2.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23094762015352568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23094762015352568 | validation: 0.23006531222323431]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.239717912644694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.239717912644694 | validation: 0.46374653238774627]
	TIME [epoch: 2.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3090592506695645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3090592506695645 | validation: 0.2810977254729809]
	TIME [epoch: 2.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454099228185608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5454099228185608 | validation: 0.770851693024448]
	TIME [epoch: 2.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5067376594252005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5067376594252005 | validation: 0.4059445286799789]
	TIME [epoch: 2.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.409043417306451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.409043417306451 | validation: 0.3112880283010948]
	TIME [epoch: 2.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792623454561972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3792623454561972 | validation: 0.34121453518083]
	TIME [epoch: 2.71 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522649203505249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2522649203505249 | validation: 0.32954098904496415]
	TIME [epoch: 2.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555135653277716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2555135653277716 | validation: 0.31225860144430523]
	TIME [epoch: 2.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27428318794969153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27428318794969153 | validation: 0.35573467603574743]
	TIME [epoch: 2.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4068322205324471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4068322205324471 | validation: 0.5381790933492121]
	TIME [epoch: 2.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4243245315963421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4243245315963421 | validation: 0.35662779597278554]
	TIME [epoch: 2.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42072164258716527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42072164258716527 | validation: 0.34326431795800727]
	TIME [epoch: 2.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23896048163758593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23896048163758593 | validation: 0.2553083225265473]
	TIME [epoch: 2.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20505298572460134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20505298572460134 | validation: 0.2785783206864411]
	TIME [epoch: 2.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20986161131689252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20986161131689252 | validation: 0.2561792194867842]
	TIME [epoch: 2.71 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29161132060959005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29161132060959005 | validation: 0.45735296655651736]
	TIME [epoch: 2.71 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5033495683346265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5033495683346265 | validation: 0.23967732022047158]
	TIME [epoch: 2.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814984533841662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2814984533841662 | validation: 0.38747697932664593]
	TIME [epoch: 2.71 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23491707542116044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23491707542116044 | validation: 0.1810474380252416]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2715223693481474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2715223693481474 | validation: 0.6826494574229132]
	TIME [epoch: 2.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42946884952044784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42946884952044784 | validation: 0.30631711919483207]
	TIME [epoch: 2.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149646153211967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3149646153211967 | validation: 0.30360004392929185]
	TIME [epoch: 2.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32514376848819154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32514376848819154 | validation: 0.2948451225872753]
	TIME [epoch: 2.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252619661740076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.252619661740076 | validation: 0.2966063653703392]
	TIME [epoch: 2.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2109826432211451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2109826432211451 | validation: 0.2259483226694825]
	TIME [epoch: 2.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24650147949827944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24650147949827944 | validation: 0.4812121156187028]
	TIME [epoch: 2.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2928724147412068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2928724147412068 | validation: 0.269076157404041]
	TIME [epoch: 2.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34149764646902186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34149764646902186 | validation: 0.43136567112926116]
	TIME [epoch: 2.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829681165531375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2829681165531375 | validation: 0.2660341379019777]
	TIME [epoch: 2.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.268560180480138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.268560180480138 | validation: 0.37844852822149067]
	TIME [epoch: 2.71 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28039825350314823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28039825350314823 | validation: 0.25684333801479264]
	TIME [epoch: 2.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26402776768485564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26402776768485564 | validation: 0.25603846867434427]
	TIME [epoch: 2.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20781309447230045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20781309447230045 | validation: 0.2817489497992544]
	TIME [epoch: 2.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2190184793977878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2190184793977878 | validation: 0.20438507854621957]
	TIME [epoch: 2.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32154324152461883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32154324152461883 | validation: 0.43728749331979416]
	TIME [epoch: 2.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29071265230385906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29071265230385906 | validation: 0.16670992041323607]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19498242998333468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19498242998333468 | validation: 0.3613370297279057]
	TIME [epoch: 2.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2060048675420137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2060048675420137 | validation: 0.23170691988088785]
	TIME [epoch: 2.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28743401635387067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28743401635387067 | validation: 0.46022606986697057]
	TIME [epoch: 2.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3167247277183985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3167247277183985 | validation: 0.2537372717791932]
	TIME [epoch: 2.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22808450283222229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22808450283222229 | validation: 0.2792265455740139]
	TIME [epoch: 2.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21719059040449046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21719059040449046 | validation: 0.25328716076500424]
	TIME [epoch: 2.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22028602278394221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22028602278394221 | validation: 0.24334976436464972]
	TIME [epoch: 2.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19480062121541836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19480062121541836 | validation: 0.22245190519686941]
	TIME [epoch: 2.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14840380528570338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14840380528570338 | validation: 0.21442691507069223]
	TIME [epoch: 2.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14435864085608954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14435864085608954 | validation: 0.20992781653571335]
	TIME [epoch: 2.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1932238210801238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1932238210801238 | validation: 0.3689204680328358]
	TIME [epoch: 2.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2855202308295492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2855202308295492 | validation: 0.37294924103989086]
	TIME [epoch: 2.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3689794111381802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3689794111381802 | validation: 0.38270785583099154]
	TIME [epoch: 2.71 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2243966240779957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2243966240779957 | validation: 0.1628315942197149]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1897907816822932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1897907816822932 | validation: 0.3810401252153619]
	TIME [epoch: 2.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2482725196021824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2482725196021824 | validation: 0.14324288467546858]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18547739723536244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18547739723536244 | validation: 0.3769990516428503]
	TIME [epoch: 2.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2134382764899523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2134382764899523 | validation: 0.20825552966464117]
	TIME [epoch: 2.71 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25487784993173956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25487784993173956 | validation: 0.2703919043068894]
	TIME [epoch: 2.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14458708563330144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14458708563330144 | validation: 0.14522839766027282]
	TIME [epoch: 2.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11455066727155633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11455066727155633 | validation: 0.18978504692165898]
	TIME [epoch: 2.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1044585979907098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1044585979907098 | validation: 0.14161926330463204]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10495466881198301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10495466881198301 | validation: 0.22568333460498727]
	TIME [epoch: 2.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18600644980794478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18600644980794478 | validation: 0.2944157804578728]
	TIME [epoch: 2.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46952730190791514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46952730190791514 | validation: 0.1615132149845908]
	TIME [epoch: 2.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20451916471719428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20451916471719428 | validation: 0.6242588296379576]
	TIME [epoch: 2.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117196788850885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117196788850885 | validation: 0.2682416866309844]
	TIME [epoch: 2.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577087373001427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577087373001427 | validation: 0.2174487461689906]
	TIME [epoch: 2.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33328124548050964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33328124548050964 | validation: 0.3212685930053148]
	TIME [epoch: 2.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19202206956603768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19202206956603768 | validation: 0.19317275412168264]
	TIME [epoch: 2.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12081915663627964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12081915663627964 | validation: 0.1865530587687065]
	TIME [epoch: 2.71 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13000358471896253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13000358471896253 | validation: 0.20212971794013926]
	TIME [epoch: 2.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11900579111084117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11900579111084117 | validation: 0.15658942472166193]
	TIME [epoch: 2.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18698598936612104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18698598936612104 | validation: 0.2685919296623448]
	TIME [epoch: 2.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25250348081626484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25250348081626484 | validation: 0.23957175030179836]
	TIME [epoch: 2.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22951848609554823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22951848609554823 | validation: 0.19398648064912427]
	TIME [epoch: 2.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16506962246101964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16506962246101964 | validation: 0.4971595233387824]
	TIME [epoch: 2.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768159166965961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2768159166965961 | validation: 0.15635390935007476]
	TIME [epoch: 2.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648700383641718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1648700383641718 | validation: 0.2712138084035354]
	TIME [epoch: 2.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1747221516277024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1747221516277024 | validation: 0.15909726402284932]
	TIME [epoch: 2.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12368028835837164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12368028835837164 | validation: 0.1438257550646657]
	TIME [epoch: 2.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11001108891062751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11001108891062751 | validation: 0.25193756508593035]
	TIME [epoch: 2.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2321995285080603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2321995285080603 | validation: 0.26897167809758765]
	TIME [epoch: 2.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26546433720554563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26546433720554563 | validation: 0.22855620913110705]
	TIME [epoch: 2.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649771222853363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1649771222853363 | validation: 0.26735717761983346]
	TIME [epoch: 2.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12837193313718603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12837193313718603 | validation: 0.12164103774360152]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15256434670675045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15256434670675045 | validation: 0.370094072435743]
	TIME [epoch: 2.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19605423715859188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19605423715859188 | validation: 0.13965265530556548]
	TIME [epoch: 2.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16182776470985655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16182776470985655 | validation: 0.2585031438244665]
	TIME [epoch: 2.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13485366483933384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13485366483933384 | validation: 0.15827587990677583]
	TIME [epoch: 2.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12528801450305765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12528801450305765 | validation: 0.23484911642995795]
	TIME [epoch: 2.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16450040686134393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16450040686134393 | validation: 0.22689564409448285]
	TIME [epoch: 2.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20778317324133952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20778317324133952 | validation: 0.25988514070736285]
	TIME [epoch: 2.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21414288107887308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21414288107887308 | validation: 0.1687896883001841]
	TIME [epoch: 2.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961181566790366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0961181566790366 | validation: 0.13793077502434878]
	TIME [epoch: 2.7 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06977286472039979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06977286472039979 | validation: 0.11816608029061311]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790860254162338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06790860254162338 | validation: 0.22190899594429936]
	TIME [epoch: 2.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310895571330389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11310895571330389 | validation: 0.24669393556662902]
	TIME [epoch: 2.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34123405527903866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34123405527903866 | validation: 0.34921663188359586]
	TIME [epoch: 2.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16156781127575864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16156781127575864 | validation: 0.20534405070012368]
	TIME [epoch: 2.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2577471939829782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2577471939829782 | validation: 0.38924116135914677]
	TIME [epoch: 2.71 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2682333112665194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2682333112665194 | validation: 0.11870179565430611]
	TIME [epoch: 2.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09580671325629264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09580671325629264 | validation: 0.22026228417240823]
	TIME [epoch: 2.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.168918240220236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.168918240220236 | validation: 0.20554549325884675]
	TIME [epoch: 2.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390004493660073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3390004493660073 | validation: 0.22725943655372918]
	TIME [epoch: 2.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1777812766759155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1777812766759155 | validation: 0.28750711700878473]
	TIME [epoch: 2.71 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16165599626282323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16165599626282323 | validation: 0.14154307923736362]
	TIME [epoch: 2.71 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12228790847609534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12228790847609534 | validation: 0.21608625666418646]
	TIME [epoch: 2.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243200337975746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10243200337975746 | validation: 0.11406445463922213]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09262960142511398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09262960142511398 | validation: 0.2336032689858547]
	TIME [epoch: 2.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10701741274929066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10701741274929066 | validation: 0.15937798379268786]
	TIME [epoch: 2.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17132337429296868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17132337429296868 | validation: 0.41471328587718004]
	TIME [epoch: 2.71 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24812892121768404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24812892121768404 | validation: 0.21041482675879825]
	TIME [epoch: 2.7 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18435125338461097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18435125338461097 | validation: 0.28585168762110663]
	TIME [epoch: 2.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13885276812907116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13885276812907116 | validation: 0.14038030002382418]
	TIME [epoch: 2.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08822006402524571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08822006402524571 | validation: 0.1343407498629512]
	TIME [epoch: 2.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181284552128846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07181284552128846 | validation: 0.14349680406252535]
	TIME [epoch: 2.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579340511562106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579340511562106 | validation: 0.11270996101604638]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288208082707517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1288208082707517 | validation: 0.2317799519433768]
	TIME [epoch: 2.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20601224954777095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20601224954777095 | validation: 0.17218769782837845]
	TIME [epoch: 2.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1827352288366167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1827352288366167 | validation: 0.1189948837162608]
	TIME [epoch: 2.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701239556398413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0701239556398413 | validation: 0.1949465478639107]
	TIME [epoch: 2.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063422450652567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08063422450652567 | validation: 0.10541635768534302]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14754263704579565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14754263704579565 | validation: 0.49589706989591537]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243411942126197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3243411942126197 | validation: 0.14246919551115306]
	TIME [epoch: 2.71 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351498065580432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1351498065580432 | validation: 0.11020334065898155]
	TIME [epoch: 2.71 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499568100175247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499568100175247 | validation: 0.2598515998434801]
	TIME [epoch: 2.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14275341051034732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14275341051034732 | validation: 0.15599036491553392]
	TIME [epoch: 2.71 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295253905962198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1295253905962198 | validation: 0.3849284258653089]
	TIME [epoch: 2.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18829619094698669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18829619094698669 | validation: 0.24588405995448506]
	TIME [epoch: 2.71 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23600608273881057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23600608273881057 | validation: 0.24791968549006396]
	TIME [epoch: 2.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18382931147488704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18382931147488704 | validation: 0.13025490801220982]
	TIME [epoch: 2.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774901432270232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774901432270232 | validation: 0.19315770056367987]
	TIME [epoch: 2.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09061174062961234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09061174062961234 | validation: 0.09811670488207536]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984428386523835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09984428386523835 | validation: 0.24325049144026598]
	TIME [epoch: 2.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11233591962028562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11233591962028562 | validation: 0.12728210401196632]
	TIME [epoch: 2.71 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616947524557684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09616947524557684 | validation: 0.17259964706518485]
	TIME [epoch: 2.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164482117036154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12164482117036154 | validation: 0.15545840350986928]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15090467525350929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15090467525350929 | validation: 0.19882154499424398]
	TIME [epoch: 2.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1645040863681897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645040863681897 | validation: 0.14689325172368617]
	TIME [epoch: 2.71 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12253390541939009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12253390541939009 | validation: 0.3077756098346749]
	TIME [epoch: 2.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804308927099256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804308927099256 | validation: 0.24065833124131011]
	TIME [epoch: 2.7 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19014267496637394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19014267496637394 | validation: 0.28225194000191045]
	TIME [epoch: 2.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381899903689985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1381899903689985 | validation: 0.07544957744029034]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09647001728842604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09647001728842604 | validation: 0.21019604590730367]
	TIME [epoch: 2.71 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10302689662425621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10302689662425621 | validation: 0.10766290738521343]
	TIME [epoch: 2.71 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10257987990768536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10257987990768536 | validation: 0.1145673729237342]
	TIME [epoch: 2.71 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08255269113138632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08255269113138632 | validation: 0.26075141738968793]
	TIME [epoch: 2.71 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15317978516988506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15317978516988506 | validation: 0.19359905517985698]
	TIME [epoch: 2.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24958534793955545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24958534793955545 | validation: 0.31838193016232785]
	TIME [epoch: 2.7 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674297373804372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674297373804372 | validation: 0.10144589335762758]
	TIME [epoch: 2.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663688725459929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663688725459929 | validation: 0.1688810433679881]
	TIME [epoch: 2.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11976193953094563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11976193953094563 | validation: 0.1436377594026905]
	TIME [epoch: 2.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15100605423504315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15100605423504315 | validation: 0.17942346703330736]
	TIME [epoch: 2.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07669669663472033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07669669663472033 | validation: 0.08932279754688711]
	TIME [epoch: 2.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09021705624301378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09021705624301378 | validation: 0.243625575450218]
	TIME [epoch: 2.71 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10000378416183757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10000378416183757 | validation: 0.14762515429299655]
	TIME [epoch: 2.7 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12707555612938376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12707555612938376 | validation: 0.37637206526970624]
	TIME [epoch: 2.71 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2356025732392358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2356025732392358 | validation: 0.28487392455275834]
	TIME [epoch: 2.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24511950460481635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24511950460481635 | validation: 0.10048926960234164]
	TIME [epoch: 2.7 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14723973299166937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14723973299166937 | validation: 0.22492788854813456]
	TIME [epoch: 2.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10367944405271541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10367944405271541 | validation: 0.18315401002936166]
	TIME [epoch: 2.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20306913348336722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20306913348336722 | validation: 0.20125211585861533]
	TIME [epoch: 2.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11788083551100503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11788083551100503 | validation: 0.10468122005032919]
	TIME [epoch: 2.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705670216952048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06705670216952048 | validation: 0.12368245821915098]
	TIME [epoch: 2.7 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884038479111258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05884038479111258 | validation: 0.13623346152042046]
	TIME [epoch: 2.71 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810442988864855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06810442988864855 | validation: 0.13141842960480635]
	TIME [epoch: 2.71 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872906359281091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0872906359281091 | validation: 0.20169508832583835]
	TIME [epoch: 2.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381827623916245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1381827623916245 | validation: 0.20269917867640827]
	TIME [epoch: 2.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652444383289609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1652444383289609 | validation: 0.2566834193857747]
	TIME [epoch: 2.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14625588074787696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14625588074787696 | validation: 0.10674297888895096]
	TIME [epoch: 2.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829921999194584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0829921999194584 | validation: 0.22268706868419763]
	TIME [epoch: 2.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175264304554257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10175264304554257 | validation: 0.17191284258310763]
	TIME [epoch: 2.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19387651762125557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19387651762125557 | validation: 0.3222625900502845]
	TIME [epoch: 2.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19283407006556857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19283407006556857 | validation: 0.11177577450478987]
	TIME [epoch: 2.71 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08249572997165958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08249572997165958 | validation: 0.07624753657818849]
	TIME [epoch: 2.71 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.087496344132779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.087496344132779 | validation: 0.3720161082135498]
	TIME [epoch: 2.71 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19359067486609882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19359067486609882 | validation: 0.14928144846123506]
	TIME [epoch: 2.71 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12506223121859186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12506223121859186 | validation: 0.13436857163535218]
	TIME [epoch: 2.71 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297005007005686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297005007005686 | validation: 0.12368281430094204]
	TIME [epoch: 2.71 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626202345278195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626202345278195 | validation: 0.1140107823519251]
	TIME [epoch: 2.71 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059173496688454785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059173496688454785 | validation: 0.11413708653020248]
	TIME [epoch: 2.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0717153345249872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0717153345249872 | validation: 0.17393794978938448]
	TIME [epoch: 2.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18855450062893503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18855450062893503 | validation: 0.18761438879768264]
	TIME [epoch: 2.71 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17373658173559456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17373658173559456 | validation: 0.1706327444649071]
	TIME [epoch: 2.73 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12115584665492451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12115584665492451 | validation: 0.24879999210140913]
	TIME [epoch: 2.71 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13021772726317202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13021772726317202 | validation: 0.19273756963950867]
	TIME [epoch: 2.71 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11524726783880923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11524726783880923 | validation: 0.1465526739079961]
	TIME [epoch: 2.71 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10023707979359164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10023707979359164 | validation: 0.1449407576166724]
	TIME [epoch: 2.71 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1093373962001768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1093373962001768 | validation: 0.13149901716190246]
	TIME [epoch: 2.71 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13154130208603115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13154130208603115 | validation: 0.281286460075959]
	TIME [epoch: 2.71 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1322248729582238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1322248729582238 | validation: 0.10215126923116974]
	TIME [epoch: 2.71 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13636611159593548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13636611159593548 | validation: 0.40278129658691175]
	TIME [epoch: 2.71 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20271442656329086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20271442656329086 | validation: 0.1332732033381845]
	TIME [epoch: 2.71 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778365811779798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05778365811779798 | validation: 0.07142281751697575]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08783141793736805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08783141793736805 | validation: 0.31455273269508116]
	TIME [epoch: 2.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14379321521647126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14379321521647126 | validation: 0.15440163228772447]
	TIME [epoch: 2.71 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448829967853564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09448829967853564 | validation: 0.10966925802868994]
	TIME [epoch: 2.71 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666019373915024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08666019373915024 | validation: 0.22645575534129614]
	TIME [epoch: 2.71 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1852059687456306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1852059687456306 | validation: 0.12731902515390056]
	TIME [epoch: 2.71 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18322589565590694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18322589565590694 | validation: 0.14605853326016258]
	TIME [epoch: 2.71 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08449056826464482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08449056826464482 | validation: 0.2049787267551494]
	TIME [epoch: 2.71 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843248246510062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0843248246510062 | validation: 0.10522442056388831]
	TIME [epoch: 2.71 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13576933064385321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13576933064385321 | validation: 0.29527542710895716]
	TIME [epoch: 2.71 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875019659928576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875019659928576 | validation: 0.15346512561316628]
	TIME [epoch: 2.71 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14166823308772605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14166823308772605 | validation: 0.10159903432583123]
	TIME [epoch: 2.71 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447080848488588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447080848488588 | validation: 0.2780339330632371]
	TIME [epoch: 2.71 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13026189414891204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13026189414891204 | validation: 0.11980223890960172]
	TIME [epoch: 2.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07129849347299316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07129849347299316 | validation: 0.08162343871689541]
	TIME [epoch: 2.71 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041744275667911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06041744275667911 | validation: 0.12483665929206658]
	TIME [epoch: 2.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07191523049427014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07191523049427014 | validation: 0.11140434087288374]
	TIME [epoch: 2.72 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10268953723161821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10268953723161821 | validation: 0.14491425682679246]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11669735395699668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11669735395699668 | validation: 0.21971408955180027]
	TIME [epoch: 2.72 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14230085340015344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14230085340015344 | validation: 0.15024087959451304]
	TIME [epoch: 2.71 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453864479064144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1453864479064144 | validation: 0.3103210850004703]
	TIME [epoch: 2.71 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14050473558533574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14050473558533574 | validation: 0.0893629240688979]
	TIME [epoch: 2.71 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06696663333775864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06696663333775864 | validation: 0.1093659236171432]
	TIME [epoch: 2.71 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05533576607959322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05533576607959322 | validation: 0.13643718475390895]
	TIME [epoch: 2.71 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902181340423315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06902181340423315 | validation: 0.1633007562951459]
	TIME [epoch: 2.71 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15497331892231309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15497331892231309 | validation: 0.30560513609709017]
	TIME [epoch: 2.71 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21238426760629964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21238426760629964 | validation: 0.12216835978841552]
	TIME [epoch: 2.71 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878276430953086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08878276430953086 | validation: 0.12045842714041043]
	TIME [epoch: 2.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792544421411927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06792544421411927 | validation: 0.16531574424190099]
	TIME [epoch: 2.71 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1503924904792341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1503924904792341 | validation: 0.16225838367768905]
	TIME [epoch: 2.71 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458971736244243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458971736244243 | validation: 0.20173463284702686]
	TIME [epoch: 2.71 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11924892041074671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11924892041074671 | validation: 0.11578951069267554]
	TIME [epoch: 2.71 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478696207845632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07478696207845632 | validation: 0.20791680428551207]
	TIME [epoch: 2.71 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698249515677047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07698249515677047 | validation: 0.06563804240002295]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08326206006522441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08326206006522441 | validation: 0.30568570049719046]
	TIME [epoch: 2.71 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13967593805480782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13967593805480782 | validation: 0.09010543244212665]
	TIME [epoch: 2.71 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183169304856626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10183169304856626 | validation: 0.2572752549610818]
	TIME [epoch: 2.71 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16123016489503422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16123016489503422 | validation: 0.12022977773039789]
	TIME [epoch: 2.71 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09606820581668302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09606820581668302 | validation: 0.06589860156490067]
	TIME [epoch: 2.71 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07778787097532212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07778787097532212 | validation: 0.17908593442054258]
	TIME [epoch: 2.71 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414303874064846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08414303874064846 | validation: 0.12115287529730634]
	TIME [epoch: 2.71 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09412697584860748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09412697584860748 | validation: 0.15017250597641318]
	TIME [epoch: 2.71 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08297789353605886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08297789353605886 | validation: 0.1225364517938272]
	TIME [epoch: 2.71 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048030093946825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07048030093946825 | validation: 0.10120427945583899]
	TIME [epoch: 2.71 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057366274246422364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057366274246422364 | validation: 0.12977857001007573]
	TIME [epoch: 2.71 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06785062792689915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06785062792689915 | validation: 0.16009920478995077]
	TIME [epoch: 2.71 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304267112298334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304267112298334 | validation: 0.2908018465411634]
	TIME [epoch: 2.71 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21283292896343242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21283292896343242 | validation: 0.22634425342754608]
	TIME [epoch: 2.71 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531226533694421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531226533694421 | validation: 0.13693783328203932]
	TIME [epoch: 2.71 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14141517136384035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14141517136384035 | validation: 0.2844380688110378]
	TIME [epoch: 2.71 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15836696696456826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15836696696456826 | validation: 0.057075675568632495]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872042663224961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0872042663224961 | validation: 0.24536926667378786]
	TIME [epoch: 2.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10945985745078307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10945985745078307 | validation: 0.09070783652101871]
	TIME [epoch: 2.71 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180651192155951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180651192155951 | validation: 0.08716499665279946]
	TIME [epoch: 2.71 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07399654946473236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07399654946473236 | validation: 0.16836084994217113]
	TIME [epoch: 2.71 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173048577587753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07173048577587753 | validation: 0.0724924151223972]
	TIME [epoch: 2.71 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06644576506877256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06644576506877256 | validation: 0.19453239026407274]
	TIME [epoch: 2.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084170695874676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.084170695874676 | validation: 0.1557680208861371]
	TIME [epoch: 2.71 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.131949949741026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.131949949741026 | validation: 0.3075634398334639]
	TIME [epoch: 2.71 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17870673679647475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17870673679647475 | validation: 0.10637879604496359]
	TIME [epoch: 2.71 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08552762009082436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08552762009082436 | validation: 0.09534767592272973]
	TIME [epoch: 2.71 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366840100204894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366840100204894 | validation: 0.1530100493890231]
	TIME [epoch: 2.71 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495345016745829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07495345016745829 | validation: 0.1228763841337141]
	TIME [epoch: 2.71 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12452366957392133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12452366957392133 | validation: 0.24746737018207768]
	TIME [epoch: 2.71 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13947544606395768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13947544606395768 | validation: 0.10295643357949949]
	TIME [epoch: 2.71 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07886642647664226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07886642647664226 | validation: 0.0623956831854675]
	TIME [epoch: 2.71 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366022296594095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366022296594095 | validation: 0.22729187173590426]
	TIME [epoch: 2.71 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09975628260715333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09975628260715333 | validation: 0.08433635479215995]
	TIME [epoch: 2.71 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11843043632966645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11843043632966645 | validation: 0.21253833332910826]
	TIME [epoch: 2.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181917741220982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09181917741220982 | validation: 0.09067038842646381]
	TIME [epoch: 2.71 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507660947308004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0507660947308004 | validation: 0.08334735891290787]
	TIME [epoch: 2.71 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04656778292489864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04656778292489864 | validation: 0.1359853575409436]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497215077442312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06497215077442312 | validation: 0.11542392469644217]
	TIME [epoch: 2.71 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222709168472948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222709168472948 | validation: 0.2666715624841044]
	TIME [epoch: 2.71 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001374465485283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2001374465485283 | validation: 0.08813954124885082]
	TIME [epoch: 2.71 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697878028880145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0697878028880145 | validation: 0.22571163922945667]
	TIME [epoch: 2.71 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557761902419394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1557761902419394 | validation: 0.26169175098250896]
	TIME [epoch: 2.71 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20914161660351469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20914161660351469 | validation: 0.20456880885370296]
	TIME [epoch: 2.71 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087953547490325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09087953547490325 | validation: 0.11246362879571743]
	TIME [epoch: 2.71 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10033869520961311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10033869520961311 | validation: 0.11240641285965965]
	TIME [epoch: 2.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09432080003398614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09432080003398614 | validation: 0.16970837968560037]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112682211559666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08112682211559666 | validation: 0.0643914783814456]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09059817496924896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09059817496924896 | validation: 0.31634005735111953]
	TIME [epoch: 2.71 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15506145034357366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15506145034357366 | validation: 0.09379470106084581]
	TIME [epoch: 2.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348306131493139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10348306131493139 | validation: 0.0994378111788596]
	TIME [epoch: 2.71 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08056576412446109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08056576412446109 | validation: 0.09869599880122272]
	TIME [epoch: 2.71 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060156959649612984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060156959649612984 | validation: 0.11150954777579032]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07651590033013891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07651590033013891 | validation: 0.1444673014994518]
	TIME [epoch: 2.71 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641044760274375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641044760274375 | validation: 0.2829032037193007]
	TIME [epoch: 2.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14112115336249448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14112115336249448 | validation: 0.10767766037069096]
	TIME [epoch: 2.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08661569045045252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08661569045045252 | validation: 0.12500069706917935]
	TIME [epoch: 2.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05759860883609964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05759860883609964 | validation: 0.0693181392099584]
	TIME [epoch: 2.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04725096227333204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04725096227333204 | validation: 0.15202950165646956]
	TIME [epoch: 2.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058540330120965114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058540330120965114 | validation: 0.090429590255378]
	TIME [epoch: 2.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829073057283264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829073057283264 | validation: 0.26405544013785215]
	TIME [epoch: 2.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14923037641365422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14923037641365422 | validation: 0.09984666796792699]
	TIME [epoch: 2.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1068007201134935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1068007201134935 | validation: 0.08990761820096083]
	TIME [epoch: 2.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0613562013803738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0613562013803738 | validation: 0.14924515760662638]
	TIME [epoch: 2.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06899192853844627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06899192853844627 | validation: 0.1282703337874543]
	TIME [epoch: 2.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12250597961914586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250597961914586 | validation: 0.3813623337104792]
	TIME [epoch: 2.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19607526211641294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19607526211641294 | validation: 0.11940161687690698]
	TIME [epoch: 2.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06904481773278459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06904481773278459 | validation: 0.05905234268700032]
	TIME [epoch: 2.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12723423719915328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12723423719915328 | validation: 0.2883748741718077]
	TIME [epoch: 2.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832390240597037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832390240597037 | validation: 0.16432404923022648]
	TIME [epoch: 2.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145122184903327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07145122184903327 | validation: 0.09475143240690306]
	TIME [epoch: 2.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15174561964981728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15174561964981728 | validation: 0.23088525091230966]
	TIME [epoch: 2.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09649231849766189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09649231849766189 | validation: 0.13882163569747105]
	TIME [epoch: 2.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060028338079473116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060028338079473116 | validation: 0.06289236160247544]
	TIME [epoch: 2.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307695876664056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06307695876664056 | validation: 0.13148652113059822]
	TIME [epoch: 2.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052630451680700664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052630451680700664 | validation: 0.0881308105747689]
	TIME [epoch: 2.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043810757554666103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043810757554666103 | validation: 0.06310547559624889]
	TIME [epoch: 2.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046253679179985456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046253679179985456 | validation: 0.16363264362186603]
	TIME [epoch: 2.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898283288983242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05898283288983242 | validation: 0.1270534055276363]
	TIME [epoch: 2.71 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444894094207701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08444894094207701 | validation: 0.29216989083040285]
	TIME [epoch: 2.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17398952341995494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17398952341995494 | validation: 0.1858158249433547]
	TIME [epoch: 2.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17583083920969858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17583083920969858 | validation: 0.13579231760776467]
	TIME [epoch: 2.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1057941631552228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1057941631552228 | validation: 0.16191378522017832]
	TIME [epoch: 2.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11823750344838024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11823750344838024 | validation: 0.11961563138277523]
	TIME [epoch: 2.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14438310821882486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14438310821882486 | validation: 0.2557154564473458]
	TIME [epoch: 2.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16586774382117042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16586774382117042 | validation: 0.06173224354379299]
	TIME [epoch: 2.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05290392858743752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05290392858743752 | validation: 0.10902395207249507]
	TIME [epoch: 2.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061199263792484845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061199263792484845 | validation: 0.09410071215555521]
	TIME [epoch: 2.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067974740274488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09067974740274488 | validation: 0.08866516776999174]
	TIME [epoch: 2.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560505370834293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04560505370834293 | validation: 0.0890649342913844]
	TIME [epoch: 2.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03711828859230841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03711828859230841 | validation: 0.07681247848704105]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770227752704559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03770227752704559 | validation: 0.0822022474901114]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04580450584278891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04580450584278891 | validation: 0.13116339264557084]
	TIME [epoch: 2.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640331756339191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07640331756339191 | validation: 0.23294605344008323]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15399880574498742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15399880574498742 | validation: 0.21381934073215714]
	TIME [epoch: 2.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1667023689894499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1667023689894499 | validation: 0.11658852107941052]
	TIME [epoch: 2.7 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986161029620321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0986161029620321 | validation: 0.10242587359616512]
	TIME [epoch: 2.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052543122070023277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052543122070023277 | validation: 0.07436448007260353]
	TIME [epoch: 2.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668328127933009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08668328127933009 | validation: 0.23451553807853376]
	TIME [epoch: 2.7 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11624893045456997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11624893045456997 | validation: 0.05631062667274616]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08882309033300725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08882309033300725 | validation: 0.25376721877084124]
	TIME [epoch: 2.71 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11934142945896682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11934142945896682 | validation: 0.1579725001792406]
	TIME [epoch: 2.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659044169659112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1659044169659112 | validation: 0.1926200253975584]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11202533908022569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11202533908022569 | validation: 0.09328520749881133]
	TIME [epoch: 2.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087105094200224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087105094200224 | validation: 0.11669027616296418]
	TIME [epoch: 2.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06094151668170587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06094151668170587 | validation: 0.07665876189141073]
	TIME [epoch: 2.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154212927061916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07154212927061916 | validation: 0.12311936402836161]
	TIME [epoch: 2.71 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057208399341422855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057208399341422855 | validation: 0.09542042885987466]
	TIME [epoch: 2.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0447828847777673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0447828847777673 | validation: 0.07203300660319249]
	TIME [epoch: 2.71 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04298152650305581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04298152650305581 | validation: 0.11720836599283171]
	TIME [epoch: 2.71 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853047758153614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04853047758153614 | validation: 0.08827925473488016]
	TIME [epoch: 2.71 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661364807526365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0661364807526365 | validation: 0.19671173198871483]
	TIME [epoch: 2.71 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10462553208638864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10462553208638864 | validation: 0.1577444729896067]
	TIME [epoch: 2.71 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411584343678022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1411584343678022 | validation: 0.12843467957834995]
	TIME [epoch: 2.71 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10637586642697193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10637586642697193 | validation: 0.15112775382961285]
	TIME [epoch: 2.71 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952035560345644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952035560345644 | validation: 0.049961215983025324]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07456855119899293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07456855119899293 | validation: 0.3009682086820273]
	TIME [epoch: 2.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13748353997130786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13748353997130786 | validation: 0.09712032359003314]
	TIME [epoch: 2.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09676180711548606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09676180711548606 | validation: 0.2550539756393031]
	TIME [epoch: 2.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200139842608312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2200139842608312 | validation: 0.14225973923934018]
	TIME [epoch: 2.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1372606846451821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1372606846451821 | validation: 0.17809194616945365]
	TIME [epoch: 2.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10973938686840215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10973938686840215 | validation: 0.13532444345299455]
	TIME [epoch: 2.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867891628446298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07867891628446298 | validation: 0.0666391466199126]
	TIME [epoch: 2.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04829161521840649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04829161521840649 | validation: 0.16125305974667672]
	TIME [epoch: 2.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06217140717138246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06217140717138246 | validation: 0.05102683331288266]
	TIME [epoch: 2.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072846392505694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07072846392505694 | validation: 0.1723819508744523]
	TIME [epoch: 2.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06022169089423361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06022169089423361 | validation: 0.07520446725594361]
	TIME [epoch: 2.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05879192203448441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05879192203448441 | validation: 0.137133719673599]
	TIME [epoch: 2.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103472409547852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08103472409547852 | validation: 0.07358852320630822]
	TIME [epoch: 2.7 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678469477439783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0678469477439783 | validation: 0.12070029576463093]
	TIME [epoch: 2.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05422185066408741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05422185066408741 | validation: 0.06514024998341163]
	TIME [epoch: 2.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052127821059705134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052127821059705134 | validation: 0.15245596891622257]
	TIME [epoch: 2.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057804057033799816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057804057033799816 | validation: 0.06846896076089178]
	TIME [epoch: 2.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07684504417407392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07684504417407392 | validation: 0.20217603100181517]
	TIME [epoch: 2.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12465813077422734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12465813077422734 | validation: 0.21042249717708797]
	TIME [epoch: 2.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16435310407333592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16435310407333592 | validation: 0.1463890464218362]
	TIME [epoch: 2.7 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12524611507554773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12524611507554773 | validation: 0.2503505368717422]
	TIME [epoch: 2.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754058749935128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11754058749935128 | validation: 0.08495383723291482]
	TIME [epoch: 2.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186882597114047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08186882597114047 | validation: 0.08824986416180941]
	TIME [epoch: 2.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09723042253082803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09723042253082803 | validation: 0.18508017073130803]
	TIME [epoch: 2.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824011257109846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0824011257109846 | validation: 0.07479163250913325]
	TIME [epoch: 2.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543343378830723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05543343378830723 | validation: 0.08382498677876349]
	TIME [epoch: 2.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03759091037419903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03759091037419903 | validation: 0.08033115509275626]
	TIME [epoch: 2.7 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03571095041791585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03571095041791585 | validation: 0.06719342850579191]
	TIME [epoch: 2.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03631312312151858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03631312312151858 | validation: 0.10883735956114574]
	TIME [epoch: 2.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046253708767035405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046253708767035405 | validation: 0.09625544561710134]
	TIME [epoch: 2.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0812750074227017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0812750074227017 | validation: 0.17920768459246741]
	TIME [epoch: 2.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115338956188606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115338956188606 | validation: 0.0881727025555899]
	TIME [epoch: 2.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09549210288688366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09549210288688366 | validation: 0.24953265535669683]
	TIME [epoch: 2.7 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11357677915623861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11357677915623861 | validation: 0.04333156622923397]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05835333471866178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05835333471866178 | validation: 0.15634130419138656]
	TIME [epoch: 2.71 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07398466486916962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07398466486916962 | validation: 0.1664642977057803]
	TIME [epoch: 2.71 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964208852241024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10964208852241024 | validation: 0.14099359882923498]
	TIME [epoch: 2.71 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705526273103531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07705526273103531 | validation: 0.10906223576100929]
	TIME [epoch: 2.71 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07268896365358518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07268896365358518 | validation: 0.08995601554035115]
	TIME [epoch: 2.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224662593409963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09224662593409963 | validation: 0.1602872761678186]
	TIME [epoch: 2.71 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1169388287104568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1169388287104568 | validation: 0.22756670834141624]
	TIME [epoch: 2.71 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14812534056340515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14812534056340515 | validation: 0.08350078303088489]
	TIME [epoch: 2.71 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08383937195615623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08383937195615623 | validation: 0.21341340321031874]
	TIME [epoch: 2.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09034286515900082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09034286515900082 | validation: 0.057725367042448794]
	TIME [epoch: 2.71 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891174966299664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04891174966299664 | validation: 0.0842766343355072]
	TIME [epoch: 2.71 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660411028029657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04660411028029657 | validation: 0.10278776552810179]
	TIME [epoch: 2.71 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439050141572306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07439050141572306 | validation: 0.11315568303610878]
	TIME [epoch: 2.71 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142974460526468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09142974460526468 | validation: 0.09186241518734187]
	TIME [epoch: 2.71 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067180655252364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06067180655252364 | validation: 0.11968446826855494]
	TIME [epoch: 2.71 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05471455012654643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05471455012654643 | validation: 0.0816972822138229]
	TIME [epoch: 2.71 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050175666743026606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050175666743026606 | validation: 0.07701421507009876]
	TIME [epoch: 2.71 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569082145683748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0569082145683748 | validation: 0.15170279312842483]
	TIME [epoch: 2.71 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874051677356325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07874051677356325 | validation: 0.12668135353858445]
	TIME [epoch: 2.71 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12333317698949764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12333317698949764 | validation: 0.3341786879082293]
	TIME [epoch: 2.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18518805847485445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18518805847485445 | validation: 0.07748651744047358]
	TIME [epoch: 2.71 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06370114338747503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06370114338747503 | validation: 0.0690426132057466]
	TIME [epoch: 2.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06981367073346627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06981367073346627 | validation: 0.16799897564024852]
	TIME [epoch: 2.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09516680439609575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09516680439609575 | validation: 0.08170295311126896]
	TIME [epoch: 2.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09607950634047623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09607950634047623 | validation: 0.06684112121865779]
	TIME [epoch: 2.71 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045598349569298494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045598349569298494 | validation: 0.13221332249594653]
	TIME [epoch: 2.71 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05138036234300512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05138036234300512 | validation: 0.05851561512244349]
	TIME [epoch: 2.71 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907671023394542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06907671023394542 | validation: 0.20727852398801427]
	TIME [epoch: 2.71 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937467832834519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0937467832834519 | validation: 0.06304641070233213]
	TIME [epoch: 2.71 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07130778712305906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07130778712305906 | validation: 0.10059383613823632]
	TIME [epoch: 2.71 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052961108006610476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052961108006610476 | validation: 0.1001634382321347]
	TIME [epoch: 2.71 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05413871486979185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05413871486979185 | validation: 0.10965581660448931]
	TIME [epoch: 2.71 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08762438888660785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08762438888660785 | validation: 0.2400807395229344]
	TIME [epoch: 2.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14436538646701005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14436538646701005 | validation: 0.10921777582001045]
	TIME [epoch: 2.71 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11502793325948756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11502793325948756 | validation: 0.21762398864583368]
	TIME [epoch: 2.71 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590686962711537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09590686962711537 | validation: 0.05874634890075924]
	TIME [epoch: 2.71 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05015957432741198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05015957432741198 | validation: 0.06687056249748495]
	TIME [epoch: 2.71 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049557837145973754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049557837145973754 | validation: 0.1376423092969521]
	TIME [epoch: 2.71 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08583409203419884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08583409203419884 | validation: 0.11961354319115391]
	TIME [epoch: 2.71 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11599846447560698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11599846447560698 | validation: 0.09173222845131519]
	TIME [epoch: 2.71 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691752200404981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05691752200404981 | validation: 0.09053959339019657]
	TIME [epoch: 2.71 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044947722099146416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044947722099146416 | validation: 0.056004781133164906]
	TIME [epoch: 2.71 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04956937968740635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04956937968740635 | validation: 0.10039517773587021]
	TIME [epoch: 2.71 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06032625780817948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06032625780817948 | validation: 0.11372825588273948]
	TIME [epoch: 2.71 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467570356067365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08467570356067365 | validation: 0.07606705329266733]
	TIME [epoch: 2.71 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062254083316711346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062254083316711346 | validation: 0.15932344047871128]
	TIME [epoch: 2.71 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06515453979778649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06515453979778649 | validation: 0.07114632284090894]
	TIME [epoch: 2.71 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07346329372976153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07346329372976153 | validation: 0.2376313850492708]
	TIME [epoch: 2.71 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11286163515167262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11286163515167262 | validation: 0.09922754244323023]
	TIME [epoch: 2.71 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09171997594094751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09171997594094751 | validation: 0.1002685507368008]
	TIME [epoch: 2.71 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09287403399022096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09287403399022096 | validation: 0.14392841650029578]
	TIME [epoch: 2.71 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08738353668208142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08738353668208142 | validation: 0.10564430948051472]
	TIME [epoch: 2.71 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261840316856328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261840316856328 | validation: 0.1551629280728204]
	TIME [epoch: 2.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06930885038417774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06930885038417774 | validation: 0.08246919356796628]
	TIME [epoch: 2.71 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655261536824883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05655261536824883 | validation: 0.05760509602135325]
	TIME [epoch: 2.71 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689221204179428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0689221204179428 | validation: 0.13790860841431796]
	TIME [epoch: 2.71 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058999606650220304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058999606650220304 | validation: 0.06667564431644336]
	TIME [epoch: 2.71 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662027653157166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662027653157166 | validation: 0.1320712923476208]
	TIME [epoch: 2.71 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063684410793465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07063684410793465 | validation: 0.1399163236112655]
	TIME [epoch: 2.71 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11977099541553343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11977099541553343 | validation: 0.13767794776705267]
	TIME [epoch: 2.71 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09064680421303739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09064680421303739 | validation: 0.06521827159125576]
	TIME [epoch: 2.71 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04705516113347812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04705516113347812 | validation: 0.07711333297225838]
	TIME [epoch: 2.71 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03432642244698394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03432642244698394 | validation: 0.046587298374528456]
	TIME [epoch: 2.71 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04789666505883988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04789666505883988 | validation: 0.13872993659281793]
	TIME [epoch: 2.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507275319029497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0507275319029497 | validation: 0.03865663709050792]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04012048216276793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04012048216276793 | validation: 0.14197125443039363]
	TIME [epoch: 2.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056955751016407284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056955751016407284 | validation: 0.13295222883545546]
	TIME [epoch: 2.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1488154245433286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1488154245433286 | validation: 0.2775875496100068]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1623962311127407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1623962311127407 | validation: 0.1347400907726192]
	TIME [epoch: 2.69 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10578973427491618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10578973427491618 | validation: 0.1300935274408344]
	TIME [epoch: 2.69 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10151713226432786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10151713226432786 | validation: 0.1643307750165537]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070199286270517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.070199286270517 | validation: 0.06338405194875674]
	TIME [epoch: 2.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048044468413854444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048044468413854444 | validation: 0.07997080790015389]
	TIME [epoch: 2.69 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041245112155632035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041245112155632035 | validation: 0.09704203352357749]
	TIME [epoch: 2.69 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735800988414399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06735800988414399 | validation: 0.07536079397051011]
	TIME [epoch: 2.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943790841726917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07943790841726917 | validation: 0.10007655986889248]
	TIME [epoch: 2.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073566985500718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04073566985500718 | validation: 0.06785698593011073]
	TIME [epoch: 2.69 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030162519569682286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030162519569682286 | validation: 0.06326497708757708]
	TIME [epoch: 2.71 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035107320533565425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035107320533565425 | validation: 0.13203407906818007]
	TIME [epoch: 2.71 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050756132279638745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050756132279638745 | validation: 0.10880751564018819]
	TIME [epoch: 2.71 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321713076273444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09321713076273444 | validation: 0.20320193721364488]
	TIME [epoch: 2.71 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11965062088713936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11965062088713936 | validation: 0.13877485864740327]
	TIME [epoch: 2.71 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983092206434388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0983092206434388 | validation: 0.05689999254178647]
	TIME [epoch: 2.71 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07975556088480022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07975556088480022 | validation: 0.24228793048709246]
	TIME [epoch: 2.71 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182347182017988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10182347182017988 | validation: 0.0882958320977979]
	TIME [epoch: 2.71 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10479459748404235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10479459748404235 | validation: 0.15333734983870695]
	TIME [epoch: 2.71 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10648162472478624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10648162472478624 | validation: 0.07710469011598135]
	TIME [epoch: 2.71 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05484712026009508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05484712026009508 | validation: 0.10554316397763873]
	TIME [epoch: 2.71 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455361234276352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0455361234276352 | validation: 0.07099563830419657]
	TIME [epoch: 2.71 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05012788677734818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05012788677734818 | validation: 0.09147035635699236]
	TIME [epoch: 2.71 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05339715487683082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05339715487683082 | validation: 0.08662973416023972]
	TIME [epoch: 2.71 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410667280927613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05410667280927613 | validation: 0.1056845188699862]
	TIME [epoch: 2.71 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057640119216759766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057640119216759766 | validation: 0.0861434883765943]
	TIME [epoch: 2.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303977781334311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05303977781334311 | validation: 0.08489745726637932]
	TIME [epoch: 2.71 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041455546391827906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041455546391827906 | validation: 0.09038053231572239]
	TIME [epoch: 2.71 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03737879857003014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03737879857003014 | validation: 0.07594030065660251]
	TIME [epoch: 2.71 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04243931170480326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04243931170480326 | validation: 0.1419026315764362]
	TIME [epoch: 2.71 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166641777691393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08166641777691393 | validation: 0.16381922149953854]
	TIME [epoch: 2.71 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18168535402233943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18168535402233943 | validation: 0.1686008245149168]
	TIME [epoch: 2.71 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0891609134100106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0891609134100106 | validation: 0.15972772954802814]
	TIME [epoch: 2.71 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057013992375267754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057013992375267754 | validation: 0.09251549136810161]
	TIME [epoch: 2.71 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11984019698732955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11984019698732955 | validation: 0.248086548484726]
	TIME [epoch: 2.71 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12074743900278893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12074743900278893 | validation: 0.09179833093784262]
	TIME [epoch: 2.71 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08483069055587247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08483069055587247 | validation: 0.053463132099906586]
	TIME [epoch: 182 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09639294780327638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09639294780327638 | validation: 0.13548027330072102]
	TIME [epoch: 5.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025894059414337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05025894059414337 | validation: 0.1059791156151587]
	TIME [epoch: 5.79 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07126670534096094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07126670534096094 | validation: 0.08110689760075307]
	TIME [epoch: 5.79 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354833380998253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07354833380998253 | validation: 0.05577368713727833]
	TIME [epoch: 5.79 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03567648059321541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03567648059321541 | validation: 0.12690886241270732]
	TIME [epoch: 5.79 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04121673968097975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04121673968097975 | validation: 0.04710422459647639]
	TIME [epoch: 5.78 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03175785023538127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03175785023538127 | validation: 0.060599149026372756]
	TIME [epoch: 5.79 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03143867992826541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03143867992826541 | validation: 0.09623414487856888]
	TIME [epoch: 5.79 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498312053099951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03498312053099951 | validation: 0.0817192018880166]
	TIME [epoch: 5.79 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557811313537434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05557811313537434 | validation: 0.24985088915577836]
	TIME [epoch: 5.79 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13477682715569828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13477682715569828 | validation: 0.15198088699028056]
	TIME [epoch: 5.79 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500622385140991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1500622385140991 | validation: 0.13228194139013658]
	TIME [epoch: 5.79 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583985986780556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06583985986780556 | validation: 0.07675158890001038]
	TIME [epoch: 5.79 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279554299344784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04279554299344784 | validation: 0.07141799144259103]
	TIME [epoch: 5.79 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975324242821082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06975324242821082 | validation: 0.20306898942279378]
	TIME [epoch: 5.79 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10640189090318661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10640189090318661 | validation: 0.09682123610334506]
	TIME [epoch: 5.79 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10848326394339398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10848326394339398 | validation: 0.1642966422704746]
	TIME [epoch: 5.79 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546059410620382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06546059410620382 | validation: 0.05888383172425676]
	TIME [epoch: 5.79 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034667298216363614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034667298216363614 | validation: 0.0625938260590611]
	TIME [epoch: 5.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04465043281816721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04465043281816721 | validation: 0.11391391048873843]
	TIME [epoch: 5.79 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347812454075732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04347812454075732 | validation: 0.05790732759672488]
	TIME [epoch: 5.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843719654461026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03843719654461026 | validation: 0.09297107891099335]
	TIME [epoch: 5.79 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347103616519675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04347103616519675 | validation: 0.08140579330702659]
	TIME [epoch: 5.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539191926134295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06539191926134295 | validation: 0.07806280230856558]
	TIME [epoch: 5.8 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09049588188418121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09049588188418121 | validation: 0.12788948667517716]
	TIME [epoch: 5.8 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06180275311106138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06180275311106138 | validation: 0.06731047693338645]
	TIME [epoch: 5.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375399515311047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04375399515311047 | validation: 0.07995858844121051]
	TIME [epoch: 5.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047940501791741696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047940501791741696 | validation: 0.18519067039366377]
	TIME [epoch: 5.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1024588026900531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1024588026900531 | validation: 0.2090682558148098]
	TIME [epoch: 5.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17618282263053314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17618282263053314 | validation: 0.11483882404491741]
	TIME [epoch: 5.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07429952068660413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07429952068660413 | validation: 0.09148483988421591]
	TIME [epoch: 5.79 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033462477460510884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033462477460510884 | validation: 0.04360155552797427]
	TIME [epoch: 5.79 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03944245762084857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03944245762084857 | validation: 0.11659701865460162]
	TIME [epoch: 5.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754762063674508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03754762063674508 | validation: 0.05270477934119971]
	TIME [epoch: 5.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038378173973735934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038378173973735934 | validation: 0.1300710326991432]
	TIME [epoch: 5.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05353891126190138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05353891126190138 | validation: 0.13366726013277339]
	TIME [epoch: 5.79 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14127591635930378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14127591635930378 | validation: 0.26616649937549214]
	TIME [epoch: 5.8 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16506532011663932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16506532011663932 | validation: 0.09172541260045823]
	TIME [epoch: 5.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768109947831681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0768109947831681 | validation: 0.09349390346190914]
	TIME [epoch: 5.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885529609061688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885529609061688 | validation: 0.09023887258645526]
	TIME [epoch: 5.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940758387217572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07940758387217572 | validation: 0.08246868937397946]
	TIME [epoch: 5.8 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04258837236372731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04258837236372731 | validation: 0.04840010408207768]
	TIME [epoch: 5.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518181312618488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03518181312618488 | validation: 0.07163290220320305]
	TIME [epoch: 5.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03591939061222519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03591939061222519 | validation: 0.06618179874503397]
	TIME [epoch: 5.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03944491609036718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03944491609036718 | validation: 0.07256847315523583]
	TIME [epoch: 5.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813214392187552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03813214392187552 | validation: 0.10030797010264765]
	TIME [epoch: 5.79 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0467144851834827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0467144851834827 | validation: 0.10145444511328856]
	TIME [epoch: 5.79 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07915145034717598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07915145034717598 | validation: 0.2491444404327067]
	TIME [epoch: 5.79 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061878319194195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11061878319194195 | validation: 0.09233019736499043]
	TIME [epoch: 5.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08389366874520254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08389366874520254 | validation: 0.07953241380113052]
	TIME [epoch: 5.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055681681636174395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055681681636174395 | validation: 0.1932074436684142]
	TIME [epoch: 5.8 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546655542971529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08546655542971529 | validation: 0.1054052495220138]
	TIME [epoch: 5.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12500189533686176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12500189533686176 | validation: 0.19565456380363153]
	TIME [epoch: 5.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09588470559736005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09588470559736005 | validation: 0.07318263291650508]
	TIME [epoch: 5.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036408437088967134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036408437088967134 | validation: 0.031264134713741634]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_1056.pth
	Model improved!!!
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043074300431960734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043074300431960734 | validation: 0.12305436749511714]
	TIME [epoch: 5.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052802000355224823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052802000355224823 | validation: 0.056046068173484224]
	TIME [epoch: 5.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08427481316321803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08427481316321803 | validation: 0.10849803821403614]
	TIME [epoch: 5.81 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048267136514342404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048267136514342404 | validation: 0.1198155881730417]
	TIME [epoch: 5.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05507440815519218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05507440815519218 | validation: 0.08992790413472634]
	TIME [epoch: 5.81 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08405274723347486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08405274723347486 | validation: 0.15449204392586957]
	TIME [epoch: 5.8 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07896956807421267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07896956807421267 | validation: 0.06394144710414007]
	TIME [epoch: 5.81 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053138737509091157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053138737509091157 | validation: 0.0644478848432168]
	TIME [epoch: 5.8 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045476423492609976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045476423492609976 | validation: 0.08554772810618318]
	TIME [epoch: 5.81 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697331407956543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0697331407956543 | validation: 0.10749281197158617]
	TIME [epoch: 5.81 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09255004414965459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09255004414965459 | validation: 0.10804324227618103]
	TIME [epoch: 5.8 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07667993145652277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07667993145652277 | validation: 0.22808783141453387]
	TIME [epoch: 5.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09522445055692319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09522445055692319 | validation: 0.05656990240277865]
	TIME [epoch: 5.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040370787905761825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040370787905761825 | validation: 0.046847361080949224]
	TIME [epoch: 5.8 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030260030228131864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030260030228131864 | validation: 0.0816366472813376]
	TIME [epoch: 5.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028037584897486402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028037584897486402 | validation: 0.045873125570061804]
	TIME [epoch: 5.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03032847042851657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03032847042851657 | validation: 0.10537020438536931]
	TIME [epoch: 5.8 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05560938183793341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05560938183793341 | validation: 0.13053578542967811]
	TIME [epoch: 5.79 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.123068489664967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.123068489664967 | validation: 0.1815024656718221]
	TIME [epoch: 5.79 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178829750366247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11178829750366247 | validation: 0.06643752185819488]
	TIME [epoch: 5.79 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202400057211287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05202400057211287 | validation: 0.13014442822835903]
	TIME [epoch: 5.8 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07472872524797622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07472872524797622 | validation: 0.10062436098796841]
	TIME [epoch: 5.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09992710976113298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09992710976113298 | validation: 0.06493401068974497]
	TIME [epoch: 5.8 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04995650297586896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04995650297586896 | validation: 0.15574677020529004]
	TIME [epoch: 5.8 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114039115595588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07114039115595588 | validation: 0.07512305166848142]
	TIME [epoch: 5.81 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456657586510685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06456657586510685 | validation: 0.0961990486172272]
	TIME [epoch: 5.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678995101061585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05678995101061585 | validation: 0.1063244068363348]
	TIME [epoch: 5.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047818912234921296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047818912234921296 | validation: 0.05510024702912325]
	TIME [epoch: 5.79 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941595360865549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04941595360865549 | validation: 0.11687610148835788]
	TIME [epoch: 5.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04961922573988253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04961922573988253 | validation: 0.07030017608262852]
	TIME [epoch: 5.79 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06673233097246234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06673233097246234 | validation: 0.1664053674329555]
	TIME [epoch: 5.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08648743093635776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08648743093635776 | validation: 0.1012565805296668]
	TIME [epoch: 5.79 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841653358710081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841653358710081 | validation: 0.10487665500793808]
	TIME [epoch: 5.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788630523429405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06788630523429405 | validation: 0.10553270149763475]
	TIME [epoch: 5.79 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06007608640266447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06007608640266447 | validation: 0.10066394561354759]
	TIME [epoch: 5.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639286554448838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0639286554448838 | validation: 0.0871978385409797]
	TIME [epoch: 5.8 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365066626787202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06365066626787202 | validation: 0.0928703779374011]
	TIME [epoch: 5.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06174791441732689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06174791441732689 | validation: 0.0551432224067894]
	TIME [epoch: 5.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399613977080915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06399613977080915 | validation: 0.08122076364824683]
	TIME [epoch: 5.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03111069115728374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03111069115728374 | validation: 0.0475951339295344]
	TIME [epoch: 5.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520250118290522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02520250118290522 | validation: 0.08361981805729117]
	TIME [epoch: 5.81 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027504087388826638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027504087388826638 | validation: 0.05760015459743907]
	TIME [epoch: 5.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039986069639187376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039986069639187376 | validation: 0.15874538127344967]
	TIME [epoch: 5.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07000473919434233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07000473919434233 | validation: 0.1270642281496801]
	TIME [epoch: 5.79 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11222086816176724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11222086816176724 | validation: 0.14157191724313312]
	TIME [epoch: 5.79 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093154120042276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07093154120042276 | validation: 0.07287715387586909]
	TIME [epoch: 5.79 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03654387647436586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03654387647436586 | validation: 0.04029453560833124]
	TIME [epoch: 5.79 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04312991974594551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04312991974594551 | validation: 0.15613185934948912]
	TIME [epoch: 5.79 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06751087690566004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06751087690566004 | validation: 0.04215572228926367]
	TIME [epoch: 5.79 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04548932681490178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04548932681490178 | validation: 0.07920988662845851]
	TIME [epoch: 5.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053914274504322124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053914274504322124 | validation: 0.2632031181397738]
	TIME [epoch: 5.79 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14513844587233568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14513844587233568 | validation: 0.17316310622594397]
	TIME [epoch: 5.79 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1480663093676494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1480663093676494 | validation: 0.1499479874503322]
	TIME [epoch: 5.79 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699480956115444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0699480956115444 | validation: 0.05745944779099221]
	TIME [epoch: 5.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03297495976831105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03297495976831105 | validation: 0.06855213597493998]
	TIME [epoch: 5.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806741002421655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03806741002421655 | validation: 0.06196535273477486]
	TIME [epoch: 5.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577990128610996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03577990128610996 | validation: 0.06993321592429964]
	TIME [epoch: 5.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038920909473680924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038920909473680924 | validation: 0.10203927615471087]
	TIME [epoch: 5.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641798508991369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641798508991369 | validation: 0.14055472679190562]
	TIME [epoch: 5.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11250432002919862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11250432002919862 | validation: 0.1056746137031249]
	TIME [epoch: 5.81 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10235869691599864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10235869691599864 | validation: 0.09177014143664307]
	TIME [epoch: 5.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040050918621095076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040050918621095076 | validation: 0.049650442524830196]
	TIME [epoch: 5.8 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03598025064614222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03598025064614222 | validation: 0.0844549138453866]
	TIME [epoch: 5.79 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170600952171586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05170600952171586 | validation: 0.06659252185656321]
	TIME [epoch: 5.79 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04648828609252142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04648828609252142 | validation: 0.1172839729387984]
	TIME [epoch: 5.79 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047293946726281746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047293946726281746 | validation: 0.059476719480075074]
	TIME [epoch: 5.79 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05234870024815834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05234870024815834 | validation: 0.14105602163082007]
	TIME [epoch: 5.79 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0602028463117985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0602028463117985 | validation: 0.06976076949470565]
	TIME [epoch: 5.79 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056178868549471905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056178868549471905 | validation: 0.08466846171007965]
	TIME [epoch: 5.79 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897885270116488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06897885270116488 | validation: 0.17338424067436567]
	TIME [epoch: 5.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10839064175636136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10839064175636136 | validation: 0.09091029337735687]
	TIME [epoch: 5.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09624884230035534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09624884230035534 | validation: 0.16192567317813789]
	TIME [epoch: 5.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0665439859563572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0665439859563572 | validation: 0.10031285634412167]
	TIME [epoch: 5.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09400498114140826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09400498114140826 | validation: 0.07145314246989849]
	TIME [epoch: 5.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0618801467685614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0618801467685614 | validation: 0.0854513160512219]
	TIME [epoch: 5.79 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030804276309959047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030804276309959047 | validation: 0.0789124769539527]
	TIME [epoch: 5.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03794900582729392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03794900582729392 | validation: 0.06716452828521459]
	TIME [epoch: 5.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035557987334212865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035557987334212865 | validation: 0.04837557589525811]
	TIME [epoch: 5.79 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02923820616299131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02923820616299131 | validation: 0.10600032599715631]
	TIME [epoch: 5.79 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03869336210481467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03869336210481467 | validation: 0.07788418768868943]
	TIME [epoch: 5.79 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07567044392346445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07567044392346445 | validation: 0.1797498409133605]
	TIME [epoch: 5.79 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09852470510515024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09852470510515024 | validation: 0.08502352470412051]
	TIME [epoch: 5.79 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06934304995104631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06934304995104631 | validation: 0.06571460759291702]
	TIME [epoch: 5.79 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321253418072327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04321253418072327 | validation: 0.1523702645111667]
	TIME [epoch: 5.79 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06393173577464788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06393173577464788 | validation: 0.08935297929112107]
	TIME [epoch: 5.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08394970826980805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08394970826980805 | validation: 0.16651894340613027]
	TIME [epoch: 5.8 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.100209590196426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.100209590196426 | validation: 0.06523884017933544]
	TIME [epoch: 5.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055496452797789024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055496452797789024 | validation: 0.06037046905473145]
	TIME [epoch: 5.8 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331638496527958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03331638496527958 | validation: 0.07292251363784645]
	TIME [epoch: 5.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033171037076465175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033171037076465175 | validation: 0.05769188793801804]
	TIME [epoch: 5.81 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044585819444538974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044585819444538974 | validation: 0.08463849313820919]
	TIME [epoch: 5.8 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716327533807327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05716327533807327 | validation: 0.07659384806743985]
	TIME [epoch: 5.8 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05335091986778085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05335091986778085 | validation: 0.08642579007229592]
	TIME [epoch: 5.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06135679028136301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06135679028136301 | validation: 0.07533084884849141]
	TIME [epoch: 5.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047454907030118586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047454907030118586 | validation: 0.04467649712514467]
	TIME [epoch: 5.79 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040152671391716015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040152671391716015 | validation: 0.0914182104516621]
	TIME [epoch: 5.8 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677714062624278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03677714062624278 | validation: 0.034889214450647445]
	TIME [epoch: 5.79 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052771682426407084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052771682426407084 | validation: 0.2162567024805001]
	TIME [epoch: 5.79 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582436983403095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582436983403095 | validation: 0.11264875112953728]
	TIME [epoch: 5.79 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10784040696047857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10784040696047857 | validation: 0.1451023590867562]
	TIME [epoch: 5.79 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127324125517214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.127324125517214 | validation: 0.09248694872165278]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_155020/states/model_phi1_4a_v_mmd2_1157.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3570.647 seconds.
