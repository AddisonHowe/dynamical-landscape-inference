Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/basic/data_phi1_3c/training', validation_data='data/training_data/basic/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 578636314

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.938235617503051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.938235617503051 | validation: 3.582410752054311]
	TIME [epoch: 250 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8731966951273207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8731966951273207 | validation: 3.1606699143138552]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5407092056127873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5407092056127873 | validation: 3.132481319442148]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4308730178064084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4308730178064084 | validation: 2.6671571872023208]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0485313527180096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0485313527180096 | validation: 2.0043721980705222]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.524584322774948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.524584322774948 | validation: 2.1723335931749026]
	TIME [epoch: 2.75 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4203654847085625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4203654847085625 | validation: 2.084214906883112]
	TIME [epoch: 2.75 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4502193118536493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4502193118536493 | validation: 1.5215475728162933]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0010381581424843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0010381581424843 | validation: 1.6798136845199723]
	TIME [epoch: 2.75 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8027994764066635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8027994764066635 | validation: 1.4427312462691602]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7924159237308603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7924159237308603 | validation: 1.2694009308846712]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6037573874766489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6037573874766489 | validation: 1.2288552486399578]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4194360201383143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4194360201383143 | validation: 1.0608320296926521]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2770017177009525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2770017177009525 | validation: 1.1789953697778637]
	TIME [epoch: 2.76 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5213046907680556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5213046907680556 | validation: 1.0800365324798553]
	TIME [epoch: 2.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190431105031035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.190431105031035 | validation: 1.3994552205317934]
	TIME [epoch: 2.76 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5268082416416784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5268082416416784 | validation: 1.1035260937912246]
	TIME [epoch: 2.75 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1695662395492912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1695662395492912 | validation: 1.0196263136563923]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125630778897377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.125630778897377 | validation: 0.9083791449026997]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0667720929903832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0667720929903832 | validation: 0.9647849996825251]
	TIME [epoch: 2.76 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.939207932731019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.939207932731019 | validation: 0.9988145225743867]
	TIME [epoch: 2.75 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9985810555724177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9985810555724177 | validation: 0.9814176577859914]
	TIME [epoch: 2.76 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728553530180927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9728553530180927 | validation: 0.9447616713943185]
	TIME [epoch: 2.76 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8482115256630063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8482115256630063 | validation: 0.9222880791127738]
	TIME [epoch: 2.76 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550827911619053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8550827911619053 | validation: 0.7820661706405162]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236784494230185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9236784494230185 | validation: 0.9146205610251187]
	TIME [epoch: 2.76 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287578000356735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8287578000356735 | validation: 0.8466109151238239]
	TIME [epoch: 2.76 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599355466739661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599355466739661 | validation: 1.0116139542887062]
	TIME [epoch: 2.76 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9538359681896456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9538359681896456 | validation: 0.88654509794726]
	TIME [epoch: 2.76 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327397850809053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327397850809053 | validation: 0.7382087091695051]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161545246810269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161545246810269 | validation: 0.8689772022587077]
	TIME [epoch: 2.76 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708200530993164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8708200530993164 | validation: 0.7505964291261629]
	TIME [epoch: 2.76 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625935073723784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625935073723784 | validation: 0.7172835707072753]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584093346463461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584093346463461 | validation: 0.8419124833447402]
	TIME [epoch: 2.76 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917172736745927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7917172736745927 | validation: 0.7294836563563845]
	TIME [epoch: 2.76 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437152548376187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437152548376187 | validation: 0.7816029929442179]
	TIME [epoch: 2.77 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556852351036807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556852351036807 | validation: 0.8389659201727083]
	TIME [epoch: 2.76 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012265464767105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8012265464767105 | validation: 0.8709008728871859]
	TIME [epoch: 2.76 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803686125511477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803686125511477 | validation: 0.817068195456306]
	TIME [epoch: 2.75 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227326362271407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8227326362271407 | validation: 0.8256965134383359]
	TIME [epoch: 2.75 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044203613796195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044203613796195 | validation: 0.685296194393199]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349961997368851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7349961997368851 | validation: 0.7583522165239434]
	TIME [epoch: 2.76 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292764600759761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292764600759761 | validation: 0.7056677024371005]
	TIME [epoch: 2.75 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005005357227297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7005005357227297 | validation: 0.7318082707621691]
	TIME [epoch: 2.76 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865370086807167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6865370086807167 | validation: 0.7520794056505148]
	TIME [epoch: 2.75 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964722136323209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964722136323209 | validation: 0.7809011877527079]
	TIME [epoch: 2.75 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571231491641627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7571231491641627 | validation: 0.8380517983960595]
	TIME [epoch: 2.75 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293658215900018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8293658215900018 | validation: 0.9122735818761591]
	TIME [epoch: 2.76 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9135459651751079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135459651751079 | validation: 0.7618778814670981]
	TIME [epoch: 2.75 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997550656913859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7997550656913859 | validation: 0.838848441913799]
	TIME [epoch: 2.75 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840909248955572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8840909248955572 | validation: 0.6985686720868574]
	TIME [epoch: 2.75 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455494801669574		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.7455494801669574 | validation: 0.781577955604573]
	TIME [epoch: 2.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454446263422088		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.7454446263422088 | validation: 0.7147069603621289]
	TIME [epoch: 2.75 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999101684998538		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.6999101684998538 | validation: 0.7151860187201162]
	TIME [epoch: 2.76 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647381967517878		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.6647381967517878 | validation: 0.726204756957177]
	TIME [epoch: 2.76 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609721895190278		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.6609721895190278 | validation: 0.7293130138541324]
	TIME [epoch: 2.76 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818810696783751		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.6818810696783751 | validation: 0.7034485039810122]
	TIME [epoch: 2.75 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693867742907776		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.693867742907776 | validation: 0.71118114234481]
	TIME [epoch: 2.76 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646910195121979		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.6646910195121979 | validation: 0.7038178281036578]
	TIME [epoch: 2.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563074395961889		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.6563074395961889 | validation: 0.7030287250202861]
	TIME [epoch: 2.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522007919871443		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.6522007919871443 | validation: 0.6990626271402963]
	TIME [epoch: 2.76 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484449182283669		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.6484449182283669 | validation: 0.7106561664894802]
	TIME [epoch: 2.76 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702662440899456		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.6702662440899456 | validation: 0.7230458279231136]
	TIME [epoch: 2.76 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362373674672459		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.7362373674672459 | validation: 0.9089869579930698]
	TIME [epoch: 2.76 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704250393507755		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.8704250393507755 | validation: 0.9650075301475347]
	TIME [epoch: 2.76 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9574308132364802		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9574308132364802 | validation: 0.7324464828976374]
	TIME [epoch: 2.75 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186696016504418		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.7186696016504418 | validation: 0.859746786464913]
	TIME [epoch: 2.75 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267843310388311		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.8267843310388311 | validation: 0.7360313000343137]
	TIME [epoch: 2.75 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302014184871034		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7302014184871034 | validation: 0.7129773481347904]
	TIME [epoch: 2.76 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825469854615008		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.6825469854615008 | validation: 0.7253358126684603]
	TIME [epoch: 2.76 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966857381396696		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.6966857381396696 | validation: 0.7059160753420812]
	TIME [epoch: 2.75 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65558512925158		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.65558512925158 | validation: 0.7001312723666365]
	TIME [epoch: 2.76 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600533481285163		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.6600533481285163 | validation: 0.6891350818497779]
	TIME [epoch: 2.75 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759169751182711		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.6759169751182711 | validation: 0.7468137241463186]
	TIME [epoch: 2.76 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760783603957952		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.6760783603957952 | validation: 0.7034643920467311]
	TIME [epoch: 2.75 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68071615412194		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.68071615412194 | validation: 0.7192079878125566]
	TIME [epoch: 2.75 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078546993701238		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.7078546993701238 | validation: 0.7068932976631905]
	TIME [epoch: 2.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374902584653257		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.7374902584653257 | validation: 0.7080210339588611]
	TIME [epoch: 2.76 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662112564544972		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.6662112564544972 | validation: 0.6938985351255763]
	TIME [epoch: 2.76 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760230535110597		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.6760230535110597 | validation: 0.6719093024459044]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6717212127857292		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.6717212127857292 | validation: 0.6949399047317695]
	TIME [epoch: 2.76 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601749115025894		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.6601749115025894 | validation: 0.6863308566608618]
	TIME [epoch: 2.76 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610952222114886		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.6610952222114886 | validation: 0.7141624023126676]
	TIME [epoch: 2.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672630281813225		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.6672630281813225 | validation: 0.6868991453428445]
	TIME [epoch: 2.76 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799992558800599		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.6799992558800599 | validation: 0.7541836999713551]
	TIME [epoch: 2.76 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7095010309019216		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.7095010309019216 | validation: 0.8210899972152984]
	TIME [epoch: 2.75 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112594784228288		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8112594784228288 | validation: 0.6777501811616469]
	TIME [epoch: 2.76 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533276200781493		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.6533276200781493 | validation: 0.6996498179217193]
	TIME [epoch: 2.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664897324541259		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.664897324541259 | validation: 0.7035967164239678]
	TIME [epoch: 2.76 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659264876097066		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.6659264876097066 | validation: 0.7072466455031661]
	TIME [epoch: 2.75 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562488409076102		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.6562488409076102 | validation: 0.6946997138934852]
	TIME [epoch: 2.76 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991062700766301		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.6991062700766301 | validation: 0.7065642028023316]
	TIME [epoch: 2.76 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088211516848344		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7088211516848344 | validation: 0.6803873059032426]
	TIME [epoch: 2.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725838873658519		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.725838873658519 | validation: 0.7564066371482403]
	TIME [epoch: 2.76 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301097824315622		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7301097824315622 | validation: 0.7965164642656717]
	TIME [epoch: 2.76 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783304517654058		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.783304517654058 | validation: 0.7058167247996189]
	TIME [epoch: 2.76 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672133845790492		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.6672133845790492 | validation: 0.6981783088121056]
	TIME [epoch: 2.76 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818986068556739		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.6818986068556739 | validation: 0.6899591849525133]
	TIME [epoch: 2.76 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494449318443285		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.6494449318443285 | validation: 0.7152076489968691]
	TIME [epoch: 2.76 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611190568008505		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.6611190568008505 | validation: 0.718494433302734]
	TIME [epoch: 2.76 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815286669553927		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.6815286669553927 | validation: 0.7725966874262192]
	TIME [epoch: 2.76 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7057500817908442		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7057500817908442 | validation: 0.7326473011817212]
	TIME [epoch: 2.76 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227662696983427		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7227662696983427 | validation: 0.7111930892841931]
	TIME [epoch: 2.75 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682406302874719		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.6682406302874719 | validation: 0.6832728204288663]
	TIME [epoch: 2.76 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670289547891995		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.670289547891995 | validation: 0.6868551979471813]
	TIME [epoch: 2.75 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509796091008779		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.6509796091008779 | validation: 0.6935511280130262]
	TIME [epoch: 2.76 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638465957580354		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.638465957580354 | validation: 0.6887877033293611]
	TIME [epoch: 2.76 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405081679081475		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.6405081679081475 | validation: 0.6896970208224128]
	TIME [epoch: 2.76 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648092017739665		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.648092017739665 | validation: 0.6901155486426108]
	TIME [epoch: 2.75 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689245551546117		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.6689245551546117 | validation: 0.7343626774647102]
	TIME [epoch: 2.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105571137054555		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7105571137054555 | validation: 0.7833017940787206]
	TIME [epoch: 2.75 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598879255613958		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.7598879255613958 | validation: 0.7320376682131905]
	TIME [epoch: 2.75 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142025049054234		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7142025049054234 | validation: 0.6836651412851649]
	TIME [epoch: 2.75 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193292291488719		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7193292291488719 | validation: 0.6677492778423181]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646968360928571		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.6646968360928571 | validation: 0.7052879877333373]
	TIME [epoch: 2.76 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022563008600231		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7022563008600231 | validation: 0.7361470334621182]
	TIME [epoch: 2.76 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120282527821894		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7120282527821894 | validation: 0.7400674334973338]
	TIME [epoch: 2.76 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873084626711837		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.6873084626711837 | validation: 0.6962707801851581]
	TIME [epoch: 2.75 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575415666249873		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.6575415666249873 | validation: 0.6826348905877866]
	TIME [epoch: 2.75 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444535473184346		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.6444535473184346 | validation: 0.7128649139794481]
	TIME [epoch: 2.76 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64973148702094		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.64973148702094 | validation: 0.6956637462821189]
	TIME [epoch: 2.76 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657712235193893		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.657712235193893 | validation: 0.707627184521455]
	TIME [epoch: 2.75 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522711588030513		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.6522711588030513 | validation: 0.6910519635140223]
	TIME [epoch: 2.75 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709071856557214		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6709071856557214 | validation: 0.7155618633488863]
	TIME [epoch: 2.75 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714841178083296		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.6714841178083296 | validation: 0.7063294434941554]
	TIME [epoch: 2.75 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651303757026793		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.6651303757026793 | validation: 0.7022047375566357]
	TIME [epoch: 2.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673140888506991		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.673140888506991 | validation: 0.6951885578633509]
	TIME [epoch: 2.75 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153324298960316		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7153324298960316 | validation: 0.6742064332052969]
	TIME [epoch: 2.76 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447380983497337		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6447380983497337 | validation: 0.7152360209431521]
	TIME [epoch: 2.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600522426790276		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6600522426790276 | validation: 0.6937166048795459]
	TIME [epoch: 2.76 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898656038399446		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.6898656038399446 | validation: 0.6834738673312992]
	TIME [epoch: 2.76 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461147194418476		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.6461147194418476 | validation: 0.6789998628701727]
	TIME [epoch: 2.76 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393910387967293		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.6393910387967293 | validation: 0.6799620189353762]
	TIME [epoch: 2.75 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389683080006333		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6389683080006333 | validation: 0.719394234536888]
	TIME [epoch: 2.76 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633588851159534		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6633588851159534 | validation: 0.7887493200633513]
	TIME [epoch: 2.76 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7792256422019905		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7792256422019905 | validation: 0.689769548569193]
	TIME [epoch: 2.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522435074168198		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6522435074168198 | validation: 0.6684349986406881]
	TIME [epoch: 2.76 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587862153360318		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.6587862153360318 | validation: 0.6927379091321338]
	TIME [epoch: 2.76 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561567589265819		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.6561567589265819 | validation: 0.6850932051980796]
	TIME [epoch: 2.76 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438457071860646		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6438457071860646 | validation: 0.6824353862535917]
	TIME [epoch: 2.76 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462230250046203		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6462230250046203 | validation: 0.6975494069786314]
	TIME [epoch: 2.76 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502016520230383		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6502016520230383 | validation: 0.700395794169335]
	TIME [epoch: 2.76 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748555245938189		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6748555245938189 | validation: 0.759981433617689]
	TIME [epoch: 2.76 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198022096866297		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7198022096866297 | validation: 0.7529482767101116]
	TIME [epoch: 2.76 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736154895380285		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.736154895380285 | validation: 0.6785950982137517]
	TIME [epoch: 2.76 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464963123397299		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6464963123397299 | validation: 0.6989065167356148]
	TIME [epoch: 2.76 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797220277038015		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.6797220277038015 | validation: 0.6981272496973356]
	TIME [epoch: 2.76 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668007106345022		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6668007106345022 | validation: 0.6792257347743167]
	TIME [epoch: 2.76 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403926216275566		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6403926216275566 | validation: 0.6867199110885946]
	TIME [epoch: 2.76 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393354007106069		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6393354007106069 | validation: 0.6856431112925006]
	TIME [epoch: 2.76 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370286389777251		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6370286389777251 | validation: 0.6873286886527323]
	TIME [epoch: 2.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378590444593507		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6378590444593507 | validation: 0.6865031861487979]
	TIME [epoch: 2.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426898708762048		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6426898708762048 | validation: 0.6866086973318851]
	TIME [epoch: 2.76 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401770748313149		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6401770748313149 | validation: 0.6748174201251178]
	TIME [epoch: 2.76 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767042911709672		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6767042911709672 | validation: 0.7223558663815111]
	TIME [epoch: 2.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220236429518301		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7220236429518301 | validation: 0.6866726627322004]
	TIME [epoch: 2.76 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726483978846583		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.726483978846583 | validation: 0.6601690069897007]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684888440752063		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6684888440752063 | validation: 0.7001685583773692]
	TIME [epoch: 2.76 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669073505771279		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.669073505771279 | validation: 0.6897501274883622]
	TIME [epoch: 2.76 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639973504417828		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.639973504417828 | validation: 0.6869806934438613]
	TIME [epoch: 2.76 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359407543890749		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6359407543890749 | validation: 0.715405638499202]
	TIME [epoch: 2.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649915115903018		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.649915115903018 | validation: 0.6947114218212089]
	TIME [epoch: 2.76 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709942466923885		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6709942466923885 | validation: 0.7014639431686325]
	TIME [epoch: 2.76 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520646231020806		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.6520646231020806 | validation: 0.6970697130322527]
	TIME [epoch: 2.76 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718920957090794		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.6718920957090794 | validation: 0.6953086733937895]
	TIME [epoch: 2.77 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505755071244577		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6505755071244577 | validation: 0.6710393383971316]
	TIME [epoch: 2.76 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.651225768157762		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.651225768157762 | validation: 0.6932692791193106]
	TIME [epoch: 2.76 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371839189542164		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.6371839189542164 | validation: 0.6883961190164635]
	TIME [epoch: 2.76 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354817737282457		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6354817737282457 | validation: 0.6825585432836676]
	TIME [epoch: 2.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332416384143755		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.6332416384143755 | validation: 0.6732378618022534]
	TIME [epoch: 2.76 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381290705021647		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.6381290705021647 | validation: 0.7114094561817028]
	TIME [epoch: 2.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902794388225794		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.6902794388225794 | validation: 0.7535409803753801]
	TIME [epoch: 2.77 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386460541945438		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7386460541945438 | validation: 0.6922312190284825]
	TIME [epoch: 2.76 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822512444043387		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6822512444043387 | validation: 0.6768572623266667]
	TIME [epoch: 2.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918326952106189		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6918326952106189 | validation: 0.66662987137418]
	TIME [epoch: 2.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559793186611297		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6559793186611297 | validation: 0.7001039004326781]
	TIME [epoch: 2.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515085748825515		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6515085748825515 | validation: 0.6879540640860283]
	TIME [epoch: 2.77 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346118386458899		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6346118386458899 | validation: 0.7126025489823422]
	TIME [epoch: 2.76 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510240523491535		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6510240523491535 | validation: 0.6895830984370591]
	TIME [epoch: 2.76 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648055512777411		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.648055512777411 | validation: 0.691525469088145]
	TIME [epoch: 2.76 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439531155502715		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6439531155502715 | validation: 0.6734994592880983]
	TIME [epoch: 2.77 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430705388934439		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6430705388934439 | validation: 0.6827312005623322]
	TIME [epoch: 2.76 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635715327511082		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.635715327511082 | validation: 0.6631136306798081]
	TIME [epoch: 2.76 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442390180364214		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6442390180364214 | validation: 0.6835845786769809]
	TIME [epoch: 2.76 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348216777953081		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6348216777953081 | validation: 0.673301791495381]
	TIME [epoch: 2.77 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389563916135714		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6389563916135714 | validation: 0.7004015093438969]
	TIME [epoch: 2.76 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532178871942259		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6532178871942259 | validation: 0.6982707440225016]
	TIME [epoch: 2.76 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67525080046999		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.67525080046999 | validation: 0.7088842593446879]
	TIME [epoch: 2.76 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6914116048841106		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6914116048841106 | validation: 0.654711608071282]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773656282848245		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6773656282848245 | validation: 0.6706950287504081]
	TIME [epoch: 2.74 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434744578572557		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.6434744578572557 | validation: 0.6832853111730196]
	TIME [epoch: 2.74 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533531341114371		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6533531341114371 | validation: 0.6532285814548964]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350441976731839		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6350441976731839 | validation: 0.6773243710169781]
	TIME [epoch: 2.75 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326541359573293		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6326541359573293 | validation: 0.6832243958750315]
	TIME [epoch: 2.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657918227056256		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.657918227056256 | validation: 0.7047068740502932]
	TIME [epoch: 2.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399718485430091		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6399718485430091 | validation: 0.6739875608783121]
	TIME [epoch: 2.74 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487649186502575		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6487649186502575 | validation: 0.7101885871878715]
	TIME [epoch: 2.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527111577355882		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6527111577355882 | validation: 0.7037841646093291]
	TIME [epoch: 2.75 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683342336914138		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.6683342336914138 | validation: 0.7050457697192818]
	TIME [epoch: 2.74 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349487296445192		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6349487296445192 | validation: 0.6611800316226225]
	TIME [epoch: 2.74 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63124575304974		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.63124575304974 | validation: 0.666249824548612]
	TIME [epoch: 268 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623954277485071		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.623954277485071 | validation: 0.6705211505111668]
	TIME [epoch: 5.93 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219500340823841		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6219500340823841 | validation: 0.6687712809728039]
	TIME [epoch: 5.92 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317927962738467		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6317927962738467 | validation: 0.6879404298896578]
	TIME [epoch: 5.92 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393334572343041		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6393334572343041 | validation: 0.6850810848448621]
	TIME [epoch: 5.92 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620395256693766		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6620395256693766 | validation: 0.7101126041083172]
	TIME [epoch: 5.92 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800289869696172		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6800289869696172 | validation: 0.6882880506099763]
	TIME [epoch: 5.92 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781172493144171		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6781172493144171 | validation: 0.6700025204433615]
	TIME [epoch: 5.92 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491928686235696		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6491928686235696 | validation: 0.6704069550923782]
	TIME [epoch: 5.93 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648435236780076		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6648435236780076 | validation: 0.6592879307295697]
	TIME [epoch: 5.93 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325138246366853		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6325138246366853 | validation: 0.6779585866768815]
	TIME [epoch: 5.92 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342713873173671		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6342713873173671 | validation: 0.6886912135753456]
	TIME [epoch: 5.92 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6455806062830657		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6455806062830657 | validation: 0.6835302338585767]
	TIME [epoch: 5.92 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400288922528975		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6400288922528975 | validation: 0.6701704978517365]
	TIME [epoch: 5.93 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658132116093561		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.658132116093561 | validation: 0.6680811160280508]
	TIME [epoch: 5.92 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221327492650686		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6221327492650686 | validation: 0.6890629375960443]
	TIME [epoch: 5.93 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557161837913209		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6557161837913209 | validation: 0.6543845525369156]
	TIME [epoch: 5.92 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280446562308805		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6280446562308805 | validation: 0.6874132870191834]
	TIME [epoch: 5.93 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276630371368682		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6276630371368682 | validation: 0.6945239022818779]
	TIME [epoch: 5.93 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532052793340091		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6532052793340091 | validation: 0.7100284866390276]
	TIME [epoch: 5.93 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486792551888084		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6486792551888084 | validation: 0.665162853053852]
	TIME [epoch: 5.92 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483867170286474		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6483867170286474 | validation: 0.7062462278919069]
	TIME [epoch: 5.92 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332004882965179		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6332004882965179 | validation: 0.6609366617958878]
	TIME [epoch: 5.92 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368316543078336		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6368316543078336 | validation: 0.6780397551060996]
	TIME [epoch: 5.92 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215591451003644		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6215591451003644 | validation: 0.675277678344175]
	TIME [epoch: 5.92 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329551264603989		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6329551264603989 | validation: 0.686596911604834]
	TIME [epoch: 5.92 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219739851901414		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6219739851901414 | validation: 0.6592050502845593]
	TIME [epoch: 5.92 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629629049142135		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.629629049142135 | validation: 0.6797319087439222]
	TIME [epoch: 5.92 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250984209222453		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6250984209222453 | validation: 0.6648313391422365]
	TIME [epoch: 5.92 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279521173286102		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6279521173286102 | validation: 0.6820064022058088]
	TIME [epoch: 5.92 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234774618191784		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6234774618191784 | validation: 0.6636490669343675]
	TIME [epoch: 5.93 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333448461800744		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6333448461800744 | validation: 0.6880704647391561]
	TIME [epoch: 5.93 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341700505385583		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6341700505385583 | validation: 0.6725214647107642]
	TIME [epoch: 5.92 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545219101435978		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6545219101435978 | validation: 0.7071568960974762]
	TIME [epoch: 5.92 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665956627319872		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6665956627319872 | validation: 0.6677904557859883]
	TIME [epoch: 5.92 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367598343116282		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6367598343116282 | validation: 0.6641321311558466]
	TIME [epoch: 5.92 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6145007862131263		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6145007862131263 | validation: 0.6407257978973483]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079232159407537		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6079232159407537 | validation: 0.6576545634221994]
	TIME [epoch: 5.92 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056116813331481		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6056116813331481 | validation: 0.6430531726711749]
	TIME [epoch: 5.92 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008247984593241		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6008247984593241 | validation: 0.6580351314127667]
	TIME [epoch: 5.92 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014693457335772		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6014693457335772 | validation: 0.6498585088401786]
	TIME [epoch: 5.92 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6044163208219332		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6044163208219332 | validation: 0.6574759391521301]
	TIME [epoch: 5.91 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.601305475868683		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.601305475868683 | validation: 0.6642609775849808]
	TIME [epoch: 5.92 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313326654560243		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6313326654560243 | validation: 0.9127867326389572]
	TIME [epoch: 5.92 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053827943534111		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9053827943534111 | validation: 0.6840387425469544]
	TIME [epoch: 5.92 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968203769519548		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6968203769519548 | validation: 0.644820142928611]
	TIME [epoch: 5.92 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121097095090149		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6121097095090149 | validation: 0.684585541595091]
	TIME [epoch: 5.92 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280748486185805		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6280748486185805 | validation: 0.63629152428578]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5990572706606608		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5990572706606608 | validation: 0.6456583426127431]
	TIME [epoch: 5.92 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5924255908133774		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.5924255908133774 | validation: 0.6408562252768424]
	TIME [epoch: 5.92 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994835016467126		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.5994835016467126 | validation: 0.6410159423811195]
	TIME [epoch: 5.92 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129991297748535		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6129991297748535 | validation: 0.6543064330260864]
	TIME [epoch: 5.92 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961015132412336		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5961015132412336 | validation: 0.6205969966641979]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967682718644097		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5967682718644097 | validation: 0.6428431798041254]
	TIME [epoch: 5.92 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58109339209991		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.58109339209991 | validation: 0.6253068493517593]
	TIME [epoch: 5.92 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5768887481833198		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5768887481833198 | validation: 0.6284570311602352]
	TIME [epoch: 5.92 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5705888283675619		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.5705888283675619 | validation: 0.6433677795388608]
	TIME [epoch: 5.92 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5730180658643934		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.5730180658643934 | validation: 0.6711843483320209]
	TIME [epoch: 5.91 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599574681329006		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6599574681329006 | validation: 1.0389971417833361]
	TIME [epoch: 5.92 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583430945303276		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0583430945303276 | validation: 0.6316714265543362]
	TIME [epoch: 5.92 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5604326689272547		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5604326689272547 | validation: 0.6687903807944013]
	TIME [epoch: 5.92 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594973063585272		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6594973063585272 | validation: 0.6680078458055321]
	TIME [epoch: 5.92 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139827265859524		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6139827265859524 | validation: 0.6217378148509929]
	TIME [epoch: 5.92 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5511616581527006		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5511616581527006 | validation: 0.6138549480123793]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5660877592357992		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.5660877592357992 | validation: 0.6283329620167852]
	TIME [epoch: 5.92 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582937950479869		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.5582937950479869 | validation: 0.6065857599382043]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543548021084325		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.543548021084325 | validation: 0.6245891880076125]
	TIME [epoch: 5.92 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5428677477835256		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5428677477835256 | validation: 0.6066774807152969]
	TIME [epoch: 5.92 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305259828454219		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.5305259828454219 | validation: 0.6057167374069664]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253768482590475		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.5253768482590475 | validation: 0.6060847626143196]
	TIME [epoch: 5.93 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5745581147190845		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5745581147190845 | validation: 0.9027034934601428]
	TIME [epoch: 5.92 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614069704902158		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9614069704902158 | validation: 0.5776751188689099]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566028380274256		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5566028380274256 | validation: 0.6298111740392556]
	TIME [epoch: 5.92 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240968984400852		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6240968984400852 | validation: 0.6346991360740397]
	TIME [epoch: 5.92 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598749976029275		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.598749976029275 | validation: 0.5721496134377043]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253393495439345		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5253393495439345 | validation: 0.5619120567694713]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480087441883378		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5480087441883378 | validation: 0.649498652477107]
	TIME [epoch: 5.92 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5556994659495486		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5556994659495486 | validation: 0.5633714685248455]
	TIME [epoch: 5.92 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5218183292227297		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.5218183292227297 | validation: 0.5950363492544704]
	TIME [epoch: 5.92 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5108892972785407		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5108892972785407 | validation: 0.5486035893540521]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5075359638624316		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5075359638624316 | validation: 0.6143621576917728]
	TIME [epoch: 5.92 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5357188658196542		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5357188658196542 | validation: 0.6452545501438778]
	TIME [epoch: 5.92 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862413599639094		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6862413599639094 | validation: 0.7864144173053182]
	TIME [epoch: 5.92 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560604512784324		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8560604512784324 | validation: 0.5773435682721315]
	TIME [epoch: 5.92 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889134320816278		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5889134320816278 | validation: 0.6478740529887452]
	TIME [epoch: 5.92 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645783741072313		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.645783741072313 | validation: 0.5706144120067602]
	TIME [epoch: 5.92 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534551446642051		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5534551446642051 | validation: 0.5853529104323816]
	TIME [epoch: 5.92 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491434053536076		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5491434053536076 | validation: 0.5622764884505351]
	TIME [epoch: 5.92 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506848700391203		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.506848700391203 | validation: 0.5416819292210068]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5160820742424713		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5160820742424713 | validation: 0.5473275511975831]
	TIME [epoch: 5.89 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48863623523811245		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.48863623523811245 | validation: 0.5431445804589877]
	TIME [epoch: 5.89 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48277523732429894		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.48277523732429894 | validation: 0.5491987644956178]
	TIME [epoch: 5.88 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48215657672136386		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.48215657672136386 | validation: 0.5258230206671558]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48888506500742496		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.48888506500742496 | validation: 0.6306235386009202]
	TIME [epoch: 5.89 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867046435735376		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5867046435735376 | validation: 0.5284186561045301]
	TIME [epoch: 5.89 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098504312758834		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5098504312758834 | validation: 0.5327675105076096]
	TIME [epoch: 5.89 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4812379140591031		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.4812379140591031 | validation: 0.5017051808959672]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4633143416216443		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.4633143416216443 | validation: 0.5142356745525662]
	TIME [epoch: 5.92 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45374412471457554		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.45374412471457554 | validation: 0.5417040416178329]
	TIME [epoch: 5.92 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5070753223147751		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.5070753223147751 | validation: 0.7422055667941462]
	TIME [epoch: 5.92 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476018852604159		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8476018852604159 | validation: 0.52338066269844]
	TIME [epoch: 5.91 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5015789496942982		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5015789496942982 | validation: 0.5693938149401468]
	TIME [epoch: 5.92 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6005884807038961		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6005884807038961 | validation: 0.5549830114898838]
	TIME [epoch: 5.92 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191589931863194		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5191589931863194 | validation: 0.5118645542718067]
	TIME [epoch: 5.92 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4675441733403872		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.4675441733403872 | validation: 0.523354298368678]
	TIME [epoch: 5.9 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222261034297014		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5222261034297014 | validation: 0.6489672421722491]
	TIME [epoch: 5.89 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298834401704632		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6298834401704632 | validation: 0.5007435870815696]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4620175421413175		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.4620175421413175 | validation: 0.4995268097759669]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026048690307248		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.5026048690307248 | validation: 0.5424325346298525]
	TIME [epoch: 5.92 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.494462387091115		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.494462387091115 | validation: 0.49270648349396934]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4433483144636142		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.4433483144636142 | validation: 0.4844453482383479]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4419461810007021		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.4419461810007021 | validation: 0.5283401714583261]
	TIME [epoch: 5.9 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47643952868324063		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.47643952868324063 | validation: 0.5904017026031471]
	TIME [epoch: 5.89 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769751195097067		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5769751195097067 | validation: 0.6452820912041052]
	TIME [epoch: 5.89 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324641579624236		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.7324641579624236 | validation: 0.5317212270317995]
	TIME [epoch: 5.89 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439180722629291		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.5439180722629291 | validation: 0.5679349286469065]
	TIME [epoch: 5.89 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5622937993081097		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.5622937993081097 | validation: 0.5019662081065515]
	TIME [epoch: 5.9 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47484493339625006		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.47484493339625006 | validation: 0.5243654880552107]
	TIME [epoch: 5.9 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47116304429318606		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.47116304429318606 | validation: 0.4988198860382541]
	TIME [epoch: 5.91 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44114424257646734		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.44114424257646734 | validation: 0.48882134420199397]
	TIME [epoch: 5.9 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4397706639607117		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4397706639607117 | validation: 0.4843178172342378]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307062190722609		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.4307062190722609 | validation: 0.4825671008108632]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359611030696667		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4359611030696667 | validation: 0.4855891410908737]
	TIME [epoch: 5.89 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617418127470936		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4617418127470936 | validation: 0.5821174349087722]
	TIME [epoch: 5.89 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6030249952822289		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6030249952822289 | validation: 0.4744911000251719]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4470622873123672		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.4470622873123672 | validation: 0.4684570558268593]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4449863797319796		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.4449863797319796 | validation: 0.5543348113031883]
	TIME [epoch: 5.89 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4917509446985477		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.4917509446985477 | validation: 0.498236245401132]
	TIME [epoch: 5.89 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46881160073419426		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.46881160073419426 | validation: 0.5520043524312559]
	TIME [epoch: 5.89 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779434592663624		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5779434592663624 | validation: 0.46491672774603]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4444609423553102		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.4444609423553102 | validation: 0.45154384804946945]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42791407779111085		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.42791407779111085 | validation: 0.5159403321862719]
	TIME [epoch: 5.89 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4461780254503971		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.4461780254503971 | validation: 0.5242537492727292]
	TIME [epoch: 5.89 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48986267909120385		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.48986267909120385 | validation: 0.6155504766180373]
	TIME [epoch: 5.89 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6670474540868543		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6670474540868543 | validation: 0.47363230902097897]
	TIME [epoch: 5.89 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46015941459184123		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.46015941459184123 | validation: 0.5108153242864598]
	TIME [epoch: 5.89 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5040557515525835		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5040557515525835 | validation: 0.4803233977117611]
	TIME [epoch: 5.89 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42856582478479943		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.42856582478479943 | validation: 0.4520840303831527]
	TIME [epoch: 5.89 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039656993567561		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.4039656993567561 | validation: 0.462922944624936]
	TIME [epoch: 5.89 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4027513538605457		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.4027513538605457 | validation: 0.4797166358946976]
	TIME [epoch: 5.89 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109097566600963		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.4109097566600963 | validation: 0.5085251164664528]
	TIME [epoch: 5.89 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4831895517307686		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.4831895517307686 | validation: 0.5617266700558802]
	TIME [epoch: 5.89 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326934388128789		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6326934388128789 | validation: 0.4738881808558425]
	TIME [epoch: 5.88 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46383103190358227		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.46383103190358227 | validation: 0.5053251181906875]
	TIME [epoch: 5.89 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193777573990513		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5193777573990513 | validation: 0.43869657873839696]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4204166539100793		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.4204166539100793 | validation: 0.4390052597132216]
	TIME [epoch: 5.89 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3930599273265887		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.3930599273265887 | validation: 0.4873961541472685]
	TIME [epoch: 5.88 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.435753252231766		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.435753252231766 | validation: 0.5780526732719143]
	TIME [epoch: 5.89 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5836290170909549		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5836290170909549 | validation: 0.45889944763747714]
	TIME [epoch: 5.88 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4187024711003125		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.4187024711003125 | validation: 0.47601324600790423]
	TIME [epoch: 5.88 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45944919720872096		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.45944919720872096 | validation: 0.4951434831457304]
	TIME [epoch: 5.88 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4533652078930433		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.4533652078930433 | validation: 0.43986744770257885]
	TIME [epoch: 5.89 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817325896582072		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.3817325896582072 | validation: 0.4706980813531189]
	TIME [epoch: 5.89 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41432779401678277		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.41432779401678277 | validation: 0.5433804871748511]
	TIME [epoch: 5.89 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5362201558639031		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5362201558639031 | validation: 0.43574409737959985]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39173089272649947		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.39173089272649947 | validation: 0.42820064998991775]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817047311450123		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.3817047311450123 | validation: 0.4636752238928388]
	TIME [epoch: 5.93 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4106856898298025		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.4106856898298025 | validation: 0.45289873700153926]
	TIME [epoch: 5.93 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3908355219154511		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.3908355219154511 | validation: 0.47214329440556124]
	TIME [epoch: 5.92 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.430550784219691		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.430550784219691 | validation: 0.4128183309157503]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37663889102127956		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.37663889102127956 | validation: 0.4231251218295411]
	TIME [epoch: 5.92 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587431602936218		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.3587431602936218 | validation: 0.428800069908681]
	TIME [epoch: 5.92 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3798416261188298		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.3798416261188298 | validation: 0.5152298187957323]
	TIME [epoch: 5.91 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4503497141219395		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.4503497141219395 | validation: 0.5563878850851934]
	TIME [epoch: 5.92 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6846255509414655		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.6846255509414655 | validation: 0.4470225654123685]
	TIME [epoch: 5.92 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46643070115103086		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.46643070115103086 | validation: 0.48926407670548333]
	TIME [epoch: 5.92 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49552330814000994		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.49552330814000994 | validation: 0.41086503992481316]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3818435541222835		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3818435541222835 | validation: 0.4339394947573889]
	TIME [epoch: 5.9 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36699976461404665		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.36699976461404665 | validation: 0.6075646009581057]
	TIME [epoch: 5.91 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5309218384632846		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5309218384632846 | validation: 0.5853032254641793]
	TIME [epoch: 5.9 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800850465036581		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6800850465036581 | validation: 0.4653377654536211]
	TIME [epoch: 5.89 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4869713288880221		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4869713288880221 | validation: 0.46351069357937674]
	TIME [epoch: 5.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4786221229263502		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.4786221229263502 | validation: 0.417575620753121]
	TIME [epoch: 5.9 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4006466110631801		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4006466110631801 | validation: 0.43655183767948585]
	TIME [epoch: 5.91 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37551706800554585		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.37551706800554585 | validation: 0.4138540861951498]
	TIME [epoch: 5.91 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578723734434698		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3578723734434698 | validation: 0.42057126581105514]
	TIME [epoch: 5.92 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34897511519164104		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.34897511519164104 | validation: 0.39593882579756595]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3421402618543024		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.3421402618543024 | validation: 0.3929533454208585]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458031942851316		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.3458031942851316 | validation: 0.38852692065586614]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34259759075307633		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.34259759075307633 | validation: 0.3881398036412116]
	TIME [epoch: 5.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3372083618474044		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.3372083618474044 | validation: 0.39151211431352473]
	TIME [epoch: 5.89 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3289978458782528		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.3289978458782528 | validation: 0.3856720161828586]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33125355614250396		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.33125355614250396 | validation: 0.3862509327891984]
	TIME [epoch: 5.89 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3267543570336896		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3267543570336896 | validation: 0.39827851598186087]
	TIME [epoch: 5.89 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34342286709190717		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.34342286709190717 | validation: 0.45373688719052385]
	TIME [epoch: 5.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4684204717307992		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.4684204717307992 | validation: 0.4163427820981988]
	TIME [epoch: 5.9 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380582995782584		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.380582995782584 | validation: 0.40359549550396745]
	TIME [epoch: 5.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3983519894998881		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3983519894998881 | validation: 0.4112971514360333]
	TIME [epoch: 5.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35729787682084363		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.35729787682084363 | validation: 0.40670982225454955]
	TIME [epoch: 5.9 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3896881913061368		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.3896881913061368 | validation: 0.4787818430435762]
	TIME [epoch: 5.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168892344587782		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4168892344587782 | validation: 0.45632491635074895]
	TIME [epoch: 5.9 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853723508523975		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.4853723508523975 | validation: 0.37466064553445405]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36524100577817925		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.36524100577817925 | validation: 0.44694450946478437]
	TIME [epoch: 5.89 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4181113517946465		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.4181113517946465 | validation: 0.397113994176783]
	TIME [epoch: 5.89 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37833431768473086		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.37833431768473086 | validation: 0.3768096986082533]
	TIME [epoch: 5.89 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3245842054338739		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.3245842054338739 | validation: 0.4840745569678248]
	TIME [epoch: 5.89 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4056915597518835		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4056915597518835 | validation: 0.46154654385257193]
	TIME [epoch: 5.9 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4844351108944099		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.4844351108944099 | validation: 0.39167908106982435]
	TIME [epoch: 5.88 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35860879736893464		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.35860879736893464 | validation: 0.4359146723779772]
	TIME [epoch: 5.89 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971774444485028		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3971774444485028 | validation: 0.374162012462103]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3349994255329773		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.3349994255329773 | validation: 0.36673977927027135]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29808620628946364		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.29808620628946364 | validation: 0.4102234014842605]
	TIME [epoch: 5.89 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3240758827580508		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.3240758827580508 | validation: 0.3919074652956606]
	TIME [epoch: 5.89 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3725666036000731		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.3725666036000731 | validation: 0.3722390734442633]
	TIME [epoch: 5.89 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30323189722210186		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.30323189722210186 | validation: 0.35499920081686476]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2967167698102079		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.2967167698102079 | validation: 0.34809040780748274]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29153420896053045		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.29153420896053045 | validation: 0.36937903164777214]
	TIME [epoch: 5.89 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29669286866256556		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.29669286866256556 | validation: 0.36529460024835836]
	TIME [epoch: 5.91 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34455232155473925		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.34455232155473925 | validation: 0.40602521973826217]
	TIME [epoch: 5.9 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.350166554911431		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.350166554911431 | validation: 0.3892375165856449]
	TIME [epoch: 5.89 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41161952970769383		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.41161952970769383 | validation: 0.34804631634498245]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30769397697636647		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.30769397697636647 | validation: 0.4370731721795208]
	TIME [epoch: 5.89 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3529052426357325		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3529052426357325 | validation: 0.4334077446383754]
	TIME [epoch: 5.89 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4394570920330979		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.4394570920330979 | validation: 0.3456773107120725]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2821800674778068		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2821800674778068 | validation: 0.4777541975904651]
	TIME [epoch: 5.89 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4013814768619725		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4013814768619725 | validation: 0.4501145126515164]
	TIME [epoch: 5.9 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771496180833968		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.4771496180833968 | validation: 0.35521902155298907]
	TIME [epoch: 5.89 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3636764417408484		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3636764417408484 | validation: 0.4271423703195371]
	TIME [epoch: 5.89 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735955536977666		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3735955536977666 | validation: 0.3315710078714701]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2853346699767578		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.2853346699767578 | validation: 0.33589510928589034]
	TIME [epoch: 5.88 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2760540120716798		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2760540120716798 | validation: 0.36313372745439115]
	TIME [epoch: 5.89 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847478901365846		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2847478901365846 | validation: 0.3477755655684049]
	TIME [epoch: 5.9 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31146514727811375		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.31146514727811375 | validation: 0.3258251520289749]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.268675554342417		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.268675554342417 | validation: 0.3242164475590315]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644430049841622		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2644430049841622 | validation: 0.32453785592715934]
	TIME [epoch: 5.93 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2635398461105835		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.2635398461105835 | validation: 0.3304059866465462]
	TIME [epoch: 5.93 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266539566560024		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.266539566560024 | validation: 0.31019774936288746]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2827711501017792		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.2827711501017792 | validation: 0.37270942559870723]
	TIME [epoch: 5.92 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007144892556839		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3007144892556839 | validation: 0.36450645093197487]
	TIME [epoch: 5.93 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3927478970505738		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.3927478970505738 | validation: 0.33080256150128023]
	TIME [epoch: 5.93 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713765804406776		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.2713765804406776 | validation: 0.3424700533203221]
	TIME [epoch: 5.94 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700977256217571		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2700977256217571 | validation: 0.3521550000096446]
	TIME [epoch: 5.93 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290892888593634		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3290892888593634 | validation: 0.3521631364458532]
	TIME [epoch: 5.93 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26917364586562864		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.26917364586562864 | validation: 0.32061861808090897]
	TIME [epoch: 5.92 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2851529536733271		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2851529536733271 | validation: 0.3201877662012552]
	TIME [epoch: 5.92 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25429811930667634		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.25429811930667634 | validation: 0.2920455942892695]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2524072701749829		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.2524072701749829 | validation: 0.3795513678826211]
	TIME [epoch: 5.93 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27074210650166747		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.27074210650166747 | validation: 0.3717017277565566]
	TIME [epoch: 5.92 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39755033454108807		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.39755033454108807 | validation: 0.3237058819570219]
	TIME [epoch: 5.92 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2584781359404487		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.2584781359404487 | validation: 0.36848781454054635]
	TIME [epoch: 5.92 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27746835981643225		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.27746835981643225 | validation: 0.39173124519632285]
	TIME [epoch: 5.92 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40217204001367224		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.40217204001367224 | validation: 0.30773502792525975]
	TIME [epoch: 5.92 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23795424692349332		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.23795424692349332 | validation: 0.3860893033109565]
	TIME [epoch: 5.92 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29393426181345456		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.29393426181345456 | validation: 0.38222863260617]
	TIME [epoch: 5.92 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041089534311006		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4041089534311006 | validation: 0.3156919884607056]
	TIME [epoch: 5.92 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2707971955948207		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.2707971955948207 | validation: 0.4477540072832783]
	TIME [epoch: 5.92 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37475190711969875		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.37475190711969875 | validation: 0.33610796513252916]
	TIME [epoch: 5.92 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34895697430605643		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.34895697430605643 | validation: 0.3161027663203037]
	TIME [epoch: 5.93 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2788345265500172		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.2788345265500172 | validation: 0.37259817582965127]
	TIME [epoch: 5.92 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27927227685503625		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.27927227685503625 | validation: 0.30530190758140674]
	TIME [epoch: 5.93 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24639584928580177		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.24639584928580177 | validation: 0.3014551992469676]
	TIME [epoch: 5.92 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22758233861537436		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.22758233861537436 | validation: 0.30348386952610196]
	TIME [epoch: 5.92 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22933948118039735		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.22933948118039735 | validation: 0.284391963221441]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24861085266402505		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.24861085266402505 | validation: 0.29810557539368693]
	TIME [epoch: 5.9 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22528358506551394		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.22528358506551394 | validation: 0.27049517984825255]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21938227297337937		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.21938227297337937 | validation: 0.29128505298140617]
	TIME [epoch: 5.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2178903700026116		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.2178903700026116 | validation: 0.2778661122989889]
	TIME [epoch: 5.93 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2268321362002346		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2268321362002346 | validation: 0.3093298480632446]
	TIME [epoch: 5.93 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23444360227052685		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.23444360227052685 | validation: 0.3056253145273754]
	TIME [epoch: 5.94 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29583782159733735		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.29583782159733735 | validation: 0.3091920917112114]
	TIME [epoch: 5.93 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22313821657571556		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.22313821657571556 | validation: 0.27221994475884165]
	TIME [epoch: 5.94 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21341249085142586		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.21341249085142586 | validation: 0.2913648176309789]
	TIME [epoch: 5.93 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21325204089808658		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.21325204089808658 | validation: 0.27741339265043313]
	TIME [epoch: 5.94 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660527445600781		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2660527445600781 | validation: 0.316621209136424]
	TIME [epoch: 5.93 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23679343464358504		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.23679343464358504 | validation: 0.2719984825357822]
	TIME [epoch: 5.94 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27055659278541433		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.27055659278541433 | validation: 0.29060951684746755]
	TIME [epoch: 5.93 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20105624997792923		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.20105624997792923 | validation: 0.29862201117414894]
	TIME [epoch: 5.93 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21169777210061733		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.21169777210061733 | validation: 0.2871956797448095]
	TIME [epoch: 5.94 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275431281270544		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.275431281270544 | validation: 0.2871953196933814]
	TIME [epoch: 5.93 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20960145294833743		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.20960145294833743 | validation: 0.2564596903709182]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20881201833474733		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.20881201833474733 | validation: 0.30328608743352037]
	TIME [epoch: 5.93 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.211467070653887		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.211467070653887 | validation: 0.2796218076607767]
	TIME [epoch: 5.93 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280481753974141		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.280481753974141 | validation: 0.31281967516693493]
	TIME [epoch: 5.94 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2078673551055074		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.2078673551055074 | validation: 0.25362345429363975]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20243163916341655		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.20243163916341655 | validation: 0.297283651397898]
	TIME [epoch: 5.93 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20042692724621355		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.20042692724621355 | validation: 0.2737655736179296]
	TIME [epoch: 5.93 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27123890011731905		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.27123890011731905 | validation: 0.29811681790896716]
	TIME [epoch: 5.94 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20583385745031899		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.20583385745031899 | validation: 0.25347296227351873]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1949836579438924		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.1949836579438924 | validation: 0.26928420868047065]
	TIME [epoch: 5.94 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18640676380432086		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.18640676380432086 | validation: 0.2502472493606437]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2143080194902937		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2143080194902937 | validation: 0.3123222295976225]
	TIME [epoch: 5.92 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22023727509953783		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.22023727509953783 | validation: 0.28559182540721734]
	TIME [epoch: 5.93 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32351624984366645		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.32351624984366645 | validation: 0.2536825468366381]
	TIME [epoch: 5.93 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20698282359906942		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.20698282359906942 | validation: 0.42073147787641507]
	TIME [epoch: 5.94 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33443525859598616		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.33443525859598616 | validation: 0.3017465649114493]
	TIME [epoch: 5.93 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3371516094454964		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.3371516094454964 | validation: 0.28061387897890633]
	TIME [epoch: 5.92 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787408242534974		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.2787408242534974 | validation: 0.29216160063690094]
	TIME [epoch: 5.93 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19892221949071284		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.19892221949071284 | validation: 0.2907274442576736]
	TIME [epoch: 5.92 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19883065660721777		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.19883065660721777 | validation: 0.2600272374048301]
	TIME [epoch: 5.94 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24116254995149425		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.24116254995149425 | validation: 0.23250548205186652]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1694752506221383		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.1694752506221383 | validation: 0.33091879844769995]
	TIME [epoch: 5.93 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2397730335162835		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2397730335162835 | validation: 0.27330480331802287]
	TIME [epoch: 5.93 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28886079939168213		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.28886079939168213 | validation: 0.25452421217017435]
	TIME [epoch: 5.94 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24702171798057354		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.24702171798057354 | validation: 0.2616913777649912]
	TIME [epoch: 5.93 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1959676943608165		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.1959676943608165 | validation: 0.27225080934136286]
	TIME [epoch: 5.94 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18278528224820098		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.18278528224820098 | validation: 0.2579534177665494]
	TIME [epoch: 5.93 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24042311424412974		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.24042311424412974 | validation: 0.22593789586907814]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16388558316027171		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.16388558316027171 | validation: 0.3320063257038282]
	TIME [epoch: 5.9 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23479282733885087		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.23479282733885087 | validation: 0.25930523511809017]
	TIME [epoch: 5.92 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29088883798098125		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.29088883798098125 | validation: 0.25451461034047645]
	TIME [epoch: 5.91 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22246034095577494		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.22246034095577494 | validation: 0.2876119742051665]
	TIME [epoch: 276 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20079891281188572		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.20079891281188572 | validation: 0.23057330051592678]
	TIME [epoch: 12.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16072313083493733		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.16072313083493733 | validation: 0.24315135415673528]
	TIME [epoch: 12.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15952846805927592		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.15952846805927592 | validation: 0.2516640714237899]
	TIME [epoch: 12.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16643111255552673		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.16643111255552673 | validation: 0.22204730648010484]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1900168285186562		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.1900168285186562 | validation: 0.23526316090630833]
	TIME [epoch: 12.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632155241895626		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.1632155241895626 | validation: 0.21245455096616475]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15601417123518546		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.15601417123518546 | validation: 0.24206675665714192]
	TIME [epoch: 12.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589029905316922		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1589029905316922 | validation: 0.2312823852797434]
	TIME [epoch: 12.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754045178281629		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1754045178281629 | validation: 0.2688292627024744]
	TIME [epoch: 12.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17576970618869753		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.17576970618869753 | validation: 0.2269967537063248]
	TIME [epoch: 12.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22920127615863595		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.22920127615863595 | validation: 0.2134752618731448]
	TIME [epoch: 12.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15501439464183372		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.15501439464183372 | validation: 0.2887423433022082]
	TIME [epoch: 12.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19933182280432124		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.19933182280432124 | validation: 0.26437774897747024]
	TIME [epoch: 12.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2963157630492972		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2963157630492972 | validation: 0.2308027565144909]
	TIME [epoch: 12.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19846725151149816		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.19846725151149816 | validation: 0.36026160743158686]
	TIME [epoch: 12.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27166678553433893		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.27166678553433893 | validation: 0.21573933285085692]
	TIME [epoch: 12.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23390174624329632		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.23390174624329632 | validation: 0.22754357540351175]
	TIME [epoch: 12.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20862375140645595		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.20862375140645595 | validation: 0.22669776503164096]
	TIME [epoch: 12.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16199567783832486		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.16199567783832486 | validation: 0.20525484561748117]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14282795654943192		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.14282795654943192 | validation: 0.21636794103158033]
	TIME [epoch: 12.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490169922189224		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1490169922189224 | validation: 0.213940529707493]
	TIME [epoch: 12.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14919497218844074		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.14919497218844074 | validation: 0.2100426736607356]
	TIME [epoch: 12.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1643633673151915		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1643633673151915 | validation: 0.2397209819551831]
	TIME [epoch: 12.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15943719741485532		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.15943719741485532 | validation: 0.20567941000575163]
	TIME [epoch: 12.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18310988428069302		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.18310988428069302 | validation: 0.21562570509835247]
	TIME [epoch: 12.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448963586019693		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.1448963586019693 | validation: 0.2114549036192647]
	TIME [epoch: 12.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14001619559924394		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.14001619559924394 | validation: 0.21364316027066677]
	TIME [epoch: 12.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329400787023653		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1329400787023653 | validation: 0.20807256075718078]
	TIME [epoch: 12.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787030686887375		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.13787030686887375 | validation: 0.20758045593380203]
	TIME [epoch: 12.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608804065450786		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.1608804065450786 | validation: 0.2991418164622574]
	TIME [epoch: 12.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22772594076091537		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.22772594076091537 | validation: 0.2658129175341308]
	TIME [epoch: 12.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.319000506654028		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.319000506654028 | validation: 0.23083179596637227]
	TIME [epoch: 12.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25602554630775126		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.25602554630775126 | validation: 0.24242432225794813]
	TIME [epoch: 12.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754099856213924		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1754099856213924 | validation: 0.2994495545967791]
	TIME [epoch: 12.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22207988815782806		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.22207988815782806 | validation: 0.2509988163800276]
	TIME [epoch: 12.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26025739984683727		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.26025739984683727 | validation: 0.23054318766297954]
	TIME [epoch: 12.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2538556908630955		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.2538556908630955 | validation: 0.209798037727995]
	TIME [epoch: 12.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15295454558343047		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.15295454558343047 | validation: 0.2972222835884014]
	TIME [epoch: 12.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21416135122543228		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.21416135122543228 | validation: 0.23177154342921955]
	TIME [epoch: 12.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22866843839903236		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.22866843839903236 | validation: 0.2118700774016166]
	TIME [epoch: 12.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19624733989404433		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.19624733989404433 | validation: 0.21355186535429893]
	TIME [epoch: 12.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479758465891555		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1479758465891555 | validation: 0.2062367436713208]
	TIME [epoch: 12.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13892770036027263		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.13892770036027263 | validation: 0.1937666308391239]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15782927738415603		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.15782927738415603 | validation: 0.2011821811319174]
	TIME [epoch: 12.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13501921127234492		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.13501921127234492 | validation: 0.17872255129175701]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032239503863585		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.13032239503863585 | validation: 0.19920130995564903]
	TIME [epoch: 12.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299201571132564		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.1299201571132564 | validation: 0.19747850032719452]
	TIME [epoch: 12.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12963453895936092		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.12963453895936092 | validation: 0.19242169386952762]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12733248352551704		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.12733248352551704 | validation: 0.17430792026573563]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12754674047322845		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.12754674047322845 | validation: 0.2082039363432336]
	TIME [epoch: 12.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13420270922604136		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.13420270922604136 | validation: 0.19594276942969444]
	TIME [epoch: 12.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1866526122756413		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1866526122756413 | validation: 0.23443045904728177]
	TIME [epoch: 12.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16190877447184626		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.16190877447184626 | validation: 0.20576103692014774]
	TIME [epoch: 12.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19860423848301223		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.19860423848301223 | validation: 0.18700067529825395]
	TIME [epoch: 12.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398844307388532		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.1398844307388532 | validation: 0.28322021322027563]
	TIME [epoch: 12.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2080510774330678		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.2080510774330678 | validation: 0.24347046335134126]
	TIME [epoch: 12.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26753204337707837		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.26753204337707837 | validation: 0.2382684041259817]
	TIME [epoch: 12.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24671529845994553		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.24671529845994553 | validation: 0.2402150036720574]
	TIME [epoch: 12.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543874867792758		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.1543874867792758 | validation: 0.2757010304549942]
	TIME [epoch: 12.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21404446740670272		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.21404446740670272 | validation: 0.20987188764894357]
	TIME [epoch: 12.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21541982300479912		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.21541982300479912 | validation: 0.18578415982295948]
	TIME [epoch: 12.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1903765762520559		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1903765762520559 | validation: 0.19181851122027463]
	TIME [epoch: 12.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14125009869394353		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.14125009869394353 | validation: 0.1891400799737051]
	TIME [epoch: 12.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12606680311880386		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.12606680311880386 | validation: 0.18412728441881626]
	TIME [epoch: 12.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14303879619115914		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.14303879619115914 | validation: 0.1885765187822402]
	TIME [epoch: 12.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12977663420651397		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.12977663420651397 | validation: 0.18558772507167198]
	TIME [epoch: 12.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12971750765704088		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.12971750765704088 | validation: 0.17614562602569162]
	TIME [epoch: 12.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12233824962646388		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.12233824962646388 | validation: 0.17607376023189897]
	TIME [epoch: 12.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12285116958068884		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.12285116958068884 | validation: 0.19086554162148867]
	TIME [epoch: 12.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12998581803095152		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.12998581803095152 | validation: 0.18398745825126542]
	TIME [epoch: 12.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15880758478865645		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.15880758478865645 | validation: 0.19456161809602823]
	TIME [epoch: 12.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13601920702982961		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.13601920702982961 | validation: 0.18025240682661103]
	TIME [epoch: 12.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1546483897257612		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.1546483897257612 | validation: 0.19576402711795088]
	TIME [epoch: 12.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359098439026422		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.1359098439026422 | validation: 0.17861948637419103]
	TIME [epoch: 12.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15246507021649028		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.15246507021649028 | validation: 0.18233896982603637]
	TIME [epoch: 12.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13285909607442434		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.13285909607442434 | validation: 0.17838229152536156]
	TIME [epoch: 12.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473297320423486		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1473297320423486 | validation: 0.19919010093423567]
	TIME [epoch: 12.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306835261845295		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.1306835261845295 | validation: 0.17864576547425873]
	TIME [epoch: 12.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15189950804315872		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.15189950804315872 | validation: 0.18856143236861803]
	TIME [epoch: 12.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12365027330822273		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.12365027330822273 | validation: 0.16902515993752298]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11985209801336033		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.11985209801336033 | validation: 0.1720705092713567]
	TIME [epoch: 12.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11571598792902506		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.11571598792902506 | validation: 0.17876090319649232]
	TIME [epoch: 12.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14261816682616674		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.14261816682616674 | validation: 0.24242284008386572]
	TIME [epoch: 12.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617211969896359		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.1617211969896359 | validation: 0.21376182911580283]
	TIME [epoch: 12.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24093595656062594		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.24093595656062594 | validation: 0.1927915061985952]
	TIME [epoch: 12.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19079423393708012		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.19079423393708012 | validation: 0.261847049702537]
	TIME [epoch: 12.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792538001492177		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.1792538001492177 | validation: 0.16966530232704036]
	TIME [epoch: 12.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449740263167493		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.14449740263167493 | validation: 0.15288531125941984]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825205665436168		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.11825205665436168 | validation: 0.2063310622588102]
	TIME [epoch: 12.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14700135132916906		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.14700135132916906 | validation: 0.19399870275126402]
	TIME [epoch: 12.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038158773233969		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.2038158773233969 | validation: 0.18185642779411162]
	TIME [epoch: 12.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15737730476620038		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.15737730476620038 | validation: 0.21202316309664454]
	TIME [epoch: 12.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571704985924054		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.1571704985924054 | validation: 0.17657146903306795]
	TIME [epoch: 12.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15376029475013378		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.15376029475013378 | validation: 0.15967984743937316]
	TIME [epoch: 12.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1236807380748768		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.1236807380748768 | validation: 0.20979271599709934]
	TIME [epoch: 12.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14785404182477327		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.14785404182477327 | validation: 0.1842435340490983]
	TIME [epoch: 12.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19174322834347904		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.19174322834347904 | validation: 0.15249358700525892]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405820874325965		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1405820874325965 | validation: 0.23407489869050765]
	TIME [epoch: 12.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797498703128365		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1797498703128365 | validation: 0.19197777365684865]
	TIME [epoch: 12.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1723075957579386		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.1723075957579386 | validation: 0.18157100353138977]
	TIME [epoch: 12.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14583269088821477		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.14583269088821477 | validation: 0.20506060888602795]
	TIME [epoch: 12.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15585390621809175		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.15585390621809175 | validation: 0.15530285751942996]
	TIME [epoch: 12.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13478660025100314		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.13478660025100314 | validation: 0.1446716777605863]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11893833678698801		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.11893833678698801 | validation: 0.1887495630968331]
	TIME [epoch: 12.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385174973453816		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.1385174973453816 | validation: 0.17381445625580427]
	TIME [epoch: 12.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15494517747041373		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.15494517747041373 | validation: 0.1644803004846695]
	TIME [epoch: 12.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12306309035100566		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.12306309035100566 | validation: 0.20830426405518776]
	TIME [epoch: 12.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14398973559196343		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.14398973559196343 | validation: 0.17458742339198316]
	TIME [epoch: 12.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15558970626174096		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.15558970626174096 | validation: 0.1612828400091192]
	TIME [epoch: 12.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12169388879328163		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.12169388879328163 | validation: 0.22677524537858745]
	TIME [epoch: 12.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709120174937727		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.1709120174937727 | validation: 0.182394721218143]
	TIME [epoch: 12.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1720030625533196		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.1720030625533196 | validation: 0.17027901732052306]
	TIME [epoch: 12.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15078426145471588		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.15078426145471588 | validation: 0.16757275629656543]
	TIME [epoch: 12.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12199496070839455		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.12199496070839455 | validation: 0.15105811270279648]
	TIME [epoch: 12.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11115346879325501		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.11115346879325501 | validation: 0.1482449595450173]
	TIME [epoch: 12.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1089285658220112		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.1089285658220112 | validation: 0.1535125166985136]
	TIME [epoch: 12.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127024077488096		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1127024077488096 | validation: 0.15629730670450204]
	TIME [epoch: 12.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10789053044301666		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.10789053044301666 | validation: 0.140631122948732]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095682405157226		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.1095682405157226 | validation: 0.15191644618349395]
	TIME [epoch: 12.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862530574690638		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.10862530574690638 | validation: 0.15053979215613192]
	TIME [epoch: 12.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536230666993597		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.11536230666993597 | validation: 0.1662559870276874]
	TIME [epoch: 12.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14736938090443524		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.14736938090443524 | validation: 0.16009205055344491]
	TIME [epoch: 12.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536028498853412		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.11536028498853412 | validation: 0.16244878728329343]
	TIME [epoch: 12.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122876928473421		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.12122876928473421 | validation: 0.1794374409121051]
	TIME [epoch: 12.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12606814491377602		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.12606814491377602 | validation: 0.17838447200046154]
	TIME [epoch: 12.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15816553202713615		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.15816553202713615 | validation: 0.151878581973991]
	TIME [epoch: 12.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10845279719428724		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10845279719428724 | validation: 0.16639864552421707]
	TIME [epoch: 12.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12867828475192422		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.12867828475192422 | validation: 0.1899357319593161]
	TIME [epoch: 12.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19742557093788043		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.19742557093788043 | validation: 0.1358084505083102]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11031160916943764		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.11031160916943764 | validation: 0.2042214314636036]
	TIME [epoch: 12.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15702608671445992		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.15702608671445992 | validation: 0.18500797269529112]
	TIME [epoch: 12.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20016612635248798		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.20016612635248798 | validation: 0.1842897749317468]
	TIME [epoch: 12.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16860742110907093		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.16860742110907093 | validation: 0.17933369641346109]
	TIME [epoch: 12.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11908107195084851		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.11908107195084851 | validation: 0.15019510994850763]
	TIME [epoch: 12.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124505567113566		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.11124505567113566 | validation: 0.13929311770551633]
	TIME [epoch: 12.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11907299244767754		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.11907299244767754 | validation: 0.14435053214532725]
	TIME [epoch: 12.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11043513588456445		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.11043513588456445 | validation: 0.15677163469986702]
	TIME [epoch: 12.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10947509480262774		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10947509480262774 | validation: 0.14809653092646]
	TIME [epoch: 12.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047272654254061		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.1047272654254061 | validation: 0.14805567811942824]
	TIME [epoch: 12.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109681696982646		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.109681696982646 | validation: 0.1461656142477906]
	TIME [epoch: 12.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10735790700173559		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.10735790700173559 | validation: 0.15174413468064996]
	TIME [epoch: 12.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11503295014319555		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.11503295014319555 | validation: 0.17366247981541943]
	TIME [epoch: 12.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12281746146828817		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.12281746146828817 | validation: 0.16306492494193178]
	TIME [epoch: 12.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15906673944569913		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.15906673944569913 | validation: 0.14313858561372628]
	TIME [epoch: 12.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10814706442195782		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.10814706442195782 | validation: 0.20352599028967866]
	TIME [epoch: 12.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16138922267972192		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.16138922267972192 | validation: 0.19221212809275745]
	TIME [epoch: 12.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20874584297509732		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.20874584297509732 | validation: 0.19178008280712808]
	TIME [epoch: 12.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16943183289764271		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.16943183289764271 | validation: 0.20724309669760044]
	TIME [epoch: 12.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13585503818012523		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.13585503818012523 | validation: 0.15076330923306483]
	TIME [epoch: 12.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11411343960643479		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.11411343960643479 | validation: 0.13330152401360398]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10461429795133652		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.10461429795133652 | validation: 0.1574979615686145]
	TIME [epoch: 12.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10639453953226148		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.10639453953226148 | validation: 0.14454363682877389]
	TIME [epoch: 12.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11699045036122228		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.11699045036122228 | validation: 0.16679917729559965]
	TIME [epoch: 12.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11263063010807113		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.11263063010807113 | validation: 0.15030696232195342]
	TIME [epoch: 12.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11593861366990883		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.11593861366990883 | validation: 0.14649018196716784]
	TIME [epoch: 12.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10595843551344566		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.10595843551344566 | validation: 0.14262259566026755]
	TIME [epoch: 12.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10519304749711114		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.10519304749711114 | validation: 0.13997857963002436]
	TIME [epoch: 12.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10159535331190155		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.10159535331190155 | validation: 0.1376891920886076]
	TIME [epoch: 12.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10493556566466367		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.10493556566466367 | validation: 0.14174941403636432]
	TIME [epoch: 12.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11112375148399359		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.11112375148399359 | validation: 0.1773896265144452]
	TIME [epoch: 12.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641540241940718		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.1641540241940718 | validation: 0.14333989274921646]
	TIME [epoch: 12.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031112337814367		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.1031112337814367 | validation: 0.20884021374679237]
	TIME [epoch: 12.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608853329931778		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.1608853329931778 | validation: 0.1854960670974712]
	TIME [epoch: 12.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20373782256776565		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.20373782256776565 | validation: 0.1833906134232867]
	TIME [epoch: 12.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18597085109701297		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.18597085109701297 | validation: 0.16640889469788506]
	TIME [epoch: 12.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11174900044163356		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.11174900044163356 | validation: 0.2158474371735903]
	TIME [epoch: 12.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16677737142940458		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.16677737142940458 | validation: 0.16915800605116182]
	TIME [epoch: 12.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16115011137073215		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.16115011137073215 | validation: 0.16683539413445223]
	TIME [epoch: 12.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332517956053745		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1332517956053745 | validation: 0.14007678732670945]
	TIME [epoch: 12.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819980906848124		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.11819980906848124 | validation: 0.14278885902642294]
	TIME [epoch: 12.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10651107858750511		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.10651107858750511 | validation: 0.1529582789586486]
	TIME [epoch: 12.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256688318063616		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.10256688318063616 | validation: 0.14152379723480735]
	TIME [epoch: 12.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10337960460927058		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.10337960460927058 | validation: 0.13898730798444614]
	TIME [epoch: 12.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015514647504903		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.1015514647504903 | validation: 0.13659990086673954]
	TIME [epoch: 12.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10511929293436718		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.10511929293436718 | validation: 0.14902650525345926]
	TIME [epoch: 12.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10904445074458818		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.10904445074458818 | validation: 0.16925131168702118]
	TIME [epoch: 12.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067148459622551		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.11067148459622551 | validation: 0.15360752405189118]
	TIME [epoch: 12.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463288740123667		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.11463288740123667 | validation: 0.1356837151295472]
	TIME [epoch: 12.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09955695121692597		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.09955695121692597 | validation: 0.1466463870451846]
	TIME [epoch: 12.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10389093225376016		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.10389093225376016 | validation: 0.13705381574670322]
	TIME [epoch: 12.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10662615269543325		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.10662615269543325 | validation: 0.15200887924831258]
	TIME [epoch: 12.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12404127013785131		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.12404127013785131 | validation: 0.13488716624573432]
	TIME [epoch: 12.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10882787751374604		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.10882787751374604 | validation: 0.1351362901997751]
	TIME [epoch: 12.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10567239814359436		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10567239814359436 | validation: 0.14060550944682573]
	TIME [epoch: 12.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059785681394017		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.1059785681394017 | validation: 0.15008388846509335]
	TIME [epoch: 12.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12902946950768152		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.12902946950768152 | validation: 0.13105205149344412]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11271286850642795		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.11271286850642795 | validation: 0.14862064731245947]
	TIME [epoch: 12.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12249985255971318		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.12249985255971318 | validation: 0.1359761675962109]
	TIME [epoch: 12.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09808313257545713		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09808313257545713 | validation: 0.14179617148185916]
	TIME [epoch: 12.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797960089623843		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.09797960089623843 | validation: 0.14153986924749842]
	TIME [epoch: 12.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10268295164904871		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.10268295164904871 | validation: 0.18416669450406065]
	TIME [epoch: 12.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14036407829511996		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.14036407829511996 | validation: 0.18392465872763056]
	TIME [epoch: 12.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1853928142504444		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.1853928142504444 | validation: 0.16684679853445517]
	TIME [epoch: 12.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559847566355977		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.14559847566355977 | validation: 0.1606223475109534]
	TIME [epoch: 12.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11682801753355286		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.11682801753355286 | validation: 0.14577396647360588]
	TIME [epoch: 12.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566827532368799		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.10566827532368799 | validation: 0.1295248785180132]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10107360436493139		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.10107360436493139 | validation: 0.1369215243588728]
	TIME [epoch: 12.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006733428244532		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1006733428244532 | validation: 0.13656137104360688]
	TIME [epoch: 12.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574583583767776		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.09574583583767776 | validation: 0.13063788845618948]
	TIME [epoch: 12.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09582445206906703		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.09582445206906703 | validation: 0.13845229552620272]
	TIME [epoch: 12.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09905003810480979		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.09905003810480979 | validation: 0.14444151981729886]
	TIME [epoch: 12.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10858664610300058		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.10858664610300058 | validation: 0.1720763352401575]
	TIME [epoch: 12.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15597785084451699		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.15597785084451699 | validation: 0.13434863307718964]
	TIME [epoch: 12.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983905683650476		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09983905683650476 | validation: 0.20741096450322033]
	TIME [epoch: 12.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16735123146137973		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.16735123146137973 | validation: 0.17421690628660566]
	TIME [epoch: 12.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17692910391781363		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.17692910391781363 | validation: 0.16984520502484254]
	TIME [epoch: 12.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16917952367749614		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.16917952367749614 | validation: 0.1380244936597943]
	TIME [epoch: 12.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10289224479946231		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.10289224479946231 | validation: 0.18358190347551112]
	TIME [epoch: 12.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13791981635318631		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.13791981635318631 | validation: 0.15695069880575574]
	TIME [epoch: 12.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13817781199658988		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.13817781199658988 | validation: 0.13055308306279068]
	TIME [epoch: 12.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465470167689617		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.10465470167689617 | validation: 0.1663381262186824]
	TIME [epoch: 12.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13837525886303553		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.13837525886303553 | validation: 0.15601545659942626]
	TIME [epoch: 12.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11690699747298235		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.11690699747298235 | validation: 0.1486796877587939]
	TIME [epoch: 12.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10986171520304922		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.10986171520304922 | validation: 0.14210860106951156]
	TIME [epoch: 12.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768020258828863		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.10768020258828863 | validation: 0.12613917235212455]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10103064780453273		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.10103064780453273 | validation: 0.11250422407566343]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756528495812936		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.09756528495812936 | validation: 0.13930452909274813]
	TIME [epoch: 12.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161589472674236		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.10161589472674236 | validation: 0.13269216448174298]
	TIME [epoch: 12.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070790170759555		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.10070790170759555 | validation: 0.1267382217538819]
	TIME [epoch: 12.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09692879789110162		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.09692879789110162 | validation: 0.12573554729875192]
	TIME [epoch: 12.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09847274654761216		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.09847274654761216 | validation: 0.12019790599585418]
	TIME [epoch: 12.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10083729178567107		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.10083729178567107 | validation: 0.12377960524034362]
	TIME [epoch: 12.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10056942982180893		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.10056942982180893 | validation: 0.13384485774824026]
	TIME [epoch: 12.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10105280212722914		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.10105280212722914 | validation: 0.14090625096941678]
	TIME [epoch: 12.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11322107759487128		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.11322107759487128 | validation: 0.13229213652237615]
	TIME [epoch: 12.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10118138294823745		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.10118138294823745 | validation: 0.12431653908321239]
	TIME [epoch: 12.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09714949252567624		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.09714949252567624 | validation: 0.1648074770447241]
	TIME [epoch: 12.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12322554336695213		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.12322554336695213 | validation: 0.177571908138482]
	TIME [epoch: 12.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15886181567239382		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.15886181567239382 | validation: 0.15649376633612155]
	TIME [epoch: 12.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546683207486791		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.11546683207486791 | validation: 0.18936313962597612]
	TIME [epoch: 12.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453240935373503		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.1453240935373503 | validation: 0.15914153818570215]
	TIME [epoch: 12.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12466527302913057		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.12466527302913057 | validation: 0.13400222450532664]
	TIME [epoch: 12.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532735351734082		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.10532735351734082 | validation: 0.1399161462290016]
	TIME [epoch: 12.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629962504886914		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.10629962504886914 | validation: 0.1374794111488854]
	TIME [epoch: 12.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986350241898775		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.0986350241898775 | validation: 0.1318175703086752]
	TIME [epoch: 12.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507951753998133		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.09507951753998133 | validation: 0.12279721239682773]
	TIME [epoch: 12.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09748168508981206		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.09748168508981206 | validation: 0.12284198049682865]
	TIME [epoch: 12.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09363271794278882		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.09363271794278882 | validation: 0.1271155542393749]
	TIME [epoch: 12.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094356255386169		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.094356255386169 | validation: 0.1284844181865694]
	TIME [epoch: 12.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032533717761678		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.10032533717761678 | validation: 0.13291716476266782]
	TIME [epoch: 12.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09971999416717406		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.09971999416717406 | validation: 0.15736089461818334]
	TIME [epoch: 12.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949165126688984		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.10949165126688984 | validation: 0.12528412068035483]
	TIME [epoch: 12.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09486355249934562		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.09486355249934562 | validation: 0.13388861783812098]
	TIME [epoch: 12.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09944865774189579		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.09944865774189579 | validation: 0.1418415113875151]
	TIME [epoch: 12.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10437203164614413		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.10437203164614413 | validation: 0.15756247744598997]
	TIME [epoch: 12.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385922639695814		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.12385922639695814 | validation: 0.1316195714323434]
	TIME [epoch: 12.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227413455629843		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.09227413455629843 | validation: 0.13018921084995047]
	TIME [epoch: 12.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576575907271835		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.09576575907271835 | validation: 0.13903878694115424]
	TIME [epoch: 12.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10004789552590937		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.10004789552590937 | validation: 0.11839513502475126]
	TIME [epoch: 12.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954775676301155		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.0954775676301155 | validation: 0.1387493051103358]
	TIME [epoch: 12.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1134008564112612		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.1134008564112612 | validation: 0.13981657539724768]
	TIME [epoch: 12.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10137183183405025		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.10137183183405025 | validation: 0.1482000519080882]
	TIME [epoch: 12.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11151020437104858		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.11151020437104858 | validation: 0.14035466526406334]
	TIME [epoch: 12.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09762927201723347		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.09762927201723347 | validation: 0.14182670811082038]
	TIME [epoch: 12.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11085017321658747		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.11085017321658747 | validation: 0.12349296870578866]
	TIME [epoch: 12.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282226696438692		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.09282226696438692 | validation: 0.1276622636996638]
	TIME [epoch: 12.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623012108665853		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.09623012108665853 | validation: 0.12221650534162476]
	TIME [epoch: 12.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09710386679263533		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.09710386679263533 | validation: 0.1277735608356871]
	TIME [epoch: 12.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764559528301037		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.09764559528301037 | validation: 0.1400814932722458]
	TIME [epoch: 12.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10529119182364889		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.10529119182364889 | validation: 0.1505587217154788]
	TIME [epoch: 12.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12851133259003653		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.12851133259003653 | validation: 0.1298911494514036]
	TIME [epoch: 12.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391079178934768		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.09391079178934768 | validation: 0.17101528760502838]
	TIME [epoch: 12.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13011631815989388		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.13011631815989388 | validation: 0.1633531554035166]
	TIME [epoch: 12.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448004632604204		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.1448004632604204 | validation: 0.13932141908705317]
	TIME [epoch: 12.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12628540739611854		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.12628540739611854 | validation: 0.13249480764359767]
	TIME [epoch: 12.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131933163665933		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.1131933163665933 | validation: 0.13035972198654885]
	TIME [epoch: 12.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09905106828175701		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.09905106828175701 | validation: 0.12978129373860364]
	TIME [epoch: 12.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302981509299656		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.09302981509299656 | validation: 0.13666414622794423]
	TIME [epoch: 12.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09854735212336628		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.09854735212336628 | validation: 0.13931424532991343]
	TIME [epoch: 12.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446834997550777		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.10446834997550777 | validation: 0.1217697115685727]
	TIME [epoch: 12.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09731331387403643		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.09731331387403643 | validation: 0.12436565907929463]
	TIME [epoch: 12.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08962533753023644		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.08962533753023644 | validation: 0.12429160209889507]
	TIME [epoch: 12.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08999025098431478		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.08999025098431478 | validation: 0.12239756747250544]
	TIME [epoch: 12.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383867449923877		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.09383867449923877 | validation: 0.12757190828111262]
	TIME [epoch: 12.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09235555527236397		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.09235555527236397 | validation: 0.13474975968080544]
	TIME [epoch: 12.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270654323165506		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.09270654323165506 | validation: 0.12846648620861367]
	TIME [epoch: 12.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0901026172093627		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.0901026172093627 | validation: 0.128597746947771]
	TIME [epoch: 12.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603515436253895		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.09603515436253895 | validation: 0.1315570889325591]
	TIME [epoch: 12.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10634376491415658		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.10634376491415658 | validation: 0.1625504086791282]
	TIME [epoch: 12.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367764203951306		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.1367764203951306 | validation: 0.11629087973705961]
	TIME [epoch: 12.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09190460806059272		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.09190460806059272 | validation: 0.16174931830148562]
	TIME [epoch: 12.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338421287639021		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.1338421287639021 | validation: 0.1651920148702997]
	TIME [epoch: 12.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14669040808498499		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.14669040808498499 | validation: 0.15195339572131436]
	TIME [epoch: 12.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12850715901542523		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.12850715901542523 | validation: 0.12438659071735157]
	TIME [epoch: 12.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10045439354758756		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.10045439354758756 | validation: 0.12699314535464237]
	TIME [epoch: 12.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09664099005763564		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.09664099005763564 | validation: 0.12802180718620929]
	TIME [epoch: 12.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10137042596396992		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.10137042596396992 | validation: 0.12114864728609978]
	TIME [epoch: 12.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08930400385928647		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.08930400385928647 | validation: 0.1237706108662219]
	TIME [epoch: 12.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469244899098417		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.09469244899098417 | validation: 0.1281146263229234]
	TIME [epoch: 12.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10126381530551269		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.10126381530551269 | validation: 0.12734966691142732]
	TIME [epoch: 12.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08961490831174039		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.08961490831174039 | validation: 0.11043853033087561]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990797460833758		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.08990797460833758 | validation: 0.11097291116820987]
	TIME [epoch: 12.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067796821799202		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.09067796821799202 | validation: 0.13209163370515878]
	TIME [epoch: 12.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08696574208714596		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.08696574208714596 | validation: 0.1251783432517062]
	TIME [epoch: 12.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09186080205605604		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.09186080205605604 | validation: 0.12613478390151386]
	TIME [epoch: 12.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09265770971259187		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.09265770971259187 | validation: 0.13986428463822262]
	TIME [epoch: 12.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1092351573315826		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.1092351573315826 | validation: 0.12716276180007222]
	TIME [epoch: 12.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09800256889470228		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.09800256889470228 | validation: 0.14537603238175195]
	TIME [epoch: 12.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10659015075198486		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.10659015075198486 | validation: 0.1290719958484988]
	TIME [epoch: 12.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133340476886011		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.09133340476886011 | validation: 0.12289990795525418]
	TIME [epoch: 12.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732071700628141		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.08732071700628141 | validation: 0.11909844755179891]
	TIME [epoch: 12.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08991341037428811		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.08991341037428811 | validation: 0.11741645979177245]
	TIME [epoch: 12.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08591223515858762		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08591223515858762 | validation: 0.11789419680727967]
	TIME [epoch: 12.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09385281803696416		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.09385281803696416 | validation: 0.11300314046940074]
	TIME [epoch: 12.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097459472318501		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.10097459472318501 | validation: 0.13868450754246237]
	TIME [epoch: 12.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11629139330627095		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.11629139330627095 | validation: 0.13147919634838307]
	TIME [epoch: 12.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09176961038639618		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.09176961038639618 | validation: 0.1258032682825356]
	TIME [epoch: 12.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09221352341540005		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.09221352341540005 | validation: 0.13437767473523618]
	TIME [epoch: 12.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09634773849791041		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09634773849791041 | validation: 0.14278293325140018]
	TIME [epoch: 12.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10428504759027866		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.10428504759027866 | validation: 0.13033021756994895]
	TIME [epoch: 12.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056159904901229		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.1056159904901229 | validation: 0.14499643066899798]
	TIME [epoch: 12.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11394898372531888		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.11394898372531888 | validation: 0.11076551597139879]
	TIME [epoch: 12.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833347379378974		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.08833347379378974 | validation: 0.14481113612600202]
	TIME [epoch: 12.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1115651509218613		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.1115651509218613 | validation: 0.14532327788273586]
	TIME [epoch: 12.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11320176610132329		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.11320176610132329 | validation: 0.11876805704602994]
	TIME [epoch: 12.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09583648854911032		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.09583648854911032 | validation: 0.1371447247856818]
	TIME [epoch: 12.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116620968542309		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.1116620968542309 | validation: 0.14242637610962433]
	TIME [epoch: 12.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10448237362041		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.10448237362041 | validation: 0.1286475796061559]
	TIME [epoch: 12.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928321522980496		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0928321522980496 | validation: 0.12796371811464338]
	TIME [epoch: 12.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031734513298785		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.1031734513298785 | validation: 0.12383825664659001]
	TIME [epoch: 12.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09910280768534736		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.09910280768534736 | validation: 0.12631292716763007]
	TIME [epoch: 12.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08895742715387993		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.08895742715387993 | validation: 0.11682996310423642]
	TIME [epoch: 12.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08609268328564426		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.08609268328564426 | validation: 0.12304844749264393]
	TIME [epoch: 12.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151974618886223		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.09151974618886223 | validation: 0.10629635248349684]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909907352737652		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.08909907352737652 | validation: 0.12125985663112393]
	TIME [epoch: 12.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09041678611164794		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.09041678611164794 | validation: 0.1132416499150198]
	TIME [epoch: 12.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09098485253103476		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.09098485253103476 | validation: 0.1311363391656955]
	TIME [epoch: 12.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457922714695052		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.09457922714695052 | validation: 0.13700301384509103]
	TIME [epoch: 12.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767577088202836		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.09767577088202836 | validation: 0.12863839854785084]
	TIME [epoch: 12.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10601103672276609		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.10601103672276609 | validation: 0.1255353469270579]
	TIME [epoch: 12.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09485840655738127		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.09485840655738127 | validation: 0.1401127328680601]
	TIME [epoch: 12.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752924233205153		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.09752924233205153 | validation: 0.11745244424967245]
	TIME [epoch: 12.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058783381274324		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.09058783381274324 | validation: 0.10643345171214397]
	TIME [epoch: 12.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08943522421881538		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.08943522421881538 | validation: 0.1054407806262854]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799778558583532		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08799778558583532 | validation: 0.12034629163639031]
	TIME [epoch: 12.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08808455622761356		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.08808455622761356 | validation: 0.12268856912508963]
	TIME [epoch: 12.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638468265054768		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.08638468265054768 | validation: 0.12903764783967772]
	TIME [epoch: 12.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127847039123183		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.09127847039123183 | validation: 0.1255846164516887]
	TIME [epoch: 12.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889339487534354		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.0889339487534354 | validation: 0.12139713553206116]
	TIME [epoch: 12.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09674980327458382		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.09674980327458382 | validation: 0.14707533942362755]
	TIME [epoch: 12.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131431368888716		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.1131431368888716 | validation: 0.12476735295457182]
	TIME [epoch: 12.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09246819288695049		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.09246819288695049 | validation: 0.12445330173260323]
	TIME [epoch: 12.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08981998957671887		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08981998957671887 | validation: 0.1309453446233973]
	TIME [epoch: 12.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720248049345333		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.08720248049345333 | validation: 0.11842228335492361]
	TIME [epoch: 12.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08422487222420484		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.08422487222420484 | validation: 0.1260359693302457]
	TIME [epoch: 12.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09068527714495915		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.09068527714495915 | validation: 0.11551576591306319]
	TIME [epoch: 12.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09313806191893625		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.09313806191893625 | validation: 0.13965273836184258]
	TIME [epoch: 12.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10371503619145932		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.10371503619145932 | validation: 0.13074963363796938]
	TIME [epoch: 12.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08973911294606772		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.08973911294606772 | validation: 0.12402611350030118]
	TIME [epoch: 12.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871151398193426		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.08871151398193426 | validation: 0.12513393807526393]
	TIME [epoch: 12.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344546803754888		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.08344546803754888 | validation: 0.12351206964958239]
	TIME [epoch: 12.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608668641266613		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.08608668641266613 | validation: 0.12765388614396675]
	TIME [epoch: 12.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0891374484799476		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.0891374484799476 | validation: 0.12287810931551058]
	TIME [epoch: 12.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08791732015719284		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.08791732015719284 | validation: 0.1428827796518144]
	TIME [epoch: 12.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10291805571688155		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.10291805571688155 | validation: 0.13192594752419898]
	TIME [epoch: 12.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11161497147750353		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.11161497147750353 | validation: 0.15579166026638452]
	TIME [epoch: 12.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12347017265728692		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.12347017265728692 | validation: 0.1375433195874853]
	TIME [epoch: 12.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778066605670429		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.08778066605670429 | validation: 0.1346241292966006]
	TIME [epoch: 12.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11112092576845757		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.11112092576845757 | validation: 0.14344505276729672]
	TIME [epoch: 12.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10213338238939225		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.10213338238939225 | validation: 0.13149467059419648]
	TIME [epoch: 12.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09216877376043929		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.09216877376043929 | validation: 0.12203720286545182]
	TIME [epoch: 12.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09150716016443426		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.09150716016443426 | validation: 0.12156500922541134]
	TIME [epoch: 12.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08908072621356963		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.08908072621356963 | validation: 0.12268918546597658]
	TIME [epoch: 12.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08896434683503529		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.08896434683503529 | validation: 0.12242461242767688]
	TIME [epoch: 12.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09002882212250073		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.09002882212250073 | validation: 0.1316724138017775]
	TIME [epoch: 12.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728261138953866		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.09728261138953866 | validation: 0.11974188130951705]
	TIME [epoch: 12.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08868326148238702		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.08868326148238702 | validation: 0.11872648167342725]
	TIME [epoch: 12.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08738024761130593		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.08738024761130593 | validation: 0.121809837929092]
	TIME [epoch: 12.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08862293740349753		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.08862293740349753 | validation: 0.11252185109541145]
	TIME [epoch: 12.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09041359385294119		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.09041359385294119 | validation: 0.14356371700793047]
	TIME [epoch: 12.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09186417644588588		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.09186417644588588 | validation: 0.12161144099343808]
	TIME [epoch: 12.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08932599520443982		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.08932599520443982 | validation: 0.11195881354006842]
	TIME [epoch: 12.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864963161278061		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.08864963161278061 | validation: 0.12200577413010345]
	TIME [epoch: 12.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091463966979958		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.091463966979958 | validation: 0.1317357051142179]
	TIME [epoch: 12.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09265159006362143		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.09265159006362143 | validation: 0.1215110663372287]
	TIME [epoch: 12.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842639313743314		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.08842639313743314 | validation: 0.12067804242992244]
	TIME [epoch: 12.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09252256711468902		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.09252256711468902 | validation: 0.1250218530109958]
	TIME [epoch: 12.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09147563798052694		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.09147563798052694 | validation: 0.1394444246966928]
	TIME [epoch: 12.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182714024037832		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.10182714024037832 | validation: 0.11948458005632584]
	TIME [epoch: 12.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08584168907910138		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.08584168907910138 | validation: 0.14530624413888535]
	TIME [epoch: 12.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182042957673267		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.10182042957673267 | validation: 0.14716118639148257]
	TIME [epoch: 12.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096063048063288		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.1096063048063288 | validation: 0.11305119651573255]
	TIME [epoch: 12.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08992303982386061		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.08992303982386061 | validation: 0.1309086073581994]
	TIME [epoch: 12.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09749110730763466		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.09749110730763466 | validation: 0.13972750262732317]
	TIME [epoch: 12.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404673876401026		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.10404673876401026 | validation: 0.11388383275270231]
	TIME [epoch: 12.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0891355738177972		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.0891355738177972 | validation: 0.12852934488710752]
	TIME [epoch: 12.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059102991680959		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.1059102991680959 | validation: 0.1296762127608739]
	TIME [epoch: 12.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10062355676105479		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.10062355676105479 | validation: 0.11719660827829387]
	TIME [epoch: 12.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09132033528258501		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.09132033528258501 | validation: 0.1270379318563186]
	TIME [epoch: 12.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09274263830693417		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.09274263830693417 | validation: 0.1325052935395637]
	TIME [epoch: 12.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115404264429393		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.09115404264429393 | validation: 0.1261341696328128]
	TIME [epoch: 12.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526595114559708		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08526595114559708 | validation: 0.1227552637380745]
	TIME [epoch: 12.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170287166111922		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.09170287166111922 | validation: 0.12956087167869074]
	TIME [epoch: 12.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09038463064217186		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.09038463064217186 | validation: 0.12293607809718819]
	TIME [epoch: 12.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09583143308281106		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.09583143308281106 | validation: 0.10582648574833811]
	TIME [epoch: 12.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08803188597912026		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.08803188597912026 | validation: 0.11782885328310962]
	TIME [epoch: 12.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663434235900976		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.08663434235900976 | validation: 0.12833475048635434]
	TIME [epoch: 12.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743347342285826		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.08743347342285826 | validation: 0.11124348018499913]
	TIME [epoch: 12.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08677394247417894		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.08677394247417894 | validation: 0.1339687683241295]
	TIME [epoch: 12.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08591761243715315		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.08591761243715315 | validation: 0.12341273795839842]
	TIME [epoch: 12.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.086255903176021		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.086255903176021 | validation: 0.12902360487944106]
	TIME [epoch: 12.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203956045512044		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.08203956045512044 | validation: 0.1284984504709996]
	TIME [epoch: 12.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08700754851658424		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.08700754851658424 | validation: 0.13241099326460434]
	TIME [epoch: 12.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09157161930198587		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.09157161930198587 | validation: 0.1252337676109991]
	TIME [epoch: 12.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812935507803908		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.08812935507803908 | validation: 0.1298169668125504]
	TIME [epoch: 12.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09080754173524674		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.09080754173524674 | validation: 0.11767224023781986]
	TIME [epoch: 12.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08400057433033772		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.08400057433033772 | validation: 0.11462977078867978]
	TIME [epoch: 12.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08647552488461223		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.08647552488461223 | validation: 0.1098219246235094]
	TIME [epoch: 12.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08684288932698646		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.08684288932698646 | validation: 0.1242481751632206]
	TIME [epoch: 12.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897804721820489		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0897804721820489 | validation: 0.14179538365556618]
	TIME [epoch: 12.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928649176526913		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0928649176526913 | validation: 0.12569823498839744]
	TIME [epoch: 12.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09193014434455528		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.09193014434455528 | validation: 0.13705199345182517]
	TIME [epoch: 12.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863850509317072		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.09863850509317072 | validation: 0.11727703584277016]
	TIME [epoch: 12.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0873906723118399		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.0873906723118399 | validation: 0.11576087923762071]
	TIME [epoch: 12.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08553731695856559		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.08553731695856559 | validation: 0.11009331669769754]
	TIME [epoch: 12.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245871683442993		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.08245871683442993 | validation: 0.11333301228672572]
	TIME [epoch: 12.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08729094889688688		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.08729094889688688 | validation: 0.10649731486048014]
	TIME [epoch: 12.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09434825410775034		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.09434825410775034 | validation: 0.11992701010691349]
	TIME [epoch: 12.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09078632825203602		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.09078632825203602 | validation: 0.12362441369082558]
	TIME [epoch: 12.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199071610118041		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.09199071610118041 | validation: 0.12336279165688091]
	TIME [epoch: 12.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10079283504849602		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.10079283504849602 | validation: 0.12535514177535131]
	TIME [epoch: 12.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538347767289662		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.10538347767289662 | validation: 0.12887698697985525]
	TIME [epoch: 12.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247309937316692		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.09247309937316692 | validation: 0.11617744783750394]
	TIME [epoch: 12.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829665059703149		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.0829665059703149 | validation: 0.11760758832626175]
	TIME [epoch: 12.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133512259112753		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.09133512259112753 | validation: 0.13138558826035215]
	TIME [epoch: 12.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885523571449306		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.0885523571449306 | validation: 0.12395719956120424]
	TIME [epoch: 12.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08917220610371322		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.08917220610371322 | validation: 0.12496788165572666]
	TIME [epoch: 12.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603164147372797		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.08603164147372797 | validation: 0.12492255104465724]
	TIME [epoch: 12.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818313671066112		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.08818313671066112 | validation: 0.11176964704865758]
	TIME [epoch: 12.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08332857729103176		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.08332857729103176 | validation: 0.11812045675521117]
	TIME [epoch: 12.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587387427857607		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.08587387427857607 | validation: 0.11917599431610074]
	TIME [epoch: 12.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188283395631285		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.08188283395631285 | validation: 0.1049247173962965]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08368680048595473		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.08368680048595473 | validation: 0.11078689148322987]
	TIME [epoch: 12.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799093034567182		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.08799093034567182 | validation: 0.12424514757476027]
	TIME [epoch: 12.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849274236035963		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0849274236035963 | validation: 0.1168861064940308]
	TIME [epoch: 12.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08421525371653578		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.08421525371653578 | validation: 0.1265155656713759]
	TIME [epoch: 12.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601001571799893		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.09601001571799893 | validation: 0.1414966672744213]
	TIME [epoch: 12.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029205636371417		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.10029205636371417 | validation: 0.110800530788935]
	TIME [epoch: 12.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0851465902284821		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0851465902284821 | validation: 0.12218090645318647]
	TIME [epoch: 12.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08822087869536344		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.08822087869536344 | validation: 0.13190869468337746]
	TIME [epoch: 12.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10058873100402992		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.10058873100402992 | validation: 0.1213136820736574]
	TIME [epoch: 12.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08584637322088126		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.08584637322088126 | validation: 0.12488244752793785]
	TIME [epoch: 12.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645650423990801		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.08645650423990801 | validation: 0.13418246546264953]
	TIME [epoch: 12.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224127776521918		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.09224127776521918 | validation: 0.12999817327853075]
	TIME [epoch: 12.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000032889577465		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.09000032889577465 | validation: 0.13392869490136539]
	TIME [epoch: 12.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842177545560297		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.08842177545560297 | validation: 0.12466813391775969]
	TIME [epoch: 12.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08275908348141943		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.08275908348141943 | validation: 0.13346703647168678]
	TIME [epoch: 12.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09167625121874458		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.09167625121874458 | validation: 0.1304079116036059]
	TIME [epoch: 12.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110812170646137		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.10110812170646137 | validation: 0.12775251873951932]
	TIME [epoch: 12.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08461511220133565		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.08461511220133565 | validation: 0.11954776081711511]
	TIME [epoch: 12.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798256499325194		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.08798256499325194 | validation: 0.12952306177819714]
	TIME [epoch: 12.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08910492680583382		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.08910492680583382 | validation: 0.12193972136594247]
	TIME [epoch: 12.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876299884970155		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.08876299884970155 | validation: 0.1099545696767189]
	TIME [epoch: 12.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08281591706541754		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.08281591706541754 | validation: 0.12137655128329881]
	TIME [epoch: 12.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08390906335356373		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.08390906335356373 | validation: 0.11724340379709962]
	TIME [epoch: 12.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08420778860576729		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.08420778860576729 | validation: 0.1252476157659146]
	TIME [epoch: 12.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329176132772816		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.08329176132772816 | validation: 0.11521179840268472]
	TIME [epoch: 12.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08623934084055046		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.08623934084055046 | validation: 0.13307761779383842]
	TIME [epoch: 12.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383236336760853		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.09383236336760853 | validation: 0.1270250753556621]
	TIME [epoch: 12.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09860003270072182		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.09860003270072182 | validation: 0.12633381987712186]
	TIME [epoch: 12.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686183731534636		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.08686183731534636 | validation: 0.11483822770936306]
	TIME [epoch: 12.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08759801084805194		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.08759801084805194 | validation: 0.1274387596930658]
	TIME [epoch: 12.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09754971041604424		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.09754971041604424 | validation: 0.10787534362478696]
	TIME [epoch: 12.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08291607257319032		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.08291607257319032 | validation: 0.11592690596609317]
	TIME [epoch: 12.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789493024309252		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0789493024309252 | validation: 0.11249298730795639]
	TIME [epoch: 12.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616390627717477		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.08616390627717477 | validation: 0.12297119467001925]
	TIME [epoch: 12.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08627532006852497		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.08627532006852497 | validation: 0.11166145844605489]
	TIME [epoch: 12.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08337639176577702		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.08337639176577702 | validation: 0.1254701321742747]
	TIME [epoch: 12.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697782087544415		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.08697782087544415 | validation: 0.13339587716185905]
	TIME [epoch: 12.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141266218816714		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.09141266218816714 | validation: 0.12618265082459842]
	TIME [epoch: 12.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035696309078336		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.08035696309078336 | validation: 0.11430764065628918]
	TIME [epoch: 12.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066229537160624		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.09066229537160624 | validation: 0.12692817144815025]
	TIME [epoch: 12.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225640196806847		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.09225640196806847 | validation: 0.12095223163469293]
	TIME [epoch: 12.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08392722405243987		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.08392722405243987 | validation: 0.11962223426806391]
	TIME [epoch: 12.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08558411011360721		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.08558411011360721 | validation: 0.11433827583972903]
	TIME [epoch: 12.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256717615122286		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.08256717615122286 | validation: 0.11808358676891301]
	TIME [epoch: 12.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694142644074392		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.08694142644074392 | validation: 0.11685080215179222]
	TIME [epoch: 12.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08332087567864578		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.08332087567864578 | validation: 0.12213500271243648]
	TIME [epoch: 12.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771264906950839		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.08771264906950839 | validation: 0.12365932774653912]
	TIME [epoch: 12.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833021035912143		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.08833021035912143 | validation: 0.11292445630188284]
	TIME [epoch: 12.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08701966934880206		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.08701966934880206 | validation: 0.11751608317680531]
	TIME [epoch: 12.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08341214106830884		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.08341214106830884 | validation: 0.11616505320563997]
	TIME [epoch: 12.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08151864148557357		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.08151864148557357 | validation: 0.11695667865916673]
	TIME [epoch: 12.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831495606772462		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.0831495606772462 | validation: 0.11410184697085862]
	TIME [epoch: 12.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08426085820177118		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.08426085820177118 | validation: 0.12343386145572083]
	TIME [epoch: 12.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688772482453876		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.08688772482453876 | validation: 0.1340718615661662]
	TIME [epoch: 12.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317020645177485		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.09317020645177485 | validation: 0.12194921778440762]
	TIME [epoch: 12.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08389676253125058		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.08389676253125058 | validation: 0.12083565655982081]
	TIME [epoch: 12.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08539672241335282		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.08539672241335282 | validation: 0.11980304849306447]
	TIME [epoch: 12.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09379946831923686		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.09379946831923686 | validation: 0.10495959832797555]
	TIME [epoch: 12.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08751762093453516		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.08751762093453516 | validation: 0.1266794071345698]
	TIME [epoch: 12.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08682523975359743		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.08682523975359743 | validation: 0.11259903711638519]
	TIME [epoch: 12.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08781390785916397		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.08781390785916397 | validation: 0.1187012470992809]
	TIME [epoch: 12.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0842973335240347		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.0842973335240347 | validation: 0.10725446527947131]
	TIME [epoch: 12.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493079189746006		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.08493079189746006 | validation: 0.11621427496635262]
	TIME [epoch: 12.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08474994351580832		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.08474994351580832 | validation: 0.1071756565955881]
	TIME [epoch: 12.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843287880335797		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.0843287880335797 | validation: 0.1181942667685758]
	TIME [epoch: 12.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08511767874970753		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.08511767874970753 | validation: 0.13289694194842655]
	TIME [epoch: 12.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859607936305105		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0859607936305105 | validation: 0.10725424883681917]
	TIME [epoch: 12.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08263270574872981		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.08263270574872981 | validation: 0.13502106930030253]
	TIME [epoch: 293 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08234007105832959		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.08234007105832959 | validation: 0.11159383264681869]
	TIME [epoch: 25.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470565589377067		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.08470565589377067 | validation: 0.12168236650203193]
	TIME [epoch: 25.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08274932275831605		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.08274932275831605 | validation: 0.10726353756042309]
	TIME [epoch: 25.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08476862030233587		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.08476862030233587 | validation: 0.11745090956872772]
	TIME [epoch: 25.8 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08737123256863198		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.08737123256863198 | validation: 0.12623644822412025]
	TIME [epoch: 25.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947242023361925		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.0947242023361925 | validation: 0.1393095581983928]
	TIME [epoch: 25.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734797993518213		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.09734797993518213 | validation: 0.12684255022547117]
	TIME [epoch: 25.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186134494134355		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.08186134494134355 | validation: 0.12707038124390835]
	TIME [epoch: 25.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09055112588497824		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.09055112588497824 | validation: 0.11889925081228028]
	TIME [epoch: 25.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818653446777962		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.08818653446777962 | validation: 0.11810623161367158]
	TIME [epoch: 25.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08284450691760067		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.08284450691760067 | validation: 0.1141853083729949]
	TIME [epoch: 25.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869189178894088		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.0869189178894088 | validation: 0.14317497862730083]
	TIME [epoch: 25.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08913671547413346		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.08913671547413346 | validation: 0.11842387967252975]
	TIME [epoch: 25.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327789194225674		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.08327789194225674 | validation: 0.1074399854528507]
	TIME [epoch: 25.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08612486569015108		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.08612486569015108 | validation: 0.11198337378405482]
	TIME [epoch: 25.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0834198441386114		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.0834198441386114 | validation: 0.11908773557170321]
	TIME [epoch: 25.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0793797931815932		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.0793797931815932 | validation: 0.12016983677759362]
	TIME [epoch: 25.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08244740414550655		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.08244740414550655 | validation: 0.12431640566326424]
	TIME [epoch: 25.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08373592975347947		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.08373592975347947 | validation: 0.10451730855239494]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08401626448483837		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.08401626448483837 | validation: 0.12482202240146695]
	TIME [epoch: 25.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07968645020832582		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.07968645020832582 | validation: 0.13289012154292087]
	TIME [epoch: 25.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08210099019950416		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.08210099019950416 | validation: 0.10735836612778758]
	TIME [epoch: 25.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871127542183353		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.0871127542183353 | validation: 0.13644056278338193]
	TIME [epoch: 25.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050107913804224		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.09050107913804224 | validation: 0.12106793801592926]
	TIME [epoch: 25.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362151404903707		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.08362151404903707 | validation: 0.1299444549216966]
	TIME [epoch: 25.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08412522985433281		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.08412522985433281 | validation: 0.11996801401758705]
	TIME [epoch: 25.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918455636124671		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.0918455636124671 | validation: 0.1179815145211526]
	TIME [epoch: 25.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668901247064893		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.08668901247064893 | validation: 0.11931797154960222]
	TIME [epoch: 25.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08199429515043523		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.08199429515043523 | validation: 0.1202164806728356]
	TIME [epoch: 25.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815317563921656		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.0815317563921656 | validation: 0.12000505267858089]
	TIME [epoch: 25.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08450902624632128		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.08450902624632128 | validation: 0.14257100529723163]
	TIME [epoch: 25.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229277371092184		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.09229277371092184 | validation: 0.1094025828763034]
	TIME [epoch: 25.8 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08419341782182546		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.08419341782182546 | validation: 0.12136771800358931]
	TIME [epoch: 25.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808512355630501		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.0808512355630501 | validation: 0.12294043395622097]
	TIME [epoch: 25.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083646235050447		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.083646235050447 | validation: 0.11885937126294453]
	TIME [epoch: 25.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471003371766914		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.08471003371766914 | validation: 0.12236532318263223]
	TIME [epoch: 25.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08934828597645549		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.08934828597645549 | validation: 0.10869717634097918]
	TIME [epoch: 25.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08522448937040598		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.08522448937040598 | validation: 0.10049226947012149]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_1039.pth
	Model improved!!!
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08933374102944061		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.08933374102944061 | validation: 0.12532951323266742]
	TIME [epoch: 25.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09276699255917485		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.09276699255917485 | validation: 0.11469013466383243]
	TIME [epoch: 25.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860515648699944		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.0860515648699944 | validation: 0.11922128751491473]
	TIME [epoch: 25.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08416761424546902		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.08416761424546902 | validation: 0.13180129965226278]
	TIME [epoch: 25.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08589512811008711		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.08589512811008711 | validation: 0.10613361590234467]
	TIME [epoch: 25.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327289957896308		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.08327289957896308 | validation: 0.13640417217311293]
	TIME [epoch: 25.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114660345971786		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.08114660345971786 | validation: 0.1081714326001546]
	TIME [epoch: 25.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08293894223368964		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.08293894223368964 | validation: 0.12787983440627992]
	TIME [epoch: 25.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08013857782863397		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.08013857782863397 | validation: 0.12964762857292642]
	TIME [epoch: 25.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08680494093908583		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.08680494093908583 | validation: 0.11506349274433125]
	TIME [epoch: 25.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08651673316077318		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.08651673316077318 | validation: 0.1290246235274208]
	TIME [epoch: 25.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0792027433086518		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.0792027433086518 | validation: 0.12817213142167888]
	TIME [epoch: 25.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414253456375971		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.08414253456375971 | validation: 0.12015470306921339]
	TIME [epoch: 25.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869254276553067		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.0869254276553067 | validation: 0.12347973501518568]
	TIME [epoch: 25.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08264263858413588		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.08264263858413588 | validation: 0.11732715780808488]
	TIME [epoch: 25.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08601425257203912		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.08601425257203912 | validation: 0.12250895971648995]
	TIME [epoch: 25.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806278847136068		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.0806278847136068 | validation: 0.13161573844593602]
	TIME [epoch: 25.8 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414569930771865		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.08414569930771865 | validation: 0.11706856224193576]
	TIME [epoch: 25.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08443285250350406		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.08443285250350406 | validation: 0.11042727068351361]
	TIME [epoch: 25.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0856386861656966		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.0856386861656966 | validation: 0.1219349454480003]
	TIME [epoch: 25.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08655915243142602		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.08655915243142602 | validation: 0.12859166560989718]
	TIME [epoch: 25.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663235796629844		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.08663235796629844 | validation: 0.11696696668223044]
	TIME [epoch: 25.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976638501987907		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.07976638501987907 | validation: 0.10673769923784718]
	TIME [epoch: 25.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08226015208775397		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.08226015208775397 | validation: 0.11884774543306192]
	TIME [epoch: 25.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734543162333744		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.08734543162333744 | validation: 0.10358603091338403]
	TIME [epoch: 25.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429596642034888		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.08429596642034888 | validation: 0.12775442316640181]
	TIME [epoch: 25.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827112713544225		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.0827112713544225 | validation: 0.11788023343720982]
	TIME [epoch: 25.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07898443217584673		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.07898443217584673 | validation: 0.1221270529192732]
	TIME [epoch: 25.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961640451025291		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.07961640451025291 | validation: 0.10865001489791837]
	TIME [epoch: 25.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928711506086075		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.07928711506086075 | validation: 0.1098905806300805]
	TIME [epoch: 25.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545346520774161		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.08545346520774161 | validation: 0.125527928260728]
	TIME [epoch: 25.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862507260144124		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0862507260144124 | validation: 0.13133082872660715]
	TIME [epoch: 25.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833486744271256		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.08833486744271256 | validation: 0.115150380614719]
	TIME [epoch: 25.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546656602889784		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.08546656602889784 | validation: 0.12004868341242068]
	TIME [epoch: 25.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130823031639099		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.08130823031639099 | validation: 0.11212178734724386]
	TIME [epoch: 25.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08690911716615435		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.08690911716615435 | validation: 0.11278642403781408]
	TIME [epoch: 25.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08217783946224035		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.08217783946224035 | validation: 0.11399058701407319]
	TIME [epoch: 25.8 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07978309530946272		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.07978309530946272 | validation: 0.1172534830707491]
	TIME [epoch: 25.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092411822975511		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.08092411822975511 | validation: 0.11761826908278186]
	TIME [epoch: 25.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08217733913551048		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.08217733913551048 | validation: 0.11991736126001853]
	TIME [epoch: 25.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08320544890412471		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.08320544890412471 | validation: 0.12236159901085966]
	TIME [epoch: 25.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784642892933127		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.0784642892933127 | validation: 0.11106691001222356]
	TIME [epoch: 25.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08161467652815162		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.08161467652815162 | validation: 0.12710593777531543]
	TIME [epoch: 25.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810350595130231		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.0810350595130231 | validation: 0.11757810977068246]
	TIME [epoch: 25.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08185902876412098		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.08185902876412098 | validation: 0.1314891190011113]
	TIME [epoch: 25.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878597381079234		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.0878597381079234 | validation: 0.11782000485410055]
	TIME [epoch: 25.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08611742891073845		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.08611742891073845 | validation: 0.12853508687888657]
	TIME [epoch: 25.8 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377914511741832		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.08377914511741832 | validation: 0.11663786453598497]
	TIME [epoch: 25.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245087193585125		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.08245087193585125 | validation: 0.11713287784316134]
	TIME [epoch: 25.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08274828637708943		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.08274828637708943 | validation: 0.11849894261575789]
	TIME [epoch: 25.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791736753301466		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0791736753301466 | validation: 0.12812243119808214]
	TIME [epoch: 25.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940757928027219		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.07940757928027219 | validation: 0.11255662320106757]
	TIME [epoch: 25.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08069340509014708		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.08069340509014708 | validation: 0.10186398876009611]
	TIME [epoch: 25.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067018590289157		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.08067018590289157 | validation: 0.11352659750508656]
	TIME [epoch: 25.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063289948528692		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.08063289948528692 | validation: 0.11553702621277516]
	TIME [epoch: 25.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08183657641772463		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.08183657641772463 | validation: 0.11897202706158408]
	TIME [epoch: 25.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435783345306085		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.08435783345306085 | validation: 0.1338237037530896]
	TIME [epoch: 25.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269311919915588		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.08269311919915588 | validation: 0.1155674229220126]
	TIME [epoch: 25.8 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08199040783034604		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.08199040783034604 | validation: 0.12019857881365373]
	TIME [epoch: 25.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07948132498950777		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.07948132498950777 | validation: 0.1132673107368174]
	TIME [epoch: 25.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08342094255041134		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.08342094255041134 | validation: 0.12719845316971953]
	TIME [epoch: 25.8 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08552611521295593		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.08552611521295593 | validation: 0.11383943482876072]
	TIME [epoch: 25.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795831667261867		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.0795831667261867 | validation: 0.11911112869474452]
	TIME [epoch: 25.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125548543687931		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.08125548543687931 | validation: 0.1287374785500077]
	TIME [epoch: 25.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07861090491127716		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.07861090491127716 | validation: 0.12080665574708083]
	TIME [epoch: 25.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314359853465046		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.08314359853465046 | validation: 0.11593411425636191]
	TIME [epoch: 25.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07971587695391723		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.07971587695391723 | validation: 0.12215925146108653]
	TIME [epoch: 25.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08343743407549237		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.08343743407549237 | validation: 0.10718800620192151]
	TIME [epoch: 25.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08275815503514922		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.08275815503514922 | validation: 0.1134034825140293]
	TIME [epoch: 25.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825145706934285		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.0825145706934285 | validation: 0.11266168063626546]
	TIME [epoch: 25.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403577916563502		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.08403577916563502 | validation: 0.1140348926148177]
	TIME [epoch: 25.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08248801852862592		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.08248801852862592 | validation: 0.12476312655291243]
	TIME [epoch: 25.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08224436177612844		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.08224436177612844 | validation: 0.11960187714211488]
	TIME [epoch: 25.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08061315669289927		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.08061315669289927 | validation: 0.11497928904617405]
	TIME [epoch: 25.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08228513550273552		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.08228513550273552 | validation: 0.13087535042109064]
	TIME [epoch: 25.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09134900209644417		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.09134900209644417 | validation: 0.11136308824053741]
	TIME [epoch: 25.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08246685271948319		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.08246685271948319 | validation: 0.11789516199241393]
	TIME [epoch: 25.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08204060235353104		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.08204060235353104 | validation: 0.1231018639910591]
	TIME [epoch: 25.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08685045891811095		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.08685045891811095 | validation: 0.10776902048128241]
	TIME [epoch: 25.8 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08150810375115854		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.08150810375115854 | validation: 0.11395586138871941]
	TIME [epoch: 25.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995486071855862		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.07995486071855862 | validation: 0.12544726903977366]
	TIME [epoch: 25.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114060997621596		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.08114060997621596 | validation: 0.117487358961036]
	TIME [epoch: 25.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08508432353769392		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.08508432353769392 | validation: 0.11203093839722969]
	TIME [epoch: 25.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784820315747889		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.0784820315747889 | validation: 0.10816136222115756]
	TIME [epoch: 25.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07832961807211668		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.07832961807211668 | validation: 0.1114833641076517]
	TIME [epoch: 25.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288795868154855		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.08288795868154855 | validation: 0.137796310758969]
	TIME [epoch: 25.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08304667313819632		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.08304667313819632 | validation: 0.1259019643883468]
	TIME [epoch: 25.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08032572886707005		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.08032572886707005 | validation: 0.11252502267839937]
	TIME [epoch: 25.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07960429010637785		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.07960429010637785 | validation: 0.10876633834930445]
	TIME [epoch: 25.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795561887243466		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.0795561887243466 | validation: 0.1042242246297537]
	TIME [epoch: 25.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08317135292562611		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.08317135292562611 | validation: 0.12951068804074747]
	TIME [epoch: 25.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08495970327631117		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.08495970327631117 | validation: 0.11655790018246266]
	TIME [epoch: 25.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252872961009906		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.08252872961009906 | validation: 0.1260008542980757]
	TIME [epoch: 25.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07915327053151779		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.07915327053151779 | validation: 0.12881690887509745]
	TIME [epoch: 25.8 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08042807876720755		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.08042807876720755 | validation: 0.12621209763789978]
	TIME [epoch: 25.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429752597262255		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.08429752597262255 | validation: 0.11502446630223824]
	TIME [epoch: 25.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186585509162914		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.08186585509162914 | validation: 0.11189551799156677]
	TIME [epoch: 25.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808931958109082		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.0808931958109082 | validation: 0.1306733808675293]
	TIME [epoch: 25.8 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639704764422323		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.08639704764422323 | validation: 0.128791273984024]
	TIME [epoch: 25.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323472308805059		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.08323472308805059 | validation: 0.1185488778261819]
	TIME [epoch: 25.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467068572451862		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.08467068572451862 | validation: 0.12226297987801221]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_164507/states/model_phi1_3c_v_mmd1_1140.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 13334.204 seconds.
