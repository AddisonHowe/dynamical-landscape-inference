Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3040448714

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.101761005768337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.101761005768337 | validation: 3.869479723783944]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.234181612124234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.234181612124234 | validation: 3.9088775944354808]
	TIME [epoch: 0.808 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9205994606565184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9205994606565184 | validation: 2.2552842528608847]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7192631714190116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7192631714190116 | validation: 2.66563763603062]
	TIME [epoch: 0.712 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7734074459341036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7734074459341036 | validation: 1.9896022616572708]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3276259433797555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3276259433797555 | validation: 1.9230475309558526]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1464794417675366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1464794417675366 | validation: 2.6150142786726107]
	TIME [epoch: 0.712 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.189745620225545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.189745620225545 | validation: 1.8714739430765261]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9992704489978599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9992704489978599 | validation: 1.8586592097394457]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7407418532181254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7407418532181254 | validation: 1.6306042327140469]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6337863737891527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6337863737891527 | validation: 1.5636467101376565]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5701362362778024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5701362362778024 | validation: 1.49871697695027]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.518813079207341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.518813079207341 | validation: 1.4645637925551758]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4792730517997195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4792730517997195 | validation: 1.4552807216111476]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4492368599197096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4492368599197096 | validation: 1.3999463839375133]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4244902533896584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4244902533896584 | validation: 1.3948322854835569]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4010882234257058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4010882234257058 | validation: 1.3993140112132103]
	TIME [epoch: 0.712 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4450035674226485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4450035674226485 | validation: 2.128591210846547]
	TIME [epoch: 0.714 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9134469513302705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9134469513302705 | validation: 1.5237954780504523]
	TIME [epoch: 0.711 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.638007690333663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.638007690333663 | validation: 1.220764481764553]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3597697415152366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3597697415152366 | validation: 1.3669316549952422]
	TIME [epoch: 0.714 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.381897192997231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.381897192997231 | validation: 1.3375013677872265]
	TIME [epoch: 0.715 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335770745731276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.335770745731276 | validation: 1.138312797721467]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2742629793674392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2742629793674392 | validation: 1.1628795759889392]
	TIME [epoch: 0.715 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2527941935201876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2527941935201876 | validation: 1.1487370939883346]
	TIME [epoch: 0.715 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2385447760998052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2385447760998052 | validation: 1.0717085684094603]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2250593075461227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2250593075461227 | validation: 1.1123805745682895]
	TIME [epoch: 0.717 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2113947878871434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2113947878871434 | validation: 1.0280446547181807]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205062517372947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.205062517372947 | validation: 1.1053274169870464]
	TIME [epoch: 0.712 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2070264179761143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2070264179761143 | validation: 1.001399565593782]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2193263254986173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2193263254986173 | validation: 1.094010560245527]
	TIME [epoch: 0.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2065866370142908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2065866370142908 | validation: 0.9888641686790539]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1846895957377876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1846895957377876 | validation: 0.9578600767093569]
	TIME [epoch: 0.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141633163771666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.141633163771666 | validation: 0.9625531312240888]
	TIME [epoch: 0.714 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1229874228276824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1229874228276824 | validation: 0.8686115537793555]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0974670591143123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0974670591143123 | validation: 0.9200996827846638]
	TIME [epoch: 0.716 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0937920614680023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0937920614680023 | validation: 0.8260999711193815]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0829390300007042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0829390300007042 | validation: 0.8735402250216779]
	TIME [epoch: 0.715 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0827548483150167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0827548483150167 | validation: 0.9524319278994811]
	TIME [epoch: 0.715 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200126018325494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1200126018325494 | validation: 0.7538801898232066]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.252296250288463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.252296250288463 | validation: 1.2116558292171231]
	TIME [epoch: 0.714 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1085366166701216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1085366166701216 | validation: 0.8843565570696832]
	TIME [epoch: 0.715 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0435147398257776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0435147398257776 | validation: 0.7425771779908188]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0463565529688632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0463565529688632 | validation: 0.783474966174274]
	TIME [epoch: 0.714 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0297562603233075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0297562603233075 | validation: 0.8517023882255806]
	TIME [epoch: 0.712 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0166475455789092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0166475455789092 | validation: 0.7394180940796519]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013465296918873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.013465296918873 | validation: 0.7797458431995417]
	TIME [epoch: 0.713 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0007718836080495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0007718836080495 | validation: 0.7737221739665042]
	TIME [epoch: 0.712 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.990242827914638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.990242827914638 | validation: 0.7386454227037564]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0117962140699943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0117962140699943 | validation: 0.8698705415948746]
	TIME [epoch: 0.712 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0282051531480751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0282051531480751 | validation: 0.7444961234994243]
	TIME [epoch: 0.713 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030888145872753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030888145872753 | validation: 0.7378003683034623]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9677252091050129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9677252091050129 | validation: 0.833080601009506]
	TIME [epoch: 0.715 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9550218824685528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9550218824685528 | validation: 0.6460189269462374]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9346841379720491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9346841379720491 | validation: 0.8716724234462601]
	TIME [epoch: 0.712 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9432446902747518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9432446902747518 | validation: 0.647883616126273]
	TIME [epoch: 0.71 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9276362866095744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9276362866095744 | validation: 0.8650483680949833]
	TIME [epoch: 0.721 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9350870564577127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9350870564577127 | validation: 0.6637328567982553]
	TIME [epoch: 0.711 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9214946823844306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9214946823844306 | validation: 0.7689052068834277]
	TIME [epoch: 0.711 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600179350573166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9600179350573166 | validation: 0.9564150586155198]
	TIME [epoch: 0.713 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029939007003119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029939007003119 | validation: 0.6503752777872643]
	TIME [epoch: 0.711 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987066598518645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9987066598518645 | validation: 0.7994412903598145]
	TIME [epoch: 0.71 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769744234657546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769744234657546 | validation: 0.806093711071107]
	TIME [epoch: 0.709 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937766684551839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937766684551839 | validation: 0.7924801541066342]
	TIME [epoch: 0.711 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242848663707806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9242848663707806 | validation: 0.7454042558106514]
	TIME [epoch: 0.711 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8932268987885341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8932268987885341 | validation: 0.9675359524166396]
	TIME [epoch: 0.71 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9254855839266458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9254855839266458 | validation: 0.5865940676260383]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9336711658221234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9336711658221234 | validation: 0.9737363914402613]
	TIME [epoch: 0.712 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9185359288047243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9185359288047243 | validation: 0.6042481037665618]
	TIME [epoch: 0.711 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9162948877758897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9162948877758897 | validation: 0.7464525754292652]
	TIME [epoch: 0.711 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851395197489567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851395197489567 | validation: 0.8571134396959667]
	TIME [epoch: 0.712 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886392275519456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886392275519456 | validation: 0.7036697146074458]
	TIME [epoch: 0.714 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384549223882811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8384549223882811 | validation: 0.6602138068706094]
	TIME [epoch: 0.712 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264568633672229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264568633672229 | validation: 0.7371607883868562]
	TIME [epoch: 0.712 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261070008372678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8261070008372678 | validation: 0.6782291860755114]
	TIME [epoch: 0.712 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8213807959578929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8213807959578929 | validation: 0.7225471554779466]
	TIME [epoch: 0.711 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199559974366938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199559974366938 | validation: 0.7200399960663667]
	TIME [epoch: 0.71 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148785103093354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8148785103093354 | validation: 0.6887310399757691]
	TIME [epoch: 0.71 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095209041893004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9095209041893004 | validation: 1.1647310205562493]
	TIME [epoch: 0.711 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195747846423425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.195747846423425 | validation: 0.6315312820555241]
	TIME [epoch: 0.711 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9331836730245283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9331836730245283 | validation: 0.7279805574612891]
	TIME [epoch: 0.709 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318535855523738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318535855523738 | validation: 0.8352031393220841]
	TIME [epoch: 0.709 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8970733660906526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8970733660906526 | validation: 0.8331540775959967]
	TIME [epoch: 0.712 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620037221200737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8620037221200737 | validation: 0.6331445273202981]
	TIME [epoch: 0.713 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449462276307679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449462276307679 | validation: 0.924964546788806]
	TIME [epoch: 0.712 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808241908041978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808241908041978 | validation: 0.5795936501426822]
	TIME [epoch: 0.774 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517625075557578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8517625075557578 | validation: 0.7529627420362567]
	TIME [epoch: 0.714 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040620403462089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8040620403462089 | validation: 0.7180406860092089]
	TIME [epoch: 0.71 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790208459474604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790208459474604 | validation: 0.6728391814376637]
	TIME [epoch: 0.711 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984091899852961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7984091899852961 | validation: 0.7483297294616591]
	TIME [epoch: 0.713 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013684406952186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8013684406952186 | validation: 0.636223975057865]
	TIME [epoch: 0.712 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320118847295622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320118847295622 | validation: 0.8348369114460472]
	TIME [epoch: 0.71 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693452100753706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8693452100753706 | validation: 0.6754983265957025]
	TIME [epoch: 0.711 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9158684808004175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9158684808004175 | validation: 0.7657455315510577]
	TIME [epoch: 0.71 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080361104062435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080361104062435 | validation: 0.8122148891927478]
	TIME [epoch: 0.709 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098086943048546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098086943048546 | validation: 0.6382695681668324]
	TIME [epoch: 0.709 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992693238317077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992693238317077 | validation: 1.0850378980591877]
	TIME [epoch: 0.713 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9208696930357678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9208696930357678 | validation: 0.6493454855810863]
	TIME [epoch: 0.712 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779239959605738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7779239959605738 | validation: 0.6440710992567094]
	TIME [epoch: 0.711 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036731663091473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036731663091473 | validation: 0.813399641490221]
	TIME [epoch: 0.71 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221447142803002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221447142803002 | validation: 0.7277139180061559]
	TIME [epoch: 0.717 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119813342157964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119813342157964 | validation: 0.6422624878282358]
	TIME [epoch: 0.712 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554566894360004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8554566894360004 | validation: 0.8120602757395416]
	TIME [epoch: 0.713 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8570069755480582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8570069755480582 | validation: 0.6066169667157765]
	TIME [epoch: 0.71 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491584680101929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491584680101929 | validation: 0.7651326890917192]
	TIME [epoch: 0.713 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787204697677644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787204697677644 | validation: 0.6226952859474437]
	TIME [epoch: 0.712 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612376651538202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612376651538202 | validation: 0.7158797014347603]
	TIME [epoch: 0.711 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656566929405121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656566929405121 | validation: 0.6751148020068428]
	TIME [epoch: 0.711 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614568268342182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614568268342182 | validation: 0.7275647658275641]
	TIME [epoch: 0.712 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943387543289037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7943387543289037 | validation: 0.7566253221662391]
	TIME [epoch: 0.71 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8627533022507572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8627533022507572 | validation: 0.8975774496209354]
	TIME [epoch: 0.709 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9548999709150181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9548999709150181 | validation: 0.7274429936637107]
	TIME [epoch: 0.707 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850047874950505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7850047874950505 | validation: 0.5632883559159483]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7682385950676672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7682385950676672 | validation: 0.8199442348848696]
	TIME [epoch: 0.717 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903592802191611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903592802191611 | validation: 0.5522756646030748]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220521763202152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8220521763202152 | validation: 0.8383028567600932]
	TIME [epoch: 0.715 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812771799275923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812771799275923 | validation: 0.5936363084432424]
	TIME [epoch: 0.714 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834887702656971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834887702656971 | validation: 0.7188498366694356]
	TIME [epoch: 0.714 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792704550524588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.792704550524588 | validation: 0.7571811415557544]
	TIME [epoch: 0.713 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121926637580231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121926637580231 | validation: 0.6883671808276541]
	TIME [epoch: 0.715 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835438806985904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7835438806985904 | validation: 0.6946167625753316]
	TIME [epoch: 0.716 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717812801674997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7717812801674997 | validation: 0.6743378525814627]
	TIME [epoch: 0.715 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506238559333528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7506238559333528 | validation: 0.6033963456330167]
	TIME [epoch: 0.715 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689945042298374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7689945042298374 | validation: 0.7851456856119792]
	TIME [epoch: 0.715 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78953620061061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.78953620061061 | validation: 0.543392881452521]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428625580878856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8428625580878856 | validation: 0.7575514924682868]
	TIME [epoch: 0.714 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890718642491025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7890718642491025 | validation: 0.6023278078853832]
	TIME [epoch: 0.716 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465845236925727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7465845236925727 | validation: 0.6610148901183067]
	TIME [epoch: 0.713 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179335311610348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179335311610348 | validation: 0.6235612077057989]
	TIME [epoch: 0.713 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717403633026851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717403633026851 | validation: 0.677475322322062]
	TIME [epoch: 0.714 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393921319353922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393921319353922 | validation: 0.8795381891473005]
	TIME [epoch: 0.713 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608495435830767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608495435830767 | validation: 0.7532921307692471]
	TIME [epoch: 0.716 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956165121081417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956165121081417 | validation: 0.7509760818021997]
	TIME [epoch: 0.714 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002211468857863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002211468857863 | validation: 0.6737071359348725]
	TIME [epoch: 0.714 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717383814795173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717383814795173 | validation: 0.5575806801972382]
	TIME [epoch: 0.714 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426449409941336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426449409941336 | validation: 0.8004281504218828]
	TIME [epoch: 0.719 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580409427942181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580409427942181 | validation: 0.5378702557761706]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525722760262588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525722760262588 | validation: 0.7099981983097845]
	TIME [epoch: 0.716 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721448959540703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721448959540703 | validation: 0.6775440470369364]
	TIME [epoch: 0.714 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169588857936969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169588857936969 | validation: 0.7095937045736553]
	TIME [epoch: 0.714 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854539573430274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854539573430274 | validation: 0.6070506395711224]
	TIME [epoch: 0.713 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229590495883284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229590495883284 | validation: 0.6065253448494551]
	TIME [epoch: 0.717 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091090599384725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7091090599384725 | validation: 0.6242833546716415]
	TIME [epoch: 0.716 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988482965106926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6988482965106926 | validation: 0.6020896481181911]
	TIME [epoch: 0.716 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982270353521226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982270353521226 | validation: 0.6592834891844455]
	TIME [epoch: 0.714 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6995446891404273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6995446891404273 | validation: 0.657559809267989]
	TIME [epoch: 0.715 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75782461557711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75782461557711 | validation: 0.8230234483757782]
	TIME [epoch: 0.716 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573593915142956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573593915142956 | validation: 0.7587242864561557]
	TIME [epoch: 0.715 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208352470704672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8208352470704672 | validation: 0.5046189462965484]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637142438173079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637142438173079 | validation: 0.7870658217830013]
	TIME [epoch: 0.717 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520715330275282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520715330275282 | validation: 0.5560070937423529]
	TIME [epoch: 0.714 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739234053194601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739234053194601 | validation: 0.7054530457673864]
	TIME [epoch: 0.713 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046470324545674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7046470324545674 | validation: 0.616477767617974]
	TIME [epoch: 0.713 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790727127778106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790727127778106 | validation: 0.5823952425957432]
	TIME [epoch: 0.715 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960375618426966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960375618426966 | validation: 0.6508079826662605]
	TIME [epoch: 0.714 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025117065486879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7025117065486879 | validation: 0.5753049156589922]
	TIME [epoch: 0.715 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268122745595299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268122745595299 | validation: 0.7152925928296104]
	TIME [epoch: 0.715 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570654821460865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570654821460865 | validation: 0.6462180333096706]
	TIME [epoch: 0.716 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472784515232814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472784515232814 | validation: 0.6165455887080844]
	TIME [epoch: 0.715 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697037641642896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.697037641642896 | validation: 0.6412157046285869]
	TIME [epoch: 0.714 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836895552093923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836895552093923 | validation: 0.54290236622192]
	TIME [epoch: 0.716 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799113823427905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6799113823427905 | validation: 0.7353104170479293]
	TIME [epoch: 0.714 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035541000814675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7035541000814675 | validation: 0.5470780797427377]
	TIME [epoch: 0.714 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993654052641526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993654052641526 | validation: 0.7187776040502327]
	TIME [epoch: 0.715 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091975030249004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7091975030249004 | validation: 0.5425076856260026]
	TIME [epoch: 0.717 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6719470903209207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6719470903209207 | validation: 0.6287597284543376]
	TIME [epoch: 0.718 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769329062287613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6769329062287613 | validation: 0.6321909856104109]
	TIME [epoch: 0.715 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922012239646453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922012239646453 | validation: 0.5101130646978816]
	TIME [epoch: 0.716 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892265544994069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7892265544994069 | validation: 0.857627274428971]
	TIME [epoch: 0.718 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276966234241163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276966234241163 | validation: 0.5063300552588913]
	TIME [epoch: 0.715 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917642973841891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917642973841891 | validation: 0.4948142690851521]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472419688397048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472419688397048 | validation: 0.6564630758797128]
	TIME [epoch: 0.719 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585576845755261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585576845755261 | validation: 0.5153744095676988]
	TIME [epoch: 0.714 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652534515901569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652534515901569 | validation: 0.5834719067184513]
	TIME [epoch: 0.715 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388769802333644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388769802333644 | validation: 0.5309818682001545]
	TIME [epoch: 0.715 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.628022352116491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.628022352116491 | validation: 0.5563164236363978]
	TIME [epoch: 0.718 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626129926440237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626129926440237 | validation: 0.6213873595982748]
	TIME [epoch: 0.717 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657577807064129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657577807064129 | validation: 0.6214457601993479]
	TIME [epoch: 0.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449050224381958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449050224381958 | validation: 0.871563835450144]
	TIME [epoch: 0.717 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827816148391469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.827816148391469 | validation: 0.6240185643432272]
	TIME [epoch: 0.716 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834302286101758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6834302286101758 | validation: 0.4305210164900702]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398227333327914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6398227333327914 | validation: 0.6903421842813443]
	TIME [epoch: 0.715 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499432916252598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6499432916252598 | validation: 0.5068424971344635]
	TIME [epoch: 0.714 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154642619004317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6154642619004317 | validation: 0.4548435999821493]
	TIME [epoch: 0.714 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156419650523881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6156419650523881 | validation: 0.6238857810219466]
	TIME [epoch: 0.715 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160323987136791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6160323987136791 | validation: 0.44869175803822475]
	TIME [epoch: 0.717 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230790319790753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6230790319790753 | validation: 0.6294660979691853]
	TIME [epoch: 0.712 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517943097674436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6517943097674436 | validation: 0.5484757845228596]
	TIME [epoch: 0.712 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008385761257483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7008385761257483 | validation: 0.6369282844006698]
	TIME [epoch: 0.715 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038099585449743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7038099585449743 | validation: 0.6183194292308024]
	TIME [epoch: 0.715 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650154144658416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650154144658416 | validation: 0.4431484487198349]
	TIME [epoch: 0.713 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189368786888708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6189368786888708 | validation: 0.611287206378154]
	TIME [epoch: 0.714 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182116496661135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6182116496661135 | validation: 0.4792816291355784]
	TIME [epoch: 0.716 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233872134930508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233872134930508 | validation: 0.4808105635217712]
	TIME [epoch: 0.715 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877616910444182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5877616910444182 | validation: 0.6256865312293451]
	TIME [epoch: 0.714 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100639536963575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6100639536963575 | validation: 0.44864227509109755]
	TIME [epoch: 0.715 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550761332716805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550761332716805 | validation: 0.719768716268911]
	TIME [epoch: 0.714 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720004502488343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720004502488343 | validation: 0.5436324088415433]
	TIME [epoch: 0.714 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092652002207385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092652002207385 | validation: 0.462015612461952]
	TIME [epoch: 0.713 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61893661088572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61893661088572 | validation: 0.5502791553483155]
	TIME [epoch: 0.714 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530210641976981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5530210641976981 | validation: 0.4859016008572441]
	TIME [epoch: 183 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5785636875450816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5785636875450816 | validation: 0.6528379783908708]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631929836373905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6631929836373905 | validation: 0.38219473239235063]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7189784720920569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7189784720920569 | validation: 0.7828208977826299]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835677804220308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835677804220308 | validation: 0.5341344157330138]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5706726676683287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5706726676683287 | validation: 0.4043811670467541]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592403520077023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592403520077023 | validation: 0.5657488448587621]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508461275019897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508461275019897 | validation: 0.5426876054125719]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5392687689317641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5392687689317641 | validation: 0.44095588216512116]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5780503105980406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5780503105980406 | validation: 0.7120984133166424]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647495661572862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647495661572862 | validation: 0.6602803721414695]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081555082694522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7081555082694522 | validation: 0.4376213050745644]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275719506889201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6275719506889201 | validation: 0.4976538535089935]
	TIME [epoch: 1.39 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469736720808637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5469736720808637 | validation: 0.6289983620682387]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5776402294439977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5776402294439977 | validation: 0.4409794481226693]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5457306655234537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5457306655234537 | validation: 0.6082405634277108]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582921993307491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5582921993307491 | validation: 0.472575360172969]
	TIME [epoch: 1.39 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048878318138874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048878318138874 | validation: 0.5198682766516322]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5647512364958671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5647512364958671 | validation: 0.5301424427905065]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4970654073159871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4970654073159871 | validation: 0.4584897393925358]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305033364252393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5305033364252393 | validation: 0.652093937937193]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542698646964286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542698646964286 | validation: 0.3588449533080382]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.542633682069302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.542633682069302 | validation: 0.7261580072682908]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5773856071299013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5773856071299013 | validation: 0.38589638434317886]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5627953148071451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5627953148071451 | validation: 0.6084792328121389]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037211837848742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6037211837848742 | validation: 0.7098966770589592]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709995794007477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.709995794007477 | validation: 0.531755742317333]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5586651330908036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5586651330908036 | validation: 0.5816919328081129]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178668878524488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6178668878524488 | validation: 0.45416465792349375]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5125261227003727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5125261227003727 | validation: 0.6212718655075907]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5002608964281672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5002608964281672 | validation: 0.41816870096475994]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4681381968944893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4681381968944893 | validation: 0.4804483059920661]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45448870596755336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45448870596755336 | validation: 0.506657104661796]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47205283323133346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47205283323133346 | validation: 0.5504138228990784]
	TIME [epoch: 1.39 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925152792522649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6925152792522649 | validation: 0.5433960119115152]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187254201100108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5187254201100108 | validation: 0.6457490877120193]
	TIME [epoch: 1.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143997750231771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143997750231771 | validation: 0.33805732784669484]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131215799099127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7131215799099127 | validation: 0.8153342397450123]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345758479128093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7345758479128093 | validation: 0.5715666414943913]
	TIME [epoch: 1.39 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5169317106226088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5169317106226088 | validation: 0.4629827357439929]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.497443295945109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.497443295945109 | validation: 0.5570595250373246]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46305379819102255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46305379819102255 | validation: 0.4459741347613281]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44145287534247746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44145287534247746 | validation: 0.4830279164200782]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40630589433494363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40630589433494363 | validation: 0.3878926973458561]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39832169661386957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39832169661386957 | validation: 0.6347492539583587]
	TIME [epoch: 1.39 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4734695239010849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4734695239010849 | validation: 0.3630524547873969]
	TIME [epoch: 1.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192019462430347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192019462430347 | validation: 0.4605462831084863]
	TIME [epoch: 1.39 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5196145550824912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5196145550824912 | validation: 0.811990146098156]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884255891194816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7884255891194816 | validation: 0.5094025163178516]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5013656365819648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5013656365819648 | validation: 0.3592420388262055]
	TIME [epoch: 1.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5521068023286175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5521068023286175 | validation: 0.6512824644276627]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5585308226893041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5585308226893041 | validation: 0.49010748316073693]
	TIME [epoch: 1.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4448089472845756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4448089472845756 | validation: 0.435663173225357]
	TIME [epoch: 1.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741599303614942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4741599303614942 | validation: 0.49485234892461477]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4011101546662361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4011101546662361 | validation: 0.43004058139674095]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3603687048281857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3603687048281857 | validation: 0.38523826296316166]
	TIME [epoch: 1.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3543806906682094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3543806906682094 | validation: 0.5644498245432102]
	TIME [epoch: 1.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4110454749783095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4110454749783095 | validation: 0.30269832501648797]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664536979174466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5664536979174466 | validation: 0.5339877822786211]
	TIME [epoch: 1.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40595783266589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40595783266589 | validation: 0.5632797963906226]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6941368054310314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6941368054310314 | validation: 0.40185854302344826]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59064646938102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.59064646938102 | validation: 0.5188768740284705]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.424341722775366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.424341722775366 | validation: 0.6169703462912648]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862900314194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6862900314194 | validation: 0.3213673262196527]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4730366442978226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4730366442978226 | validation: 0.7638093993533305]
	TIME [epoch: 1.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235374650325239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7235374650325239 | validation: 0.5787123394303629]
	TIME [epoch: 1.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4624165492286973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4624165492286973 | validation: 0.35910289249669525]
	TIME [epoch: 1.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42134472356456926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42134472356456926 | validation: 0.4494988535182982]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3610133509646983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3610133509646983 | validation: 0.5665834745241175]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768549955407377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4768549955407377 | validation: 0.4558368012843623]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231360576746951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5231360576746951 | validation: 0.4109709953377822]
	TIME [epoch: 1.39 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38565652692407043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38565652692407043 | validation: 0.5419179562753236]
	TIME [epoch: 1.39 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44897685904809354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44897685904809354 | validation: 0.5048975966691017]
	TIME [epoch: 1.39 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4744335007304508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4744335007304508 | validation: 0.3532270047473855]
	TIME [epoch: 1.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39315325631763276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39315325631763276 | validation: 0.639532192735701]
	TIME [epoch: 1.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4776740934513653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4776740934513653 | validation: 0.38435146009277005]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3588829791874811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3588829791874811 | validation: 0.4674141390021027]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4031004996747596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4031004996747596 | validation: 0.4495980217263725]
	TIME [epoch: 1.39 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.370665106020363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.370665106020363 | validation: 0.43885700174419]
	TIME [epoch: 1.39 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4453551150998179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4453551150998179 | validation: 0.4272981481920264]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3279800926292989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3279800926292989 | validation: 0.3886767409438073]
	TIME [epoch: 1.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3511330361453656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3511330361453656 | validation: 0.4847837410626585]
	TIME [epoch: 1.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701694359347914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3701694359347914 | validation: 0.4495572046201341]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574160116747628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574160116747628 | validation: 0.4618016163203234]
	TIME [epoch: 1.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35463780919807975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35463780919807975 | validation: 0.46885024699502864]
	TIME [epoch: 1.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969581355089218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5969581355089218 | validation: 0.4066613855180696]
	TIME [epoch: 1.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3563977386055433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3563977386055433 | validation: 0.7615111737704017]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7139972265201984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7139972265201984 | validation: 0.3936782425680901]
	TIME [epoch: 1.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5234698060259803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5234698060259803 | validation: 0.6037819176064054]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44366193374919527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44366193374919527 | validation: 0.49367401246457876]
	TIME [epoch: 1.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4487444031026193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4487444031026193 | validation: 0.4333907652712882]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183395991670132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183395991670132 | validation: 0.4047347456697667]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34675196547553727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34675196547553727 | validation: 0.43810489131329977]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3532437029219909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3532437029219909 | validation: 0.37128648334279984]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032713133364661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032713133364661 | validation: 0.44945722595469245]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30979359336809426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30979359336809426 | validation: 0.3122217812419983]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4084779205805106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4084779205805106 | validation: 0.7419402048359054]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499627266815996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5499627266815996 | validation: 0.35420313108791746]
	TIME [epoch: 1.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808496655415846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2808496655415846 | validation: 0.3099856552396454]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162716815249273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3162716815249273 | validation: 0.5614387877159542]
	TIME [epoch: 1.39 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4439232577990025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4439232577990025 | validation: 0.42793103427863644]
	TIME [epoch: 1.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286925756866682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4286925756866682 | validation: 0.3641845218975225]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417671744845982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5417671744845982 | validation: 0.6179959110161661]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40631849956099386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40631849956099386 | validation: 0.3929007957391329]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27412428267159367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27412428267159367 | validation: 0.31629322533712506]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3734775633957211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3734775633957211 | validation: 0.5724911672004775]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40229910343823777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40229910343823777 | validation: 0.46629588927423227]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45496745385319887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45496745385319887 | validation: 0.28407887123022263]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36264262616600124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36264262616600124 | validation: 0.5689638990387849]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3779593089689686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3779593089689686 | validation: 0.376341684497694]
	TIME [epoch: 1.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2663705045553321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2663705045553321 | validation: 0.29853481956371425]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36612485941370165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36612485941370165 | validation: 0.5056069793428121]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4113892052739371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4113892052739371 | validation: 0.42789051737000483]
	TIME [epoch: 1.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37661213765474566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37661213765474566 | validation: 0.31962257282336043]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4052048583160267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4052048583160267 | validation: 0.45543466476284583]
	TIME [epoch: 1.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29678829510185445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29678829510185445 | validation: 0.33655587524286434]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376206031821545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2376206031821545 | validation: 0.2984062491980078]
	TIME [epoch: 1.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542609871405978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2542609871405978 | validation: 0.48933628619714775]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36008745134619885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36008745134619885 | validation: 0.5135423061352213]
	TIME [epoch: 1.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582664930198431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.582664930198431 | validation: 0.28815787560071165]
	TIME [epoch: 1.39 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31002197094549516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31002197094549516 | validation: 0.7081613519008023]
	TIME [epoch: 1.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.577482294727626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.577482294727626 | validation: 0.42133687718201307]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3571774906417249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3571774906417249 | validation: 0.27062700983269444]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4522622264238122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4522622264238122 | validation: 0.5509536994147209]
	TIME [epoch: 1.39 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36934534156907445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36934534156907445 | validation: 0.42761397888407837]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060796689258679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3060796689258679 | validation: 0.3123463145703984]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30149971551330607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30149971551330607 | validation: 0.33685483344245737]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24194212165020687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24194212165020687 | validation: 0.37317201250498566]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23882465210302695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23882465210302695 | validation: 0.27004243536450684]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21993383747075318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21993383747075318 | validation: 0.34497032687308105]
	TIME [epoch: 1.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20615151785220998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20615151785220998 | validation: 0.2585580167697075]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121312122108857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2121312122108857 | validation: 0.4469156480661265]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032754133997384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032754133997384 | validation: 0.44104691882610825]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359602146986688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359602146986688 | validation: 0.390669995702442]
	TIME [epoch: 1.39 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5570943107491464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5570943107491464 | validation: 0.8634784515147214]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161925692960997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161925692960997 | validation: 0.9292239659863881]
	TIME [epoch: 1.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823838421980869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6823838421980869 | validation: 0.5687076125663032]
	TIME [epoch: 1.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393755436815919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.393755436815919 | validation: 0.30683944841150324]
	TIME [epoch: 1.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878528710608781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2878528710608781 | validation: 0.3894026615776046]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3181410484050846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3181410484050846 | validation: 0.352469059224235]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28389481701234687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28389481701234687 | validation: 0.3909933100959722]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24732871659210823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24732871659210823 | validation: 0.34203383610645566]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24016748675658658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24016748675658658 | validation: 0.32018327325255824]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568753474019191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2568753474019191 | validation: 0.3845780376673421]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36221648153648767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36221648153648767 | validation: 0.29778125973238584]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.236253260586483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.236253260586483 | validation: 0.3350825476279065]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21883651737385115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21883651737385115 | validation: 0.28171238533329035]
	TIME [epoch: 1.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332543916580164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2332543916580164 | validation: 0.34979484756018764]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31134160486472623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31134160486472623 | validation: 0.3239988114577601]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2821621778445435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2821621778445435 | validation: 0.3701139204998678]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39153721259615426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39153721259615426 | validation: 0.36016745463006405]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2137782046211705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2137782046211705 | validation: 0.31645358036751625]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014348781207307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4014348781207307 | validation: 0.6365659322083641]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4076466508477403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4076466508477403 | validation: 0.27582746939162545]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2311007239771629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2311007239771629 | validation: 0.28803863287394754]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28751061983341364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28751061983341364 | validation: 0.3825307491802088]
	TIME [epoch: 1.39 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2839838645951871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2839838645951871 | validation: 0.3167254687639642]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26911675393167084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26911675393167084 | validation: 0.27520556556658765]
	TIME [epoch: 1.39 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27032479126931547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27032479126931547 | validation: 0.3755802043701413]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.238924372319823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.238924372319823 | validation: 0.26359726854621485]
	TIME [epoch: 1.39 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2353964794783687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2353964794783687 | validation: 0.3507133165031042]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27432813340949774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27432813340949774 | validation: 0.31403196852956744]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28684761577516027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28684761577516027 | validation: 0.25034107061698935]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1779859254301127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1779859254301127 | validation: 0.3261109694550771]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19850710020479148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19850710020479148 | validation: 0.23133530384670192]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24414000332500466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24414000332500466 | validation: 0.5833508824699889]
	TIME [epoch: 1.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39424203586158174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39424203586158174 | validation: 0.3270398660231879]
	TIME [epoch: 1.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34065211880557855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34065211880557855 | validation: 0.31986937712561503]
	TIME [epoch: 1.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4237026291060284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4237026291060284 | validation: 0.3915636207364095]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23213579443442334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23213579443442334 | validation: 0.4140233141071315]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41192285037522747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41192285037522747 | validation: 0.22123813658807392]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25361093865433576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25361093865433576 | validation: 0.3836715722646482]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518351959153214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2518351959153214 | validation: 0.3632943166078837]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3168339407303609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3168339407303609 | validation: 0.22583703909394967]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19676905249451104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19676905249451104 | validation: 0.30390162890539196]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18818159715049834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18818159715049834 | validation: 0.2692021495911716]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038510847091372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2038510847091372 | validation: 0.30893925792750676]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22486882629581678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22486882629581678 | validation: 0.26644499719144576]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23949477868703067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23949477868703067 | validation: 0.30045953898625677]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18801632875243765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18801632875243765 | validation: 0.24650880771077777]
	TIME [epoch: 1.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2117160703947771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2117160703947771 | validation: 0.30576977052873505]
	TIME [epoch: 1.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22120050175110836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22120050175110836 | validation: 0.2622378657182516]
	TIME [epoch: 1.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23705791531138082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23705791531138082 | validation: 0.2885801350557395]
	TIME [epoch: 1.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22566843824608407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22566843824608407 | validation: 0.22382200520789333]
	TIME [epoch: 1.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21644137461966162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21644137461966162 | validation: 0.22489592008281578]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17764627423771567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17764627423771567 | validation: 0.37383914352270364]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24210581078842638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24210581078842638 | validation: 0.28311881494312835]
	TIME [epoch: 1.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870744576222067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3870744576222067 | validation: 0.5121599861820925]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3288006198532884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3288006198532884 | validation: 0.22426215016349513]
	TIME [epoch: 1.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22990685704554042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22990685704554042 | validation: 0.2205855027869993]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19974495263710673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19974495263710673 | validation: 0.2804654792620737]
	TIME [epoch: 1.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16549329852380637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16549329852380637 | validation: 0.19262675853634403]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15383758951445706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15383758951445706 | validation: 0.2803761458002606]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814370892749183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18814370892749183 | validation: 0.21558811689975121]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24410385152659778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24410385152659778 | validation: 0.27033325693423366]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857492972390476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1857492972390476 | validation: 0.2383117545787733]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19343066159882355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19343066159882355 | validation: 0.28079943782241723]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20611335067501302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20611335067501302 | validation: 0.29778090038496885]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21135167354644363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21135167354644363 | validation: 0.20807335488935463]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14512996968852845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14512996968852845 | validation: 0.22181385639649545]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13423515822505855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13423515822505855 | validation: 0.23097336240321864]
	TIME [epoch: 1.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031054327545622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2031054327545622 | validation: 0.5156548910970015]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33730533880485014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33730533880485014 | validation: 0.20851378510946264]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534323678611191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2534323678611191 | validation: 0.22908492412660253]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318080505712548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12318080505712548 | validation: 0.17852228275147683]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127045183128514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1127045183128514 | validation: 0.22812496506628188]
	TIME [epoch: 1.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15346918022251754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15346918022251754 | validation: 0.18475786663765958]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2569312358646191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2569312358646191 | validation: 0.37403527999907205]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2719742625762893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2719742625762893 | validation: 0.17909421339667345]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660785386815527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2660785386815527 | validation: 0.2873038034467152]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14706099467824904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14706099467824904 | validation: 0.2593385361479923]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18510472704519906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18510472704519906 | validation: 0.2605980613556493]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2140733579787797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2140733579787797 | validation: 0.21302599610640788]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15880605643814005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15880605643814005 | validation: 0.24817983489878734]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15157074902369563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15157074902369563 | validation: 0.15741820207932378]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12091182025878272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12091182025878272 | validation: 0.2213360889346063]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206482203148821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206482203148821 | validation: 0.14589475796074652]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11612592981654957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11612592981654957 | validation: 0.2598349273448081]
	TIME [epoch: 1.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.143296597770907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.143296597770907 | validation: 0.14198880030483238]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508781188599077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1508781188599077 | validation: 0.35162957436076514]
	TIME [epoch: 1.39 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21893533865199707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21893533865199707 | validation: 0.18023104501357257]
	TIME [epoch: 1.39 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2352570585123604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2352570585123604 | validation: 0.4492685603417945]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35731738443784333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35731738443784333 | validation: 0.2731453222180824]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17261651739726105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17261651739726105 | validation: 0.23230988330575883]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12536684689930613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12536684689930613 | validation: 0.1669114567377586]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11919903020462946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11919903020462946 | validation: 0.1800755011726266]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13857226123459163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13857226123459163 | validation: 0.22487023432824024]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1941986373999135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1941986373999135 | validation: 0.15257948046354092]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17381079161996332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17381079161996332 | validation: 0.23667488554379182]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1281455226542717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1281455226542717 | validation: 0.14402045153426984]
	TIME [epoch: 1.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14794046179261477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14794046179261477 | validation: 0.4406282815750873]
	TIME [epoch: 1.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27487523061756386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27487523061756386 | validation: 0.2730869391634563]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1994944056909277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1994944056909277 | validation: 0.18270998838637742]
	TIME [epoch: 1.39 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152747045789358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10152747045789358 | validation: 0.1562726940241253]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08824782691732015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08824782691732015 | validation: 0.17684438369058586]
	TIME [epoch: 1.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14362735919539016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14362735919539016 | validation: 0.2062734340766011]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17910345966915409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17910345966915409 | validation: 0.1601794945417911]
	TIME [epoch: 1.39 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13399126798912575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13399126798912575 | validation: 0.20491710852004755]
	TIME [epoch: 1.39 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13031290737372284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13031290737372284 | validation: 0.14588763567850682]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714497632120984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1714497632120984 | validation: 0.3925788749090239]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502678284507082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502678284507082 | validation: 0.24963694959719415]
	TIME [epoch: 1.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21371373599850074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21371373599850074 | validation: 0.22835910221984396]
	TIME [epoch: 1.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299041454225534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12299041454225534 | validation: 0.14499853828300324]
	TIME [epoch: 1.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615667850879923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07615667850879923 | validation: 0.11566103505703894]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405185253916484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06405185253916484 | validation: 0.1437726681638388]
	TIME [epoch: 1.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987798869383585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06987798869383585 | validation: 0.13719727452449682]
	TIME [epoch: 1.39 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12708771017290546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12708771017290546 | validation: 0.24937572840795028]
	TIME [epoch: 1.39 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23530705734534912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23530705734534912 | validation: 0.19172054907449604]
	TIME [epoch: 1.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14262480201912597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14262480201912597 | validation: 0.17138721407809032]
	TIME [epoch: 1.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10185275729028571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10185275729028571 | validation: 0.13023431789727932]
	TIME [epoch: 1.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09418029300984772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09418029300984772 | validation: 0.17942927992074262]
	TIME [epoch: 1.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603958246528846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09603958246528846 | validation: 0.17112124269353593]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2128038184137752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2128038184137752 | validation: 0.6362056600637225]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44385304134768305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44385304134768305 | validation: 0.2231534545062014]
	TIME [epoch: 1.4 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.177207262939829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.177207262939829 | validation: 0.11546344657178764]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09894283282054722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09894283282054722 | validation: 0.22068338834732226]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12332714633658597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12332714633658597 | validation: 0.1615152997308775]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11571523293005725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11571523293005725 | validation: 0.17795329420182543]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12544269093077925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12544269093077925 | validation: 0.1623003367419748]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553801185128737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1553801185128737 | validation: 0.1622592387783386]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09424529449526617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09424529449526617 | validation: 0.16822065663476607]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693080210984568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09693080210984568 | validation: 0.1758400607035614]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260618981349862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260618981349862 | validation: 0.24686144670783983]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13399796183162116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13399796183162116 | validation: 0.21520629841091288]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14661122595086762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14661122595086762 | validation: 0.23210619561493903]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15345297762497156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15345297762497156 | validation: 0.130990136703302]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17532635365946533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17532635365946533 | validation: 0.25755414051876324]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606017558637084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606017558637084 | validation: 0.12514858513210028]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448891506537934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448891506537934 | validation: 0.15867346476262015]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10278200758815352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10278200758815352 | validation: 0.17512496446224635]
	TIME [epoch: 1.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09675655376811845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09675655376811845 | validation: 0.11800729177079167]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07701437451451044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07701437451451044 | validation: 0.15715652315948364]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08071427288946388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08071427288946388 | validation: 0.1449254006593469]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361428530900703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10361428530900703 | validation: 0.3455125710299818]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20305669155646094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20305669155646094 | validation: 0.26395280937820287]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21730337609689948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21730337609689948 | validation: 0.19890829640942323]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176621457321684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176621457321684 | validation: 0.1252194644513691]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12766395531829486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12766395531829486 | validation: 0.2049593851661527]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474828495659271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12474828495659271 | validation: 0.13809603881481577]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09612535894464332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09612535894464332 | validation: 0.12006044448475924]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759275598145906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0759275598145906 | validation: 0.15662184931524972]
	TIME [epoch: 1.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806294584113983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0806294584113983 | validation: 0.10456249088387624]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127847590859574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09127847590859574 | validation: 0.22618901375745357]
	TIME [epoch: 1.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14700677365247358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14700677365247358 | validation: 0.194767348991409]
	TIME [epoch: 1.4 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21009402591701837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21009402591701837 | validation: 0.2874622567058084]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715518066860849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1715518066860849 | validation: 0.1517512706962852]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11801407190727561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11801407190727561 | validation: 0.14261954504427413]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127519562656809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09127519562656809 | validation: 0.11851241280615715]
	TIME [epoch: 1.4 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09965541652292782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09965541652292782 | validation: 0.1670663468559821]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10877930226083056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10877930226083056 | validation: 0.16933182825026305]
	TIME [epoch: 1.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737195167684614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10737195167684614 | validation: 0.15425379200993616]
	TIME [epoch: 1.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566767904207805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10566767904207805 | validation: 0.2038570441859268]
	TIME [epoch: 1.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11367406074894922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11367406074894922 | validation: 0.11686170904287217]
	TIME [epoch: 1.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029223343689149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10029223343689149 | validation: 0.2012694877592175]
	TIME [epoch: 1.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10827985829138545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10827985829138545 | validation: 0.12300362198238163]
	TIME [epoch: 1.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302551605636556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302551605636556 | validation: 0.20375454936839282]
	TIME [epoch: 1.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13782889958331776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13782889958331776 | validation: 0.14705982504577012]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419339175979126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1419339175979126 | validation: 0.16098520313463202]
	TIME [epoch: 1.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08026645551713717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08026645551713717 | validation: 0.11215070177365323]
	TIME [epoch: 1.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0605585157705068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0605585157705068 | validation: 0.133312398971794]
	TIME [epoch: 1.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628668760828957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06628668760828957 | validation: 0.1684659929510342]
	TIME [epoch: 185 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087336991073017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09087336991073017 | validation: 0.2157700212105323]
	TIME [epoch: 2.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13007974620436685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13007974620436685 | validation: 0.1496527376631482]
	TIME [epoch: 2.75 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336582049907798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336582049907798 | validation: 0.15512359035343667]
	TIME [epoch: 2.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08884416959413448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08884416959413448 | validation: 0.09353213287294389]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818060041311781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0818060041311781 | validation: 0.18443129714907824]
	TIME [epoch: 2.75 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418592327569067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418592327569067 | validation: 0.15147174928318208]
	TIME [epoch: 2.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20542767769271095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20542767769271095 | validation: 0.3344092643489832]
	TIME [epoch: 2.75 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2088957763044889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2088957763044889 | validation: 0.2055049758643266]
	TIME [epoch: 2.75 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11270973752839643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11270973752839643 | validation: 0.1357441027415491]
	TIME [epoch: 2.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10219886934220068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10219886934220068 | validation: 0.18351751563416582]
	TIME [epoch: 2.76 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861106671810914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06861106671810914 | validation: 0.09358067700095289]
	TIME [epoch: 2.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05667452180399304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05667452180399304 | validation: 0.13888143388781882]
	TIME [epoch: 2.75 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056811610324644726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056811610324644726 | validation: 0.1209303876040273]
	TIME [epoch: 2.76 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838228011836718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07838228011836718 | validation: 0.23864204994136742]
	TIME [epoch: 2.75 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15342163829611835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15342163829611835 | validation: 0.2713325522641709]
	TIME [epoch: 2.76 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2367651006722614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2367651006722614 | validation: 0.20557158344773918]
	TIME [epoch: 2.76 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258762585097993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1258762585097993 | validation: 0.08379535893551787]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07153457106652467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07153457106652467 | validation: 0.10737145983905692]
	TIME [epoch: 2.75 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053316274002226355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053316274002226355 | validation: 0.11069890590934928]
	TIME [epoch: 2.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868962096355456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06868962096355456 | validation: 0.13974648465856543]
	TIME [epoch: 2.75 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275324031865915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1275324031865915 | validation: 0.18728407371324038]
	TIME [epoch: 2.75 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12390078112426542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12390078112426542 | validation: 0.17387415394017142]
	TIME [epoch: 2.75 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12268676430668901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12268676430668901 | validation: 0.17272245053071397]
	TIME [epoch: 2.76 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08712355045431223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08712355045431223 | validation: 0.1675743310454494]
	TIME [epoch: 2.76 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893938791966184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0893938791966184 | validation: 0.12206795091169848]
	TIME [epoch: 2.75 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14783958997268598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14783958997268598 | validation: 0.24541301354076417]
	TIME [epoch: 2.76 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1704912519653982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1704912519653982 | validation: 0.11615745450644784]
	TIME [epoch: 2.75 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10662616474963514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10662616474963514 | validation: 0.11339599583254296]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05146176506430001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05146176506430001 | validation: 0.1057438429034804]
	TIME [epoch: 2.75 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344420602258375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04344420602258375 | validation: 0.08438059490900693]
	TIME [epoch: 2.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041890253420306466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041890253420306466 | validation: 0.1186448908689902]
	TIME [epoch: 2.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045237384035743705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045237384035743705 | validation: 0.12271203027383007]
	TIME [epoch: 2.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694094278950717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06694094278950717 | validation: 0.21567266583142208]
	TIME [epoch: 2.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775880943891548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11775880943891548 | validation: 0.2572868470438796]
	TIME [epoch: 2.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18867366311279604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18867366311279604 | validation: 0.15144320187937055]
	TIME [epoch: 2.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11415971766934309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11415971766934309 | validation: 0.1029248889127744]
	TIME [epoch: 2.76 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06764396658237665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06764396658237665 | validation: 0.16274533289262846]
	TIME [epoch: 2.76 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175658830231072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09175658830231072 | validation: 0.11517052525524454]
	TIME [epoch: 2.75 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08031884934404815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08031884934404815 | validation: 0.124873131317614]
	TIME [epoch: 2.74 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08797446689830082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08797446689830082 | validation: 0.11954239913421161]
	TIME [epoch: 2.75 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08336232100554344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08336232100554344 | validation: 0.11450995508587543]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10257208040459677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10257208040459677 | validation: 0.1434933191113262]
	TIME [epoch: 2.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10296363554030968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10296363554030968 | validation: 0.11849264344341011]
	TIME [epoch: 2.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12250200666520712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250200666520712 | validation: 0.3375795004478204]
	TIME [epoch: 2.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20946240588684467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20946240588684467 | validation: 0.18170216601119812]
	TIME [epoch: 2.75 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17357919618801318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17357919618801318 | validation: 0.21406690600996675]
	TIME [epoch: 2.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13175964621241268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13175964621241268 | validation: 0.1297704443905515]
	TIME [epoch: 2.75 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058016235491405777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058016235491405777 | validation: 0.0990213045399222]
	TIME [epoch: 2.75 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112949733796073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05112949733796073 | validation: 0.1059831809566557]
	TIME [epoch: 2.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859747076017277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03859747076017277 | validation: 0.08424225085869211]
	TIME [epoch: 2.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03660388893053058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03660388893053058 | validation: 0.08878801355734298]
	TIME [epoch: 2.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03454711245717663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03454711245717663 | validation: 0.08349799042864159]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03845795621306692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03845795621306692 | validation: 0.13218516473537614]
	TIME [epoch: 2.75 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835895820290927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06835895820290927 | validation: 0.23774971294782646]
	TIME [epoch: 2.75 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18937761073026715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18937761073026715 | validation: 0.37453184705606735]
	TIME [epoch: 2.75 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080252841216042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3080252841216042 | validation: 0.2239296059966132]
	TIME [epoch: 2.75 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12082057722305546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12082057722305546 | validation: 0.10789997871462283]
	TIME [epoch: 2.75 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11820063162383733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11820063162383733 | validation: 0.17226225338072024]
	TIME [epoch: 2.75 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09565733298804499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09565733298804499 | validation: 0.0866548837582965]
	TIME [epoch: 2.75 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04290879830188023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04290879830188023 | validation: 0.10040399452103857]
	TIME [epoch: 2.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047057767693160474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047057767693160474 | validation: 0.11238483862884148]
	TIME [epoch: 2.75 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05168143965807064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05168143965807064 | validation: 0.12201473114000355]
	TIME [epoch: 2.75 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551866877498212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06551866877498212 | validation: 0.13309464750423874]
	TIME [epoch: 2.74 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693607803104858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693607803104858 | validation: 0.22951228352988703]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16029915365454475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16029915365454475 | validation: 0.17988721540585448]
	TIME [epoch: 2.75 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19655320481363434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19655320481363434 | validation: 0.2277568688763937]
	TIME [epoch: 2.76 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11703652139670226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11703652139670226 | validation: 0.10547001967362234]
	TIME [epoch: 2.76 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426255040040237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426255040040237 | validation: 0.08755497516509775]
	TIME [epoch: 2.75 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06238565072080407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06238565072080407 | validation: 0.12496267911112563]
	TIME [epoch: 2.75 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404642636439828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05404642636439828 | validation: 0.06728566401305566]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05191571898019779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05191571898019779 | validation: 0.09910097581394638]
	TIME [epoch: 2.75 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04435018425968042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04435018425968042 | validation: 0.06275403524083459]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047475239870192865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047475239870192865 | validation: 0.10569643961065821]
	TIME [epoch: 2.75 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06602592502675853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06602592502675853 | validation: 0.11992667447532877]
	TIME [epoch: 2.75 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11313224488086521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11313224488086521 | validation: 0.18330908672167243]
	TIME [epoch: 2.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13418909664498455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13418909664498455 | validation: 0.18555745045151187]
	TIME [epoch: 2.75 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13790248844328543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13790248844328543 | validation: 0.33609824623594053]
	TIME [epoch: 2.75 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18022009156111282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18022009156111282 | validation: 0.14438620847622327]
	TIME [epoch: 2.75 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727559789423697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06727559789423697 | validation: 0.0745541603525921]
	TIME [epoch: 2.75 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810015790679062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05810015790679062 | validation: 0.15082327237080098]
	TIME [epoch: 2.76 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692599275201164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692599275201164 | validation: 0.09666517236845809]
	TIME [epoch: 2.76 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10025896763842167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10025896763842167 | validation: 0.1341588100243556]
	TIME [epoch: 2.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070037491430237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.070037491430237 | validation: 0.08226929275781876]
	TIME [epoch: 2.75 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05120091095373762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05120091095373762 | validation: 0.10448010717929042]
	TIME [epoch: 2.76 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06139029383738391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06139029383738391 | validation: 0.13189775120906444]
	TIME [epoch: 2.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1066653463358782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1066653463358782 | validation: 0.19408098478417207]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14417642851580437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14417642851580437 | validation: 0.16912348317858938]
	TIME [epoch: 2.76 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12409644617001707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12409644617001707 | validation: 0.17552867811695863]
	TIME [epoch: 2.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345582760325364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08345582760325364 | validation: 0.07807571619504311]
	TIME [epoch: 2.75 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0553807639324744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0553807639324744 | validation: 0.10568438083129116]
	TIME [epoch: 2.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04850197870993626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04850197870993626 | validation: 0.06929988333919337]
	TIME [epoch: 2.76 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04967317867850864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04967317867850864 | validation: 0.11354493768808137]
	TIME [epoch: 2.75 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056327416841947094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056327416841947094 | validation: 0.10119875695443128]
	TIME [epoch: 2.75 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07762385734129236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07762385734129236 | validation: 0.18795252460201786]
	TIME [epoch: 2.75 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09148114904665558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09148114904665558 | validation: 0.11339279400957317]
	TIME [epoch: 2.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08041090071222304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08041090071222304 | validation: 0.11970322318100508]
	TIME [epoch: 2.75 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07406135167794586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07406135167794586 | validation: 0.08018173018857372]
	TIME [epoch: 2.75 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09896878407139716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09896878407139716 | validation: 0.1314110201168907]
	TIME [epoch: 2.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841798177065859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841798177065859 | validation: 0.16561113137055555]
	TIME [epoch: 2.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12996661371632148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12996661371632148 | validation: 0.3488152430779279]
	TIME [epoch: 2.75 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984867709987633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1984867709987633 | validation: 0.07849754087398311]
	TIME [epoch: 2.76 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06257768513955327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06257768513955327 | validation: 0.10950077774493312]
	TIME [epoch: 2.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06239257938862596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06239257938862596 | validation: 0.09387692997942518]
	TIME [epoch: 2.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052121318285405636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052121318285405636 | validation: 0.10428711526457542]
	TIME [epoch: 2.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052012402648025875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052012402648025875 | validation: 0.0906927235699504]
	TIME [epoch: 2.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05780573424077435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05780573424077435 | validation: 0.13527708640835426]
	TIME [epoch: 2.75 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061975508423237714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061975508423237714 | validation: 0.06256218448959708]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05312207271107978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05312207271107978 | validation: 0.1100199214404887]
	TIME [epoch: 2.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041782613930723564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041782613930723564 | validation: 0.07091846103179904]
	TIME [epoch: 2.75 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049242839600964664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049242839600964664 | validation: 0.14185628272282563]
	TIME [epoch: 2.75 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12120826322725979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12120826322725979 | validation: 0.2806110164279634]
	TIME [epoch: 2.76 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670736372090942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2670736372090942 | validation: 0.33953358306992754]
	TIME [epoch: 2.76 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18020101217221943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18020101217221943 | validation: 0.10095174156883309]
	TIME [epoch: 2.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248692397675801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05248692397675801 | validation: 0.11212725155891508]
	TIME [epoch: 2.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06651294194400395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06651294194400395 | validation: 0.1169482946650585]
	TIME [epoch: 2.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050442175931660664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050442175931660664 | validation: 0.07380645155524088]
	TIME [epoch: 2.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03986394768682825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03986394768682825 | validation: 0.10655223882452743]
	TIME [epoch: 2.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042979861364033256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042979861364033256 | validation: 0.06568931676681017]
	TIME [epoch: 2.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058827414314497645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058827414314497645 | validation: 0.11624114188350979]
	TIME [epoch: 2.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07973758728911072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07973758728911072 | validation: 0.10264416392342311]
	TIME [epoch: 2.75 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10048288061566481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10048288061566481 | validation: 0.16150461412353068]
	TIME [epoch: 2.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10195686864704413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10195686864704413 | validation: 0.16061233229840266]
	TIME [epoch: 2.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405958082116249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405958082116249 | validation: 0.15086393568887507]
	TIME [epoch: 2.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141160882883415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1141160882883415 | validation: 0.15936439324071938]
	TIME [epoch: 2.76 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07617975487344096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07617975487344096 | validation: 0.11095325746154688]
	TIME [epoch: 2.76 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05309489965782887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05309489965782887 | validation: 0.07575031311486032]
	TIME [epoch: 2.75 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036624473106100314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036624473106100314 | validation: 0.06053909202668768]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03305249266512134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03305249266512134 | validation: 0.0958696417197279]
	TIME [epoch: 2.74 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115923141817519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04115923141817519 | validation: 0.09923819956363444]
	TIME [epoch: 2.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860266143527442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06860266143527442 | validation: 0.17106001601088572]
	TIME [epoch: 2.74 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09503025751974324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09503025751974324 | validation: 0.13786314672569255]
	TIME [epoch: 2.75 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894970578543156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894970578543156 | validation: 0.13165868673205855]
	TIME [epoch: 2.74 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07491885749424464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07491885749424464 | validation: 0.13551941210320437]
	TIME [epoch: 2.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12475332050265421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12475332050265421 | validation: 0.2665434203587071]
	TIME [epoch: 2.75 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16531145343659462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16531145343659462 | validation: 0.10598696470134693]
	TIME [epoch: 2.76 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910967257760212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06910967257760212 | validation: 0.09606770952627386]
	TIME [epoch: 2.75 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03573874505748762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03573874505748762 | validation: 0.07216137076282149]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03823893695140445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03823893695140445 | validation: 0.08415433112587165]
	TIME [epoch: 2.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04155778550352258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04155778550352258 | validation: 0.06384156392613126]
	TIME [epoch: 2.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057451936537616655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057451936537616655 | validation: 0.0998234549751766]
	TIME [epoch: 2.75 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078194784210428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.078194784210428 | validation: 0.07691277870546281]
	TIME [epoch: 2.76 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859229021470345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06859229021470345 | validation: 0.07777843897126308]
	TIME [epoch: 2.76 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04499206299330325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04499206299330325 | validation: 0.11367210414438145]
	TIME [epoch: 2.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131476349492157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06131476349492157 | validation: 0.19664239381470316]
	TIME [epoch: 2.76 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14650477812034257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14650477812034257 | validation: 0.2563032455557063]
	TIME [epoch: 2.76 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16324573308454313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16324573308454313 | validation: 0.10371511529734212]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10877754570002773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10877754570002773 | validation: 0.0928566219685209]
	TIME [epoch: 2.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657706573315588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05657706573315588 | validation: 0.11232479274856817]
	TIME [epoch: 2.76 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256809659188308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05256809659188308 | validation: 0.07977883235133892]
	TIME [epoch: 2.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0685572858024085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0685572858024085 | validation: 0.1185494949493188]
	TIME [epoch: 2.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05291135627674169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05291135627674169 | validation: 0.0683747345542211]
	TIME [epoch: 2.76 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038479960986871335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038479960986871335 | validation: 0.09241338597458923]
	TIME [epoch: 2.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512826161414207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03512826161414207 | validation: 0.11458781881512331]
	TIME [epoch: 2.75 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602325062842786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04602325062842786 | validation: 0.12678459357899033]
	TIME [epoch: 2.75 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06010327317746369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06010327317746369 | validation: 0.10493909610620614]
	TIME [epoch: 2.76 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764451364503339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05764451364503339 | validation: 0.10695000322397696]
	TIME [epoch: 2.75 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05605043140460479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05605043140460479 | validation: 0.06329518170821886]
	TIME [epoch: 2.76 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988228710225018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06988228710225018 | validation: 0.1148116587216651]
	TIME [epoch: 2.75 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511210878475676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511210878475676 | validation: 0.09902883986332141]
	TIME [epoch: 2.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10890635020394268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10890635020394268 | validation: 0.11107273793596889]
	TIME [epoch: 2.75 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937097879564324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937097879564324 | validation: 0.34089991043203294]
	TIME [epoch: 2.75 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1994626007969804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1994626007969804 | validation: 0.10005526357277648]
	TIME [epoch: 2.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07169160697802496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07169160697802496 | validation: 0.10818888620004104]
	TIME [epoch: 2.75 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07081086856128078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07081086856128078 | validation: 0.08118235994059894]
	TIME [epoch: 2.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377095661491939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377095661491939 | validation: 0.06345613050095811]
	TIME [epoch: 2.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03035884763613389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03035884763613389 | validation: 0.07844534526873587]
	TIME [epoch: 2.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033164167268640805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033164167268640805 | validation: 0.10944860793198213]
	TIME [epoch: 2.76 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434490669195165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04434490669195165 | validation: 0.11552284350931474]
	TIME [epoch: 2.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08266827392458526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08266827392458526 | validation: 0.1923540748203673]
	TIME [epoch: 2.75 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348256764389742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1348256764389742 | validation: 0.10760270305175229]
	TIME [epoch: 2.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11583609846601813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11583609846601813 | validation: 0.07780609929689992]
	TIME [epoch: 2.76 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031171955140036865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031171955140036865 | validation: 0.06423238584429934]
	TIME [epoch: 2.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02994094864230534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02994094864230534 | validation: 0.05780423704197685]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034032714601167385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034032714601167385 | validation: 0.09892183942920302]
	TIME [epoch: 2.75 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033214287053308984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033214287053308984 | validation: 0.08506496401019824]
	TIME [epoch: 2.75 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047789565836553013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047789565836553013 | validation: 0.194462342296974]
	TIME [epoch: 2.75 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112291608833133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1112291608833133 | validation: 0.36038142253044037]
	TIME [epoch: 2.75 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435885952987454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2435885952987454 | validation: 0.11407415921742113]
	TIME [epoch: 2.76 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13145901199564825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13145901199564825 | validation: 0.1279000184307631]
	TIME [epoch: 2.75 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362037588078179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06362037588078179 | validation: 0.08507072297239629]
	TIME [epoch: 2.75 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799097841864972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06799097841864972 | validation: 0.0738069713664356]
	TIME [epoch: 2.75 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041597266318310525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041597266318310525 | validation: 0.06855816669902981]
	TIME [epoch: 2.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029223010247983794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029223010247983794 | validation: 0.06635295397737322]
	TIME [epoch: 2.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03360347983718237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03360347983718237 | validation: 0.0662600124822241]
	TIME [epoch: 2.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032253603295517304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032253603295517304 | validation: 0.07214265342041998]
	TIME [epoch: 2.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0394279619754027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0394279619754027 | validation: 0.07545183180118195]
	TIME [epoch: 2.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591132289038806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05591132289038806 | validation: 0.15889630574316033]
	TIME [epoch: 2.76 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085762052945577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10085762052945577 | validation: 0.23309147500973812]
	TIME [epoch: 2.76 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19654852441348575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19654852441348575 | validation: 0.14180448166092666]
	TIME [epoch: 2.75 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590020023906551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09590020023906551 | validation: 0.06957036499115986]
	TIME [epoch: 2.76 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03300332853471824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03300332853471824 | validation: 0.055860905921813135]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399231219585897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03399231219585897 | validation: 0.08957845624526908]
	TIME [epoch: 2.74 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037709563025010544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037709563025010544 | validation: 0.05429839518816196]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03798463871753564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03798463871753564 | validation: 0.07976800275035097]
	TIME [epoch: 2.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030244108297938176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030244108297938176 | validation: 0.05968475771922065]
	TIME [epoch: 2.75 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029396124538674108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029396124538674108 | validation: 0.09081621663663156]
	TIME [epoch: 2.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034266028636380286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034266028636380286 | validation: 0.10378760437551614]
	TIME [epoch: 2.75 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07371843353680295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07371843353680295 | validation: 0.19794099766563988]
	TIME [epoch: 2.75 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12287049468687535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12287049468687535 | validation: 0.25682611228634306]
	TIME [epoch: 2.75 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13523307228017845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13523307228017845 | validation: 0.04555767204324848]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044694379720503725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044694379720503725 | validation: 0.12663252188794547]
	TIME [epoch: 2.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08439180648564513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08439180648564513 | validation: 0.07195601576984532]
	TIME [epoch: 2.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369004591235172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07369004591235172 | validation: 0.10777282789310275]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05429967592281728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05429967592281728 | validation: 0.09545140163726346]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052700636902334994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052700636902334994 | validation: 0.11927982609922655]
	TIME [epoch: 2.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938238472979706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06938238472979706 | validation: 0.14025624553895702]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10955323529522375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10955323529522375 | validation: 0.17129662498630696]
	TIME [epoch: 2.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12718344557235978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12718344557235978 | validation: 0.12573950579545573]
	TIME [epoch: 2.74 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07102457242936853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07102457242936853 | validation: 0.07671438337793719]
	TIME [epoch: 2.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03167895825549244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03167895825549244 | validation: 0.04896409745356659]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026074416919701294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026074416919701294 | validation: 0.08254350388750334]
	TIME [epoch: 2.74 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027460942082820072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027460942082820072 | validation: 0.05411247169495514]
	TIME [epoch: 2.74 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031168652152912663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031168652152912663 | validation: 0.09794488281571817]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038939285522590364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038939285522590364 | validation: 0.10755032679775854]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06620738684286695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06620738684286695 | validation: 0.15752329581007013]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11312579652425533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11312579652425533 | validation: 0.22210998953246516]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15433130481785834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15433130481785834 | validation: 0.08473384579158795]
	TIME [epoch: 2.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10014142740755387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10014142740755387 | validation: 0.10633749030544065]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049681143324815914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049681143324815914 | validation: 0.0727559598830712]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852385842538672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03852385842538672 | validation: 0.06556340516512865]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334342803149523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0334342803149523 | validation: 0.06912582642145365]
	TIME [epoch: 2.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030102053314508544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030102053314508544 | validation: 0.07640111271316719]
	TIME [epoch: 2.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03246659608272572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03246659608272572 | validation: 0.10856029144449453]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056090068140710055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056090068140710055 | validation: 0.11429861265428523]
	TIME [epoch: 2.74 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06284922088116393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06284922088116393 | validation: 0.14683722488806267]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08706292146543197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08706292146543197 | validation: 0.10912629881173208]
	TIME [epoch: 2.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10771645325047484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10771645325047484 | validation: 0.09465922579927888]
	TIME [epoch: 2.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471223394083426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07471223394083426 | validation: 0.06771843059447856]
	TIME [epoch: 2.75 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04095947109509652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04095947109509652 | validation: 0.054669496373152174]
	TIME [epoch: 2.75 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05048994306582183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05048994306582183 | validation: 0.08999619955477833]
	TIME [epoch: 2.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878144481270863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04878144481270863 | validation: 0.06955901146646346]
	TIME [epoch: 2.76 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608217971487723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03608217971487723 | validation: 0.07675364809498177]
	TIME [epoch: 2.77 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04302341398619132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04302341398619132 | validation: 0.13764608435976472]
	TIME [epoch: 2.76 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06257185788775313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06257185788775313 | validation: 0.10694967592806455]
	TIME [epoch: 2.75 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142503165824528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07142503165824528 | validation: 0.10566926288255313]
	TIME [epoch: 2.77 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292228296936214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05292228296936214 | validation: 0.06507456213467785]
	TIME [epoch: 2.76 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03951273299838208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03951273299838208 | validation: 0.04695699820521997]
	TIME [epoch: 2.76 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032354545678434946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032354545678434946 | validation: 0.09091348566486093]
	TIME [epoch: 2.76 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980545908229286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980545908229286 | validation: 0.0728030705684596]
	TIME [epoch: 2.75 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806152519407953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05806152519407953 | validation: 0.1688309060236326]
	TIME [epoch: 2.76 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14214308328054973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14214308328054973 | validation: 0.35921000660845637]
	TIME [epoch: 2.76 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22462618321240563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22462618321240563 | validation: 0.08191266654781415]
	TIME [epoch: 2.76 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749860596837389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0749860596837389 | validation: 0.10062563511681721]
	TIME [epoch: 2.76 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052231013207935176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052231013207935176 | validation: 0.08182423561200844]
	TIME [epoch: 2.76 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0444014428085912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0444014428085912 | validation: 0.04739228360591459]
	TIME [epoch: 2.76 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026580564213851462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026580564213851462 | validation: 0.06555753832734632]
	TIME [epoch: 2.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019242435374907284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019242435374907284 | validation: 0.0614647020360754]
	TIME [epoch: 2.75 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021650781230297608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021650781230297608 | validation: 0.07103440084946099]
	TIME [epoch: 2.76 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038084158290802035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038084158290802035 | validation: 0.14283471983424717]
	TIME [epoch: 2.77 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469597102008542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09469597102008542 | validation: 0.17127874036046775]
	TIME [epoch: 2.76 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14607467472896773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14607467472896773 | validation: 0.09383189808150552]
	TIME [epoch: 2.76 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953660386406129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953660386406129 | validation: 0.07827986374354456]
	TIME [epoch: 2.76 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04122365595035638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04122365595035638 | validation: 0.050671780824255545]
	TIME [epoch: 2.76 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04634043372711071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04634043372711071 | validation: 0.058737498338804886]
	TIME [epoch: 2.76 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028292037202290902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028292037202290902 | validation: 0.04493446113519082]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02058817248800234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02058817248800234 | validation: 0.06185062238001098]
	TIME [epoch: 2.74 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023090001836598525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023090001836598525 | validation: 0.08121108606220469]
	TIME [epoch: 2.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04226738791182782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04226738791182782 | validation: 0.10876277690812229]
	TIME [epoch: 2.74 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784480909885892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07784480909885892 | validation: 0.10846024817877238]
	TIME [epoch: 2.75 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11720186912601008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11720186912601008 | validation: 0.1730855219828905]
	TIME [epoch: 2.74 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1079604774360313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1079604774360313 | validation: 0.17026012807839105]
	TIME [epoch: 2.74 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09910342862070529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09910342862070529 | validation: 0.1786040106512543]
	TIME [epoch: 2.75 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802523319520398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802523319520398 | validation: 0.09032594630718126]
	TIME [epoch: 2.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03276846939973464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03276846939973464 | validation: 0.061694167412357295]
	TIME [epoch: 2.75 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02796721321211482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02796721321211482 | validation: 0.08438235657996633]
	TIME [epoch: 2.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035449445835068386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035449445835068386 | validation: 0.08428025761932909]
	TIME [epoch: 2.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031464888119614016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031464888119614016 | validation: 0.08015339923454312]
	TIME [epoch: 2.74 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037207417775229505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037207417775229505 | validation: 0.11180445383056573]
	TIME [epoch: 2.75 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010463222247065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05010463222247065 | validation: 0.11051095874289679]
	TIME [epoch: 2.75 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06959162695008436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06959162695008436 | validation: 0.08965905648411726]
	TIME [epoch: 2.75 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10575568535128163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10575568535128163 | validation: 0.09564669311597025]
	TIME [epoch: 2.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811177109227345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07811177109227345 | validation: 0.05223124105981149]
	TIME [epoch: 2.75 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628648742236692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04628648742236692 | validation: 0.09032607455222623]
	TIME [epoch: 2.74 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051463790503661955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051463790503661955 | validation: 0.06524878702571241]
	TIME [epoch: 2.74 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700235136257088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03700235136257088 | validation: 0.05303909859208747]
	TIME [epoch: 2.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029876721028482284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029876721028482284 | validation: 0.052561493092749045]
	TIME [epoch: 2.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01644710877616302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01644710877616302 | validation: 0.055701050506972716]
	TIME [epoch: 2.75 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02271524781120977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02271524781120977 | validation: 0.10347697376250556]
	TIME [epoch: 2.74 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049987176775513564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049987176775513564 | validation: 0.17470017408464522]
	TIME [epoch: 2.75 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1060752358499003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1060752358499003 | validation: 0.1376533787208326]
	TIME [epoch: 2.74 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1368181546820985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1368181546820985 | validation: 0.10378392274091651]
	TIME [epoch: 2.74 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810754462209182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0810754462209182 | validation: 0.05653955610139036]
	TIME [epoch: 2.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031189705756617645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031189705756617645 | validation: 0.06124306453152655]
	TIME [epoch: 2.75 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03066796902466397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03066796902466397 | validation: 0.0739830922292567]
	TIME [epoch: 2.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033158080698673924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033158080698673924 | validation: 0.058054138481040476]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032196365998188356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032196365998188356 | validation: 0.0987236727026214]
	TIME [epoch: 2.74 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05599567056379605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05599567056379605 | validation: 0.13922876563990944]
	TIME [epoch: 2.74 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149600895040447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149600895040447 | validation: 0.23147501755265398]
	TIME [epoch: 2.74 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535961376294005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535961376294005 | validation: 0.08449006581639895]
	TIME [epoch: 2.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296113350752731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06296113350752731 | validation: 0.06800045317195157]
	TIME [epoch: 2.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03988600706859098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03988600706859098 | validation: 0.10333864182195703]
	TIME [epoch: 2.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047801934294203666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047801934294203666 | validation: 0.06664646436549426]
	TIME [epoch: 2.77 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031111937807283427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031111937807283427 | validation: 0.055979426671321914]
	TIME [epoch: 2.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027844807439181687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027844807439181687 | validation: 0.08181068145343705]
	TIME [epoch: 2.76 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030558668501499964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030558668501499964 | validation: 0.0859719207750635]
	TIME [epoch: 2.76 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035656690040856864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035656690040856864 | validation: 0.10273735306284221]
	TIME [epoch: 2.76 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395153188948174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0395153188948174 | validation: 0.11289294305742073]
	TIME [epoch: 2.76 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955142301755607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05955142301755607 | validation: 0.12843977037248694]
	TIME [epoch: 2.76 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06148470086891443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06148470086891443 | validation: 0.08048171218999473]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03899538452824145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03899538452824145 | validation: 0.0469351426135837]
	TIME [epoch: 2.76 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938451754568483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04938451754568483 | validation: 0.11229224840186487]
	TIME [epoch: 2.76 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875463820930921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10875463820930921 | validation: 0.08794024477704626]
	TIME [epoch: 2.77 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10381413188395758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10381413188395758 | validation: 0.08635693724071017]
	TIME [epoch: 2.76 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050688133684736576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050688133684736576 | validation: 0.08641827849467759]
	TIME [epoch: 2.76 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04014940492901653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04014940492901653 | validation: 0.04562374273367235]
	TIME [epoch: 2.76 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03276401946834265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03276401946834265 | validation: 0.08966486135195982]
	TIME [epoch: 2.75 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03282179765925635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03282179765925635 | validation: 0.08627260274243025]
	TIME [epoch: 2.76 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0551337414519113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0551337414519113 | validation: 0.08987619533103816]
	TIME [epoch: 2.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099480743392929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099480743392929 | validation: 0.10078352084130832]
	TIME [epoch: 2.76 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060739313525547865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060739313525547865 | validation: 0.0962343591481431]
	TIME [epoch: 2.76 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046544162675188384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046544162675188384 | validation: 0.11237268709754007]
	TIME [epoch: 2.76 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892975564988584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08892975564988584 | validation: 0.12630210174979634]
	TIME [epoch: 2.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1225576300293532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1225576300293532 | validation: 0.07060396630399761]
	TIME [epoch: 2.76 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04834591711910365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04834591711910365 | validation: 0.05089303843498064]
	TIME [epoch: 2.76 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02112881223908655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02112881223908655 | validation: 0.05100463196493343]
	TIME [epoch: 2.76 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363493928492088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03363493928492088 | validation: 0.08323351957979457]
	TIME [epoch: 2.76 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034889778344690786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034889778344690786 | validation: 0.056670419384980145]
	TIME [epoch: 2.76 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038001959057692734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038001959057692734 | validation: 0.09267077467202017]
	TIME [epoch: 2.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620138583579579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03620138583579579 | validation: 0.0923388112165045]
	TIME [epoch: 2.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043133083167868236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043133083167868236 | validation: 0.13238298519916888]
	TIME [epoch: 2.76 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06311342721777295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06311342721777295 | validation: 0.11375334829993895]
	TIME [epoch: 2.76 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0643351402743662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0643351402743662 | validation: 0.13733519290555862]
	TIME [epoch: 2.75 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09503037583727121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09503037583727121 | validation: 0.09923926567619107]
	TIME [epoch: 2.74 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10896874578935119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10896874578935119 | validation: 0.07038919125395902]
	TIME [epoch: 2.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474258843972145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0474258843972145 | validation: 0.10464491679665136]
	TIME [epoch: 2.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043230845099831904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043230845099831904 | validation: 0.06618305202912973]
	TIME [epoch: 2.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296463661964107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06296463661964107 | validation: 0.10024909487866684]
	TIME [epoch: 2.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03992674757697828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03992674757697828 | validation: 0.062034250721108755]
	TIME [epoch: 2.75 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029119107380639157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029119107380639157 | validation: 0.053055127846595454]
	TIME [epoch: 2.74 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033816218508497406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033816218508497406 | validation: 0.06501592891852863]
	TIME [epoch: 2.75 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038206032992941875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038206032992941875 | validation: 0.06581336291873012]
	TIME [epoch: 2.75 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04029569573270376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04029569573270376 | validation: 0.10101714162922813]
	TIME [epoch: 2.75 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06129251922615599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06129251922615599 | validation: 0.14881985631211822]
	TIME [epoch: 2.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692893281775313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692893281775313 | validation: 0.10081192554238405]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252289107991502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06252289107991502 | validation: 0.0794035481212525]
	TIME [epoch: 2.75 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04607891623608561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04607891623608561 | validation: 0.06537490632675125]
	TIME [epoch: 2.75 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05113473945161564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05113473945161564 | validation: 0.07660776246707242]
	TIME [epoch: 2.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055292756707220725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055292756707220725 | validation: 0.09405300073431169]
	TIME [epoch: 2.82 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039708571388582956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039708571388582956 | validation: 0.058014583110808454]
	TIME [epoch: 2.75 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026778627590309308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026778627590309308 | validation: 0.08077525146782882]
	TIME [epoch: 2.76 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504372141336263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04504372141336263 | validation: 0.10988413339750105]
	TIME [epoch: 2.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484210394943606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07484210394943606 | validation: 0.10217187655566083]
	TIME [epoch: 2.75 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885350034334521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0885350034334521 | validation: 0.07349326816054777]
	TIME [epoch: 2.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04290338384186318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04290338384186318 | validation: 0.047075414683960384]
	TIME [epoch: 2.74 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025872042399588124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025872042399588124 | validation: 0.05123731601643161]
	TIME [epoch: 2.75 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030506600343194225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030506600343194225 | validation: 0.05601326286495967]
	TIME [epoch: 2.76 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0314973717218871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0314973717218871 | validation: 0.07598630154353325]
	TIME [epoch: 2.75 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929044371748033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03929044371748033 | validation: 0.08870955128565539]
	TIME [epoch: 2.75 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04883246000165147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04883246000165147 | validation: 0.14017379712706043]
	TIME [epoch: 2.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0615587692876303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0615587692876303 | validation: 0.15245160050002343]
	TIME [epoch: 2.74 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.079954150700619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.079954150700619 | validation: 0.10146426182143792]
	TIME [epoch: 2.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705468909170062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05705468909170062 | validation: 0.09730128924998671]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895994438789109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06895994438789109 | validation: 0.07895503101802978]
	TIME [epoch: 2.74 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987815351632613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0987815351632613 | validation: 0.07277267539500441]
	TIME [epoch: 2.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044752241518339064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044752241518339064 | validation: 0.05627417335800863]
	TIME [epoch: 2.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027744645425267632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027744645425267632 | validation: 0.08555430969965609]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191313/states/model_phi1_4a_v_mmd2_857.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2145.591 seconds.
