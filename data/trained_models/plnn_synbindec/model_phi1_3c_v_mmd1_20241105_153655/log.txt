Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/basic/data_phi1_3c/training', validation_data='data/training_data/basic/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4164720866

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.8051845933041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8051845933041 | validation: 3.3362230765883965]
	TIME [epoch: 262 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8523949617046402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8523949617046402 | validation: 4.03406038022218]
	TIME [epoch: 2.89 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.148164373117738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.148164373117738 | validation: 2.9084575454248967]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8182182195080236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8182182195080236 | validation: 6.068863719710645]
	TIME [epoch: 2.87 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.125539881365854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.125539881365854 | validation: 4.243599817972434]
	TIME [epoch: 2.87 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7169883625488005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7169883625488005 | validation: 2.877211769594108]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.373595798187831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.373595798187831 | validation: 2.076317101296405]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.321815507810918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.321815507810918 | validation: 2.4814048289374497]
	TIME [epoch: 2.89 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5782097224232503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5782097224232503 | validation: 1.9182135932117284]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0595882225378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0595882225378 | validation: 1.7887965750458055]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0182919638077808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0182919638077808 | validation: 1.7048618962470432]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.827725897644921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.827725897644921 | validation: 1.654323797911834]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.741464610486374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.741464610486374 | validation: 1.4878432312050158]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.64350294734629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.64350294734629 | validation: 1.4356095826964603]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5433318934231681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5433318934231681 | validation: 1.3486152759187373]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4632541816177818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4632541816177818 | validation: 1.3514619609743823]
	TIME [epoch: 2.89 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4521478264122873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4521478264122873 | validation: 1.2630057323039343]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4059081963223117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4059081963223117 | validation: 1.2928705625187686]
	TIME [epoch: 2.89 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4842771344993322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4842771344993322 | validation: 1.5066703820752572]
	TIME [epoch: 2.87 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5403866539689401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5403866539689401 | validation: 1.2118691200787537]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.441731219187107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.441731219187107 | validation: 1.1323921169491926]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2610637657762571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2610637657762571 | validation: 1.114858571353049]
	TIME [epoch: 2.87 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2678847697395645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2678847697395645 | validation: 1.1673915699813062]
	TIME [epoch: 2.89 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2795026881342435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2795026881342435 | validation: 1.1532846557028835]
	TIME [epoch: 2.89 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2053963121870575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2053963121870575 | validation: 1.0499606227580152]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222780307943223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.222780307943223 | validation: 1.0240642940169653]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1510854329487372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1510854329487372 | validation: 1.0081136932778407]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0937566111828907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0937566111828907 | validation: 1.0864153425125604]
	TIME [epoch: 2.89 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093336369935924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1093336369935924 | validation: 0.928265091474793]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1134247795876666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1134247795876666 | validation: 1.178028613642027]
	TIME [epoch: 2.89 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746482829920473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1746482829920473 | validation: 1.0573779018026868]
	TIME [epoch: 2.89 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2031715754136303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2031715754136303 | validation: 1.0580082793953736]
	TIME [epoch: 2.89 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0436522427416288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0436522427416288 | validation: 0.9113101027866138]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472146114292944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9472146114292944 | validation: 0.9098618127757544]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919046895742331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.919046895742331 | validation: 0.9354157629326757]
	TIME [epoch: 2.88 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9319688153645398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9319688153645398 | validation: 0.9606623073462353]
	TIME [epoch: 2.87 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.032509100803662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.032509100803662 | validation: 1.0199934725666324]
	TIME [epoch: 2.88 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01190664769009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01190664769009 | validation: 0.8131768950535432]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281483441161114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9281483441161114 | validation: 0.9147598036541343]
	TIME [epoch: 2.89 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398062070632298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9398062070632298 | validation: 0.8254129991047595]
	TIME [epoch: 2.89 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391704748062649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8391704748062649 | validation: 0.8531093739654912]
	TIME [epoch: 2.89 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750881946255245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750881946255245 | validation: 0.815585809025535]
	TIME [epoch: 2.89 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476413086845378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8476413086845378 | validation: 0.928674796665926]
	TIME [epoch: 2.89 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157754761674933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9157754761674933 | validation: 0.8962374090657003]
	TIME [epoch: 2.89 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506427847585684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9506427847585684 | validation: 0.8245877132625963]
	TIME [epoch: 2.89 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488847534183507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8488847534183507 | validation: 0.7638909331416247]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764822250480435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764822250480435 | validation: 0.7847053857305916]
	TIME [epoch: 2.93 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114276453456438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114276453456438 | validation: 0.8175196371510461]
	TIME [epoch: 2.89 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941813359404981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941813359404981 | validation: 0.7696173231536174]
	TIME [epoch: 2.89 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769120779442534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769120779442534 | validation: 0.7711303212331724]
	TIME [epoch: 2.89 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584887892734359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584887892734359 | validation: 0.7540567307569774]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680575175725062		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.7680575175725062 | validation: 0.7846654682772641]
	TIME [epoch: 2.89 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494991951434274		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.7494991951434274 | validation: 0.7606484550063768]
	TIME [epoch: 2.88 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566834922186516		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.7566834922186516 | validation: 0.7835149700836885]
	TIME [epoch: 2.89 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759891888722847		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.759891888722847 | validation: 0.7624314133841461]
	TIME [epoch: 2.89 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811923408305773		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.7811923408305773 | validation: 0.8138994399042411]
	TIME [epoch: 2.89 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768214095430935		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.7768214095430935 | validation: 0.7453881102990509]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449251914761184		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.7449251914761184 | validation: 0.7423713319406399]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216112844725759		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.7216112844725759 | validation: 0.7260547446716107]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134371300997784		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.7134371300997784 | validation: 0.7426664173987129]
	TIME [epoch: 2.89 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038924459387621		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.7038924459387621 | validation: 0.7177001671142972]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171614330511206		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.7171614330511206 | validation: 0.7670160713906878]
	TIME [epoch: 2.89 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079160294573807		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.7079160294573807 | validation: 0.7932764918747399]
	TIME [epoch: 2.89 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745148885727923		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.7745148885727923 | validation: 0.7332275019403702]
	TIME [epoch: 2.89 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980668674760822		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.6980668674760822 | validation: 0.7178089885895869]
	TIME [epoch: 2.89 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731585233734562		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.731585233734562 | validation: 0.7287426085882406]
	TIME [epoch: 2.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705641874361358		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.705641874361358 | validation: 0.7528704864009916]
	TIME [epoch: 2.89 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308917917159977		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.7308917917159977 | validation: 0.8241019467440108]
	TIME [epoch: 2.89 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400132115446866		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7400132115446866 | validation: 0.7863202910612754]
	TIME [epoch: 2.88 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900487536602592		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.7900487536602592 | validation: 0.7358955021623741]
	TIME [epoch: 2.89 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7405526802301762		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.7405526802301762 | validation: 0.7075138626744343]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943033758455038		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.6943033758455038 | validation: 0.7503310992696175]
	TIME [epoch: 2.89 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685259163371942		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.685259163371942 | validation: 0.7146398347159112]
	TIME [epoch: 2.89 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739268473181821		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.6739268473181821 | validation: 0.7074448095064761]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826811717124659		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.6826811717124659 | validation: 0.7022948732291949]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757882213286093		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.6757882213286093 | validation: 0.7461510580493382]
	TIME [epoch: 2.89 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744416289903242		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.6744416289903242 | validation: 0.7084342763658833]
	TIME [epoch: 2.89 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765655034250279		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.6765655034250279 | validation: 0.7334441098754376]
	TIME [epoch: 2.89 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875930046944637		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.6875930046944637 | validation: 0.7451192604396502]
	TIME [epoch: 2.89 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252394659903372		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7252394659903372 | validation: 0.8178244351015471]
	TIME [epoch: 2.89 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694495496685144		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.7694495496685144 | validation: 0.7293554573340079]
	TIME [epoch: 2.89 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302350665100507		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7302350665100507 | validation: 0.6827851132264793]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691963782731324		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.6691963782731324 | validation: 0.7248586685407599]
	TIME [epoch: 2.89 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691632511270558		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.6691632511270558 | validation: 0.7143215320055966]
	TIME [epoch: 2.88 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706014688799357		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.6706014688799357 | validation: 0.7249047339411394]
	TIME [epoch: 2.87 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673401146454657		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.6673401146454657 | validation: 0.684159520717643]
	TIME [epoch: 2.88 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549190912477663		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.6549190912477663 | validation: 0.6894840920015005]
	TIME [epoch: 2.87 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616372022118727		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.6616372022118727 | validation: 0.6847401098966004]
	TIME [epoch: 2.88 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639323957069602		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.6639323957069602 | validation: 0.6988399561842545]
	TIME [epoch: 2.88 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6632915143074917		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.6632915143074917 | validation: 0.6898966753073618]
	TIME [epoch: 2.88 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780936418721593		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.6780936418721593 | validation: 0.7049740629324286]
	TIME [epoch: 2.88 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66744515298023		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.66744515298023 | validation: 0.674699125669517]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673698289187729		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.673698289187729 | validation: 0.7044060357593557]
	TIME [epoch: 2.89 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664415792426025		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.664415792426025 | validation: 0.6841870319885106]
	TIME [epoch: 2.89 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516225833542535		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.6516225833542535 | validation: 0.689336612879839]
	TIME [epoch: 2.89 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665062760238685		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.665062760238685 | validation: 0.7025000306814315]
	TIME [epoch: 2.89 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758989544395346		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.6758989544395346 | validation: 0.6883219698799129]
	TIME [epoch: 2.89 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598195177343642		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.6598195177343642 | validation: 0.7071823272665887]
	TIME [epoch: 2.89 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707962256829804		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.6707962256829804 | validation: 0.6833845128529124]
	TIME [epoch: 2.89 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574556309400985		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.6574556309400985 | validation: 0.6607262826191989]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458454230403666		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.6458454230403666 | validation: 0.6907116795884823]
	TIME [epoch: 2.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462546261059794		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.6462546261059794 | validation: 0.67705426075042]
	TIME [epoch: 2.89 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638262023558465		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.638262023558465 | validation: 0.6912679770028016]
	TIME [epoch: 2.89 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699625485986991		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.699625485986991 | validation: 0.7030483550910757]
	TIME [epoch: 2.89 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388338006003704		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7388338006003704 | validation: 0.7203494788541871]
	TIME [epoch: 2.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479892774319845		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7479892774319845 | validation: 0.6656783748933871]
	TIME [epoch: 2.89 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679460785039217		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.679460785039217 | validation: 0.684844578703268]
	TIME [epoch: 2.89 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450967500872449		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.6450967500872449 | validation: 0.7067576561795961]
	TIME [epoch: 2.89 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461039963062701		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.6461039963062701 | validation: 0.6891295235575857]
	TIME [epoch: 2.89 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538560858341884		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.6538560858341884 | validation: 0.6744863590556561]
	TIME [epoch: 2.89 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398598552022675		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.6398598552022675 | validation: 0.649332735423126]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407917449464905		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.6407917449464905 | validation: 0.6794096846649709]
	TIME [epoch: 2.89 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408288973435498		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.6408288973435498 | validation: 0.6779933986011559]
	TIME [epoch: 2.89 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645886139208275		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.645886139208275 | validation: 0.7036961433138222]
	TIME [epoch: 2.89 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701728843349691		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.6701728843349691 | validation: 0.7251325513800232]
	TIME [epoch: 2.89 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109270914473903		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7109270914473903 | validation: 0.698381794641173]
	TIME [epoch: 2.89 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576021793480699		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.6576021793480699 | validation: 0.6832917097742449]
	TIME [epoch: 2.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380897574016337		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.6380897574016337 | validation: 0.6710931595117209]
	TIME [epoch: 2.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380689289024627		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.6380689289024627 | validation: 0.6821808969929388]
	TIME [epoch: 2.89 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457629392940203		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.6457629392940203 | validation: 0.6575676069608026]
	TIME [epoch: 2.89 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388609439222341		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.6388609439222341 | validation: 0.6788644092504772]
	TIME [epoch: 2.89 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395818021862755		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.6395818021862755 | validation: 0.6680908023747074]
	TIME [epoch: 2.89 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637934219436337		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.637934219436337 | validation: 0.6799604869608995]
	TIME [epoch: 2.89 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407408447372095		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6407408447372095 | validation: 0.6906199666906194]
	TIME [epoch: 2.89 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647509253863706		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.6647509253863706 | validation: 0.7265829496447824]
	TIME [epoch: 2.89 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7269060263455966		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7269060263455966 | validation: 0.7129606521850494]
	TIME [epoch: 2.89 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900131018089363		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.6900131018089363 | validation: 0.6902407283953216]
	TIME [epoch: 2.89 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459894303450324		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6459894303450324 | validation: 0.6841765307750819]
	TIME [epoch: 2.89 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345512181075487		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6345512181075487 | validation: 0.6604517658913767]
	TIME [epoch: 2.89 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315087209418833		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6315087209418833 | validation: 0.6616056884532847]
	TIME [epoch: 2.89 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386595448846102		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.6386595448846102 | validation: 0.6815672065427587]
	TIME [epoch: 2.89 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364738642161085		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.6364738642161085 | validation: 0.671482796533013]
	TIME [epoch: 2.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344514502262737		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.6344514502262737 | validation: 0.6658693034478433]
	TIME [epoch: 2.89 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419478222000835		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6419478222000835 | validation: 0.6598634614242969]
	TIME [epoch: 2.89 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375227592684075		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6375227592684075 | validation: 0.6948922425442176]
	TIME [epoch: 2.89 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491245337477868		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.6491245337477868 | validation: 0.7095234001311064]
	TIME [epoch: 2.89 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687481827704838		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6687481827704838 | validation: 0.7030319720019217]
	TIME [epoch: 2.89 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691604380357866		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.6691604380357866 | validation: 0.6913218434961259]
	TIME [epoch: 2.89 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67903720594971		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.67903720594971 | validation: 0.6433867701837269]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393463171217325		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6393463171217325 | validation: 0.6817703087338017]
	TIME [epoch: 2.89 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384232041541503		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6384232041541503 | validation: 0.6777747595123067]
	TIME [epoch: 2.9 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447840645527815		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6447840645527815 | validation: 0.6742562718335035]
	TIME [epoch: 2.89 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427771510470714		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6427771510470714 | validation: 0.6686160672039071]
	TIME [epoch: 2.89 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331698013431698		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6331698013431698 | validation: 0.6862548639900532]
	TIME [epoch: 2.89 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382810176081833		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.6382810176081833 | validation: 0.6567021106848026]
	TIME [epoch: 2.89 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457429722337408		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6457429722337408 | validation: 0.6936369623370828]
	TIME [epoch: 2.89 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556487606876334		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.6556487606876334 | validation: 0.7202618124454521]
	TIME [epoch: 2.9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835140011857843		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6835140011857843 | validation: 0.7186625889891566]
	TIME [epoch: 2.89 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001271976455486		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7001271976455486 | validation: 0.6548452180042785]
	TIME [epoch: 2.89 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317108438525145		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6317108438525145 | validation: 0.6615137125745469]
	TIME [epoch: 2.89 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403826233392224		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6403826233392224 | validation: 0.6834727953225039]
	TIME [epoch: 2.89 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446665331428046		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6446665331428046 | validation: 0.6687545424919411]
	TIME [epoch: 2.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290931148639065		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6290931148639065 | validation: 0.6616088077486515]
	TIME [epoch: 2.89 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634030674631522		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.634030674631522 | validation: 0.6703788024313457]
	TIME [epoch: 2.89 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286364989844989		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6286364989844989 | validation: 0.6687676701238578]
	TIME [epoch: 2.89 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429604613763169		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6429604613763169 | validation: 0.660798617404041]
	TIME [epoch: 2.89 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349136619908948		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6349136619908948 | validation: 0.659925296647359]
	TIME [epoch: 2.89 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320856037723069		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6320856037723069 | validation: 0.6801692114455163]
	TIME [epoch: 2.89 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363577626144005		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6363577626144005 | validation: 0.7084487848210703]
	TIME [epoch: 2.89 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959057810411909		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6959057810411909 | validation: 0.6705539071365433]
	TIME [epoch: 2.89 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499441801712544		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6499441801712544 | validation: 0.6536477208054736]
	TIME [epoch: 2.89 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451131114636885		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.6451131114636885 | validation: 0.6701526850230857]
	TIME [epoch: 2.89 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284561849631526		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6284561849631526 | validation: 0.6737107741050347]
	TIME [epoch: 2.89 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328755582294897		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.6328755582294897 | validation: 0.6678554978470933]
	TIME [epoch: 2.89 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522848476650219		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.6522848476650219 | validation: 0.679651328272465]
	TIME [epoch: 2.89 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454203885547471		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6454203885547471 | validation: 0.6480026355313427]
	TIME [epoch: 2.89 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387093368349506		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6387093368349506 | validation: 0.662231960657705]
	TIME [epoch: 2.89 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225841496287586		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.6225841496287586 | validation: 0.6721767350231678]
	TIME [epoch: 2.89 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259101686131511		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6259101686131511 | validation: 0.6618134705219529]
	TIME [epoch: 2.89 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619188066989116		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.619188066989116 | validation: 0.662087425331707]
	TIME [epoch: 2.89 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286675367042398		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.6286675367042398 | validation: 0.6514705618331252]
	TIME [epoch: 2.89 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379524832844596		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.6379524832844596 | validation: 0.6727899104119631]
	TIME [epoch: 2.89 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399083182392477		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6399083182392477 | validation: 0.661659030026558]
	TIME [epoch: 2.89 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345664220794804		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6345664220794804 | validation: 0.6561158880409015]
	TIME [epoch: 2.89 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200902353985336		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6200902353985336 | validation: 0.6614900269520918]
	TIME [epoch: 2.89 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418193531694627		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6418193531694627 | validation: 0.6773790483225698]
	TIME [epoch: 2.89 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385446273233687		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6385446273233687 | validation: 0.6923824516314592]
	TIME [epoch: 2.89 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662683005621753		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.662683005621753 | validation: 0.686917787019749]
	TIME [epoch: 2.89 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584315737670806		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6584315737670806 | validation: 0.6471068971030433]
	TIME [epoch: 2.88 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648977393103639		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.648977393103639 | validation: 0.6421743366088095]
	TIME [epoch: 2.89 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316567555003719		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6316567555003719 | validation: 0.6508036030330179]
	TIME [epoch: 2.89 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6207255025507555		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6207255025507555 | validation: 0.6652021894557587]
	TIME [epoch: 2.89 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268636103431346		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6268636103431346 | validation: 0.6738054455875746]
	TIME [epoch: 2.89 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6176239047927797		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6176239047927797 | validation: 0.6567950736274573]
	TIME [epoch: 2.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622449063232353		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.622449063232353 | validation: 0.6551090728162078]
	TIME [epoch: 2.89 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6137593968823144		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6137593968823144 | validation: 0.6653502321756648]
	TIME [epoch: 2.89 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189488983521894		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6189488983521894 | validation: 0.6449265516558645]
	TIME [epoch: 2.89 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624737033546814		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.624737033546814 | validation: 0.6744042213846444]
	TIME [epoch: 2.89 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261442655783375		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6261442655783375 | validation: 0.673208891948155]
	TIME [epoch: 2.88 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493644530051623		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6493644530051623 | validation: 0.6765754934355637]
	TIME [epoch: 2.88 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635814336456483		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.6635814336456483 | validation: 0.6518245167222956]
	TIME [epoch: 2.89 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406534265786598		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6406534265786598 | validation: 0.6312004141777955]
	TIME [epoch: 2.88 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174902225768483		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6174902225768483 | validation: 0.656022032762476]
	TIME [epoch: 2.89 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204857974057766		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6204857974057766 | validation: 0.6467983177242976]
	TIME [epoch: 2.89 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148061366237909		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6148061366237909 | validation: 0.6541540813006743]
	TIME [epoch: 2.89 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133570431740999		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6133570431740999 | validation: 0.6711466954199816]
	TIME [epoch: 2.89 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147302058323327		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6147302058323327 | validation: 0.6448934903137227]
	TIME [epoch: 2.89 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239439012304594		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6239439012304594 | validation: 0.6596718532385871]
	TIME [epoch: 2.9 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656546056366551		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.656546056366551 | validation: 0.6444958053228744]
	TIME [epoch: 2.89 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258811027054191		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6258811027054191 | validation: 0.6339042613947492]
	TIME [epoch: 2.89 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014735103814005		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6014735103814005 | validation: 0.6253479108652279]
	TIME [epoch: 279 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5995307121538835		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.5995307121538835 | validation: 0.64724195800111]
	TIME [epoch: 6.19 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021843521985596		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6021843521985596 | validation: 0.6737897241345228]
	TIME [epoch: 6.19 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6089866892776905		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6089866892776905 | validation: 0.6623442394979486]
	TIME [epoch: 6.18 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489413832323008		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6489413832323008 | validation: 0.6696123845069265]
	TIME [epoch: 6.18 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865513879340779		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6865513879340779 | validation: 0.6022110813271627]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5980171203862328		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.5980171203862328 | validation: 0.5904594588665305]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037615828331873		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6037615828331873 | validation: 0.6026858337709765]
	TIME [epoch: 6.24 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.576659994323403		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.576659994323403 | validation: 0.6056529861818446]
	TIME [epoch: 6.23 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506284185272985		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5506284185272985 | validation: 0.590423040492999]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5348058234647973		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5348058234647973 | validation: 1.634147553144629]
	TIME [epoch: 6.23 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9031553705144257		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.9031553705144257 | validation: 0.6623264349694429]
	TIME [epoch: 6.21 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985621806133		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6985621806133 | validation: 0.6375705981639825]
	TIME [epoch: 6.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917914765856421		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6917914765856421 | validation: 0.5623201089081921]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5479032394903952		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.5479032394903952 | validation: 0.5396151993628666]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132249754771698		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5132249754771698 | validation: 0.5306563498184633]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381628383789421		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.5381628383789421 | validation: 0.5188895913569805]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011394277213793		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5011394277213793 | validation: 0.5427347538594146]
	TIME [epoch: 6.22 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250925848718341		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.5250925848718341 | validation: 0.5240718953321316]
	TIME [epoch: 6.22 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001786572714934		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.5001786572714934 | validation: 0.5000040983544137]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49413074413155744		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.49413074413155744 | validation: 0.5100963712191641]
	TIME [epoch: 6.23 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47635248118937185		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.47635248118937185 | validation: 0.4943035366333133]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496692198133699		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.4496692198133699 | validation: 0.47081047337115334]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4457578975922138		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.4457578975922138 | validation: 0.490172487021332]
	TIME [epoch: 6.24 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4549641482016681		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4549641482016681 | validation: 0.4739683788491125]
	TIME [epoch: 6.23 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4324655142139661		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4324655142139661 | validation: 0.4398444611673643]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.429900289978952		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.429900289978952 | validation: 0.4659615390855688]
	TIME [epoch: 6.18 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46618280244542776		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.46618280244542776 | validation: 0.5489087334260286]
	TIME [epoch: 6.22 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487892367439939		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7487892367439939 | validation: 0.5234433408449585]
	TIME [epoch: 6.23 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4802100135995117		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.4802100135995117 | validation: 1.1334733157126708]
	TIME [epoch: 6.22 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.432910786262751		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.432910786262751 | validation: 0.5371500077419965]
	TIME [epoch: 6.23 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5646737406475557		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.5646737406475557 | validation: 0.5808342121349875]
	TIME [epoch: 6.21 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905690041192475		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.905690041192475 | validation: 0.5720874300447]
	TIME [epoch: 6.23 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330319581828053		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8330319581828053 | validation: 0.5365810857632006]
	TIME [epoch: 6.21 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5902928015603859		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.5902928015603859 | validation: 0.5210450025446057]
	TIME [epoch: 6.23 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5247228202428407		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.5247228202428407 | validation: 0.4923464808111634]
	TIME [epoch: 6.23 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021121950597357		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.5021121950597357 | validation: 0.4847718133946459]
	TIME [epoch: 6.24 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49070158614947756		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.49070158614947756 | validation: 0.4763819934450094]
	TIME [epoch: 6.22 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4651194976069589		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.4651194976069589 | validation: 0.4753151801436861]
	TIME [epoch: 6.24 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4704852484404184		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4704852484404184 | validation: 0.45963279276696184]
	TIME [epoch: 6.22 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44888518360634777		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.44888518360634777 | validation: 0.4794443071122489]
	TIME [epoch: 6.23 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4764715121252009		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.4764715121252009 | validation: 0.48837446617856595]
	TIME [epoch: 6.22 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47258121913810175		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.47258121913810175 | validation: 0.47199487403312024]
	TIME [epoch: 6.23 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4345068865024143		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.4345068865024143 | validation: 0.4556531865720329]
	TIME [epoch: 6.22 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45440700241837945		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.45440700241837945 | validation: 0.47416863725403186]
	TIME [epoch: 6.23 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469959606091768		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.4469959606091768 | validation: 0.44866279273721565]
	TIME [epoch: 6.22 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4218317446372671		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.4218317446372671 | validation: 0.4250839390134756]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4187216346497759		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.4187216346497759 | validation: 0.4331579404625394]
	TIME [epoch: 6.21 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4244378130149613		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.4244378130149613 | validation: 0.4479215135682368]
	TIME [epoch: 6.22 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4576262740673056		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4576262740673056 | validation: 0.4248921387705691]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136774380930809		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.4136774380930809 | validation: 0.4124464904314442]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4030504939675348		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.4030504939675348 | validation: 0.4138770249003658]
	TIME [epoch: 6.23 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014511947327655		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.4014511947327655 | validation: 0.4028167251538281]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4341926481399878		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.4341926481399878 | validation: 0.4392376463126315]
	TIME [epoch: 6.23 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5115020757467308		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5115020757467308 | validation: 0.5436142854618129]
	TIME [epoch: 6.23 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754177476082361		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6754177476082361 | validation: 0.4908996209400407]
	TIME [epoch: 6.22 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.512570143033652		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.512570143033652 | validation: 0.46264596650077827]
	TIME [epoch: 6.22 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46444084204934477		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.46444084204934477 | validation: 0.43071717478471644]
	TIME [epoch: 6.23 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40169018511364085		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.40169018511364085 | validation: 0.3999032203252298]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4030332588905526		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4030332588905526 | validation: 0.41566936993243947]
	TIME [epoch: 6.22 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39605013885908064		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.39605013885908064 | validation: 0.38488578480846586]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38023906361349474		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.38023906361349474 | validation: 0.3891349078933527]
	TIME [epoch: 6.22 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3813710032747617		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.3813710032747617 | validation: 0.38818115560426164]
	TIME [epoch: 6.22 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649631568683119		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.3649631568683119 | validation: 0.3711056666452819]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590565510886296		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.3590565510886296 | validation: 0.37410785952702175]
	TIME [epoch: 6.22 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578822786472854		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3578822786472854 | validation: 0.3626210347103587]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35578557189819876		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.35578557189819876 | validation: 0.3509623814530626]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459579514065579		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.3459579514065579 | validation: 0.35268308353816363]
	TIME [epoch: 6.19 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3541294107019117		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.3541294107019117 | validation: 0.410225982342498]
	TIME [epoch: 6.18 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44481768529533894		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.44481768529533894 | validation: 0.493781115754537]
	TIME [epoch: 6.19 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745495736361727		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7745495736361727 | validation: 0.48779055655482995]
	TIME [epoch: 6.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274927325611743		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6274927325611743 | validation: 0.40412000112568336]
	TIME [epoch: 6.18 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4273738805799786		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.4273738805799786 | validation: 0.36442556818711014]
	TIME [epoch: 6.19 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792423386386452		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.3792423386386452 | validation: 0.37132824368496214]
	TIME [epoch: 6.19 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35845709815078025		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.35845709815078025 | validation: 0.34192516363485376]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351719181340422		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.3351719181340422 | validation: 0.35550917702511237]
	TIME [epoch: 6.21 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38325892385430416		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.38325892385430416 | validation: 0.40874610541047485]
	TIME [epoch: 6.22 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45293391632718355		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.45293391632718355 | validation: 0.35541254593461574]
	TIME [epoch: 6.23 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3328016152862389		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.3328016152862389 | validation: 0.3476361697115757]
	TIME [epoch: 6.23 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3524060776879124		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3524060776879124 | validation: 0.39540903365260816]
	TIME [epoch: 6.22 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3945409068874236		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.3945409068874236 | validation: 0.3606382351680335]
	TIME [epoch: 6.23 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34968342375853567		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.34968342375853567 | validation: 0.402334441536796]
	TIME [epoch: 6.22 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46326049113271667		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.46326049113271667 | validation: 0.388579965521331]
	TIME [epoch: 6.23 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619948230043854		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.4619948230043854 | validation: 0.35517991111371217]
	TIME [epoch: 6.18 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3976183014093572		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.3976183014093572 | validation: 0.34131125277684027]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37632290236975025		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.37632290236975025 | validation: 0.34574063763641183]
	TIME [epoch: 6.22 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32098870993153533		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.32098870993153533 | validation: 0.3233443664484038]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317616451711275		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.317616451711275 | validation: 0.2987799264687318]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018839915427493		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3018839915427493 | validation: 0.290583239412783]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29412551041893387		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.29412551041893387 | validation: 0.3043704420993497]
	TIME [epoch: 6.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.287235581676258		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.287235581676258 | validation: 0.2886440898565728]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28516086807224533		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.28516086807224533 | validation: 0.3247698404616462]
	TIME [epoch: 6.21 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32730400553308014		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.32730400553308014 | validation: 0.30092311949875405]
	TIME [epoch: 6.21 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32733406864771225		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.32733406864771225 | validation: 0.3186367674023077]
	TIME [epoch: 6.21 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36017527890601925		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.36017527890601925 | validation: 0.31311133769041977]
	TIME [epoch: 6.21 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077028303404607		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3077028303404607 | validation: 0.28466096426306314]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098676115970839		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.3098676115970839 | validation: 0.3326311490368638]
	TIME [epoch: 6.22 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35312973942446746		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.35312973942446746 | validation: 0.2803101409774742]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28063081426367814		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.28063081426367814 | validation: 0.39645193686166863]
	TIME [epoch: 6.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5063270318760057		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.5063270318760057 | validation: 0.3703588428165301]
	TIME [epoch: 6.22 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47983976261072686		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.47983976261072686 | validation: 0.37763013758079245]
	TIME [epoch: 6.21 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763154681772494		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5763154681772494 | validation: 0.386711100444986]
	TIME [epoch: 6.22 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49372179071992206		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.49372179071992206 | validation: 0.35285326836900555]
	TIME [epoch: 6.22 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39273279489991914		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.39273279489991914 | validation: 0.2958180433521293]
	TIME [epoch: 6.21 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29564732956667233		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.29564732956667233 | validation: 0.27116920672737427]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28784244895194055		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.28784244895194055 | validation: 0.2950618974201488]
	TIME [epoch: 6.21 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28608270971362165		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.28608270971362165 | validation: 0.26181898251531244]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26971057521132846		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.26971057521132846 | validation: 0.25650906135407736]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2576540341209456		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.2576540341209456 | validation: 0.24606176895783982]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2479077875692396		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.2479077875692396 | validation: 0.2396604515990254]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2637642184739342		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.2637642184739342 | validation: 0.29083873546342104]
	TIME [epoch: 6.22 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2891901675718079		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2891901675718079 | validation: 0.25941701713967497]
	TIME [epoch: 6.21 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24296384281274988		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.24296384281274988 | validation: 0.2777512303419764]
	TIME [epoch: 6.21 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31215824120452973		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.31215824120452973 | validation: 0.32939085784926486]
	TIME [epoch: 6.22 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3944879163054626		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.3944879163054626 | validation: 0.3256194707673077]
	TIME [epoch: 6.21 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088273549638967		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.4088273549638967 | validation: 0.22199493264779974]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2414106414513177		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.2414106414513177 | validation: 0.327635341095984]
	TIME [epoch: 6.21 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3972190186607661		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3972190186607661 | validation: 0.3457447269155756]
	TIME [epoch: 6.22 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4138171452521664		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.4138171452521664 | validation: 0.4322190620559139]
	TIME [epoch: 6.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763596738917911		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.5763596738917911 | validation: 0.263613489634366]
	TIME [epoch: 6.21 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574273428730276		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.2574273428730276 | validation: 0.499215550739184]
	TIME [epoch: 6.21 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659563079994848		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.659563079994848 | validation: 0.263831213169334]
	TIME [epoch: 6.21 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881528410713519		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.2881528410713519 | validation: 0.30154333853061066]
	TIME [epoch: 6.21 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33048095472588174		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.33048095472588174 | validation: 0.24994622293561558]
	TIME [epoch: 6.21 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600770389323811		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.2600770389323811 | validation: 0.23830238306369855]
	TIME [epoch: 6.21 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24207491252511504		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.24207491252511504 | validation: 0.2264754601420124]
	TIME [epoch: 6.22 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357245179337643		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.2357245179337643 | validation: 0.22775906067552376]
	TIME [epoch: 6.21 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2384529573537477		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2384529573537477 | validation: 0.22215036514125544]
	TIME [epoch: 6.21 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22783364993011213		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.22783364993011213 | validation: 0.22412237695878345]
	TIME [epoch: 6.21 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2225352741221058		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.2225352741221058 | validation: 0.21519087124528974]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2199559131089604		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.2199559131089604 | validation: 0.21853073787661975]
	TIME [epoch: 6.21 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22070172056533868		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.22070172056533868 | validation: 0.20889425486621624]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2084735622396041		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.2084735622396041 | validation: 0.21540816650896694]
	TIME [epoch: 6.21 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21076207782572617		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.21076207782572617 | validation: 0.2014689451059396]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2084863370956559		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.2084863370956559 | validation: 0.19980905135545812]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058277616343828		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.2058277616343828 | validation: 0.20731758019572133]
	TIME [epoch: 6.17 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20655799598795405		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.20655799598795405 | validation: 0.19659067369169383]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20541103378417214		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.20541103378417214 | validation: 0.2078104660242942]
	TIME [epoch: 6.21 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20524899724016402		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.20524899724016402 | validation: 0.19569266397519566]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946550517806519		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.1946550517806519 | validation: 0.2071205848774295]
	TIME [epoch: 6.23 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19707427801078362		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.19707427801078362 | validation: 0.19502932299879772]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2100613513874254		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2100613513874254 | validation: 0.25856215839602376]
	TIME [epoch: 6.22 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26288068353014665		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.26288068353014665 | validation: 0.22050965628754282]
	TIME [epoch: 6.23 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19635622735427635		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.19635622735427635 | validation: 0.24822127961374935]
	TIME [epoch: 6.23 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775460225408045		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2775460225408045 | validation: 0.31567861932583596]
	TIME [epoch: 6.23 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3648634030523331		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.3648634030523331 | validation: 0.3126585725002893]
	TIME [epoch: 6.23 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555543502708373		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.3555543502708373 | validation: 0.2545937156860488]
	TIME [epoch: 6.23 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22079601496229287		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.22079601496229287 | validation: 0.333486648022133]
	TIME [epoch: 6.23 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42971863320545384		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.42971863320545384 | validation: 0.2895384870841003]
	TIME [epoch: 6.23 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849120219290848		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2849120219290848 | validation: 0.2976269094381495]
	TIME [epoch: 6.23 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3003884824267878		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.3003884824267878 | validation: 0.26762189129551894]
	TIME [epoch: 6.23 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25672099816708416		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.25672099816708416 | validation: 0.19530108885428388]
	TIME [epoch: 6.23 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19445876834021447		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.19445876834021447 | validation: 0.205091934669825]
	TIME [epoch: 6.24 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20444832426904294		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.20444832426904294 | validation: 0.20658135226858287]
	TIME [epoch: 6.22 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2006689599441549		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.2006689599441549 | validation: 0.20174241131871243]
	TIME [epoch: 6.23 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19060712868529892		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.19060712868529892 | validation: 0.18667613233599376]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1839029102478673		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.1839029102478673 | validation: 0.1782908122986614]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18312110611558818		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.18312110611558818 | validation: 0.1851283902397552]
	TIME [epoch: 6.23 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17767131851846166		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.17767131851846166 | validation: 0.1683386977611308]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17818154157461877		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.17818154157461877 | validation: 0.1826778782864627]
	TIME [epoch: 6.23 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18052253984419522		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.18052253984419522 | validation: 0.17639681620381778]
	TIME [epoch: 6.22 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16677483126021408		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.16677483126021408 | validation: 0.1721210707686274]
	TIME [epoch: 6.23 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17082499025664988		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.17082499025664988 | validation: 0.18543424725956947]
	TIME [epoch: 6.22 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17954889080128353		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.17954889080128353 | validation: 0.17038882626214988]
	TIME [epoch: 6.22 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16792988701409936		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.16792988701409936 | validation: 0.1701789022749176]
	TIME [epoch: 6.22 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16913209724638015		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.16913209724638015 | validation: 0.17708694873771103]
	TIME [epoch: 6.22 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17701553465810924		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.17701553465810924 | validation: 0.21496406376931942]
	TIME [epoch: 6.23 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21044060936046385		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.21044060936046385 | validation: 0.16526121976439445]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16428394291054393		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.16428394291054393 | validation: 0.19922060366674277]
	TIME [epoch: 6.23 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23801867420098394		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.23801867420098394 | validation: 0.30890652736835267]
	TIME [epoch: 6.23 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35402251567525045		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.35402251567525045 | validation: 0.3048232112919924]
	TIME [epoch: 6.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3451364060525824		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.3451364060525824 | validation: 0.26207784192214584]
	TIME [epoch: 6.23 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27135341121762524		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.27135341121762524 | validation: 0.16256160295678954]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15437229600477564		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.15437229600477564 | validation: 0.21046162873725027]
	TIME [epoch: 6.23 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762763168161497		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2762763168161497 | validation: 0.2448629531620326]
	TIME [epoch: 6.23 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28875901772702006		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.28875901772702006 | validation: 0.23331532657131732]
	TIME [epoch: 6.22 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28418512276220304		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.28418512276220304 | validation: 0.2105307870194788]
	TIME [epoch: 6.24 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21330099474750583		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.21330099474750583 | validation: 0.15410901781767838]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16555727787746663		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.16555727787746663 | validation: 0.16906948057317228]
	TIME [epoch: 6.23 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16689320627544527		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.16689320627544527 | validation: 0.15157512243470408]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15360294020844664		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.15360294020844664 | validation: 0.15110453368417004]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531684889713629		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.1531684889713629 | validation: 0.14996384037082433]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14920169265571315		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.14920169265571315 | validation: 0.14460431735902654]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517464766388003		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1517464766388003 | validation: 0.16657212395680177]
	TIME [epoch: 6.18 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610317477605404		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.1610317477605404 | validation: 0.14498079567032512]
	TIME [epoch: 6.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15575956191967158		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.15575956191967158 | validation: 0.15913604462127845]
	TIME [epoch: 6.17 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16126330474958273		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.16126330474958273 | validation: 0.14901312522328608]
	TIME [epoch: 6.17 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1569747848085852		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.1569747848085852 | validation: 0.1810928334897244]
	TIME [epoch: 6.17 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176221110450443		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.176221110450443 | validation: 0.1429286335992801]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388832710455946		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.1388832710455946 | validation: 0.13705074923812424]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375265750772805		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.1375265750772805 | validation: 0.13786705911402727]
	TIME [epoch: 6.21 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364013120129941		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.1364013120129941 | validation: 0.14266586467064177]
	TIME [epoch: 6.21 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14120628790767292		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.14120628790767292 | validation: 0.1476522267526949]
	TIME [epoch: 6.21 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1530776889434068		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.1530776889434068 | validation: 0.22696973221228386]
	TIME [epoch: 6.21 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21333265760672648		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.21333265760672648 | validation: 0.1562126309414933]
	TIME [epoch: 6.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485336929044022		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.1485336929044022 | validation: 0.15216877274849916]
	TIME [epoch: 6.21 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17222622225100917		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.17222622225100917 | validation: 0.24856234568987423]
	TIME [epoch: 6.21 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25748939045694574		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.25748939045694574 | validation: 0.19469220092875714]
	TIME [epoch: 6.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2123572571441816		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.2123572571441816 | validation: 0.15928737852291577]
	TIME [epoch: 6.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159695725968001		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.159695725968001 | validation: 0.14317769582162385]
	TIME [epoch: 6.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13752853837271137		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.13752853837271137 | validation: 0.13601549110334296]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12919907057277008		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.12919907057277008 | validation: 0.13045197185749854]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308279588243054		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.1308279588243054 | validation: 0.1233052814641479]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14047132431664763		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.14047132431664763 | validation: 0.12954788498721626]
	TIME [epoch: 6.23 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13482373805786466		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.13482373805786466 | validation: 0.17442010738300429]
	TIME [epoch: 6.22 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18032566966629632		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.18032566966629632 | validation: 0.133553675752398]
	TIME [epoch: 6.22 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13524352217733981		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.13524352217733981 | validation: 0.11960870627231308]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12338641285571278		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.12338641285571278 | validation: 0.11419787755384632]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12455427986546604		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.12455427986546604 | validation: 0.1416463028184098]
	TIME [epoch: 6.21 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448413747065999		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.1448413747065999 | validation: 0.13743517395090052]
	TIME [epoch: 6.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1399240346284772		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.1399240346284772 | validation: 0.18502123243658364]
	TIME [epoch: 6.21 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17382248202791473		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.17382248202791473 | validation: 0.10815105628984725]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11734803251710269		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.11734803251710269 | validation: 0.13228996966616333]
	TIME [epoch: 6.18 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14160083888406325		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.14160083888406325 | validation: 0.20898439794742366]
	TIME [epoch: 6.17 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2162238836321775		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2162238836321775 | validation: 0.15420812339939036]
	TIME [epoch: 6.17 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16628320486940548		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.16628320486940548 | validation: 0.15959207770154454]
	TIME [epoch: 6.17 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17458973901822744		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.17458973901822744 | validation: 0.2083625014273498]
	TIME [epoch: 6.17 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20826294654932476		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.20826294654932476 | validation: 0.13066665082613668]
	TIME [epoch: 6.16 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14554227924903257		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.14554227924903257 | validation: 0.15716150913132254]
	TIME [epoch: 6.17 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16829012653580605		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.16829012653580605 | validation: 0.14454951924542853]
	TIME [epoch: 6.17 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15017130361142073		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.15017130361142073 | validation: 0.1321886044729233]
	TIME [epoch: 6.17 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12756848331804796		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.12756848331804796 | validation: 0.1294985431436543]
	TIME [epoch: 6.18 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14589141263415367		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.14589141263415367 | validation: 0.15173016800355119]
	TIME [epoch: 6.17 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14528425203681203		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.14528425203681203 | validation: 0.11158550640619708]
	TIME [epoch: 6.16 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903018461094053		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.11903018461094053 | validation: 0.12813371471811139]
	TIME [epoch: 6.17 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13798747086720936		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.13798747086720936 | validation: 0.15295607935882186]
	TIME [epoch: 6.18 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496253087368284		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.1496253087368284 | validation: 0.11592726362845081]
	TIME [epoch: 6.17 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12303264997256505		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.12303264997256505 | validation: 0.11675898672854966]
	TIME [epoch: 6.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12409431033789092		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.12409431033789092 | validation: 0.1253484405445802]
	TIME [epoch: 6.16 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12913766218383235		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.12913766218383235 | validation: 0.1017681206733963]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10774143847983765		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.10774143847983765 | validation: 0.09775245129051824]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10671417785471318		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.10671417785471318 | validation: 0.11901788108333022]
	TIME [epoch: 6.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11313660459376007		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.11313660459376007 | validation: 0.10215454797497117]
	TIME [epoch: 6.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1172629881984992		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1172629881984992 | validation: 0.11362098468178053]
	TIME [epoch: 6.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1151118370704765		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.1151118370704765 | validation: 0.10627002179648942]
	TIME [epoch: 6.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11905453393543881		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.11905453393543881 | validation: 0.13656678151393972]
	TIME [epoch: 6.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14322712105908533		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.14322712105908533 | validation: 0.10291896404361092]
	TIME [epoch: 6.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10861808579041615		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.10861808579041615 | validation: 0.09653863362356502]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10344423541073133		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.10344423541073133 | validation: 0.089739091873208]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155294032340695		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.10155294032340695 | validation: 0.09337645339175127]
	TIME [epoch: 6.21 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172188497133786		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.10172188497133786 | validation: 0.09706417784376845]
	TIME [epoch: 6.21 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405261011902052		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.10405261011902052 | validation: 0.10481404312541827]
	TIME [epoch: 6.21 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12261190532061317		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.12261190532061317 | validation: 0.19692040044346784]
	TIME [epoch: 6.21 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21079423082528906		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.21079423082528906 | validation: 0.1743221091617157]
	TIME [epoch: 6.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16577571479404868		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.16577571479404868 | validation: 0.10669532338120931]
	TIME [epoch: 6.21 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270620058278155		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.1270620058278155 | validation: 0.14535602514106039]
	TIME [epoch: 6.21 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14503843016073822		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.14503843016073822 | validation: 0.1082712025079085]
	TIME [epoch: 6.21 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217277501687288		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.1217277501687288 | validation: 0.12235828824998128]
	TIME [epoch: 6.21 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12526111128679346		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.12526111128679346 | validation: 0.09779664165129716]
	TIME [epoch: 6.21 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029317366021338		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.10029317366021338 | validation: 0.09284850237595133]
	TIME [epoch: 6.21 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10790920961915078		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.10790920961915078 | validation: 0.1230217732465535]
	TIME [epoch: 6.21 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13272184996053565		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.13272184996053565 | validation: 0.08683038724330806]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304484396085674		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.10304484396085674 | validation: 0.09319385247485888]
	TIME [epoch: 6.17 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849090747033437		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.09849090747033437 | validation: 0.10970075388569103]
	TIME [epoch: 6.16 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11137591293209972		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.11137591293209972 | validation: 0.10558713641065257]
	TIME [epoch: 6.19 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11905367238129526		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.11905367238129526 | validation: 0.1397769604105386]
	TIME [epoch: 6.16 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14407619953416897		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.14407619953416897 | validation: 0.09087058013567662]
	TIME [epoch: 6.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09804107683949578		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.09804107683949578 | validation: 0.10175138504973834]
	TIME [epoch: 6.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12799100863715848		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.12799100863715848 | validation: 0.19417761933543368]
	TIME [epoch: 6.17 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20899614758181195		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.20899614758181195 | validation: 0.15742301397826344]
	TIME [epoch: 6.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16154628656922795		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.16154628656922795 | validation: 0.10674495169840037]
	TIME [epoch: 6.21 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127969055549728		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.1127969055549728 | validation: 0.10374706933232819]
	TIME [epoch: 6.21 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603238384346057		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.11603238384346057 | validation: 0.11646173555489003]
	TIME [epoch: 6.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11707105575034493		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.11707105575034493 | validation: 0.0881703643042396]
	TIME [epoch: 6.21 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10896459065242996		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.10896459065242996 | validation: 0.0837234952359196]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10000945922467501		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.10000945922467501 | validation: 0.08403772391770362]
	TIME [epoch: 6.21 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584336073319343		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.09584336073319343 | validation: 0.07892785496488876]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431186624029358		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.09431186624029358 | validation: 0.07903031526946576]
	TIME [epoch: 6.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09709900263070953		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.09709900263070953 | validation: 0.12385706856679862]
	TIME [epoch: 6.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11780181954587389		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.11780181954587389 | validation: 0.1021897340588685]
	TIME [epoch: 6.21 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11538000953425394		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.11538000953425394 | validation: 0.13683922438127957]
	TIME [epoch: 6.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13914477886489887		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.13914477886489887 | validation: 0.08362700602217132]
	TIME [epoch: 6.21 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09569093309553185		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.09569093309553185 | validation: 0.08337687368718456]
	TIME [epoch: 6.21 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09749246448789432		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.09749246448789432 | validation: 0.13210102258267017]
	TIME [epoch: 6.22 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12863117269504495		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.12863117269504495 | validation: 0.09485221066532076]
	TIME [epoch: 6.21 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704218239545471		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.10704218239545471 | validation: 0.0993288264649761]
	TIME [epoch: 6.21 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09605698923572273		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.09605698923572273 | validation: 0.08613132800868578]
	TIME [epoch: 6.22 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446039223044371		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.09446039223044371 | validation: 0.08742685732911695]
	TIME [epoch: 6.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09436511893816484		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.09436511893816484 | validation: 0.07943226672565923]
	TIME [epoch: 6.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08749755877352061		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.08749755877352061 | validation: 0.08162640898214504]
	TIME [epoch: 6.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09159851161970732		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.09159851161970732 | validation: 0.09523306992321373]
	TIME [epoch: 6.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000637443534794		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.11000637443534794 | validation: 0.16025494265615237]
	TIME [epoch: 6.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15651389520970535		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.15651389520970535 | validation: 0.09822815087401021]
	TIME [epoch: 6.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10614333864813101		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.10614333864813101 | validation: 0.10886883381130244]
	TIME [epoch: 6.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289823997612682		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.1289823997612682 | validation: 0.1880767204661556]
	TIME [epoch: 6.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17437332261608154		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.17437332261608154 | validation: 0.11244139970116657]
	TIME [epoch: 6.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11870354719538247		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.11870354719538247 | validation: 0.09938881698084322]
	TIME [epoch: 6.21 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11359342230293105		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.11359342230293105 | validation: 0.09684235566521326]
	TIME [epoch: 6.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396435018285473		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.10396435018285473 | validation: 0.07954229828722612]
	TIME [epoch: 6.21 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09164913588911935		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.09164913588911935 | validation: 0.08282614742715466]
	TIME [epoch: 6.21 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08596929462029454		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.08596929462029454 | validation: 0.08670005769686952]
	TIME [epoch: 6.21 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066601043737359		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.09066601043737359 | validation: 0.08329567182976629]
	TIME [epoch: 6.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08633268893630212		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.08633268893630212 | validation: 0.07912532397972527]
	TIME [epoch: 6.21 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08510242362984449		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.08510242362984449 | validation: 0.07687087642807247]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0842944517717999		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.0842944517717999 | validation: 0.07382049918153775]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08380248953208286		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.08380248953208286 | validation: 0.07488741719176265]
	TIME [epoch: 6.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434870077068687		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.08434870077068687 | validation: 0.07719078710573742]
	TIME [epoch: 6.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08499664201222754		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.08499664201222754 | validation: 0.07730106242200041]
	TIME [epoch: 6.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09500899157751783		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.09500899157751783 | validation: 0.14450956703138337]
	TIME [epoch: 6.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507746068710675		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1507746068710675 | validation: 0.09559649922861643]
	TIME [epoch: 6.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004369526936045		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.1004369526936045 | validation: 0.09371452709373611]
	TIME [epoch: 286 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11074184490135298		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.11074184490135298 | validation: 0.19476451622767055]
	TIME [epoch: 13.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.200996232109741		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.200996232109741 | validation: 0.09663133066620629]
	TIME [epoch: 13.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11075612462791004		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.11075612462791004 | validation: 0.12427385509088103]
	TIME [epoch: 13.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147900874520943		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.147900874520943 | validation: 0.1599657704589529]
	TIME [epoch: 13.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15919973034975965		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.15919973034975965 | validation: 0.11551738760824093]
	TIME [epoch: 13.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11338528150972557		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.11338528150972557 | validation: 0.09309591023662805]
	TIME [epoch: 13.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11166417104177731		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.11166417104177731 | validation: 0.08135770685281696]
	TIME [epoch: 13.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08789563518664044		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.08789563518664044 | validation: 0.07791312358534007]
	TIME [epoch: 13.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08661877767700583		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.08661877767700583 | validation: 0.08008217997380951]
	TIME [epoch: 13.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303286001950756		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.09303286001950756 | validation: 0.08457931622348847]
	TIME [epoch: 13.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205245086675676		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.09205245086675676 | validation: 0.0766038165615835]
	TIME [epoch: 13.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838160320969244		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.0838160320969244 | validation: 0.07402873800747695]
	TIME [epoch: 13.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880723680142784		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.0880723680142784 | validation: 0.09790049625960033]
	TIME [epoch: 13.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039732996815432		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.1039732996815432 | validation: 0.07251172943439897]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08251378236410123		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.08251378236410123 | validation: 0.07733091003242441]
	TIME [epoch: 13.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0830944155679066		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.0830944155679066 | validation: 0.10491710223190474]
	TIME [epoch: 13.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1017001989213148		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1017001989213148 | validation: 0.09036336482399387]
	TIME [epoch: 13.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1107323771043082		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.1107323771043082 | validation: 0.10861017246387253]
	TIME [epoch: 13.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10888350930758481		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.10888350930758481 | validation: 0.09023009757826961]
	TIME [epoch: 13.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08763103241361557		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.08763103241361557 | validation: 0.09654652374787741]
	TIME [epoch: 13.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964482228958321		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.10964482228958321 | validation: 0.11481189146559939]
	TIME [epoch: 13.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11660077896958225		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.11660077896958225 | validation: 0.08013005118877614]
	TIME [epoch: 13.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798614384283804		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.08798614384283804 | validation: 0.07853448653326028]
	TIME [epoch: 13.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876815777912374		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.08876815777912374 | validation: 0.0944583647239877]
	TIME [epoch: 13.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09236187268900486		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09236187268900486 | validation: 0.07085388938883798]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08163799446475857		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.08163799446475857 | validation: 0.07191618448057306]
	TIME [epoch: 13.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07897419874727907		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.07897419874727907 | validation: 0.07188085720277584]
	TIME [epoch: 13.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07618951256083498		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.07618951256083498 | validation: 0.07173603175283383]
	TIME [epoch: 13.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07957587115155444		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.07957587115155444 | validation: 0.06716400404879794]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08301744062053279		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.08301744062053279 | validation: 0.09417884037149765]
	TIME [epoch: 13.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094456675018362		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.094456675018362 | validation: 0.07115721620999126]
	TIME [epoch: 13.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08472814734948117		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.08472814734948117 | validation: 0.09260720061162261]
	TIME [epoch: 13.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08953460817657737		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.08953460817657737 | validation: 0.07451625023911355]
	TIME [epoch: 13.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09021722925948045		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.09021722925948045 | validation: 0.10233253826908213]
	TIME [epoch: 13.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584888054624582		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.09584888054624582 | validation: 0.08009215824584623]
	TIME [epoch: 13.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08885184646684698		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.08885184646684698 | validation: 0.09231265323205844]
	TIME [epoch: 13.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481947065556884		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.08481947065556884 | validation: 0.07050778460300858]
	TIME [epoch: 13.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777234000856067		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.0777234000856067 | validation: 0.0750224229085796]
	TIME [epoch: 13.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0799481771729803		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.0799481771729803 | validation: 0.0744661869112674]
	TIME [epoch: 13.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08273768333441343		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.08273768333441343 | validation: 0.08935247458470981]
	TIME [epoch: 13.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09201743263121147		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.09201743263121147 | validation: 0.08141225507098482]
	TIME [epoch: 13.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876255244386304		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.0876255244386304 | validation: 0.09780420290048585]
	TIME [epoch: 13.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10212695735526776		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.10212695735526776 | validation: 0.06941217409041764]
	TIME [epoch: 13.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07977020831020466		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.07977020831020466 | validation: 0.07182964523682642]
	TIME [epoch: 13.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07641150701650151		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.07641150701650151 | validation: 0.0887807174795166]
	TIME [epoch: 13.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08577860714327529		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.08577860714327529 | validation: 0.07600859496248036]
	TIME [epoch: 13.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09096233846274335		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.09096233846274335 | validation: 0.1098594676261822]
	TIME [epoch: 13.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10639886219783459		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.10639886219783459 | validation: 0.07330026700199908]
	TIME [epoch: 13.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07637048262926105		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.07637048262926105 | validation: 0.06862379501128962]
	TIME [epoch: 13.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07180695906057931		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.07180695906057931 | validation: 0.0776386177613516]
	TIME [epoch: 13.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07521889269120534		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.07521889269120534 | validation: 0.06573335232554399]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07510593960242061		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.07510593960242061 | validation: 0.07484668466328949]
	TIME [epoch: 13.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07528820238893381		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.07528820238893381 | validation: 0.07370413225273674]
	TIME [epoch: 13.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08776259214945033		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.08776259214945033 | validation: 0.125825774071688]
	TIME [epoch: 13.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12176712486927806		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.12176712486927806 | validation: 0.07790705242819541]
	TIME [epoch: 13.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243060092056269		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.08243060092056269 | validation: 0.07447120070705142]
	TIME [epoch: 13.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07506499032286883		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.07506499032286883 | validation: 0.10145685258071727]
	TIME [epoch: 13.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08532476188523382		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.08532476188523382 | validation: 0.08055306804072845]
	TIME [epoch: 13.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679448423302205		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.09679448423302205 | validation: 0.12578473366123824]
	TIME [epoch: 13.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096410241687016		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.1096410241687016 | validation: 0.07901659854365681]
	TIME [epoch: 13.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07693926023031225		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.07693926023031225 | validation: 0.09621189669189018]
	TIME [epoch: 13.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11971275219430073		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.11971275219430073 | validation: 0.135621020263027]
	TIME [epoch: 13.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12290070512672568		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.12290070512672568 | validation: 0.09916562271966822]
	TIME [epoch: 13.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022524734357049		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.09022524734357049 | validation: 0.08229459539587236]
	TIME [epoch: 13.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887926100738769		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.08887926100738769 | validation: 0.08203013611292156]
	TIME [epoch: 13.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014397185436875		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.08014397185436875 | validation: 0.06965851492881135]
	TIME [epoch: 13.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07362921196826537		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.07362921196826537 | validation: 0.06826444489874726]
	TIME [epoch: 13.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07664031435239822		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.07664031435239822 | validation: 0.07270244042975403]
	TIME [epoch: 13.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07588812504639783		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.07588812504639783 | validation: 0.06327809304848495]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242217345660958		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.07242217345660958 | validation: 0.06635923122976066]
	TIME [epoch: 13.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07238402090983036		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07238402090983036 | validation: 0.07768858586380295]
	TIME [epoch: 13.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339329538199951		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.07339329538199951 | validation: 0.07240944414697961]
	TIME [epoch: 13.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709704512380044		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.0709704512380044 | validation: 0.0772040585458919]
	TIME [epoch: 13.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484668747628828		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.07484668747628828 | validation: 0.07165973216101222]
	TIME [epoch: 13.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076341441449353		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.08076341441449353 | validation: 0.11014027682664605]
	TIME [epoch: 13.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10120223476412625		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.10120223476412625 | validation: 0.07695794852718509]
	TIME [epoch: 13.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092508367878974		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.08092508367878974 | validation: 0.07197654463805155]
	TIME [epoch: 13.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095701353207982		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.07095701353207982 | validation: 0.06982418635586732]
	TIME [epoch: 13.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033869347768767		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.07033869347768767 | validation: 0.06359732293235508]
	TIME [epoch: 13.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712166228274633		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.0712166228274633 | validation: 0.06877764290325672]
	TIME [epoch: 13.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226931328319468		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07226931328319468 | validation: 0.0701661650821802]
	TIME [epoch: 13.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265594055636411		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.07265594055636411 | validation: 0.06333170585061966]
	TIME [epoch: 13.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0721431204280115		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.0721431204280115 | validation: 0.07778294550955]
	TIME [epoch: 13.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527296488741804		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.07527296488741804 | validation: 0.07439569757844282]
	TIME [epoch: 13.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08307730084080042		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.08307730084080042 | validation: 0.1194340272070809]
	TIME [epoch: 13.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10880790999524904		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.10880790999524904 | validation: 0.06619170451520943]
	TIME [epoch: 13.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07382773905397431		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.07382773905397431 | validation: 0.07009999704923925]
	TIME [epoch: 13.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031088988519174		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.07031088988519174 | validation: 0.09089576130952493]
	TIME [epoch: 13.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936953554733701		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.07936953554733701 | validation: 0.07187994002125082]
	TIME [epoch: 13.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790398781195756		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.07790398781195756 | validation: 0.09148515526391021]
	TIME [epoch: 13.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07830692127250806		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.07830692127250806 | validation: 0.06555229333438381]
	TIME [epoch: 13.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07654661042562076		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.07654661042562076 | validation: 0.09514723744424843]
	TIME [epoch: 13.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620278958588229		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.08620278958588229 | validation: 0.06419408849120657]
	TIME [epoch: 13.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458938888835974		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07458938888835974 | validation: 0.07272942757075551]
	TIME [epoch: 13.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784331923863836		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.06784331923863836 | validation: 0.06348849404606681]
	TIME [epoch: 13.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06605599057438821		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.06605599057438821 | validation: 0.07492674996130481]
	TIME [epoch: 13.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269177663491071		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.07269177663491071 | validation: 0.06806700450385175]
	TIME [epoch: 13.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06954801865252584		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.06954801865252584 | validation: 0.08285999095732413]
	TIME [epoch: 13.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186166024496947		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.07186166024496947 | validation: 0.07159499547775121]
	TIME [epoch: 13.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785629714642874		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.0785629714642874 | validation: 0.12191769489946816]
	TIME [epoch: 13.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10725053345631676		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.10725053345631676 | validation: 0.07991219002447267]
	TIME [epoch: 13.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07197852576628372		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.07197852576628372 | validation: 0.0732631155055081]
	TIME [epoch: 13.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0740826642166617		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.0740826642166617 | validation: 0.11539125413306746]
	TIME [epoch: 13.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09844720884084293		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.09844720884084293 | validation: 0.06705792715502257]
	TIME [epoch: 13.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225651346069728		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07225651346069728 | validation: 0.06660893403255867]
	TIME [epoch: 13.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641769590263941		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.0641769590263941 | validation: 0.0765779720587553]
	TIME [epoch: 13.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06941977711249334		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.06941977711249334 | validation: 0.060900823388785386]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724243648386785		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.0724243648386785 | validation: 0.08746319298279485]
	TIME [epoch: 13.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08440566867732159		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.08440566867732159 | validation: 0.06974156661868279]
	TIME [epoch: 13.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889419498248579		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.06889419498248579 | validation: 0.07518889375900555]
	TIME [epoch: 13.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06803874217563513		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.06803874217563513 | validation: 0.07037158389480373]
	TIME [epoch: 13.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06660105839781931		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.06660105839781931 | validation: 0.07139481834528698]
	TIME [epoch: 13.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441027143657153		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.06441027143657153 | validation: 0.06772207650141499]
	TIME [epoch: 13.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06554931905846799		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.06554931905846799 | validation: 0.06655410433449913]
	TIME [epoch: 13.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330879890300313		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.06330879890300313 | validation: 0.06251849396003521]
	TIME [epoch: 13.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06878251895969296		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.06878251895969296 | validation: 0.10163784313855447]
	TIME [epoch: 13.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09413597955180397		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.09413597955180397 | validation: 0.08159099775036682]
	TIME [epoch: 13.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08321059202200529		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.08321059202200529 | validation: 0.08555037318538929]
	TIME [epoch: 13.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07683170588900354		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.07683170588900354 | validation: 0.06300620391651278]
	TIME [epoch: 13.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810562691163576		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.06810562691163576 | validation: 0.06382577952278004]
	TIME [epoch: 13.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646196330970874		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.0646196330970874 | validation: 0.060686163622125394]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061327669034594606		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.061327669034594606 | validation: 0.06275215609308508]
	TIME [epoch: 13.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060666098258238876		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.060666098258238876 | validation: 0.06353710513002457]
	TIME [epoch: 13.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783614887390918		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.06783614887390918 | validation: 0.08962606686772451]
	TIME [epoch: 13.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458250522629373		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07458250522629373 | validation: 0.07435881464327547]
	TIME [epoch: 13.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126075293816839		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.09126075293816839 | validation: 0.115733117263802]
	TIME [epoch: 13.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10744192842378969		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10744192842378969 | validation: 0.07897670652198091]
	TIME [epoch: 13.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024546362587382		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07024546362587382 | validation: 0.07624148033188116]
	TIME [epoch: 13.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09477261539959664		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.09477261539959664 | validation: 0.09921397029756339]
	TIME [epoch: 13.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0853628711653154		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.0853628711653154 | validation: 0.07206947045806628]
	TIME [epoch: 13.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06893201335562367		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.06893201335562367 | validation: 0.06551731223712472]
	TIME [epoch: 13.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708475054266094		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.0708475054266094 | validation: 0.06749460340049136]
	TIME [epoch: 13.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06501935553851229		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.06501935553851229 | validation: 0.06719733780783625]
	TIME [epoch: 13.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06158934133564977		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.06158934133564977 | validation: 0.0668993469380285]
	TIME [epoch: 13.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07021715807985096		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.07021715807985096 | validation: 0.06277271361872537]
	TIME [epoch: 13.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657643594038485		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.0657643594038485 | validation: 0.06337515374819574]
	TIME [epoch: 13.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06116814419532379		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.06116814419532379 | validation: 0.0570145215345405]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644375036372666		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.0644375036372666 | validation: 0.06193206318618527]
	TIME [epoch: 13.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445258110176776		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06445258110176776 | validation: 0.07161969437348256]
	TIME [epoch: 13.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591999018773521		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.06591999018773521 | validation: 0.06358892554016818]
	TIME [epoch: 13.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584236308855189		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06584236308855189 | validation: 0.08468066876441956]
	TIME [epoch: 13.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07082074814856162		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.07082074814856162 | validation: 0.06741298532466858]
	TIME [epoch: 13.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07123175306010675		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07123175306010675 | validation: 0.0790980172798457]
	TIME [epoch: 13.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07188954309229473		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.07188954309229473 | validation: 0.06114650186397307]
	TIME [epoch: 13.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06370246528327239		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06370246528327239 | validation: 0.059799368146109935]
	TIME [epoch: 13.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06072407635853286		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.06072407635853286 | validation: 0.07243177575395283]
	TIME [epoch: 13.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456744411695288		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06456744411695288 | validation: 0.06775579568340702]
	TIME [epoch: 13.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06506476580913147		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.06506476580913147 | validation: 0.07754326920032889]
	TIME [epoch: 13.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07040475439366724		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.07040475439366724 | validation: 0.07034052961096923]
	TIME [epoch: 13.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495167348913795		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.07495167348913795 | validation: 0.10118230928437262]
	TIME [epoch: 13.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07861141287840816		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.07861141287840816 | validation: 0.058884362628426494]
	TIME [epoch: 13.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061606745874937		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.061606745874937 | validation: 0.06063582141620824]
	TIME [epoch: 13.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06932330464380913		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.06932330464380913 | validation: 0.09764554857858465]
	TIME [epoch: 13.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640427876627383		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.07640427876627383 | validation: 0.06660417846549954]
	TIME [epoch: 13.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06273977240803888		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06273977240803888 | validation: 0.06643662864414994]
	TIME [epoch: 13.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06338836003679109		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.06338836003679109 | validation: 0.06841475418885239]
	TIME [epoch: 13.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06384550496073037		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.06384550496073037 | validation: 0.06090024760057269]
	TIME [epoch: 13.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06834578116315694		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.06834578116315694 | validation: 0.07703598765587559]
	TIME [epoch: 13.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06684883018177416		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.06684883018177416 | validation: 0.07016457220504914]
	TIME [epoch: 13.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671927723434699		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.07671927723434699 | validation: 0.08804479190644673]
	TIME [epoch: 13.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075332726955817		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.075332726955817 | validation: 0.06814758435726355]
	TIME [epoch: 13.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06517636641187688		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06517636641187688 | validation: 0.05495426737822304]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061650149543390834		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.061650149543390834 | validation: 0.07264236333053845]
	TIME [epoch: 13.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06615193846467768		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06615193846467768 | validation: 0.06932511353603575]
	TIME [epoch: 13.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887227212390264		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06887227212390264 | validation: 0.08172465910899304]
	TIME [epoch: 13.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177280095474219		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.07177280095474219 | validation: 0.06692617341301814]
	TIME [epoch: 13.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228875427102332		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.06228875427102332 | validation: 0.06456009611749679]
	TIME [epoch: 13.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060657715514638395		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.060657715514638395 | validation: 0.0590793858750118]
	TIME [epoch: 13.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06174466274939255		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06174466274939255 | validation: 0.0642659513178842]
	TIME [epoch: 13.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06012316817138595		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06012316817138595 | validation: 0.061279592534846355]
	TIME [epoch: 13.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059647961396789316		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.059647961396789316 | validation: 0.06016116293713164]
	TIME [epoch: 13.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061046285218818996		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.061046285218818996 | validation: 0.06259244711730744]
	TIME [epoch: 13.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06174453051916757		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.06174453051916757 | validation: 0.055584900876364654]
	TIME [epoch: 13.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058884587024701195		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.058884587024701195 | validation: 0.058916542819708095]
	TIME [epoch: 13.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05833153759920288		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05833153759920288 | validation: 0.0641434204372091]
	TIME [epoch: 13.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573984374736326		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.0573984374736326 | validation: 0.05923108021964725]
	TIME [epoch: 13.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05919255196775737		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.05919255196775737 | validation: 0.06693582100166867]
	TIME [epoch: 13.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086178356667797		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.06086178356667797 | validation: 0.06347484001661027]
	TIME [epoch: 13.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148861335884116		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.07148861335884116 | validation: 0.12238259711795277]
	TIME [epoch: 13.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940013466977654		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.0940013466977654 | validation: 0.05680328528295501]
	TIME [epoch: 13.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659681176390426		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.0659681176390426 | validation: 0.058144467760322116]
	TIME [epoch: 13.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05870998426931929		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.05870998426931929 | validation: 0.06744446049987733]
	TIME [epoch: 13.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06504507028720367		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06504507028720367 | validation: 0.062043406431266615]
	TIME [epoch: 13.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07385398623707314		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07385398623707314 | validation: 0.08303154559406688]
	TIME [epoch: 13.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040716021083139		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.08040716021083139 | validation: 0.06436130982773884]
	TIME [epoch: 13.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06082892283882075		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.06082892283882075 | validation: 0.06134161028420873]
	TIME [epoch: 13.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938811057018113		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.06938811057018113 | validation: 0.09517934172569072]
	TIME [epoch: 13.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07977801382187785		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07977801382187785 | validation: 0.06327467757909526]
	TIME [epoch: 13.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061723324624319995		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.061723324624319995 | validation: 0.05682278500200506]
	TIME [epoch: 13.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05981085845572299		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.05981085845572299 | validation: 0.07841803171902312]
	TIME [epoch: 13.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06884233988257017		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06884233988257017 | validation: 0.06061154263920975]
	TIME [epoch: 13.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606124745538011		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0606124745538011 | validation: 0.06851246206791232]
	TIME [epoch: 13.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05976585335230222		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.05976585335230222 | validation: 0.05865188871695542]
	TIME [epoch: 13.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060014649716192164		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.060014649716192164 | validation: 0.06563041454113457]
	TIME [epoch: 13.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917963031021389		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.05917963031021389 | validation: 0.05844060401302296]
	TIME [epoch: 13.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748804128930639		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.05748804128930639 | validation: 0.05823059223551265]
	TIME [epoch: 13.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05483801326802389		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.05483801326802389 | validation: 0.05812519570902596]
	TIME [epoch: 13.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05706652222020691		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.05706652222020691 | validation: 0.05575260459610291]
	TIME [epoch: 13.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294775527740287		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.06294775527740287 | validation: 0.09067916871616831]
	TIME [epoch: 13.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821588233003271		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.07821588233003271 | validation: 0.05762758835931593]
	TIME [epoch: 13.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984146522458689		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.05984146522458689 | validation: 0.06289515785753805]
	TIME [epoch: 13.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06204943808011373		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.06204943808011373 | validation: 0.07590176186248536]
	TIME [epoch: 13.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06269784576572965		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06269784576572965 | validation: 0.06001025953825151]
	TIME [epoch: 13.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06380413772971114		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.06380413772971114 | validation: 0.09177098045141302]
	TIME [epoch: 13.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07568267932246728		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.07568267932246728 | validation: 0.060667455109388195]
	TIME [epoch: 13.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06081071671054224		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06081071671054224 | validation: 0.05683095873279579]
	TIME [epoch: 13.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531573432380284		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05531573432380284 | validation: 0.06055140551705088]
	TIME [epoch: 13.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598923909187943		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.0598923909187943 | validation: 0.059205059000935645]
	TIME [epoch: 13.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841072566902856		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06841072566902856 | validation: 0.08538311744399311]
	TIME [epoch: 13.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07460216049274136		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.07460216049274136 | validation: 0.0652409505408161]
	TIME [epoch: 13.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05800351178640408		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05800351178640408 | validation: 0.05690726827677511]
	TIME [epoch: 13.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371188217194826		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06371188217194826 | validation: 0.06810109753175729]
	TIME [epoch: 13.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06729078969563994		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.06729078969563994 | validation: 0.06436002146446006]
	TIME [epoch: 13.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06550099424497838		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06550099424497838 | validation: 0.06886166854130177]
	TIME [epoch: 13.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06397382670196373		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06397382670196373 | validation: 0.05864646300057933]
	TIME [epoch: 13.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05479003373438593		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05479003373438593 | validation: 0.05563995287703508]
	TIME [epoch: 13.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748598520229832		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.05748598520229832 | validation: 0.05983214276588529]
	TIME [epoch: 13.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060520858918723726		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.060520858918723726 | validation: 0.055422196724455725]
	TIME [epoch: 13.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05672946173764528		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05672946173764528 | validation: 0.056798740082453475]
	TIME [epoch: 13.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05904370891375782		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.05904370891375782 | validation: 0.05922927958752658]
	TIME [epoch: 13.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522513549735831		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.05522513549735831 | validation: 0.05499944390946602]
	TIME [epoch: 13.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05517970266103065		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.05517970266103065 | validation: 0.05779092953314783]
	TIME [epoch: 13.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0566676946832402		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.0566676946832402 | validation: 0.056440650509541425]
	TIME [epoch: 13.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056418795657801585		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.056418795657801585 | validation: 0.07126262075446901]
	TIME [epoch: 13.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138897362232044		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.06138897362232044 | validation: 0.06787985715998364]
	TIME [epoch: 13.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07368551164929288		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.07368551164929288 | validation: 0.10782325680223877]
	TIME [epoch: 13.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08665780833550957		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.08665780833550957 | validation: 0.07111488225434152]
	TIME [epoch: 13.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06212346584501792		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.06212346584501792 | validation: 0.061697221248431404]
	TIME [epoch: 13.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06821339307749844		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.06821339307749844 | validation: 0.07563921116225154]
	TIME [epoch: 13.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343912947259692		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.06343912947259692 | validation: 0.06436835158092608]
	TIME [epoch: 13.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05734323033558585		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05734323033558585 | validation: 0.05886414891929344]
	TIME [epoch: 13.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05905398507933047		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.05905398507933047 | validation: 0.068975054079064]
	TIME [epoch: 13.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05776483092603029		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05776483092603029 | validation: 0.058585930264149245]
	TIME [epoch: 13.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05902180103473558		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.05902180103473558 | validation: 0.058847424357537814]
	TIME [epoch: 13.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055823470457433175		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.055823470457433175 | validation: 0.05325049493316922]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055774737382113714		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.055774737382113714 | validation: 0.05504397963786985]
	TIME [epoch: 13.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056320977034591116		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.056320977034591116 | validation: 0.05624980146309714]
	TIME [epoch: 13.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058464872013130886		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.058464872013130886 | validation: 0.05634202169161531]
	TIME [epoch: 13.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812237592250697		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05812237592250697 | validation: 0.07371396572404877]
	TIME [epoch: 13.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059556569492961446		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.059556569492961446 | validation: 0.059422593372034455]
	TIME [epoch: 13.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05905465304582698		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05905465304582698 | validation: 0.07038942385851953]
	TIME [epoch: 13.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958872940099763		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.05958872940099763 | validation: 0.056611872647880083]
	TIME [epoch: 13.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547146725525377		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.05547146725525377 | validation: 0.059169272292839796]
	TIME [epoch: 13.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055976546944615534		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.055976546944615534 | validation: 0.06154219000052607]
	TIME [epoch: 13.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06312873119889041		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.06312873119889041 | validation: 0.08656556020367927]
	TIME [epoch: 13.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852354956110596		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06852354956110596 | validation: 0.05881016874049392]
	TIME [epoch: 13.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05809912520848779		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05809912520848779 | validation: 0.06188189596823995]
	TIME [epoch: 13.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05298467355888584		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.05298467355888584 | validation: 0.053112387662953245]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519485928511843		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.05519485928511843 | validation: 0.0624668444008831]
	TIME [epoch: 13.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0552459118882979		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.0552459118882979 | validation: 0.05665661279997869]
	TIME [epoch: 13.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05709013037340043		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05709013037340043 | validation: 0.06021055598125748]
	TIME [epoch: 13.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461945078910126		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.05461945078910126 | validation: 0.0635547466406334]
	TIME [epoch: 13.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055382164412955036		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.055382164412955036 | validation: 0.05311309352931176]
	TIME [epoch: 13.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05492321830049342		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.05492321830049342 | validation: 0.05565355085654772]
	TIME [epoch: 13.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054059869641746375		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.054059869641746375 | validation: 0.05602858201364851]
	TIME [epoch: 13.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668217574516444		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.05668217574516444 | validation: 0.06514189179843216]
	TIME [epoch: 13.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060706154305088904		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.060706154305088904 | validation: 0.05987263635411588]
	TIME [epoch: 13.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06587003865426964		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.06587003865426964 | validation: 0.08135229219524834]
	TIME [epoch: 13.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279844972019874		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.06279844972019874 | validation: 0.053076996555079185]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618157026042897		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05618157026042897 | validation: 0.05581215725229266]
	TIME [epoch: 13.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056867717887422196		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.056867717887422196 | validation: 0.07420068179733096]
	TIME [epoch: 13.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06354358089962257		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.06354358089962257 | validation: 0.054849601454760914]
	TIME [epoch: 13.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060709339260471284		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.060709339260471284 | validation: 0.06037692233268013]
	TIME [epoch: 13.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694446579839992		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05694446579839992 | validation: 0.05789848183608705]
	TIME [epoch: 13.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054632306223611975		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.054632306223611975 | validation: 0.0528263243104514]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_766.pth
	Model improved!!!
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05525123245034192		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.05525123245034192 | validation: 0.07036613405928858]
	TIME [epoch: 13.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058966891901153035		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.058966891901153035 | validation: 0.05444049680972273]
	TIME [epoch: 13.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05280048440732591		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05280048440732591 | validation: 0.059540709855978806]
	TIME [epoch: 13.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054784730557965816		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.054784730557965816 | validation: 0.05709956354199568]
	TIME [epoch: 13.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05566964997908919		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.05566964997908919 | validation: 0.059401689262724]
	TIME [epoch: 13.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241502909743842		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.05241502909743842 | validation: 0.058245484419011555]
	TIME [epoch: 13.1 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052992259625974134		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.052992259625974134 | validation: 0.0507874698695487]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509199023715148		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.05509199023715148 | validation: 0.06813088261399886]
	TIME [epoch: 13.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055048394155892064		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.055048394155892064 | validation: 0.05592995371277534]
	TIME [epoch: 13.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056882449418941924		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.056882449418941924 | validation: 0.05727976476946799]
	TIME [epoch: 13.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050618485406947925		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.050618485406947925 | validation: 0.05232058788630738]
	TIME [epoch: 13.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054872411460118765		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.054872411460118765 | validation: 0.05918130856070765]
	TIME [epoch: 13.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056774464339837814		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.056774464339837814 | validation: 0.05809289221139002]
	TIME [epoch: 13.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527912256864181		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06527912256864181 | validation: 0.08885402462978398]
	TIME [epoch: 13.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665658448155356		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.07665658448155356 | validation: 0.05745523827137431]
	TIME [epoch: 13.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05499321125098258		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05499321125098258 | validation: 0.05350033935293377]
	TIME [epoch: 13.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052274229323519626		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.052274229323519626 | validation: 0.06311865231207829]
	TIME [epoch: 13.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05409820882357837		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.05409820882357837 | validation: 0.05423293318864129]
	TIME [epoch: 13.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055938791775087766		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.055938791775087766 | validation: 0.060083672230728524]
	TIME [epoch: 13.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289621191483947		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.05289621191483947 | validation: 0.051867051197439555]
	TIME [epoch: 13.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0523952284510504		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.0523952284510504 | validation: 0.05971858043249198]
	TIME [epoch: 13.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141894138456098		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.05141894138456098 | validation: 0.05551416057475716]
	TIME [epoch: 13.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051994564087780734		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.051994564087780734 | validation: 0.056231144508415876]
	TIME [epoch: 13.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390348113176573		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.05390348113176573 | validation: 0.06733753167741843]
	TIME [epoch: 13.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700510515437312		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.05700510515437312 | validation: 0.05501073828294474]
	TIME [epoch: 13.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05475131750164497		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.05475131750164497 | validation: 0.05996344500258721]
	TIME [epoch: 13.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371907029380581		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05371907029380581 | validation: 0.05739536091757898]
	TIME [epoch: 13.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05336517113616491		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05336517113616491 | validation: 0.05539835385754587]
	TIME [epoch: 13.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05288962959550103		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05288962959550103 | validation: 0.0657000187513614]
	TIME [epoch: 13.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825087625992067		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.05825087625992067 | validation: 0.054262620796810594]
	TIME [epoch: 13.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05633645591491767		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.05633645591491767 | validation: 0.06517849506606026]
	TIME [epoch: 13.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055839974833043156		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.055839974833043156 | validation: 0.05148916123568718]
	TIME [epoch: 13.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053580383564974014		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.053580383564974014 | validation: 0.06557872467880348]
	TIME [epoch: 13.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0592110997069064		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.0592110997069064 | validation: 0.054199371069769624]
	TIME [epoch: 13.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05617688641954482		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.05617688641954482 | validation: 0.06615017855503756]
	TIME [epoch: 13.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05879094505769736		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.05879094505769736 | validation: 0.056119750814245487]
	TIME [epoch: 13.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346291396164654		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.05346291396164654 | validation: 0.0635590327684512]
	TIME [epoch: 13.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05340154032127947		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.05340154032127947 | validation: 0.059159285835409504]
	TIME [epoch: 13.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05407080350600689		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.05407080350600689 | validation: 0.057713620403426195]
	TIME [epoch: 13.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051692441633032174		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.051692441633032174 | validation: 0.05047056111926569]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05232293388980463		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.05232293388980463 | validation: 0.053890110884884726]
	TIME [epoch: 13.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05227409319753983		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.05227409319753983 | validation: 0.05195887156239759]
	TIME [epoch: 13.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052288210238736664		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.052288210238736664 | validation: 0.05697791539142241]
	TIME [epoch: 13.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051823768715257165		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.051823768715257165 | validation: 0.0523768064143229]
	TIME [epoch: 13.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525681609380933		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.0525681609380933 | validation: 0.05391584297763106]
	TIME [epoch: 13.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05490873581068824		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.05490873581068824 | validation: 0.04881878708318236]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05129944731581666		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05129944731581666 | validation: 0.07038460661497759]
	TIME [epoch: 13.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055786432121291585		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.055786432121291585 | validation: 0.06055886612131916]
	TIME [epoch: 13.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868970334230957		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.06868970334230957 | validation: 0.07115743205435315]
	TIME [epoch: 13.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625448080514001		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.0625448080514001 | validation: 0.05240477082087857]
	TIME [epoch: 13.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506344501824535		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.0506344501824535 | validation: 0.0510435004148773]
	TIME [epoch: 13.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05774661292137811		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.05774661292137811 | validation: 0.0666954890093374]
	TIME [epoch: 13.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789138530420918		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05789138530420918 | validation: 0.06005535948389467]
	TIME [epoch: 13.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05178135587754742		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.05178135587754742 | validation: 0.05100879547518686]
	TIME [epoch: 13.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05119446594825321		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.05119446594825321 | validation: 0.05935410384687269]
	TIME [epoch: 13.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532070762716358		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0532070762716358 | validation: 0.050228766770940174]
	TIME [epoch: 13.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049289931686007184		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.049289931686007184 | validation: 0.05230717631194362]
	TIME [epoch: 13.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973534889774921		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.04973534889774921 | validation: 0.050162482052134784]
	TIME [epoch: 13.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231582744249245		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.05231582744249245 | validation: 0.049313394930511845]
	TIME [epoch: 13.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519959366120492		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.05519959366120492 | validation: 0.07458368222987283]
	TIME [epoch: 13.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634693029678399		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0634693029678399 | validation: 0.05461926007038671]
	TIME [epoch: 13.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05029462917521902		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.05029462917521902 | validation: 0.05175853079222162]
	TIME [epoch: 13.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05275894813850659		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05275894813850659 | validation: 0.060952339948102924]
	TIME [epoch: 13.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053112185354165085		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.053112185354165085 | validation: 0.049993934607102025]
	TIME [epoch: 13.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051672800055340744		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.051672800055340744 | validation: 0.052963748618342445]
	TIME [epoch: 13.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052218776008304275		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.052218776008304275 | validation: 0.06213949804869902]
	TIME [epoch: 13.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669742289587961		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.05669742289587961 | validation: 0.052299116545444106]
	TIME [epoch: 13.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181831197705186		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.05181831197705186 | validation: 0.05604225300460837]
	TIME [epoch: 13.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053122079745501434		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.053122079745501434 | validation: 0.049651721499410266]
	TIME [epoch: 13.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053663239428827		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.053663239428827 | validation: 0.050983683431590265]
	TIME [epoch: 13.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779194488748508		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04779194488748508 | validation: 0.07088256923990868]
	TIME [epoch: 13.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270792117223072		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.05270792117223072 | validation: 0.052855263500200866]
	TIME [epoch: 13.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053398026386152236		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.053398026386152236 | validation: 0.0626670721675176]
	TIME [epoch: 13.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700350560777395		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.05700350560777395 | validation: 0.04882406108167764]
	TIME [epoch: 13.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473782041934294		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.05473782041934294 | validation: 0.05005394744089373]
	TIME [epoch: 13.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963793072591905		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04963793072591905 | validation: 0.0499375589402026]
	TIME [epoch: 13.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030760758316064		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.05030760758316064 | validation: 0.05135624928196641]
	TIME [epoch: 13.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054634132717943915		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.054634132717943915 | validation: 0.04833714329911776]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051063575102574914		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.051063575102574914 | validation: 0.05537857901822868]
	TIME [epoch: 13.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04940075232810778		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04940075232810778 | validation: 0.05120615655678058]
	TIME [epoch: 13.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051746555698011516		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.051746555698011516 | validation: 0.05439432820375642]
	TIME [epoch: 13.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051957575889554485		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.051957575889554485 | validation: 0.06372164257843621]
	TIME [epoch: 13.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05385587220508956		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.05385587220508956 | validation: 0.05148516619940277]
	TIME [epoch: 13.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05453335052377841		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.05453335052377841 | validation: 0.06682477769395623]
	TIME [epoch: 13.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057608466274469916		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.057608466274469916 | validation: 0.04892949898420537]
	TIME [epoch: 13.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04882152801779416		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04882152801779416 | validation: 0.05018407989061485]
	TIME [epoch: 13.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05199305323245172		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.05199305323245172 | validation: 0.06518415647523844]
	TIME [epoch: 13.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05837934238024818		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.05837934238024818 | validation: 0.051307673575702796]
	TIME [epoch: 13.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05297253871142573		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.05297253871142573 | validation: 0.046673609109634445]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050960327238812615		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.050960327238812615 | validation: 0.06353575546100579]
	TIME [epoch: 13.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05701664607844951		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.05701664607844951 | validation: 0.05183808084220465]
	TIME [epoch: 13.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340442655649955		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06340442655649955 | validation: 0.05996171287801575]
	TIME [epoch: 13.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051387593235637054		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.051387593235637054 | validation: 0.05264811166697706]
	TIME [epoch: 13.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170036543934904		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.05170036543934904 | validation: 0.0515996321823628]
	TIME [epoch: 13.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05071494214824316		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.05071494214824316 | validation: 0.058750201033323596]
	TIME [epoch: 13.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474816092797336		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.0474816092797336 | validation: 0.05784791634039826]
	TIME [epoch: 13.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051488407987744615		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.051488407987744615 | validation: 0.047505018189696505]
	TIME [epoch: 13.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053110276177103406		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.053110276177103406 | validation: 0.054150269317056354]
	TIME [epoch: 13.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051527622390811496		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.051527622390811496 | validation: 0.05688868040124167]
	TIME [epoch: 13.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05127419413465829		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.05127419413465829 | validation: 0.04956447246391335]
	TIME [epoch: 13.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05049421318949276		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.05049421318949276 | validation: 0.05386971564939506]
	TIME [epoch: 13.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136430613018973		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.05136430613018973 | validation: 0.049035424519876296]
	TIME [epoch: 13.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051388792078304435		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.051388792078304435 | validation: 0.06265813835153648]
	TIME [epoch: 13.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05124075362730762		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.05124075362730762 | validation: 0.05365290898914452]
	TIME [epoch: 13.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000730988033006		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.05000730988033006 | validation: 0.05644199446297957]
	TIME [epoch: 13.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506912409082348		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0506912409082348 | validation: 0.04947343925490836]
	TIME [epoch: 13.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050430372830843256		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.050430372830843256 | validation: 0.06440749299728397]
	TIME [epoch: 13.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255377537309113		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.05255377537309113 | validation: 0.05290226996248443]
	TIME [epoch: 13.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049473601956331645		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.049473601956331645 | validation: 0.05169991135593266]
	TIME [epoch: 13.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022937489414624		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.05022937489414624 | validation: 0.05046883251734862]
	TIME [epoch: 13.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050023081522149564		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.050023081522149564 | validation: 0.046650400104658686]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05070222016112311		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.05070222016112311 | validation: 0.05872140735404562]
	TIME [epoch: 13.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093825496352967		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.05093825496352967 | validation: 0.048722138488527515]
	TIME [epoch: 13.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054364385778780354		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.054364385778780354 | validation: 0.056021239303044014]
	TIME [epoch: 13.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170571101290914		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.05170571101290914 | validation: 0.04951932428670244]
	TIME [epoch: 13.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04824340086454579		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.04824340086454579 | validation: 0.04983134320463899]
	TIME [epoch: 13.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048930627585443025		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.048930627585443025 | validation: 0.052513534353486616]
	TIME [epoch: 13.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051201385188240175		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.051201385188240175 | validation: 0.04794637784544792]
	TIME [epoch: 13.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053596052168746564		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.053596052168746564 | validation: 0.05601864358740746]
	TIME [epoch: 13.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207523581676983		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.05207523581676983 | validation: 0.04942885737114941]
	TIME [epoch: 13.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04968667537974847		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.04968667537974847 | validation: 0.04866420974416508]
	TIME [epoch: 13.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05045686387108489		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.05045686387108489 | validation: 0.05514696293105884]
	TIME [epoch: 13.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054126358279986145		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.054126358279986145 | validation: 0.05018549518178205]
	TIME [epoch: 13.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04912728206302055		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04912728206302055 | validation: 0.04705093859136226]
	TIME [epoch: 13.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974969725486365		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04974969725486365 | validation: 0.05876592484658926]
	TIME [epoch: 13.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962849793220026		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.04962849793220026 | validation: 0.0486228399220252]
	TIME [epoch: 13.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046193069665780094		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.046193069665780094 | validation: 0.047187862716198896]
	TIME [epoch: 13.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04753682950812242		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.04753682950812242 | validation: 0.05464751116570768]
	TIME [epoch: 13.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895360133043102		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.04895360133043102 | validation: 0.048059927361673783]
	TIME [epoch: 13.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04897457513070896		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.04897457513070896 | validation: 0.05689695909966186]
	TIME [epoch: 13.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051984135149010326		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.051984135149010326 | validation: 0.052007844852342736]
	TIME [epoch: 13.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231986551795348		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.05231986551795348 | validation: 0.04936881737727381]
	TIME [epoch: 13.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05076008994845806		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.05076008994845806 | validation: 0.0474155762285021]
	TIME [epoch: 13.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493184474048051		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.0493184474048051 | validation: 0.05132420464837785]
	TIME [epoch: 13.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858453689997455		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.04858453689997455 | validation: 0.04991848505925931]
	TIME [epoch: 13.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951285189931281		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.04951285189931281 | validation: 0.04722724590018702]
	TIME [epoch: 13.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04875510532439111		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.04875510532439111 | validation: 0.053689698431837456]
	TIME [epoch: 13.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049384712460805814		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.049384712460805814 | validation: 0.04465469349648096]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04720567267033581		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04720567267033581 | validation: 0.05136721752964009]
	TIME [epoch: 13.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901663940701334		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04901663940701334 | validation: 0.04692696395511419]
	TIME [epoch: 13.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04903431397944173		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04903431397944173 | validation: 0.048027044186584655]
	TIME [epoch: 13.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047916466575286115		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.047916466575286115 | validation: 0.054117637050187344]
	TIME [epoch: 13.3 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05152359164281508		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.05152359164281508 | validation: 0.05308250648532547]
	TIME [epoch: 13.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05368669338136657		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05368669338136657 | validation: 0.054983196553715356]
	TIME [epoch: 13.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05015898163483133		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.05015898163483133 | validation: 0.05110716797285878]
	TIME [epoch: 13.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04587259879828058		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.04587259879828058 | validation: 0.047771155529261915]
	TIME [epoch: 13.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820086816352827		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.04820086816352827 | validation: 0.05011777670000211]
	TIME [epoch: 13.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048626745050226336		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.048626745050226336 | validation: 0.04816235641347442]
	TIME [epoch: 13.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04741974253873397		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.04741974253873397 | validation: 0.05796258588767525]
	TIME [epoch: 13.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04762190551086087		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04762190551086087 | validation: 0.05211963990964441]
	TIME [epoch: 13.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04874879544264243		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.04874879544264243 | validation: 0.050366381758055405]
	TIME [epoch: 13.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581780759652552		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.04581780759652552 | validation: 0.05345915231840847]
	TIME [epoch: 13.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817580578482595		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.04817580578482595 | validation: 0.048444960997543376]
	TIME [epoch: 13.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010659699467853		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.05010659699467853 | validation: 0.059147209476021284]
	TIME [epoch: 13.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063965922781088		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.05063965922781088 | validation: 0.04784999926600032]
	TIME [epoch: 13.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963696413328766		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.04963696413328766 | validation: 0.04271456770640906]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_922.pth
	Model improved!!!
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04785186082353835		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.04785186082353835 | validation: 0.05292491291929885]
	TIME [epoch: 13.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048748587763458695		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.048748587763458695 | validation: 0.04962154185888153]
	TIME [epoch: 13.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04892254616874782		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.04892254616874782 | validation: 0.04689554823211034]
	TIME [epoch: 13.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051406365669144145		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.051406365669144145 | validation: 0.06671084575358811]
	TIME [epoch: 13.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05137125187323013		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.05137125187323013 | validation: 0.04781357008142802]
	TIME [epoch: 13.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05081413884712532		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.05081413884712532 | validation: 0.0507444844972445]
	TIME [epoch: 13.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545767894679766		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04545767894679766 | validation: 0.04689232667583784]
	TIME [epoch: 13.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723316503287066		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.04723316503287066 | validation: 0.04665491873200141]
	TIME [epoch: 13.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048280530374033286		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.048280530374033286 | validation: 0.04669433651636672]
	TIME [epoch: 13.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045259694385925135		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.045259694385925135 | validation: 0.04780147726451544]
	TIME [epoch: 13.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04824668936810709		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.04824668936810709 | validation: 0.04570294963730392]
	TIME [epoch: 13.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047531630274743844		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.047531630274743844 | validation: 0.0643836988987727]
	TIME [epoch: 13.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500501475888812		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.0500501475888812 | validation: 0.04653142352473948]
	TIME [epoch: 13.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0476111362172893		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0476111362172893 | validation: 0.052407908485965465]
	TIME [epoch: 13.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612167594877475		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.04612167594877475 | validation: 0.0452301403278986]
	TIME [epoch: 13.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047034230933709176		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.047034230933709176 | validation: 0.04772569739142573]
	TIME [epoch: 13.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04861617594727111		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.04861617594727111 | validation: 0.046555201124765966]
	TIME [epoch: 13.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0457907597602119		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0457907597602119 | validation: 0.05088580223431739]
	TIME [epoch: 13.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04936928348625358		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.04936928348625358 | validation: 0.04865279013365791]
	TIME [epoch: 13.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04652389662697234		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.04652389662697234 | validation: 0.04549142994575758]
	TIME [epoch: 13.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04634684228956245		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.04634684228956245 | validation: 0.050502336881626364]
	TIME [epoch: 13.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862640293949042		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.04862640293949042 | validation: 0.04365803083832555]
	TIME [epoch: 13.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048710990011967006		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.048710990011967006 | validation: 0.05083519552740493]
	TIME [epoch: 13.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046236745137625344		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.046236745137625344 | validation: 0.05527776170830576]
	TIME [epoch: 13.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135447504160941		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05135447504160941 | validation: 0.04640376806310551]
	TIME [epoch: 13.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055120967742983		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.055120967742983 | validation: 0.05069683364949318]
	TIME [epoch: 13.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04925113082014464		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04925113082014464 | validation: 0.04913578187233197]
	TIME [epoch: 13.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04712322517306689		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.04712322517306689 | validation: 0.043841368002156295]
	TIME [epoch: 13.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048062438691317405		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.048062438691317405 | validation: 0.045812930990112036]
	TIME [epoch: 13.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04771006796960376		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.04771006796960376 | validation: 0.04850856204608878]
	TIME [epoch: 13.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04526489761047559		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.04526489761047559 | validation: 0.04687948102769447]
	TIME [epoch: 13.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594625427310299		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.04594625427310299 | validation: 0.050816970234119054]
	TIME [epoch: 13.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670690126965618		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.04670690126965618 | validation: 0.051562004896870434]
	TIME [epoch: 13.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0451723642042629		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.0451723642042629 | validation: 0.04354373587291177]
	TIME [epoch: 13.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047246235263826604		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.047246235263826604 | validation: 0.048926614046688854]
	TIME [epoch: 13.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474185563850831		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.04474185563850831 | validation: 0.04616361657635341]
	TIME [epoch: 13.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04747771156707762		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.04747771156707762 | validation: 0.05093296332514982]
	TIME [epoch: 13.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04535894394824709		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.04535894394824709 | validation: 0.04784470165135488]
	TIME [epoch: 13.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433690469950989		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.04433690469950989 | validation: 0.04430325026729142]
	TIME [epoch: 13.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682044830560085		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.04682044830560085 | validation: 0.04969134418320148]
	TIME [epoch: 13.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045226427813931304		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.045226427813931304 | validation: 0.043681394210022796]
	TIME [epoch: 13.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04427720264659462		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.04427720264659462 | validation: 0.05026478906503201]
	TIME [epoch: 13.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046801108501734544		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.046801108501734544 | validation: 0.0475188966764695]
	TIME [epoch: 13.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519041063188138		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0519041063188138 | validation: 0.052370371757923355]
	TIME [epoch: 13.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04986853609422557		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.04986853609422557 | validation: 0.04926584286227695]
	TIME [epoch: 13.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04890820362322533		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.04890820362322533 | validation: 0.04923406568310467]
	TIME [epoch: 13.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0449172714079368		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.0449172714079368 | validation: 0.046033815961049475]
	TIME [epoch: 13.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045874313895649		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.045874313895649 | validation: 0.04552264069138609]
	TIME [epoch: 13.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472918853898831		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.04472918853898831 | validation: 0.04899026195129589]
	TIME [epoch: 13.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0462319369030532		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.0462319369030532 | validation: 0.042105092670546664]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_972.pth
	Model improved!!!
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691967932137276		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.04691967932137276 | validation: 0.04421715118464671]
	TIME [epoch: 13.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616044015861544		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.04616044015861544 | validation: 0.05144048083776305]
	TIME [epoch: 13.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068559883075375		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.05068559883075375 | validation: 0.04540133015381401]
	TIME [epoch: 13.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047977964327255956		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.047977964327255956 | validation: 0.0503228696955606]
	TIME [epoch: 13.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672214792698983		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.04672214792698983 | validation: 0.05369844135704441]
	TIME [epoch: 13.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05083227233683265		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05083227233683265 | validation: 0.04330193956518818]
	TIME [epoch: 13.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998932230888383		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.04998932230888383 | validation: 0.04685304052666345]
	TIME [epoch: 13.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045260934734328115		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.045260934734328115 | validation: 0.047815347150450084]
	TIME [epoch: 13.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760869968561389		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.04760869968561389 | validation: 0.04377181255242694]
	TIME [epoch: 13.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739515422722092		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.04739515422722092 | validation: 0.04621622699881322]
	TIME [epoch: 13.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04352192886341941		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.04352192886341941 | validation: 0.0462314542895304]
	TIME [epoch: 13.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046754823837480065		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.046754823837480065 | validation: 0.041450496358083316]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_984.pth
	Model improved!!!
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046702146687573345		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.046702146687573345 | validation: 0.052071104820160134]
	TIME [epoch: 13.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707709912608689		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.04707709912608689 | validation: 0.050325344252990484]
	TIME [epoch: 13.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045878933365353966		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.045878933365353966 | validation: 0.04684442890749671]
	TIME [epoch: 13.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047027330642853034		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.047027330642853034 | validation: 0.0468619915210033]
	TIME [epoch: 13.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569152595016766		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.04569152595016766 | validation: 0.042153007085826935]
	TIME [epoch: 13.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04625896258153575		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.04625896258153575 | validation: 0.04996734908752686]
	TIME [epoch: 13.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707386771220305		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.04707386771220305 | validation: 0.050289427639630284]
	TIME [epoch: 13.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045558354964158235		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.045558354964158235 | validation: 0.0464246154715566]
	TIME [epoch: 13.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631436468363008		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.04631436468363008 | validation: 0.049347196323463384]
	TIME [epoch: 13.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045647789621842474		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.045647789621842474 | validation: 0.043959421419052916]
	TIME [epoch: 13.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048863789397440294		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.048863789397440294 | validation: 0.051616714898688326]
	TIME [epoch: 13.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0467505816473036		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.0467505816473036 | validation: 0.04334753928770144]
	TIME [epoch: 13.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04550025460665255		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.04550025460665255 | validation: 0.04567531964295249]
	TIME [epoch: 13.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04440256877252531		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.04440256877252531 | validation: 0.049881115688209424]
	TIME [epoch: 13.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791122933183447		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.04791122933183447 | validation: 0.044192373451674066]
	TIME [epoch: 13.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671422191701957		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.04671422191701957 | validation: 0.04546630930319641]
	TIME [epoch: 13.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043589483826112295		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.043589483826112295 | validation: 0.04459522406206844]
	TIME [epoch: 304 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525970663537754		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.04525970663537754 | validation: 0.04450152626966667]
	TIME [epoch: 27.1 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04404437530668713		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.04404437530668713 | validation: 0.04201160048252126]
	TIME [epoch: 27.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579936275418205		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.04579936275418205 | validation: 0.04646330710334295]
	TIME [epoch: 27.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04846501413155531		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.04846501413155531 | validation: 0.04794577769815851]
	TIME [epoch: 27.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047449743987858534		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.047449743987858534 | validation: 0.04882728480940383]
	TIME [epoch: 27.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660604889762055		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.04660604889762055 | validation: 0.04198113490942954]
	TIME [epoch: 27.1 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641175034949928		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.04641175034949928 | validation: 0.04522182703568853]
	TIME [epoch: 27.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513689593731319		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.04513689593731319 | validation: 0.04743590207781173]
	TIME [epoch: 27.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044306279062982803		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.044306279062982803 | validation: 0.042925026878297405]
	TIME [epoch: 27.1 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04438411631756595		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.04438411631756595 | validation: 0.04905175748987566]
	TIME [epoch: 27.1 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04405866868988834		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.04405866868988834 | validation: 0.047229309577688644]
	TIME [epoch: 27.1 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04695614843713332		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.04695614843713332 | validation: 0.056014280633643665]
	TIME [epoch: 27.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051965767357866184		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.051965767357866184 | validation: 0.04255306177163749]
	TIME [epoch: 27.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278717647600051		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.04278717647600051 | validation: 0.04386265530675851]
	TIME [epoch: 27.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778511239474237		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.04778511239474237 | validation: 0.044801320493583645]
	TIME [epoch: 27.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513136730600408		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.04513136730600408 | validation: 0.04580898900899563]
	TIME [epoch: 27.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04644748298286335		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.04644748298286335 | validation: 0.04023048764288831]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844219067590218		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.04844219067590218 | validation: 0.04341633597455306]
	TIME [epoch: 27.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04452709967425113		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.04452709967425113 | validation: 0.04401298302197089]
	TIME [epoch: 27.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592461525809454		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.04592461525809454 | validation: 0.043886958927703425]
	TIME [epoch: 27.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044243225212227025		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.044243225212227025 | validation: 0.04151868870745373]
	TIME [epoch: 27.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598427328493746		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.04598427328493746 | validation: 0.04475393623647134]
	TIME [epoch: 27.1 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04480835578698377		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.04480835578698377 | validation: 0.046660436779213825]
	TIME [epoch: 27.1 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04283508362785268		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.04283508362785268 | validation: 0.04403198525928215]
	TIME [epoch: 27.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046406920245027836		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.046406920245027836 | validation: 0.04031815118992832]
	TIME [epoch: 27.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043337759015895365		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.043337759015895365 | validation: 0.04729933634029948]
	TIME [epoch: 27.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375885867286377		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.04375885867286377 | validation: 0.04916859291103653]
	TIME [epoch: 27.1 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04483470909966457		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.04483470909966457 | validation: 0.04393111995034206]
	TIME [epoch: 27.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281665339580683		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.04281665339580683 | validation: 0.04319935578079163]
	TIME [epoch: 27.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045626026998355586		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.045626026998355586 | validation: 0.04486867418233206]
	TIME [epoch: 27.1 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576068439309082		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.04576068439309082 | validation: 0.04155866685155505]
	TIME [epoch: 27.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04315271992675394		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.04315271992675394 | validation: 0.04405851443760268]
	TIME [epoch: 27.1 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322998889776817		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.04322998889776817 | validation: 0.04318854514912859]
	TIME [epoch: 27.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04502365419186992		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.04502365419186992 | validation: 0.044921641299100215]
	TIME [epoch: 27.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04621023009217379		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.04621023009217379 | validation: 0.04171167655965379]
	TIME [epoch: 27.1 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04572231176026347		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.04572231176026347 | validation: 0.05118790068689969]
	TIME [epoch: 27.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0492276505698021		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.0492276505698021 | validation: 0.0435539941149949]
	TIME [epoch: 27.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841704538815268		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.04841704538815268 | validation: 0.04834393634474904]
	TIME [epoch: 27.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042637752787695965		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.042637752787695965 | validation: 0.04454656203517624]
	TIME [epoch: 27.1 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044750759246273954		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.044750759246273954 | validation: 0.0425624962959325]
	TIME [epoch: 27.1 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04518795563281113		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.04518795563281113 | validation: 0.0467247101096612]
	TIME [epoch: 27.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0427801554970751		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.0427801554970751 | validation: 0.04248843221310762]
	TIME [epoch: 27.1 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043689503240355414		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.043689503240355414 | validation: 0.0478883864010449]
	TIME [epoch: 27.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436074224900363		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.04436074224900363 | validation: 0.04387145438066296]
	TIME [epoch: 27.1 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424393839898549		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.04424393839898549 | validation: 0.042946902678508064]
	TIME [epoch: 27.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045648255153020074		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.045648255153020074 | validation: 0.04698718028479855]
	TIME [epoch: 27.1 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878638471899028		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.04878638471899028 | validation: 0.0413281177042429]
	TIME [epoch: 27.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043148990102221		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.043148990102221 | validation: 0.04417300439257005]
	TIME [epoch: 27.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04273531802204797		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.04273531802204797 | validation: 0.04543409794482885]
	TIME [epoch: 27.1 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04790962556904553		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.04790962556904553 | validation: 0.04651276544752412]
	TIME [epoch: 27.1 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04483463897207192		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.04483463897207192 | validation: 0.04128022129742768]
	TIME [epoch: 27.1 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04287552569683529		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.04287552569683529 | validation: 0.054317945876062645]
	TIME [epoch: 27.1 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714188454759586		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.04714188454759586 | validation: 0.04239074796865179]
	TIME [epoch: 27.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327180019743685		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.04327180019743685 | validation: 0.04295454959831002]
	TIME [epoch: 27.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04482858875450141		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.04482858875450141 | validation: 0.046805038517048037]
	TIME [epoch: 27.1 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276881108130544		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.04276881108130544 | validation: 0.04603452106518413]
	TIME [epoch: 27.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424448020889158		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.04424448020889158 | validation: 0.0407975831677214]
	TIME [epoch: 27.1 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042084332710082134		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.042084332710082134 | validation: 0.04626414585642788]
	TIME [epoch: 27.1 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04391264963210861		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.04391264963210861 | validation: 0.04503595732771953]
	TIME [epoch: 27.1 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043187163257285256		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.043187163257285256 | validation: 0.038904259602846505]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1061.pth
	Model improved!!!
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043315040171421834		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.043315040171421834 | validation: 0.04367321668381124]
	TIME [epoch: 27.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04506962870821157		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.04506962870821157 | validation: 0.039846871920966226]
	TIME [epoch: 27.1 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0431333040362598		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.0431333040362598 | validation: 0.0406479576153744]
	TIME [epoch: 27.1 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041670191606037195		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.041670191606037195 | validation: 0.047746961973324335]
	TIME [epoch: 27.1 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045992658437886455		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.045992658437886455 | validation: 0.04098830833448969]
	TIME [epoch: 27.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04505997129682354		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.04505997129682354 | validation: 0.04298466884392348]
	TIME [epoch: 27.1 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043148745150206204		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.043148745150206204 | validation: 0.03847629741748776]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1068.pth
	Model improved!!!
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419110279294891		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.0419110279294891 | validation: 0.045601102646402325]
	TIME [epoch: 27.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430981833070273		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.0430981833070273 | validation: 0.04086761539549188]
	TIME [epoch: 27.1 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346607210752242		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.04346607210752242 | validation: 0.039501006035724595]
	TIME [epoch: 27.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562804374445243		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.04562804374445243 | validation: 0.044768924092299704]
	TIME [epoch: 27.1 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046050414949526025		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.046050414949526025 | validation: 0.042522982702420754]
	TIME [epoch: 27.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04446692966381411		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.04446692966381411 | validation: 0.04297657064552205]
	TIME [epoch: 27.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330596924084384		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.04330596924084384 | validation: 0.04691651221075515]
	TIME [epoch: 27.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043609311900287315		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.043609311900287315 | validation: 0.03819505056828709]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1076.pth
	Model improved!!!
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04153597188826708		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.04153597188826708 | validation: 0.04114944972088579]
	TIME [epoch: 27.1 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04350112383709469		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.04350112383709469 | validation: 0.05447077334802375]
	TIME [epoch: 27.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708255371538615		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.04708255371538615 | validation: 0.044014328018104024]
	TIME [epoch: 27.1 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043894384046840956		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.043894384046840956 | validation: 0.044730048284865875]
	TIME [epoch: 27.1 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04427408461389961		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.04427408461389961 | validation: 0.03996181866478358]
	TIME [epoch: 27.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238425910361629		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.04238425910361629 | validation: 0.04051556721199853]
	TIME [epoch: 27.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04467874847316514		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.04467874847316514 | validation: 0.045649409334419445]
	TIME [epoch: 27.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043987857816302725		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.043987857816302725 | validation: 0.045793630608469166]
	TIME [epoch: 27.1 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04366648116838375		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.04366648116838375 | validation: 0.041930871273149095]
	TIME [epoch: 27.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044729242138436896		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.044729242138436896 | validation: 0.04145135854376029]
	TIME [epoch: 27.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043698490214496497		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.043698490214496497 | validation: 0.04367241958665198]
	TIME [epoch: 27.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04369982793939459		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.04369982793939459 | validation: 0.03890349832775706]
	TIME [epoch: 27.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311257187213634		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.04311257187213634 | validation: 0.04774593332355203]
	TIME [epoch: 27.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04099127402346072		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04099127402346072 | validation: 0.04048279047612146]
	TIME [epoch: 27.1 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214762306138337		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.04214762306138337 | validation: 0.042867600499684924]
	TIME [epoch: 27.1 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04277456151963553		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.04277456151963553 | validation: 0.0379562994864186]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1092.pth
	Model improved!!!
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439858087255608		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.04439858087255608 | validation: 0.04292480193270492]
	TIME [epoch: 27.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043305860892828306		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.043305860892828306 | validation: 0.04322068832668239]
	TIME [epoch: 27.1 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303784874962071		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.04303784874962071 | validation: 0.040181629368042364]
	TIME [epoch: 27.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430179566195509		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.0430179566195509 | validation: 0.03876981362371445]
	TIME [epoch: 27.1 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04317010786929275		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.04317010786929275 | validation: 0.04090625099108966]
	TIME [epoch: 27.1 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044229899816813634		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.044229899816813634 | validation: 0.03960842955150165]
	TIME [epoch: 27.1 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588839384302508		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.04588839384302508 | validation: 0.04364737688466637]
	TIME [epoch: 27.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411611187391464		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.04411611187391464 | validation: 0.04213716558576056]
	TIME [epoch: 27.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04450520250823818		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.04450520250823818 | validation: 0.03893296774668133]
	TIME [epoch: 27.1 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042670416317633894		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.042670416317633894 | validation: 0.03770827111229133]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293886117528941		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.04293886117528941 | validation: 0.04079080236021004]
	TIME [epoch: 27.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04161116830061426		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.04161116830061426 | validation: 0.040838650249169564]
	TIME [epoch: 27.1 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042730312777019004		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.042730312777019004 | validation: 0.039351779554112815]
	TIME [epoch: 27.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238549167249198		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.04238549167249198 | validation: 0.04348703581385959]
	TIME [epoch: 27.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042890988914299745		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.042890988914299745 | validation: 0.03798603286474651]
	TIME [epoch: 27.1 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042548331247068555		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.042548331247068555 | validation: 0.040002774521683104]
	TIME [epoch: 27.1 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04220937519413825		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.04220937519413825 | validation: 0.03958301122139767]
	TIME [epoch: 27.1 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0459411714037608		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.0459411714037608 | validation: 0.04678851909313533]
	TIME [epoch: 27.1 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03998042612807667		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.03998042612807667 | validation: 0.04040053009565475]
	TIME [epoch: 27.1 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04132932693728496		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.04132932693728496 | validation: 0.039650340444926595]
	TIME [epoch: 27.1 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04234612913554463		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.04234612913554463 | validation: 0.043990802118043265]
	TIME [epoch: 27.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04494321046551514		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.04494321046551514 | validation: 0.041059299655684105]
	TIME [epoch: 27.1 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04174676057924365		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.04174676057924365 | validation: 0.04198026288373763]
	TIME [epoch: 27.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395211848664178		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.0395211848664178 | validation: 0.04236417233279005]
	TIME [epoch: 27.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04263891191842158		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.04263891191842158 | validation: 0.042610876255546715]
	TIME [epoch: 27.1 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04217284760215948		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.04217284760215948 | validation: 0.03846823259698228]
	TIME [epoch: 27.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042718497167421515		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.042718497167421515 | validation: 0.041788437970657115]
	TIME [epoch: 27.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042184866962753066		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.042184866962753066 | validation: 0.04455618692855798]
	TIME [epoch: 27.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04261667237064856		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.04261667237064856 | validation: 0.040162085110842696]
	TIME [epoch: 27.1 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041612919861992295		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.041612919861992295 | validation: 0.04762824571329591]
	TIME [epoch: 27.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04251742848349294		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.04251742848349294 | validation: 0.03951971240283158]
	TIME [epoch: 27.1 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04144241863566782		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.04144241863566782 | validation: 0.04108972403299049]
	TIME [epoch: 27.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233089636104466		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.04233089636104466 | validation: 0.0420184860159873]
	TIME [epoch: 27.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043052632939205554		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.043052632939205554 | validation: 0.04509258810330913]
	TIME [epoch: 27.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04823686162282449		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.04823686162282449 | validation: 0.04373142577771551]
	TIME [epoch: 27.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043795271698677106		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.043795271698677106 | validation: 0.03985073907284279]
	TIME [epoch: 27.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335776431079671		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.04335776431079671 | validation: 0.036957531399157154]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1129.pth
	Model improved!!!
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178532702124747		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.04178532702124747 | validation: 0.039026445650900736]
	TIME [epoch: 27.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04304033827948676		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.04304033827948676 | validation: 0.04354409111657814]
	TIME [epoch: 27.1 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042222275172621865		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.042222275172621865 | validation: 0.03648681665446518]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1132.pth
	Model improved!!!
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169063976495179		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.04169063976495179 | validation: 0.03880783413666616]
	TIME [epoch: 27.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042086124614717245		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.042086124614717245 | validation: 0.03838810145897118]
	TIME [epoch: 27.1 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04141406175617121		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.04141406175617121 | validation: 0.04189043054520091]
	TIME [epoch: 27.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0417048322131382		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.0417048322131382 | validation: 0.04172698090280705]
	TIME [epoch: 27.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04213620727249588		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.04213620727249588 | validation: 0.03736015120470213]
	TIME [epoch: 27.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0425541402516905		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.0425541402516905 | validation: 0.036599676679532865]
	TIME [epoch: 27.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041265780404884725		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.041265780404884725 | validation: 0.03981425057961413]
	TIME [epoch: 27.1 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169346967484109		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.04169346967484109 | validation: 0.03959089772793776]
	TIME [epoch: 27.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0416895488308796		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.0416895488308796 | validation: 0.03941540464334323]
	TIME [epoch: 27.1 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04271546491889433		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.04271546491889433 | validation: 0.041592244410102064]
	TIME [epoch: 27.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047352450954425		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.04047352450954425 | validation: 0.045696133807382903]
	TIME [epoch: 27.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994448969443398		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.03994448969443398 | validation: 0.04228615185443671]
	TIME [epoch: 27.1 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042914204907263664		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.042914204907263664 | validation: 0.0358009305542652]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1145.pth
	Model improved!!!
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983924609949116		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.03983924609949116 | validation: 0.038271121856282876]
	TIME [epoch: 27.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041467154467866967		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.041467154467866967 | validation: 0.040112867020294984]
	TIME [epoch: 27.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04160387131769776		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04160387131769776 | validation: 0.04099077740475868]
	TIME [epoch: 27.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03977955805953692		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.03977955805953692 | validation: 0.04136889537162856]
	TIME [epoch: 27.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04171560078624017		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.04171560078624017 | validation: 0.03974765234046395]
	TIME [epoch: 27.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103399685463043		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.04103399685463043 | validation: 0.036835878593135106]
	TIME [epoch: 27.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178604091934629		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.04178604091934629 | validation: 0.03799619236397922]
	TIME [epoch: 27.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0410499461662683		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.0410499461662683 | validation: 0.038151345570410236]
	TIME [epoch: 27.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286330121613755		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.04286330121613755 | validation: 0.042764847355846895]
	TIME [epoch: 27.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279471899264344		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.04279471899264344 | validation: 0.03861874231197774]
	TIME [epoch: 27.1 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087628930589766		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04087628930589766 | validation: 0.03562152931270425]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1156.pth
	Model improved!!!
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039313761452455645		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.039313761452455645 | validation: 0.04220387742881894]
	TIME [epoch: 27.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891020351165981		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.03891020351165981 | validation: 0.039824448458344756]
	TIME [epoch: 27.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04010593678498383		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.04010593678498383 | validation: 0.0396652794020943]
	TIME [epoch: 27.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041849567854640855		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.041849567854640855 | validation: 0.03880089457347133]
	TIME [epoch: 27.2 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0418935120530903		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.0418935120530903 | validation: 0.03777019820096864]
	TIME [epoch: 27.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04240404551869102		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04240404551869102 | validation: 0.03739830226086044]
	TIME [epoch: 27.2 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04204564473951626		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.04204564473951626 | validation: 0.03966919418908718]
	TIME [epoch: 27.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041769408247895275		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.041769408247895275 | validation: 0.037598747890004336]
	TIME [epoch: 27.2 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04143643062988405		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.04143643062988405 | validation: 0.03720943015816232]
	TIME [epoch: 27.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04050481088987262		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.04050481088987262 | validation: 0.0412221039885038]
	TIME [epoch: 27.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085523683568695		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.04085523683568695 | validation: 0.036831832669866726]
	TIME [epoch: 27.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041373259352176034		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.041373259352176034 | validation: 0.0386081747173172]
	TIME [epoch: 27.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04116333851209299		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.04116333851209299 | validation: 0.04086103656909115]
	TIME [epoch: 27.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040520489201898824		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.040520489201898824 | validation: 0.04711993502377163]
	TIME [epoch: 27.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166143445899789		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.04166143445899789 | validation: 0.03634026466204448]
	TIME [epoch: 27.1 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04136316297089698		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.04136316297089698 | validation: 0.04130107338172924]
	TIME [epoch: 27.1 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040098844449732836		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.040098844449732836 | validation: 0.046988801568915266]
	TIME [epoch: 27.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188087125143635		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04188087125143635 | validation: 0.04015063875672823]
	TIME [epoch: 27.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03963323277180224		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.03963323277180224 | validation: 0.0375286660038707]
	TIME [epoch: 27.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04124709125782406		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.04124709125782406 | validation: 0.041144585034872797]
	TIME [epoch: 27.1 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04130843289958543		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.04130843289958543 | validation: 0.043382465903731675]
	TIME [epoch: 27.1 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041137209299286964		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.041137209299286964 | validation: 0.03566090519331453]
	TIME [epoch: 27.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993091313920674		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.03993091313920674 | validation: 0.03654442612210428]
	TIME [epoch: 27.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0423104814257179		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.0423104814257179 | validation: 0.04132186770408294]
	TIME [epoch: 27.1 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043258498450514535		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.043258498450514535 | validation: 0.036373594340464124]
	TIME [epoch: 27.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267407328707108		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.04267407328707108 | validation: 0.039912384305517745]
	TIME [epoch: 27.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137710784266251		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.04137710784266251 | validation: 0.03745884574284099]
	TIME [epoch: 27.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04075191239090151		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.04075191239090151 | validation: 0.03680711592392153]
	TIME [epoch: 27.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852713281560144		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.03852713281560144 | validation: 0.041147552166841586]
	TIME [epoch: 27.1 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04191003581036273		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.04191003581036273 | validation: 0.037806451699378296]
	TIME [epoch: 27.1 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967196290443903		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.03967196290443903 | validation: 0.03388565476550094]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1187.pth
	Model improved!!!
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938659531607592		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.03938659531607592 | validation: 0.036655196950478884]
	TIME [epoch: 27.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04065943617012288		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.04065943617012288 | validation: 0.042104549336349044]
	TIME [epoch: 27.2 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041228532093552914		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.041228532093552914 | validation: 0.039290702811916434]
	TIME [epoch: 27.1 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0412265721420693		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.0412265721420693 | validation: 0.03979032456108579]
	TIME [epoch: 27.1 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047427582816943		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.04047427582816943 | validation: 0.033947730114646184]
	TIME [epoch: 27.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042820790555934976		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.042820790555934976 | validation: 0.037889244276543256]
	TIME [epoch: 27.1 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041703906459364413		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.041703906459364413 | validation: 0.038302464155513076]
	TIME [epoch: 27.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041819988483371394		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.041819988483371394 | validation: 0.037038054527809165]
	TIME [epoch: 27.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04123343483910777		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.04123343483910777 | validation: 0.038604400504121505]
	TIME [epoch: 27.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039602834442876385		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.039602834442876385 | validation: 0.035042820590654514]
	TIME [epoch: 27.1 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04005864481185467		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.04005864481185467 | validation: 0.03600374653048697]
	TIME [epoch: 27.1 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03912819445071334		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.03912819445071334 | validation: 0.035522292030718064]
	TIME [epoch: 27.1 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087281688288227		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.04087281688288227 | validation: 0.036307427331432676]
	TIME [epoch: 27.1 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04268237046525559		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.04268237046525559 | validation: 0.041103593863172264]
	TIME [epoch: 27.2 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024117236483776		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.04024117236483776 | validation: 0.03853424941793833]
	TIME [epoch: 27.1 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159702214801402		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.04159702214801402 | validation: 0.040891081878992]
	TIME [epoch: 27.1 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059247769140253		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.04059247769140253 | validation: 0.038388125007984025]
	TIME [epoch: 27.1 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040631574485197		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.040631574485197 | validation: 0.03708375083114718]
	TIME [epoch: 27.1 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038543153543518005		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.038543153543518005 | validation: 0.03590302333653899]
	TIME [epoch: 27.1 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03934514311473212		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.03934514311473212 | validation: 0.038581132797504]
	TIME [epoch: 27.1 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942077151889014		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.03942077151889014 | validation: 0.03625099625642685]
	TIME [epoch: 27.1 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03982830786043289		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.03982830786043289 | validation: 0.03717664630055193]
	TIME [epoch: 27.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042202236022057936		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.042202236022057936 | validation: 0.038756842662780545]
	TIME [epoch: 27.1 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04240628856372485		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.04240628856372485 | validation: 0.04432300174446974]
	TIME [epoch: 27.1 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03976237341877518		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.03976237341877518 | validation: 0.033651372700223836]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1212.pth
	Model improved!!!
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039058458957332676		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.039058458957332676 | validation: 0.03586465121691762]
	TIME [epoch: 27 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038770615415083205		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.038770615415083205 | validation: 0.0423445036364425]
	TIME [epoch: 27 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03884663782600837		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.03884663782600837 | validation: 0.042472375852344584]
	TIME [epoch: 27 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968763390810119		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.03968763390810119 | validation: 0.033937902286740616]
	TIME [epoch: 27 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04121588456557876		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.04121588456557876 | validation: 0.03739195535300244]
	TIME [epoch: 27 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837687240480652		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.03837687240480652 | validation: 0.03602887451955169]
	TIME [epoch: 27 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942876802023853		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.03942876802023853 | validation: 0.03770924012236837]
	TIME [epoch: 27 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03832155369403766		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.03832155369403766 | validation: 0.037944004977945124]
	TIME [epoch: 27 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040146124117221335		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.040146124117221335 | validation: 0.038845550073202784]
	TIME [epoch: 27 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037417770732919		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.04037417770732919 | validation: 0.03434221945412871]
	TIME [epoch: 27 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942859350580746		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.03942859350580746 | validation: 0.0375042991457643]
	TIME [epoch: 27.1 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04005668334080818		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.04005668334080818 | validation: 0.03627360557068967]
	TIME [epoch: 27.1 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03818914666520232		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.03818914666520232 | validation: 0.04201311919115237]
	TIME [epoch: 27.1 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966888448510293		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.03966888448510293 | validation: 0.03515806440113552]
	TIME [epoch: 27.1 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039765859490872205		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.039765859490872205 | validation: 0.03543873519381393]
	TIME [epoch: 27.1 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03991091340053063		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.03991091340053063 | validation: 0.037477535452660325]
	TIME [epoch: 27.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019190080136999		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.04019190080136999 | validation: 0.03858025962877559]
	TIME [epoch: 27.2 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039733954948722436		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.039733954948722436 | validation: 0.03561926637765537]
	TIME [epoch: 27.2 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231399044562799		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.04231399044562799 | validation: 0.04114327973396517]
	TIME [epoch: 27.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967346203857858		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.03967346203857858 | validation: 0.03730319738106399]
	TIME [epoch: 27.2 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040432613353298584		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.040432613353298584 | validation: 0.035929961385765395]
	TIME [epoch: 27.1 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038380969404961246		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.038380969404961246 | validation: 0.035942445592744746]
	TIME [epoch: 27.2 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040891417654004406		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.040891417654004406 | validation: 0.03679985951183478]
	TIME [epoch: 27.2 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04055371361766123		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.04055371361766123 | validation: 0.04269796599295633]
	TIME [epoch: 27.1 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054683573553415		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.04054683573553415 | validation: 0.036262059991517874]
	TIME [epoch: 27.2 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04074441930151778		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.04074441930151778 | validation: 0.03615599381901753]
	TIME [epoch: 27.1 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039429462461062964		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.039429462461062964 | validation: 0.03739394799270786]
	TIME [epoch: 27.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03890042486943735		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.03890042486943735 | validation: 0.03795941868303754]
	TIME [epoch: 27.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04077295935942911		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.04077295935942911 | validation: 0.0381174697585734]
	TIME [epoch: 27.2 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039441337177841755		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.039441337177841755 | validation: 0.03945162387014975]
	TIME [epoch: 27.1 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039590515015344975		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.039590515015344975 | validation: 0.03873086189352748]
	TIME [epoch: 27.1 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039730921432988564		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.039730921432988564 | validation: 0.03850715959619422]
	TIME [epoch: 27.1 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039365406146176916		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.039365406146176916 | validation: 0.03635737395989715]
	TIME [epoch: 27.1 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03912541524650638		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.03912541524650638 | validation: 0.03486012279817759]
	TIME [epoch: 27.1 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03866749462621071		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.03866749462621071 | validation: 0.03510551284828378]
	TIME [epoch: 27.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041569992448766094		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.041569992448766094 | validation: 0.039679072764963325]
	TIME [epoch: 27.1 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03896532132735514		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.03896532132735514 | validation: 0.039604962069089436]
	TIME [epoch: 27.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898225613418681		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.03898225613418681 | validation: 0.03520722852747448]
	TIME [epoch: 27.1 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039486645387325936		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.039486645387325936 | validation: 0.036856429546751446]
	TIME [epoch: 27.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038162687360749716		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.038162687360749716 | validation: 0.03702929612197114]
	TIME [epoch: 27.1 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902879482770878		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.03902879482770878 | validation: 0.041247749542192416]
	TIME [epoch: 27.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061706740472393		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.04061706740472393 | validation: 0.03864512510351462]
	TIME [epoch: 27.1 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040296783103891294		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.040296783103891294 | validation: 0.035107148946932856]
	TIME [epoch: 27.1 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03758585038169892		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.03758585038169892 | validation: 0.036184917920198005]
	TIME [epoch: 27.1 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0383012519020393		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.0383012519020393 | validation: 0.03193105711879154]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1257.pth
	Model improved!!!
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040354268964279225		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.040354268964279225 | validation: 0.03607787121612906]
	TIME [epoch: 27.1 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03797786606085029		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.03797786606085029 | validation: 0.03685400299935628]
	TIME [epoch: 27.1 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037803583010276635		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.037803583010276635 | validation: 0.036693542644605504]
	TIME [epoch: 27.1 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037941034668378414		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.037941034668378414 | validation: 0.036581680069910694]
	TIME [epoch: 27.1 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03987603954490985		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.03987603954490985 | validation: 0.03994883049907449]
	TIME [epoch: 27.1 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03907443744963047		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.03907443744963047 | validation: 0.03667293995845383]
	TIME [epoch: 27.1 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778590833221773		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.03778590833221773 | validation: 0.039357750984514085]
	TIME [epoch: 27.1 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864632230453417		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.03864632230453417 | validation: 0.0367918049712627]
	TIME [epoch: 27.1 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03963284714626933		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.03963284714626933 | validation: 0.03485959591931205]
	TIME [epoch: 27.1 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0374014605090585		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.0374014605090585 | validation: 0.03508343086062212]
	TIME [epoch: 27.1 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836235973921514		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.03836235973921514 | validation: 0.03656745659683646]
	TIME [epoch: 27.1 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764481643947498		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.03764481643947498 | validation: 0.03625034598046485]
	TIME [epoch: 27.2 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041215775472154895		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.041215775472154895 | validation: 0.042339872274580664]
	TIME [epoch: 27.1 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913495776323203		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.03913495776323203 | validation: 0.03394434978226545]
	TIME [epoch: 27.1 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03960858143043503		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.03960858143043503 | validation: 0.034485325153309154]
	TIME [epoch: 27.1 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037878102527077004		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.037878102527077004 | validation: 0.03921445938186735]
	TIME [epoch: 27.1 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369688207413597		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.0369688207413597 | validation: 0.03603785328781722]
	TIME [epoch: 27.1 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898800058356398		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.03898800058356398 | validation: 0.033762949699632575]
	TIME [epoch: 27.1 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039958287631009046		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.039958287631009046 | validation: 0.03717948651391784]
	TIME [epoch: 27.1 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038525188457640434		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.038525188457640434 | validation: 0.03452499460105623]
	TIME [epoch: 27.1 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148194290568746		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.04148194290568746 | validation: 0.03546458529887236]
	TIME [epoch: 27.1 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040286931834269915		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.040286931834269915 | validation: 0.036895455188680835]
	TIME [epoch: 27.1 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873153323573748		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.03873153323573748 | validation: 0.04500211701620904]
	TIME [epoch: 27.1 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04026320657669408		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.04026320657669408 | validation: 0.034990010997590075]
	TIME [epoch: 27.1 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038793174616938264		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.038793174616938264 | validation: 0.034337130609952596]
	TIME [epoch: 27.1 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037821382951068176		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.037821382951068176 | validation: 0.033204601964891035]
	TIME [epoch: 27.1 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858815830331537		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.03858815830331537 | validation: 0.03458194244274981]
	TIME [epoch: 27.1 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885714797405964		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.03885714797405964 | validation: 0.031780987463114745]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1285.pth
	Model improved!!!
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037004945497441134		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.037004945497441134 | validation: 0.03177914237600367]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1286.pth
	Model improved!!!
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03832121140882129		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.03832121140882129 | validation: 0.03778163747597252]
	TIME [epoch: 27.2 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03695954862673813		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.03695954862673813 | validation: 0.037557034373247214]
	TIME [epoch: 27.1 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843639870323539		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.03843639870323539 | validation: 0.03734436415046277]
	TIME [epoch: 27.1 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038576198161709034		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.038576198161709034 | validation: 0.04184868312538879]
	TIME [epoch: 27.1 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0399993670291201		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.0399993670291201 | validation: 0.033966919487703716]
	TIME [epoch: 27.1 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850548117541501		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.03850548117541501 | validation: 0.033978845413773454]
	TIME [epoch: 27.2 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038318085735079055		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.038318085735079055 | validation: 0.03647721109480589]
	TIME [epoch: 27.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040147629344181465		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.040147629344181465 | validation: 0.0338105906294067]
	TIME [epoch: 27.1 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03805771349643189		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.03805771349643189 | validation: 0.034316136228258576]
	TIME [epoch: 27.2 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066379835669479		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04066379835669479 | validation: 0.038584496227918375]
	TIME [epoch: 27 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03972681172552225		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.03972681172552225 | validation: 0.039803040512337975]
	TIME [epoch: 27 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03900676766422956		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.03900676766422956 | validation: 0.03489037266987142]
	TIME [epoch: 27 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036675730379528475		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.036675730379528475 | validation: 0.03361917410200029]
	TIME [epoch: 27 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03796183603726888		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.03796183603726888 | validation: 0.03638393918191878]
	TIME [epoch: 27 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037701800481499737		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.037701800481499737 | validation: 0.03648828139285416]
	TIME [epoch: 27.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04051928474590011		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.04051928474590011 | validation: 0.036578177314381004]
	TIME [epoch: 27.2 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038188364798237694		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.038188364798237694 | validation: 0.03427977953058516]
	TIME [epoch: 27.2 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038330712974841		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.038330712974841 | validation: 0.03360183971262678]
	TIME [epoch: 27.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03750045998940388		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.03750045998940388 | validation: 0.0338604119211448]
	TIME [epoch: 27.2 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04049468545809553		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.04049468545809553 | validation: 0.03235804166856722]
	TIME [epoch: 27.2 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038317154071081244		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.038317154071081244 | validation: 0.034834852445151075]
	TIME [epoch: 27.2 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748447391021772		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.03748447391021772 | validation: 0.03474085866258491]
	TIME [epoch: 27.2 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03710858610837154		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.03710858610837154 | validation: 0.034918234161819765]
	TIME [epoch: 27.2 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0390114842897478		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.0390114842897478 | validation: 0.03427004152467907]
	TIME [epoch: 27.2 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037856907470128605		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.037856907470128605 | validation: 0.03568385988856657]
	TIME [epoch: 27.2 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202460395432484		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.04202460395432484 | validation: 0.03377802548031046]
	TIME [epoch: 27.2 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038798146250570695		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.038798146250570695 | validation: 0.039657985523967966]
	TIME [epoch: 27.2 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092969363765656		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.04092969363765656 | validation: 0.036384228869282886]
	TIME [epoch: 27.2 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927901961044074		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.03927901961044074 | validation: 0.03430467079173458]
	TIME [epoch: 27 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929225244414359		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.03929225244414359 | validation: 0.03692272741637117]
	TIME [epoch: 27 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03869063802044506		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.03869063802044506 | validation: 0.03244822165871314]
	TIME [epoch: 27.1 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039177906073649925		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.039177906073649925 | validation: 0.03467804158915474]
	TIME [epoch: 27 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03863896408538069		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.03863896408538069 | validation: 0.032430260044214156]
	TIME [epoch: 27 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037820654939861725		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.037820654939861725 | validation: 0.03654101967336414]
	TIME [epoch: 27 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806863246823264		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.03806863246823264 | validation: 0.03758340922394704]
	TIME [epoch: 27 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732163045486129		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.03732163045486129 | validation: 0.03572216966617897]
	TIME [epoch: 27 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0381559356722233		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.0381559356722233 | validation: 0.03338039464205017]
	TIME [epoch: 27 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03849358016577994		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.03849358016577994 | validation: 0.03214247929753428]
	TIME [epoch: 27 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03709749058070121		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.03709749058070121 | validation: 0.03172577385217906]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1325.pth
	Model improved!!!
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039271692162458745		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.039271692162458745 | validation: 0.03211880851230149]
	TIME [epoch: 27.2 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702013547603905		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.03702013547603905 | validation: 0.03337800074741836]
	TIME [epoch: 27.2 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037813135985007455		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.037813135985007455 | validation: 0.03167592903959076]
	TIME [epoch: 27.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1328.pth
	Model improved!!!
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0377583480007418		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.0377583480007418 | validation: 0.03288987519803721]
	TIME [epoch: 27.1 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03879778112723459		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.03879778112723459 | validation: 0.033152232601256416]
	TIME [epoch: 27.1 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03769307773485202		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.03769307773485202 | validation: 0.03364782567737539]
	TIME [epoch: 27.1 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03586053750933854		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.03586053750933854 | validation: 0.0328116316992107]
	TIME [epoch: 27.1 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671131522209074		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.03671131522209074 | validation: 0.034350113930527205]
	TIME [epoch: 27.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036279856863006506		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.036279856863006506 | validation: 0.035635427106653794]
	TIME [epoch: 27.1 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03648267984816294		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.03648267984816294 | validation: 0.03305197101907433]
	TIME [epoch: 27.2 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852642367854259		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.03852642367854259 | validation: 0.03000132571465547]
	TIME [epoch: 27.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1336.pth
	Model improved!!!
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736270103185998		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.03736270103185998 | validation: 0.03204421497370415]
	TIME [epoch: 27.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770649878312678		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.03770649878312678 | validation: 0.03341851700418286]
	TIME [epoch: 27.2 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657098582388026		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.03657098582388026 | validation: 0.032872961427694705]
	TIME [epoch: 27.2 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632054016925624		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.03632054016925624 | validation: 0.03413470290934139]
	TIME [epoch: 27.2 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037121229305298827		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.037121229305298827 | validation: 0.03321689307304685]
	TIME [epoch: 27.2 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721032376244717		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.03721032376244717 | validation: 0.034488878211291554]
	TIME [epoch: 27.2 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03638648489451996		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.03638648489451996 | validation: 0.031123256331420514]
	TIME [epoch: 27.2 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037959822212693076		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.037959822212693076 | validation: 0.03909784312903861]
	TIME [epoch: 27.2 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038585666444341256		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.038585666444341256 | validation: 0.0364142161699915]
	TIME [epoch: 27 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722045895933667		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.03722045895933667 | validation: 0.03195547836773529]
	TIME [epoch: 27 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03698122360203427		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.03698122360203427 | validation: 0.032454039554377735]
	TIME [epoch: 27 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707142634085004		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.03707142634085004 | validation: 0.03299273388379061]
	TIME [epoch: 27 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03745050782368368		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.03745050782368368 | validation: 0.03413094454567781]
	TIME [epoch: 27 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037890781618982995		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.037890781618982995 | validation: 0.03464526870395216]
	TIME [epoch: 27 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680541672524046		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.03680541672524046 | validation: 0.03168179718048222]
	TIME [epoch: 27 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03745978819650987		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.03745978819650987 | validation: 0.03358519979550088]
	TIME [epoch: 27 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036800212467417756		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.036800212467417756 | validation: 0.03181987643442241]
	TIME [epoch: 27 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03921346574062687		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.03921346574062687 | validation: 0.03274600438776814]
	TIME [epoch: 27 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038075900923358476		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.038075900923358476 | validation: 0.032480091610238564]
	TIME [epoch: 27 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038134420952475254		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.038134420952475254 | validation: 0.03428978394308157]
	TIME [epoch: 27 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735788524164274		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.03735788524164274 | validation: 0.038325840201317254]
	TIME [epoch: 27 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623796470509425		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.03623796470509425 | validation: 0.03251118416833482]
	TIME [epoch: 27 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03674133708632902		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.03674133708632902 | validation: 0.034248204410233254]
	TIME [epoch: 27 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03775378996128579		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.03775378996128579 | validation: 0.034512509399108784]
	TIME [epoch: 27 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770002943636968		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.03770002943636968 | validation: 0.03205978519441482]
	TIME [epoch: 27 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651641682668299		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.03651641682668299 | validation: 0.034851977266383775]
	TIME [epoch: 27 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037692688646243706		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.037692688646243706 | validation: 0.034195646905697374]
	TIME [epoch: 27 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740155524342919		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.03740155524342919 | validation: 0.03403046159009846]
	TIME [epoch: 27 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03853073682859757		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.03853073682859757 | validation: 0.031241633305695873]
	TIME [epoch: 27 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668266677037183		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.03668266677037183 | validation: 0.032043908464358284]
	TIME [epoch: 27 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806950966306165		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.03806950966306165 | validation: 0.03242228505083674]
	TIME [epoch: 27 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03869128779000831		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.03869128779000831 | validation: 0.032828136174782564]
	TIME [epoch: 27 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038484369426518616		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.038484369426518616 | validation: 0.03426312729872649]
	TIME [epoch: 27 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03767383085021837		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.03767383085021837 | validation: 0.03432062568336373]
	TIME [epoch: 27 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024545018079666		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.04024545018079666 | validation: 0.03479751630838818]
	TIME [epoch: 27 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0367213733531452		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.0367213733531452 | validation: 0.0338213726975803]
	TIME [epoch: 27 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03723360327437266		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.03723360327437266 | validation: 0.03191800967153557]
	TIME [epoch: 27 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037503892312002074		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.037503892312002074 | validation: 0.03195791074543177]
	TIME [epoch: 27 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03589707149062853		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.03589707149062853 | validation: 0.03318363565211365]
	TIME [epoch: 27 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036573224900123893		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.036573224900123893 | validation: 0.03198734629020272]
	TIME [epoch: 27 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595063462005342		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.03595063462005342 | validation: 0.033058231929589336]
	TIME [epoch: 27 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927676306111885		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.03927676306111885 | validation: 0.03329134676167394]
	TIME [epoch: 27 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360757058965434		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.0360757058965434 | validation: 0.031313389037259734]
	TIME [epoch: 27 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564283878645822		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.03564283878645822 | validation: 0.03687628868750086]
	TIME [epoch: 27 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037017722875204405		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.037017722875204405 | validation: 0.03205158335623864]
	TIME [epoch: 27 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037048874359078825		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.037048874359078825 | validation: 0.03204313469646586]
	TIME [epoch: 27 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035637124982264956		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.035637124982264956 | validation: 0.03264047153505777]
	TIME [epoch: 27 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038345410237960224		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.038345410237960224 | validation: 0.03215198419355545]
	TIME [epoch: 27 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037030176782626135		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.037030176782626135 | validation: 0.03358876457631151]
	TIME [epoch: 27 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035582269106213114		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.035582269106213114 | validation: 0.03624657433297468]
	TIME [epoch: 27 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037683040596387954		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.037683040596387954 | validation: 0.034418325987404234]
	TIME [epoch: 27 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712611599773874		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.03712611599773874 | validation: 0.03593989293819655]
	TIME [epoch: 27 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03514467887298198		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.03514467887298198 | validation: 0.03428010079148326]
	TIME [epoch: 27 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03713828083554478		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.03713828083554478 | validation: 0.029856632252212613]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1390.pth
	Model improved!!!
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906288790138469		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.03906288790138469 | validation: 0.030582566101378494]
	TIME [epoch: 27 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036044749093400226		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.036044749093400226 | validation: 0.036026570861089145]
	TIME [epoch: 27 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03688946573212534		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.03688946573212534 | validation: 0.03384747963795489]
	TIME [epoch: 27 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712602933710287		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.03712602933710287 | validation: 0.03259795047596684]
	TIME [epoch: 27 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562356158413177		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.03562356158413177 | validation: 0.03520994090314738]
	TIME [epoch: 27 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036703910094822066		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.036703910094822066 | validation: 0.0305923155268232]
	TIME [epoch: 27 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806532927897451		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.03806532927897451 | validation: 0.035022899882421264]
	TIME [epoch: 27 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712220966568853		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.03712220966568853 | validation: 0.03488909476921731]
	TIME [epoch: 27 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03710454221226776		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.03710454221226776 | validation: 0.033002752890236034]
	TIME [epoch: 27 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036636512718726505		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.036636512718726505 | validation: 0.03284587356120227]
	TIME [epoch: 27 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552272937642024		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.03552272937642024 | validation: 0.032774871193482484]
	TIME [epoch: 27 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03629185370897113		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.03629185370897113 | validation: 0.034141565462189216]
	TIME [epoch: 27 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644641054959069		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.03644641054959069 | validation: 0.033707656458084816]
	TIME [epoch: 27 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036140001713565795		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.036140001713565795 | validation: 0.03247630741242864]
	TIME [epoch: 27 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740378902400292		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.03740378902400292 | validation: 0.03201707557719541]
	TIME [epoch: 27 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03674158673697664		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.03674158673697664 | validation: 0.03216571983322324]
	TIME [epoch: 27 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400815769075096		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.03400815769075096 | validation: 0.03129605776456586]
	TIME [epoch: 27 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558083617849938		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.03558083617849938 | validation: 0.033210109967782155]
	TIME [epoch: 27 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03650068099191037		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.03650068099191037 | validation: 0.03654644603385132]
	TIME [epoch: 27 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461267262974882		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.03461267262974882 | validation: 0.030098374843949893]
	TIME [epoch: 27 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035455564700759806		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.035455564700759806 | validation: 0.03524053562355496]
	TIME [epoch: 27 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039694601352735254		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.039694601352735254 | validation: 0.034555620821502396]
	TIME [epoch: 27 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360782591936998		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.0360782591936998 | validation: 0.031726415581009526]
	TIME [epoch: 27 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753681097572572		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.03753681097572572 | validation: 0.03383129775937995]
	TIME [epoch: 27 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037006367427268386		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.037006367427268386 | validation: 0.03173163070860583]
	TIME [epoch: 27 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035485432902650445		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.035485432902650445 | validation: 0.03568372464552094]
	TIME [epoch: 27 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037008792425907426		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.037008792425907426 | validation: 0.03390661963218351]
	TIME [epoch: 27 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03642570834453737		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.03642570834453737 | validation: 0.03000949005459711]
	TIME [epoch: 27 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649184080072768		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.03649184080072768 | validation: 0.031294256464662995]
	TIME [epoch: 27 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857676040761544		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.03857676040761544 | validation: 0.035054529838441574]
	TIME [epoch: 27 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03741759419930783		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.03741759419930783 | validation: 0.035256288918415556]
	TIME [epoch: 27 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03534406671192154		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.03534406671192154 | validation: 0.03573268334497265]
	TIME [epoch: 27 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485065147358608		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.03485065147358608 | validation: 0.03893269870162356]
	TIME [epoch: 27 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036115514539702694		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.036115514539702694 | validation: 0.03442694857736267]
	TIME [epoch: 27 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03582885220445352		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.03582885220445352 | validation: 0.033715505227842855]
	TIME [epoch: 27 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038562479229048825		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.038562479229048825 | validation: 0.03396534046126242]
	TIME [epoch: 27 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692423736324968		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.03692423736324968 | validation: 0.03679287284531847]
	TIME [epoch: 27 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676644549156566		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.03676644549156566 | validation: 0.03198137101004205]
	TIME [epoch: 27 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03910549715493211		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.03910549715493211 | validation: 0.032696889556631574]
	TIME [epoch: 27 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036311870798597455		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.036311870798597455 | validation: 0.03300077206006672]
	TIME [epoch: 27 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037464225670021216		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.037464225670021216 | validation: 0.03056991566951757]
	TIME [epoch: 27 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378175451362948		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.0378175451362948 | validation: 0.031039610371864236]
	TIME [epoch: 27 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035294732604491175		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.035294732604491175 | validation: 0.03194268643597258]
	TIME [epoch: 27 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03521295550371723		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.03521295550371723 | validation: 0.030230448027961446]
	TIME [epoch: 27 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03482880836934656		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.03482880836934656 | validation: 0.031145233909902793]
	TIME [epoch: 27 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035671461072456764		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.035671461072456764 | validation: 0.033366190957107036]
	TIME [epoch: 27 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036187074825861094		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.036187074825861094 | validation: 0.03293953985601111]
	TIME [epoch: 27.1 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532029218320957		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.03532029218320957 | validation: 0.037377306300993264]
	TIME [epoch: 27 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037186235562802865		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.037186235562802865 | validation: 0.03447231449085302]
	TIME [epoch: 27 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653223945036209		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.03653223945036209 | validation: 0.03382837237642964]
	TIME [epoch: 27 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600872277367686		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.03600872277367686 | validation: 0.03302590854871852]
	TIME [epoch: 27 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03708161202363459		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.03708161202363459 | validation: 0.03234367348640038]
	TIME [epoch: 27 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03661243940287845		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.03661243940287845 | validation: 0.030246540153974613]
	TIME [epoch: 27 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035722295714933745		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.035722295714933745 | validation: 0.03032020290389087]
	TIME [epoch: 27 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036450985683985394		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.036450985683985394 | validation: 0.032336616913150595]
	TIME [epoch: 27 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036756916477120004		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.036756916477120004 | validation: 0.03255291846382128]
	TIME [epoch: 27 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037105758025569646		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.037105758025569646 | validation: 0.034993266595439054]
	TIME [epoch: 27 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668894664460588		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.03668894664460588 | validation: 0.031798980162634195]
	TIME [epoch: 27 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644520604408372		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.03644520604408372 | validation: 0.03018863991899963]
	TIME [epoch: 27 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541998205982768		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.03541998205982768 | validation: 0.028612882412655583]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1450.pth
	Model improved!!!
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732499771967594		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.03732499771967594 | validation: 0.03779046661232344]
	TIME [epoch: 27 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035410315679723844		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.035410315679723844 | validation: 0.03200540847582968]
	TIME [epoch: 27 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03536906753413719		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.03536906753413719 | validation: 0.029230494718041112]
	TIME [epoch: 27 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036398731781879495		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.036398731781879495 | validation: 0.029237578122552454]
	TIME [epoch: 27 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0355766798255711		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.0355766798255711 | validation: 0.030887862765235297]
	TIME [epoch: 27 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035909652444014936		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.035909652444014936 | validation: 0.03377260062285727]
	TIME [epoch: 27 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03458357890679477		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.03458357890679477 | validation: 0.03064251508686049]
	TIME [epoch: 27 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663182708244397		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.03663182708244397 | validation: 0.032431177780705124]
	TIME [epoch: 27 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035951704386098744		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.035951704386098744 | validation: 0.030665160300775887]
	TIME [epoch: 27 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622305412353569		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.03622305412353569 | validation: 0.03385552089695722]
	TIME [epoch: 27 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790421376260816		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.03790421376260816 | validation: 0.03076425358595597]
	TIME [epoch: 27 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03631520893910063		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.03631520893910063 | validation: 0.03238071003604725]
	TIME [epoch: 27 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608687003304188		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.03608687003304188 | validation: 0.03385415121502241]
	TIME [epoch: 27 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584707242081312		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.03584707242081312 | validation: 0.032024250253296284]
	TIME [epoch: 27 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558493486409235		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.03558493486409235 | validation: 0.02861834856295611]
	TIME [epoch: 27 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03610337537268708		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.03610337537268708 | validation: 0.03066606799151869]
	TIME [epoch: 27 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03377675156048455		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.03377675156048455 | validation: 0.03153195558937179]
	TIME [epoch: 27 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037261887115169805		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.037261887115169805 | validation: 0.031539027158621856]
	TIME [epoch: 27 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461299155336179		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.03461299155336179 | validation: 0.030537475666610192]
	TIME [epoch: 27 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03645081047434963		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.03645081047434963 | validation: 0.033388598158981965]
	TIME [epoch: 27 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03627280441379297		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.03627280441379297 | validation: 0.03169398601775421]
	TIME [epoch: 27 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03851803079104248		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.03851803079104248 | validation: 0.03346387989881671]
	TIME [epoch: 27 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036044706937979586		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.036044706937979586 | validation: 0.03185951144304657]
	TIME [epoch: 27 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03630947361029921		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.03630947361029921 | validation: 0.031246616262075047]
	TIME [epoch: 27 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037030327457648136		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.037030327457648136 | validation: 0.03289459153958781]
	TIME [epoch: 27 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03826422101026552		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.03826422101026552 | validation: 0.029282763633540255]
	TIME [epoch: 27 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03772550270943265		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.03772550270943265 | validation: 0.029016352748808174]
	TIME [epoch: 27 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0351449757575034		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.0351449757575034 | validation: 0.030313087561417706]
	TIME [epoch: 27 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03442844891387355		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.03442844891387355 | validation: 0.03110722880229663]
	TIME [epoch: 27 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035670494256288085		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.035670494256288085 | validation: 0.032710670571281554]
	TIME [epoch: 27 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034734721555666995		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.034734721555666995 | validation: 0.03151251262375806]
	TIME [epoch: 27 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03883898405465037		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.03883898405465037 | validation: 0.030247085955387568]
	TIME [epoch: 27 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034783410857727175		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.034783410857727175 | validation: 0.030317240727464202]
	TIME [epoch: 27 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03665438892239883		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.03665438892239883 | validation: 0.029478295569557368]
	TIME [epoch: 27 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03495856255122232		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.03495856255122232 | validation: 0.032565386962682574]
	TIME [epoch: 27 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651121245043295		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.03651121245043295 | validation: 0.030321453635044515]
	TIME [epoch: 27 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623337283990955		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.03623337283990955 | validation: 0.028070252352673433]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1487.pth
	Model improved!!!
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034536093053570374		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.034536093053570374 | validation: 0.031473628296696955]
	TIME [epoch: 27 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037697121759701524		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.037697121759701524 | validation: 0.02762941442397179]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1489.pth
	Model improved!!!
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03586384224356488		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.03586384224356488 | validation: 0.029776737990958226]
	TIME [epoch: 27 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03446579423568246		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.03446579423568246 | validation: 0.029457853752355868]
	TIME [epoch: 27 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03605111729594402		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.03605111729594402 | validation: 0.030045916148063314]
	TIME [epoch: 27 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03631788699924401		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.03631788699924401 | validation: 0.030242214518440905]
	TIME [epoch: 27 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03543678585267752		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.03543678585267752 | validation: 0.029935263028739292]
	TIME [epoch: 27 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03681752613406175		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.03681752613406175 | validation: 0.03290914252482065]
	TIME [epoch: 27 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036355701339922594		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.036355701339922594 | validation: 0.027819992138021533]
	TIME [epoch: 27 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035934221346326733		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.035934221346326733 | validation: 0.034185601736617326]
	TIME [epoch: 27 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497346641790497		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.03497346641790497 | validation: 0.030715053464845002]
	TIME [epoch: 27 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581637137839592		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.03581637137839592 | validation: 0.033456230575762934]
	TIME [epoch: 27 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651851208872975		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.03651851208872975 | validation: 0.030898971628101637]
	TIME [epoch: 27 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03611325167693196		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.03611325167693196 | validation: 0.032622605989407485]
	TIME [epoch: 27 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03667592424008485		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.03667592424008485 | validation: 0.03121985402002836]
	TIME [epoch: 27 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525839453106168		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.03525839453106168 | validation: 0.034359302536558015]
	TIME [epoch: 27 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035724418697885216		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.035724418697885216 | validation: 0.03121088057508874]
	TIME [epoch: 27 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03605741979907277		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.03605741979907277 | validation: 0.031141030652704772]
	TIME [epoch: 27 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578118620221372		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.03578118620221372 | validation: 0.032726477432451696]
	TIME [epoch: 27 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035047321629173185		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.035047321629173185 | validation: 0.029602460664522535]
	TIME [epoch: 27 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509831111139352		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.03509831111139352 | validation: 0.031111753614235307]
	TIME [epoch: 27 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545151373533886		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.03545151373533886 | validation: 0.03414341994114042]
	TIME [epoch: 27 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03409631358503555		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.03409631358503555 | validation: 0.028742512238959697]
	TIME [epoch: 27 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034577899000060246		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.034577899000060246 | validation: 0.03285371088817223]
	TIME [epoch: 27 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541652163783238		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.03541652163783238 | validation: 0.030379421712712486]
	TIME [epoch: 27 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034023290904241305		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.034023290904241305 | validation: 0.032036299956348935]
	TIME [epoch: 27 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590910775451788		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.03590910775451788 | validation: 0.03353665768119666]
	TIME [epoch: 27 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034786773688217254		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.034786773688217254 | validation: 0.03076054763687326]
	TIME [epoch: 27 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035088547110919216		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.035088547110919216 | validation: 0.03066106793108632]
	TIME [epoch: 27 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036481979695418984		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.036481979695418984 | validation: 0.030453656418309827]
	TIME [epoch: 27 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564452807647042		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.03564452807647042 | validation: 0.030095389381709817]
	TIME [epoch: 27 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497297906298703		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.03497297906298703 | validation: 0.03224809444158885]
	TIME [epoch: 27 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03535884948775251		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.03535884948775251 | validation: 0.029419075442553934]
	TIME [epoch: 27 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03599683289282435		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.03599683289282435 | validation: 0.030277102591609963]
	TIME [epoch: 27 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034842961660211114		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.034842961660211114 | validation: 0.03177830711235422]
	TIME [epoch: 27 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03599405232784531		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.03599405232784531 | validation: 0.0314115853363448]
	TIME [epoch: 27 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564566985223669		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.03564566985223669 | validation: 0.029878124038599675]
	TIME [epoch: 27 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03555597642087746		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.03555597642087746 | validation: 0.029171211299113577]
	TIME [epoch: 27 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03494841450258693		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.03494841450258693 | validation: 0.03377966466581487]
	TIME [epoch: 27 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474094113865769		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.03474094113865769 | validation: 0.03273771109108455]
	TIME [epoch: 27 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762917338102424		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.03762917338102424 | validation: 0.032300442486461876]
	TIME [epoch: 27 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03690038466504148		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.03690038466504148 | validation: 0.032889128331112794]
	TIME [epoch: 27 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537203843718397		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.03537203843718397 | validation: 0.03159318624887885]
	TIME [epoch: 27 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441427691408893		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.03441427691408893 | validation: 0.030119986433134273]
	TIME [epoch: 27 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036397237368580304		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.036397237368580304 | validation: 0.03168240636903578]
	TIME [epoch: 27 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358785407464671		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.0358785407464671 | validation: 0.029014795990779864]
	TIME [epoch: 27 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034761843042235056		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.034761843042235056 | validation: 0.033708310325540995]
	TIME [epoch: 27 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03559348896292059		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.03559348896292059 | validation: 0.030636741340701736]
	TIME [epoch: 27 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036163930375113076		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.036163930375113076 | validation: 0.03190093699751264]
	TIME [epoch: 27 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501449258546299		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.03501449258546299 | validation: 0.02829848185983749]
	TIME [epoch: 27 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036463502260922154		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.036463502260922154 | validation: 0.032649115985515634]
	TIME [epoch: 27 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574311681295382		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.03574311681295382 | validation: 0.03076653504855177]
	TIME [epoch: 27 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036707390414141776		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.036707390414141776 | validation: 0.029773831395897565]
	TIME [epoch: 27 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552254845118477		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.03552254845118477 | validation: 0.02858950913658567]
	TIME [epoch: 27 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033851470598176774		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.033851470598176774 | validation: 0.027641294567574137]
	TIME [epoch: 27 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474688232692203		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.03474688232692203 | validation: 0.031191071216946864]
	TIME [epoch: 27 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035292878474426574		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.035292878474426574 | validation: 0.0320867111131789]
	TIME [epoch: 27 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565559217691442		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.03565559217691442 | validation: 0.030172393791815244]
	TIME [epoch: 27 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034696967252871656		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.034696967252871656 | validation: 0.030985621658614262]
	TIME [epoch: 27 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034489498393596926		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.034489498393596926 | validation: 0.030994871787059487]
	TIME [epoch: 27 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03440004869884621		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.03440004869884621 | validation: 0.03019217487345155]
	TIME [epoch: 27 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035794676444325185		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.035794676444325185 | validation: 0.03085979886920839]
	TIME [epoch: 27 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593192524746095		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.03593192524746095 | validation: 0.03238884979682779]
	TIME [epoch: 27 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386050359698474		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.03386050359698474 | validation: 0.03145485249903051]
	TIME [epoch: 27 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361220847827		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.0361220847827 | validation: 0.03230624861792897]
	TIME [epoch: 27 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721067432416413		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.03721067432416413 | validation: 0.030323631098826378]
	TIME [epoch: 27 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037112922084928494		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.037112922084928494 | validation: 0.0289286047206628]
	TIME [epoch: 27 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03555625659285227		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.03555625659285227 | validation: 0.029993722956709802]
	TIME [epoch: 27 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626513443684964		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.03626513443684964 | validation: 0.033120778338514546]
	TIME [epoch: 27 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03443208253461144		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.03443208253461144 | validation: 0.031439348063390306]
	TIME [epoch: 27 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035519140126296235		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.035519140126296235 | validation: 0.030571840804285322]
	TIME [epoch: 27 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474504027011337		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.03474504027011337 | validation: 0.029702300132387796]
	TIME [epoch: 27 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035886742365031504		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.035886742365031504 | validation: 0.03271875607216135]
	TIME [epoch: 27.1 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03412586662371283		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.03412586662371283 | validation: 0.032784776378307456]
	TIME [epoch: 27 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03335224338867625		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.03335224338867625 | validation: 0.030097049715878954]
	TIME [epoch: 27 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344254243291775		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.0344254243291775 | validation: 0.03172379731374049]
	TIME [epoch: 27 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03636072891668398		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.03636072891668398 | validation: 0.028889532267017994]
	TIME [epoch: 27 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03425164180553973		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.03425164180553973 | validation: 0.02925636173260675]
	TIME [epoch: 27 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034899227408835276		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.034899227408835276 | validation: 0.03212351227567147]
	TIME [epoch: 27.1 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523162917565852		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.03523162917565852 | validation: 0.02899112166906466]
	TIME [epoch: 27 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03457061586117135		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.03457061586117135 | validation: 0.028444597643015636]
	TIME [epoch: 27.1 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03408646016567684		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.03408646016567684 | validation: 0.02939851472972631]
	TIME [epoch: 27 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03697414439832326		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.03697414439832326 | validation: 0.02938525230417487]
	TIME [epoch: 27 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036270577035825965		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.036270577035825965 | validation: 0.03246645566461538]
	TIME [epoch: 27 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03507412303043102		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.03507412303043102 | validation: 0.034707048632725904]
	TIME [epoch: 27 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034077296381306924		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.034077296381306924 | validation: 0.029699176834018782]
	TIME [epoch: 27 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03476314559630152		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.03476314559630152 | validation: 0.028073865851230107]
	TIME [epoch: 27 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497197523789974		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.03497197523789974 | validation: 0.02941230595436363]
	TIME [epoch: 27.1 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03508961688053566		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.03508961688053566 | validation: 0.02977788097581542]
	TIME [epoch: 27 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036358882921488006		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.036358882921488006 | validation: 0.032999470030781276]
	TIME [epoch: 27 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404644324115306		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.03404644324115306 | validation: 0.029423607729341706]
	TIME [epoch: 27 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03382426353902997		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.03382426353902997 | validation: 0.031567633768969004]
	TIME [epoch: 27 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03402510909329842		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.03402510909329842 | validation: 0.029010500244544837]
	TIME [epoch: 27 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03336773827003112		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.03336773827003112 | validation: 0.032236867584084984]
	TIME [epoch: 27.1 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034461212884417176		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.034461212884417176 | validation: 0.03291209661161755]
	TIME [epoch: 27 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478759754086829		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.03478759754086829 | validation: 0.0310205049068643]
	TIME [epoch: 27 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542094297456281		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.03542094297456281 | validation: 0.029732134564062086]
	TIME [epoch: 27 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461011849024281		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.03461011849024281 | validation: 0.029309650292546507]
	TIME [epoch: 27 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03570479551556218		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.03570479551556218 | validation: 0.02968804382129031]
	TIME [epoch: 27 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033860431489803465		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.033860431489803465 | validation: 0.028964502480148004]
	TIME [epoch: 27 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035059701851935174		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.035059701851935174 | validation: 0.03557906482907075]
	TIME [epoch: 27 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568828908085493		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.03568828908085493 | validation: 0.03175450849587824]
	TIME [epoch: 27 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03360064792885613		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.03360064792885613 | validation: 0.030468998616737776]
	TIME [epoch: 27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20241105_153655/states/model_phi1_3c_v_mmd1_1590.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 26254.289 seconds.
