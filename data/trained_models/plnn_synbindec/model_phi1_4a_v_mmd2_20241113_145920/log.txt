Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3443806163

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.251948561465349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.251948561465349 | validation: 6.232627571894266]
	TIME [epoch: 186 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.160788996434776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.160788996434776 | validation: 6.117846779702273]
	TIME [epoch: 0.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.117696764262765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.117696764262765 | validation: 6.041439647736389]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.006381652522007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.006381652522007 | validation: 5.954033638899249]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.997061346403235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.997061346403235 | validation: 5.878353041415354]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.87999118011423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.87999118011423 | validation: 5.7710177111235526]
	TIME [epoch: 0.749 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.822261680714196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.822261680714196 | validation: 5.63037201453891]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.757596420052255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.757596420052255 | validation: 5.428320860218481]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.654711106981713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.654711106981713 | validation: 4.889069941138676]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.448027738512913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.448027738512913 | validation: 4.452005917650699]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.309419616731591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.309419616731591 | validation: 3.4490736755680347]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.627769345299758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.627769345299758 | validation: 3.0962161941008106]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.466366299652905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466366299652905 | validation: 3.0254805971631766]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.328147921917732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328147921917732 | validation: 2.9867338986929743]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2978856650672554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2978856650672554 | validation: 2.8636117672572077]
	TIME [epoch: 0.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.302885566258028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.302885566258028 | validation: 2.8255576197142314]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.221971482046795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221971482046795 | validation: 2.72341600227146]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.20220135082038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20220135082038 | validation: 2.7069936997214903]
	TIME [epoch: 0.721 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.181378804028962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.181378804028962 | validation: 2.647132394370319]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.117393469062962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.117393469062962 | validation: 2.934698529034489]
	TIME [epoch: 0.715 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.080077580666709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.080077580666709 | validation: 2.8297736114677075]
	TIME [epoch: 0.718 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.430937590423233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.430937590423233 | validation: 3.56374294778814]
	TIME [epoch: 0.719 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.333746285754188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.333746285754188 | validation: 2.8084867894406447]
	TIME [epoch: 0.715 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3942169417062895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3942169417062895 | validation: 2.6572835535291865]
	TIME [epoch: 0.714 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.311512133607774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.311512133607774 | validation: 2.810513976503712]
	TIME [epoch: 0.717 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3575895336623103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3575895336623103 | validation: 2.8525853879407035]
	TIME [epoch: 0.715 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.708086065363757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.708086065363757 | validation: 2.4956971383181363]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1365002121830723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1365002121830723 | validation: 2.559577791754333]
	TIME [epoch: 0.715 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.111748832099778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.111748832099778 | validation: 2.2745783122665633]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1140583194027887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1140583194027887 | validation: 2.2793516894109227]
	TIME [epoch: 0.715 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.935469949691169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.935469949691169 | validation: 2.210073468846378]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9364119813615963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9364119813615963 | validation: 2.1507213867062744]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8893135111105117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8893135111105117 | validation: 1.995318559664284]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8846770201118117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8846770201118117 | validation: 2.0997547921968547]
	TIME [epoch: 0.718 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8009943605684435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8009943605684435 | validation: 1.9055578576629686]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.753103035418645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.753103035418645 | validation: 1.9453858023651873]
	TIME [epoch: 0.717 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.66489556395239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.66489556395239 | validation: 1.820711050001731]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6154423458030385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6154423458030385 | validation: 1.8031805290100484]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5337493437235268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5337493437235268 | validation: 1.6464233861970468]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5086072468322596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5086072468322596 | validation: 1.7517918250987412]
	TIME [epoch: 0.72 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.518357927767583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.518357927767583 | validation: 1.5916353289664578]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.647736143803306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.647736143803306 | validation: 1.701074585846861]
	TIME [epoch: 0.715 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5154502955560991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5154502955560991 | validation: 1.4638903547091051]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.40486115967189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.40486115967189 | validation: 1.3725282208401635]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3387986306103494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3387986306103494 | validation: 1.4831046684729159]
	TIME [epoch: 0.717 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3249658434587996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3249658434587996 | validation: 1.0503785190270893]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31814670548866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.31814670548866 | validation: 1.2374388412175]
	TIME [epoch: 0.717 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2613554485110792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2613554485110792 | validation: 0.8683762666884015]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228130888811349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.228130888811349 | validation: 0.7736242823502706]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2197555262884348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2197555262884348 | validation: 0.9193060931871021]
	TIME [epoch: 0.719 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4158958489798914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4158958489798914 | validation: 1.900656653984455]
	TIME [epoch: 0.713 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6756974402520752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6756974402520752 | validation: 1.2455712014512061]
	TIME [epoch: 0.712 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2709039389118797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2709039389118797 | validation: 0.9901885379298975]
	TIME [epoch: 0.712 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3187812748602923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3187812748602923 | validation: 0.7920213935386791]
	TIME [epoch: 0.713 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1905119184711523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1905119184711523 | validation: 1.029430718565724]
	TIME [epoch: 0.71 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1251757205783557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1251757205783557 | validation: 0.6255478092513688]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0509570961763002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0509570961763002 | validation: 0.5509749947721451]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044431440686645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.044431440686645 | validation: 0.9862187353466045]
	TIME [epoch: 0.716 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181407689721479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1181407689721479 | validation: 0.5321405640574572]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0369089831741176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0369089831741176 | validation: 0.6021011282546402]
	TIME [epoch: 0.715 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0038923166620153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0038923166620153 | validation: 0.7915341710563236]
	TIME [epoch: 0.714 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0523625781878057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0523625781878057 | validation: 0.5664898625713272]
	TIME [epoch: 0.714 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0327903498692093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0327903498692093 | validation: 1.1650354348851266]
	TIME [epoch: 0.716 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1305494132508822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1305494132508822 | validation: 0.7687897954012446]
	TIME [epoch: 0.715 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9821022501996822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9821022501996822 | validation: 0.6369610883740144]
	TIME [epoch: 0.714 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095985765728353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1095985765728353 | validation: 1.0400238832568829]
	TIME [epoch: 0.714 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0524769513776218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0524769513776218 | validation: 0.7592594547944008]
	TIME [epoch: 0.713 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9610935941476945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9610935941476945 | validation: 0.5395677963944184]
	TIME [epoch: 0.713 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0143451256896945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0143451256896945 | validation: 0.6634267685467267]
	TIME [epoch: 0.712 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360312202382807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9360312202382807 | validation: 0.6597904049926542]
	TIME [epoch: 0.713 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9296018853573733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9296018853573733 | validation: 0.5310753276876351]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9733205334394461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9733205334394461 | validation: 0.7686067995608656]
	TIME [epoch: 0.716 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566054704623201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9566054704623201 | validation: 0.6314121097358202]
	TIME [epoch: 0.717 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9286716242210276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9286716242210276 | validation: 0.5984303136699468]
	TIME [epoch: 0.717 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835350252081747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9835350252081747 | validation: 0.8269702814159392]
	TIME [epoch: 0.715 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0267538447916296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0267538447916296 | validation: 0.681462303134034]
	TIME [epoch: 0.716 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9628721838530407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9628721838530407 | validation: 0.5413930811622768]
	TIME [epoch: 0.716 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0207636565139466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0207636565139466 | validation: 0.8102130421283253]
	TIME [epoch: 0.716 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507827996862406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507827996862406 | validation: 0.5566454535713609]
	TIME [epoch: 0.714 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9441707824254124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9441707824254124 | validation: 0.678574465078776]
	TIME [epoch: 0.715 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065572925287367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9065572925287367 | validation: 0.5960691173669423]
	TIME [epoch: 0.716 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918569601818034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918569601818034 | validation: 0.5567919477719658]
	TIME [epoch: 0.714 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9071192008236415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9071192008236415 | validation: 0.8546494392716828]
	TIME [epoch: 0.713 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9198547890404557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9198547890404557 | validation: 0.6008308784122094]
	TIME [epoch: 0.714 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049655683589759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049655683589759 | validation: 0.7409276015257821]
	TIME [epoch: 0.715 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925027431469523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.925027431469523 | validation: 0.7359252095237583]
	TIME [epoch: 0.713 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9667806966432493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9667806966432493 | validation: 0.6920224504045082]
	TIME [epoch: 0.713 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0682685294148244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0682685294148244 | validation: 0.8212070370139524]
	TIME [epoch: 0.715 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9169055978683698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9169055978683698 | validation: 0.5471321432467571]
	TIME [epoch: 0.716 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060793120109844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9060793120109844 | validation: 0.6826306775290722]
	TIME [epoch: 0.714 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861535864053938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861535864053938 | validation: 0.6662599096947881]
	TIME [epoch: 0.715 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8792982755989206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8792982755989206 | validation: 0.7443936550164754]
	TIME [epoch: 0.716 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021411231928136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9021411231928136 | validation: 0.6539992687112492]
	TIME [epoch: 0.717 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9673425887839989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9673425887839989 | validation: 0.9107869264767647]
	TIME [epoch: 0.715 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9735864185543787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9735864185543787 | validation: 0.70697786723212]
	TIME [epoch: 0.712 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9061842482029963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9061842482029963 | validation: 0.574925064507203]
	TIME [epoch: 0.713 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996416468999333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8996416468999333 | validation: 0.9529960504856003]
	TIME [epoch: 0.715 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9510131898372687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9510131898372687 | validation: 0.547874402396656]
	TIME [epoch: 0.715 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986695005881953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9986695005881953 | validation: 0.7740941413336153]
	TIME [epoch: 0.721 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657706510341703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657706510341703 | validation: 0.6893602468378588]
	TIME [epoch: 0.715 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545670223271349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545670223271349 | validation: 0.5887537760268149]
	TIME [epoch: 0.718 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797331563915495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8797331563915495 | validation: 0.837039865987308]
	TIME [epoch: 0.718 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933653081722233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8933653081722233 | validation: 0.5813823728671065]
	TIME [epoch: 0.718 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815371921359492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815371921359492 | validation: 0.8663487590542123]
	TIME [epoch: 0.716 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134842092007334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9134842092007334 | validation: 0.6568327882184646]
	TIME [epoch: 0.716 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945369947658329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945369947658329 | validation: 0.8021091312127254]
	TIME [epoch: 0.715 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9841714537259152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9841714537259152 | validation: 1.0377904052009452]
	TIME [epoch: 0.714 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0414590920125013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0414590920125013 | validation: 0.6227203699483295]
	TIME [epoch: 0.713 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552151731959856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8552151731959856 | validation: 0.6438249156247253]
	TIME [epoch: 0.712 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496143352806315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496143352806315 | validation: 0.8293023800434549]
	TIME [epoch: 0.72 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983333218021318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983333218021318 | validation: 0.6292101182612312]
	TIME [epoch: 0.713 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095031502575962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9095031502575962 | validation: 0.8142725779715434]
	TIME [epoch: 0.713 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041886516915164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9041886516915164 | validation: 0.6972469806589126]
	TIME [epoch: 0.716 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680590627043463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680590627043463 | validation: 0.6172162344280847]
	TIME [epoch: 0.72 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770661892177123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8770661892177123 | validation: 0.7460805972452749]
	TIME [epoch: 0.714 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694948618184338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8694948618184338 | validation: 0.6094593750289323]
	TIME [epoch: 0.713 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.910562555096241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.910562555096241 | validation: 0.9273429187437021]
	TIME [epoch: 0.712 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.947745787850419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.947745787850419 | validation: 0.5665581336781467]
	TIME [epoch: 0.714 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349821257171768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9349821257171768 | validation: 0.8493238940534233]
	TIME [epoch: 0.712 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677648200095149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8677648200095149 | validation: 0.6000785189404407]
	TIME [epoch: 0.712 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730514661579131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730514661579131 | validation: 0.9137172661542361]
	TIME [epoch: 0.713 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199016533025335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9199016533025335 | validation: 0.7678675374405499]
	TIME [epoch: 0.715 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9272270337441646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9272270337441646 | validation: 0.8368697497331294]
	TIME [epoch: 0.714 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600961634467331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9600961634467331 | validation: 0.6693460502134061]
	TIME [epoch: 0.713 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651848682537636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651848682537636 | validation: 0.8559987311681317]
	TIME [epoch: 0.712 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653587564191076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8653587564191076 | validation: 0.5894640412191814]
	TIME [epoch: 0.714 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8619468430511056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8619468430511056 | validation: 0.8070807279043735]
	TIME [epoch: 0.714 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860078760357182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860078760357182 | validation: 0.5751630508456115]
	TIME [epoch: 0.713 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620057078033517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8620057078033517 | validation: 0.7894328387460082]
	TIME [epoch: 0.712 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84621541774995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84621541774995 | validation: 0.5584377087962563]
	TIME [epoch: 0.717 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669978067199651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8669978067199651 | validation: 0.827570660385055]
	TIME [epoch: 0.713 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887174081117409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8887174081117409 | validation: 0.6248867697692546]
	TIME [epoch: 0.712 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9498935191008464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9498935191008464 | validation: 0.748584522965174]
	TIME [epoch: 0.725 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9118667332168824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9118667332168824 | validation: 0.9953621873196327]
	TIME [epoch: 0.716 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9817069625764537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9817069625764537 | validation: 0.6169189054250848]
	TIME [epoch: 0.713 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749704308334926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8749704308334926 | validation: 0.7231636205745575]
	TIME [epoch: 0.713 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8435012591751514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8435012591751514 | validation: 0.6820768793746729]
	TIME [epoch: 0.713 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264052605784121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264052605784121 | validation: 0.6759731895817032]
	TIME [epoch: 0.717 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207511587039034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207511587039034 | validation: 0.7075885602850595]
	TIME [epoch: 0.717 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8225765759446322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8225765759446322 | validation: 0.5996312586558016]
	TIME [epoch: 0.715 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318616999720283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318616999720283 | validation: 0.8102180067504353]
	TIME [epoch: 0.717 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490355596757142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8490355596757142 | validation: 0.5377743181375553]
	TIME [epoch: 0.715 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926746143062213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926746143062213 | validation: 1.0416998726505935]
	TIME [epoch: 0.715 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9615373955644373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9615373955644373 | validation: 0.569534351898804]
	TIME [epoch: 0.714 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848669108216831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8848669108216831 | validation: 0.6597043422442787]
	TIME [epoch: 0.716 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82702101594446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82702101594446 | validation: 0.9433930092368503]
	TIME [epoch: 0.717 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911542868560475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911542868560475 | validation: 0.782277211971016]
	TIME [epoch: 0.714 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9972027665186807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9972027665186807 | validation: 0.9038621817029802]
	TIME [epoch: 0.715 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043210284681838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.043210284681838 | validation: 0.732517961958252]
	TIME [epoch: 0.715 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296037700996749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296037700996749 | validation: 0.6760796032710884]
	TIME [epoch: 0.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179049289136993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8179049289136993 | validation: 0.7024615201701583]
	TIME [epoch: 0.716 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629893041710887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8629893041710887 | validation: 0.7847258092143113]
	TIME [epoch: 0.715 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713399727346726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713399727346726 | validation: 0.6865401353673843]
	TIME [epoch: 0.715 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786033873053316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8786033873053316 | validation: 0.6911442587224054]
	TIME [epoch: 0.716 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843409467180669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.843409467180669 | validation: 0.7884618499037143]
	TIME [epoch: 0.716 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586686993198842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8586686993198842 | validation: 0.6813153342525514]
	TIME [epoch: 0.716 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394619465229611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394619465229611 | validation: 0.7607824460943701]
	TIME [epoch: 0.715 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516936047209143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516936047209143 | validation: 0.753570210804131]
	TIME [epoch: 0.713 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631748873940679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8631748873940679 | validation: 0.7444304516698343]
	TIME [epoch: 0.715 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9061982330436891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9061982330436891 | validation: 0.7414673950226637]
	TIME [epoch: 0.715 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740155153501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740155153501 | validation: 0.7826025448348739]
	TIME [epoch: 0.715 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646001292408583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646001292408583 | validation: 0.6621133542111786]
	TIME [epoch: 0.713 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8423738393743838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423738393743838 | validation: 0.8626139333565246]
	TIME [epoch: 0.715 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564413513440167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8564413513440167 | validation: 0.6366400034516315]
	TIME [epoch: 0.716 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388316915361747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388316915361747 | validation: 0.7317687455121176]
	TIME [epoch: 0.717 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260118892523414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260118892523414 | validation: 0.6747128976722249]
	TIME [epoch: 0.715 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239365211668193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8239365211668193 | validation: 0.6539462536872708]
	TIME [epoch: 0.717 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885894226569988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885894226569988 | validation: 0.9293157463103168]
	TIME [epoch: 0.716 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9698905482933541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9698905482933541 | validation: 0.5894069340272469]
	TIME [epoch: 0.717 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386452767886312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9386452767886312 | validation: 0.6715830530148756]
	TIME [epoch: 0.714 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947618372679205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7947618372679205 | validation: 0.7977740273091376]
	TIME [epoch: 0.713 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448008845607196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448008845607196 | validation: 0.5657404895860807]
	TIME [epoch: 0.714 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870011942595318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.870011942595318 | validation: 0.7478114381811153]
	TIME [epoch: 0.714 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061158141121101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061158141121101 | validation: 0.6597755974811745]
	TIME [epoch: 0.713 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881776749303651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881776749303651 | validation: 0.6919364617740296]
	TIME [epoch: 0.713 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959424489617536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959424489617536 | validation: 0.7677341293620328]
	TIME [epoch: 0.713 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984255203844823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984255203844823 | validation: 1.33192351978764]
	TIME [epoch: 0.712 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1810243086281123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1810243086281123 | validation: 0.6617566051153517]
	TIME [epoch: 0.712 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196316166937081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8196316166937081 | validation: 0.5774726484724687]
	TIME [epoch: 0.713 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082177973301103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082177973301103 | validation: 0.8051583404062381]
	TIME [epoch: 0.713 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177282164754753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177282164754753 | validation: 0.6327377193601074]
	TIME [epoch: 0.712 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173192964025282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8173192964025282 | validation: 0.7081577270549964]
	TIME [epoch: 0.716 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465207561661788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8465207561661788 | validation: 0.7505997376996515]
	TIME [epoch: 0.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599336339863003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599336339863003 | validation: 0.6915510632221654]
	TIME [epoch: 0.715 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028525333050863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9028525333050863 | validation: 0.7427701063275927]
	TIME [epoch: 0.713 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826942095844422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826942095844422 | validation: 0.6972922638234618]
	TIME [epoch: 0.713 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815949939554665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815949939554665 | validation: 0.6158113332052217]
	TIME [epoch: 0.714 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083731631235145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083731631235145 | validation: 0.8686913473485405]
	TIME [epoch: 0.715 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420731196278146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420731196278146 | validation: 0.6416373990203449]
	TIME [epoch: 0.713 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8699114475763026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699114475763026 | validation: 0.8786837353865334]
	TIME [epoch: 0.715 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.880838639164323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.880838639164323 | validation: 0.7100700403148708]
	TIME [epoch: 0.713 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473085958051215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473085958051215 | validation: 0.6124931110041953]
	TIME [epoch: 0.713 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748164133676545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8748164133676545 | validation: 0.7185623718920415]
	TIME [epoch: 0.712 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039309348616988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8039309348616988 | validation: 0.6555079446753788]
	TIME [epoch: 0.712 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785836698553519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785836698553519 | validation: 0.6305060470143997]
	TIME [epoch: 0.713 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878384695311309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7878384695311309 | validation: 0.6911237393467751]
	TIME [epoch: 0.715 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78741793004298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.78741793004298 | validation: 0.6741580332199191]
	TIME [epoch: 0.713 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318180588632658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318180588632658 | validation: 1.0162999058887043]
	TIME [epoch: 0.713 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000755617638574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.000755617638574 | validation: 0.7622667056963484]
	TIME [epoch: 0.716 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074361109761914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074361109761914 | validation: 0.6808890621085238]
	TIME [epoch: 0.713 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233541810789086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8233541810789086 | validation: 0.6611205952996241]
	TIME [epoch: 185 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771572957715652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771572957715652 | validation: 0.6051915485868239]
	TIME [epoch: 1.44 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620725369926867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7620725369926867 | validation: 0.6900258172729618]
	TIME [epoch: 1.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7755050868120159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7755050868120159 | validation: 0.6186189782543432]
	TIME [epoch: 1.4 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774209093282133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774209093282133 | validation: 0.7360179104035279]
	TIME [epoch: 1.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800685983855321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.800685983855321 | validation: 0.6998427763677102]
	TIME [epoch: 1.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722704538756031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8722704538756031 | validation: 0.9653902826105549]
	TIME [epoch: 1.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970526865943949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9970526865943949 | validation: 0.7497931398207056]
	TIME [epoch: 1.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857403156112388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857403156112388 | validation: 0.5803892733402937]
	TIME [epoch: 1.4 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8226653645433737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8226653645433737 | validation: 0.7371240178796399]
	TIME [epoch: 1.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846778586547657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846778586547657 | validation: 0.5735404192192248]
	TIME [epoch: 1.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753281748080457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7753281748080457 | validation: 0.6534270597604328]
	TIME [epoch: 1.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642996190770598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642996190770598 | validation: 0.6322397457236693]
	TIME [epoch: 1.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625008816298556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7625008816298556 | validation: 0.7072833941888337]
	TIME [epoch: 1.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898138547003375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898138547003375 | validation: 0.7274719475142967]
	TIME [epoch: 1.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974652164785774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8974652164785774 | validation: 0.9570907043289314]
	TIME [epoch: 1.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0434546282964703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0434546282964703 | validation: 0.678237461225386]
	TIME [epoch: 1.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949826553596477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7949826553596477 | validation: 0.672824700059775]
	TIME [epoch: 1.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645227662555776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645227662555776 | validation: 0.5906922078645898]
	TIME [epoch: 1.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553128806528191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553128806528191 | validation: 0.6929649451582821]
	TIME [epoch: 1.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564305052039595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564305052039595 | validation: 0.5864824942850477]
	TIME [epoch: 1.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831164547729051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7831164547729051 | validation: 0.7075476652285873]
	TIME [epoch: 1.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833208763382848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7833208763382848 | validation: 0.6484864236557337]
	TIME [epoch: 1.41 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84085286512623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84085286512623 | validation: 0.7221419131244899]
	TIME [epoch: 1.41 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524629721019666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524629721019666 | validation: 0.9479121467481382]
	TIME [epoch: 1.41 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088353527764137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9088353527764137 | validation: 0.6483244693496226]
	TIME [epoch: 1.4 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126537746803406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126537746803406 | validation: 0.7254833729095793]
	TIME [epoch: 1.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663074634120258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7663074634120258 | validation: 0.6143003076774156]
	TIME [epoch: 1.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352780116222986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352780116222986 | validation: 0.6416901068602341]
	TIME [epoch: 1.41 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729354292417459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729354292417459 | validation: 0.636529058872005]
	TIME [epoch: 1.41 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317793404559653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317793404559653 | validation: 0.5889410631879917]
	TIME [epoch: 1.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7164042668795445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7164042668795445 | validation: 0.6440087312198726]
	TIME [epoch: 1.41 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325067203165798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7325067203165798 | validation: 0.6440125991891156]
	TIME [epoch: 1.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560677217395936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8560677217395936 | validation: 0.9975991261559454]
	TIME [epoch: 1.41 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1470255558720728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1470255558720728 | validation: 0.9302064414028384]
	TIME [epoch: 1.41 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818193896250338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818193896250338 | validation: 0.5346787087470157]
	TIME [epoch: 1.41 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669405763889674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669405763889674 | validation: 0.6509804037768199]
	TIME [epoch: 1.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602459747783107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602459747783107 | validation: 0.7409006914802477]
	TIME [epoch: 1.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796955463536085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796955463536085 | validation: 0.590873449881833]
	TIME [epoch: 1.41 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536209774745658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536209774745658 | validation: 0.6890022902985268]
	TIME [epoch: 1.44 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392087234326259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7392087234326259 | validation: 0.6442757314438462]
	TIME [epoch: 1.41 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250488015225449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250488015225449 | validation: 0.6367470668931325]
	TIME [epoch: 1.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7279429129849325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7279429129849325 | validation: 0.6789309414888701]
	TIME [epoch: 1.4 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827507912653261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827507912653261 | validation: 0.7593563405879166]
	TIME [epoch: 1.4 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8822245339547302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8822245339547302 | validation: 0.710518027746354]
	TIME [epoch: 1.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871474575147975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871474575147975 | validation: 0.7324607244042993]
	TIME [epoch: 1.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762270202136925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762270202136925 | validation: 0.560583389914342]
	TIME [epoch: 1.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100618914974749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7100618914974749 | validation: 0.6059750999926959]
	TIME [epoch: 1.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112771991921837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7112771991921837 | validation: 0.6695543449566828]
	TIME [epoch: 1.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.724609124543758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.724609124543758 | validation: 0.6537578138893908]
	TIME [epoch: 1.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478541326109378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478541326109378 | validation: 0.6928944174952727]
	TIME [epoch: 1.4 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776641688097859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776641688097859 | validation: 0.6714820324440993]
	TIME [epoch: 1.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930845078761385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7930845078761385 | validation: 0.6343845795854242]
	TIME [epoch: 1.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748260737935433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.748260737935433 | validation: 0.5992105745168098]
	TIME [epoch: 1.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060587905435171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060587905435171 | validation: 0.5990272545478926]
	TIME [epoch: 1.41 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831692551283554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6831692551283554 | validation: 0.5934371988874217]
	TIME [epoch: 1.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806860068873715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6806860068873715 | validation: 0.6198670351832174]
	TIME [epoch: 1.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804599608926719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6804599608926719 | validation: 0.5426527967914289]
	TIME [epoch: 1.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793168707734656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6793168707734656 | validation: 0.6878173619499992]
	TIME [epoch: 1.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6914461297158703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6914461297158703 | validation: 0.5983596355256028]
	TIME [epoch: 1.41 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820377603180475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7820377603180475 | validation: 0.7886477873770709]
	TIME [epoch: 1.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324872804529351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324872804529351 | validation: 0.6599688262356707]
	TIME [epoch: 1.4 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650508708072624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650508708072624 | validation: 0.4849868581743043]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683474440232042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683474440232042 | validation: 0.6137359683361545]
	TIME [epoch: 1.41 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617528294317507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617528294317507 | validation: 0.591037874009923]
	TIME [epoch: 1.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829629056470893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829629056470893 | validation: 0.5834726497513962]
	TIME [epoch: 1.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722165863507916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722165863507916 | validation: 0.5731941161216753]
	TIME [epoch: 1.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781238759783201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781238759783201 | validation: 0.5899932560560605]
	TIME [epoch: 1.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649506857034158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6649506857034158 | validation: 0.5264740792667141]
	TIME [epoch: 1.4 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6719847894810095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6719847894810095 | validation: 0.6169184048354022]
	TIME [epoch: 1.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729150895457141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729150895457141 | validation: 0.5526878300877246]
	TIME [epoch: 1.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177874999277998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7177874999277998 | validation: 0.6002531240947773]
	TIME [epoch: 1.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630168300268067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630168300268067 | validation: 0.5679707317998192]
	TIME [epoch: 1.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652068150877295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652068150877295 | validation: 0.5422125511079339]
	TIME [epoch: 1.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836757533863513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836757533863513 | validation: 0.5828465857074845]
	TIME [epoch: 1.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649414055579603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6649414055579603 | validation: 0.5536207845208957]
	TIME [epoch: 1.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575231404613558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6575231404613558 | validation: 0.4820849910263433]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418581239871136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6418581239871136 | validation: 0.6222136443012833]
	TIME [epoch: 1.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66768277783359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66768277783359 | validation: 0.5995781789653399]
	TIME [epoch: 1.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280333210697854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7280333210697854 | validation: 0.618646273623155]
	TIME [epoch: 1.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290622701254946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290622701254946 | validation: 0.5217133893258605]
	TIME [epoch: 1.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072831607318399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7072831607318399 | validation: 0.6010273021854133]
	TIME [epoch: 1.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544542728515075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544542728515075 | validation: 0.5726831146365493]
	TIME [epoch: 1.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493294979993409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6493294979993409 | validation: 0.506388580420908]
	TIME [epoch: 1.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843508734209129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6843508734209129 | validation: 0.5826653031257758]
	TIME [epoch: 1.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634667522144281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.634667522144281 | validation: 0.5357996123206908]
	TIME [epoch: 1.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063421072559702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6063421072559702 | validation: 0.4625701048595288]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163197129892387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163197129892387 | validation: 0.5695891045704855]
	TIME [epoch: 1.41 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169992320170914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6169992320170914 | validation: 0.5427871528299328]
	TIME [epoch: 1.41 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653355084078711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.653355084078711 | validation: 0.48698876991953066]
	TIME [epoch: 1.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629015598202489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.629015598202489 | validation: 0.4803671408527051]
	TIME [epoch: 1.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632507994909664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632507994909664 | validation: 0.6049297510470197]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6301314047786315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6301314047786315 | validation: 0.534434453850927]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699005070335033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699005070335033 | validation: 0.4713292515809442]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983425877354056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983425877354056 | validation: 0.6494158885134756]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7000589605488509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7000589605488509 | validation: 0.5539597307870768]
	TIME [epoch: 1.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176913428570887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176913428570887 | validation: 0.6074423894990781]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109105627401627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109105627401627 | validation: 0.4859842689627784]
	TIME [epoch: 1.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947424257769263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5947424257769263 | validation: 0.5601409073836153]
	TIME [epoch: 1.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440567658069192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6440567658069192 | validation: 0.5093820014273809]
	TIME [epoch: 1.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236081014273078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236081014273078 | validation: 0.5457591791797305]
	TIME [epoch: 1.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263921654085572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6263921654085572 | validation: 0.48470086348951114]
	TIME [epoch: 1.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854548652982802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5854548652982802 | validation: 0.47206816785907935]
	TIME [epoch: 1.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5759395612753636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5759395612753636 | validation: 0.4767801288329982]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915766399144885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5915766399144885 | validation: 0.5069516074198169]
	TIME [epoch: 1.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845207341472185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5845207341472185 | validation: 0.48445155180149996]
	TIME [epoch: 1.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6193521051634597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6193521051634597 | validation: 0.5925803777108717]
	TIME [epoch: 1.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446009129654504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446009129654504 | validation: 0.4822541593375352]
	TIME [epoch: 1.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468951443894332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468951443894332 | validation: 0.4769685847542697]
	TIME [epoch: 1.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5494328316682381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5494328316682381 | validation: 0.49347468799517036]
	TIME [epoch: 1.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978168249952076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5978168249952076 | validation: 0.4930930726970629]
	TIME [epoch: 1.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776066936769513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776066936769513 | validation: 0.5431154367853385]
	TIME [epoch: 1.4 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582729175135189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.582729175135189 | validation: 0.5110134846502635]
	TIME [epoch: 1.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936848165665516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5936848165665516 | validation: 0.4701166936837563]
	TIME [epoch: 1.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472476611805789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472476611805789 | validation: 0.4823635437018675]
	TIME [epoch: 1.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870112893031265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870112893031265 | validation: 0.48000296811823234]
	TIME [epoch: 1.4 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5389840894113574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5389840894113574 | validation: 0.45435888456440277]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863388997120239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5863388997120239 | validation: 0.4923991061174066]
	TIME [epoch: 1.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860692798359477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860692798359477 | validation: 0.45526412617953343]
	TIME [epoch: 1.39 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5525498803304315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5525498803304315 | validation: 0.43758934531209226]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5182642781400102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5182642781400102 | validation: 0.40687335914368633]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5244517566397074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5244517566397074 | validation: 0.5141449233157355]
	TIME [epoch: 1.41 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458949856159621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458949856159621 | validation: 0.469106671769745]
	TIME [epoch: 1.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291733966920214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6291733966920214 | validation: 0.4155799276514602]
	TIME [epoch: 1.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.511083534162554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.511083534162554 | validation: 0.3892254038709441]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4955830245232927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4955830245232927 | validation: 0.4208873874232717]
	TIME [epoch: 1.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5229809809786624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5229809809786624 | validation: 0.49726148392565234]
	TIME [epoch: 1.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419222552533647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6419222552533647 | validation: 0.4392575206478691]
	TIME [epoch: 1.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982731517798331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982731517798331 | validation: 0.3794956693958466]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5109974178529294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5109974178529294 | validation: 0.46466983097780235]
	TIME [epoch: 1.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590438999417996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590438999417996 | validation: 0.4851250987352856]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5205057125856776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5205057125856776 | validation: 0.3545844966108754]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.539981428751861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.539981428751861 | validation: 0.5338154218045404]
	TIME [epoch: 1.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5379096537293458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5379096537293458 | validation: 0.46383950900506704]
	TIME [epoch: 1.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240643580720873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6240643580720873 | validation: 0.3892319421099244]
	TIME [epoch: 1.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4605634442113457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4605634442113457 | validation: 0.43136905055766306]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49367250399651114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49367250399651114 | validation: 0.4689094415288933]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201179011097007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6201179011097007 | validation: 0.4631812324842667]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47970857458855676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47970857458855676 | validation: 0.4087795914976628]
	TIME [epoch: 1.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4970899150513292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4970899150513292 | validation: 0.45544341864192006]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.562948516752259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.562948516752259 | validation: 0.45749071731403934]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098711108997226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5098711108997226 | validation: 0.40658224063489645]
	TIME [epoch: 1.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48288877039292744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48288877039292744 | validation: 0.45382402623289864]
	TIME [epoch: 1.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4849071188210979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4849071188210979 | validation: 0.4433160962712796]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5853831848899627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5853831848899627 | validation: 0.4548930639890021]
	TIME [epoch: 1.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46170974550546584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46170974550546584 | validation: 0.32051407819681055]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4565751896110484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565751896110484 | validation: 0.49794356786116156]
	TIME [epoch: 1.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4585277230186819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4585277230186819 | validation: 0.4073651008105932]
	TIME [epoch: 1.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574896220995912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5574896220995912 | validation: 0.41444663905415213]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4801664352031182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4801664352031182 | validation: 0.3966354467686033]
	TIME [epoch: 1.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4659749866046211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4659749866046211 | validation: 0.5591583102465226]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381844239808121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5381844239808121 | validation: 0.3873486192814164]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023674727714877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023674727714877 | validation: 0.4352258021353529]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47751567050310284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47751567050310284 | validation: 0.6018212033338495]
	TIME [epoch: 1.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327431236699389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6327431236699389 | validation: 0.33215543147323645]
	TIME [epoch: 1.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350256182551139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5350256182551139 | validation: 0.4316156770703352]
	TIME [epoch: 1.4 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46724067140398035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46724067140398035 | validation: 0.4617662899804982]
	TIME [epoch: 1.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46972428208254685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46972428208254685 | validation: 0.39694917105806243]
	TIME [epoch: 1.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46724763565541366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46724763565541366 | validation: 0.46036912613652636]
	TIME [epoch: 1.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4462360829812589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4462360829812589 | validation: 0.397093141047993]
	TIME [epoch: 1.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3942764998077608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3942764998077608 | validation: 0.4594175370785413]
	TIME [epoch: 1.46 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39848424882349004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39848424882349004 | validation: 0.33812759818702554]
	TIME [epoch: 1.41 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4104311015727688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4104311015727688 | validation: 0.5449913742276764]
	TIME [epoch: 1.41 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4804049736244214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4804049736244214 | validation: 0.6171396282131537]
	TIME [epoch: 1.4 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182755097401758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182755097401758 | validation: 0.4771966343532137]
	TIME [epoch: 1.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534554505995432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534554505995432 | validation: 0.5126975365981344]
	TIME [epoch: 1.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5330879558367563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5330879558367563 | validation: 0.39271158244494053]
	TIME [epoch: 1.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542842320216022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4542842320216022 | validation: 0.44130908128116375]
	TIME [epoch: 1.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3988627528322187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3988627528322187 | validation: 0.3824527634018742]
	TIME [epoch: 1.4 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35622472862592686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35622472862592686 | validation: 0.37429310133563]
	TIME [epoch: 1.4 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3619338471216881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3619338471216881 | validation: 0.45063955722116494]
	TIME [epoch: 1.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853349288675466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3853349288675466 | validation: 0.3966253835900482]
	TIME [epoch: 1.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4943357955228532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4943357955228532 | validation: 0.4938663408921453]
	TIME [epoch: 1.4 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41956227018757103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41956227018757103 | validation: 0.35800827311132116]
	TIME [epoch: 1.4 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4199357307454985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4199357307454985 | validation: 0.4153194115724783]
	TIME [epoch: 1.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3467806915659016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3467806915659016 | validation: 0.3202731865872011]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3595079187988429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3595079187988429 | validation: 0.43014330014078567]
	TIME [epoch: 1.4 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35640565067747454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35640565067747454 | validation: 0.3816989781751057]
	TIME [epoch: 1.4 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48460315234684254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48460315234684254 | validation: 0.42966644621340583]
	TIME [epoch: 1.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41924650184795825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41924650184795825 | validation: 0.35536915025824534]
	TIME [epoch: 1.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124784291166334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4124784291166334 | validation: 0.6737803745644106]
	TIME [epoch: 1.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953813744999053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4953813744999053 | validation: 0.3829024819426502]
	TIME [epoch: 1.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39286258266561624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39286258266561624 | validation: 0.2723933854727423]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3896317983473984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3896317983473984 | validation: 0.3514470678340956]
	TIME [epoch: 1.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28658594148912375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28658594148912375 | validation: 0.3667088689554854]
	TIME [epoch: 1.42 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28114077423393996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28114077423393996 | validation: 0.3096119796749738]
	TIME [epoch: 1.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649963691705353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2649963691705353 | validation: 0.3005824103965482]
	TIME [epoch: 1.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296523960533773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.296523960533773 | validation: 0.6102250068139956]
	TIME [epoch: 1.4 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5476123297002119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5476123297002119 | validation: 0.5780932714846094]
	TIME [epoch: 1.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978168453324043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978168453324043 | validation: 0.4425396965068205]
	TIME [epoch: 1.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061028561425101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061028561425101 | validation: 0.3295040259213223]
	TIME [epoch: 1.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29721868120226974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29721868120226974 | validation: 0.543444988591603]
	TIME [epoch: 1.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5064972866064648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5064972866064648 | validation: 0.3720322319269979]
	TIME [epoch: 1.41 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.371764746558114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.371764746558114 | validation: 0.32659098255520214]
	TIME [epoch: 1.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3181601481439536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3181601481439536 | validation: 0.3648518356471626]
	TIME [epoch: 1.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625369826082483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625369826082483 | validation: 0.3648817137275362]
	TIME [epoch: 1.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25247075063603935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25247075063603935 | validation: 0.30351758153750574]
	TIME [epoch: 1.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24023142766664696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24023142766664696 | validation: 0.30310963424936843]
	TIME [epoch: 1.4 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2268207084746329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2268207084746329 | validation: 0.33562611222828326]
	TIME [epoch: 1.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23886973175944895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23886973175944895 | validation: 0.36123880716206447]
	TIME [epoch: 1.4 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088958650094755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4088958650094755 | validation: 0.7045935394194023]
	TIME [epoch: 1.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413703498243277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413703498243277 | validation: 0.3059919108804791]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43365729562128624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43365729562128624 | validation: 0.24154299078474362]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2582128044149898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2582128044149898 | validation: 0.4530201214817437]
	TIME [epoch: 1.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32675850815831764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32675850815831764 | validation: 0.33682410618777014]
	TIME [epoch: 1.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2902482818357754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2902482818357754 | validation: 0.23325939425490985]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25308059985216264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25308059985216264 | validation: 0.3095935028241262]
	TIME [epoch: 1.4 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21017177590982372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21017177590982372 | validation: 0.270441294157798]
	TIME [epoch: 1.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19036407113624243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19036407113624243 | validation: 0.2537133410807159]
	TIME [epoch: 1.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19458337666126047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19458337666126047 | validation: 0.30617609016866376]
	TIME [epoch: 1.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29136609195262714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29136609195262714 | validation: 0.6819844744715775]
	TIME [epoch: 1.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659450611183678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659450611183678 | validation: 0.27435635353647186]
	TIME [epoch: 1.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37818202711635224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37818202711635224 | validation: 0.2849008215697515]
	TIME [epoch: 1.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.192710522305216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.192710522305216 | validation: 0.31189429958111564]
	TIME [epoch: 1.4 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23059623250258704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23059623250258704 | validation: 0.2878457054567902]
	TIME [epoch: 1.4 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2741493361997942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2741493361997942 | validation: 0.3021827902692561]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25944358518407495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25944358518407495 | validation: 0.3871604640933146]
	TIME [epoch: 1.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840752270554842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2840752270554842 | validation: 0.37339896164963254]
	TIME [epoch: 1.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717514812812666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2717514812812666 | validation: 0.25350764333375525]
	TIME [epoch: 1.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22255809196136217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22255809196136217 | validation: 0.3378710758930119]
	TIME [epoch: 1.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24808173151341958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24808173151341958 | validation: 0.2892326357925802]
	TIME [epoch: 1.4 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482725324666586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482725324666586 | validation: 0.2271416129843464]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815349761196724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815349761196724 | validation: 0.35308706934395245]
	TIME [epoch: 1.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2069742508947006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2069742508947006 | validation: 0.18384121437627085]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784711147854874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1784711147854874 | validation: 0.2959144791237876]
	TIME [epoch: 1.41 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19085452953611262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19085452953611262 | validation: 0.31865535458284616]
	TIME [epoch: 1.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3531447978783112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3531447978783112 | validation: 0.3190785450640341]
	TIME [epoch: 1.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399828315318731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3399828315318731 | validation: 0.2369439963041863]
	TIME [epoch: 1.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20316265655009857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20316265655009857 | validation: 0.31821980612516493]
	TIME [epoch: 1.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17304980907749565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17304980907749565 | validation: 0.19823871583544161]
	TIME [epoch: 1.41 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16503312775850984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16503312775850984 | validation: 0.2667115233448346]
	TIME [epoch: 1.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20223488368482487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20223488368482487 | validation: 0.3333678783208633]
	TIME [epoch: 1.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37672958656756644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37672958656756644 | validation: 0.32808062423159845]
	TIME [epoch: 1.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2438583178150518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2438583178150518 | validation: 0.24393518694052363]
	TIME [epoch: 1.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20644997615186494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20644997615186494 | validation: 0.3693811318313454]
	TIME [epoch: 1.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24269907447239064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24269907447239064 | validation: 0.2471026759417998]
	TIME [epoch: 1.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18634719860616322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18634719860616322 | validation: 0.2441155240293377]
	TIME [epoch: 1.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15225570829407828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15225570829407828 | validation: 0.23936469049323855]
	TIME [epoch: 1.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14195900831564698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14195900831564698 | validation: 0.21845691324585548]
	TIME [epoch: 1.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17652393647341305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17652393647341305 | validation: 0.2977600878656551]
	TIME [epoch: 1.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590912738953756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3590912738953756 | validation: 0.3670822288334936]
	TIME [epoch: 1.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23439757191116659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23439757191116659 | validation: 0.258594799476031]
	TIME [epoch: 1.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19510726494522174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19510726494522174 | validation: 0.2358788477945502]
	TIME [epoch: 1.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17566839610627938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17566839610627938 | validation: 0.2298295193600844]
	TIME [epoch: 1.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16756831130832375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16756831130832375 | validation: 0.20210643694514197]
	TIME [epoch: 1.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459087943517277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459087943517277 | validation: 0.19853898720012159]
	TIME [epoch: 1.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18425864110308987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18425864110308987 | validation: 0.3210025075744097]
	TIME [epoch: 1.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22183340698310158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22183340698310158 | validation: 0.24698095430283162]
	TIME [epoch: 1.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3426325041432065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3426325041432065 | validation: 0.2602908889233659]
	TIME [epoch: 1.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2122989688495143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2122989688495143 | validation: 0.26016723824110527]
	TIME [epoch: 1.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15822659540035472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15822659540035472 | validation: 0.15897275999481028]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13654724213566113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13654724213566113 | validation: 0.22301761289625974]
	TIME [epoch: 1.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12246683613385098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12246683613385098 | validation: 0.12749994897567185]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10351265814759232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10351265814759232 | validation: 0.1609529403318378]
	TIME [epoch: 1.4 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681617943592585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09681617943592585 | validation: 0.15668730481690904]
	TIME [epoch: 1.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11972820954161899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11972820954161899 | validation: 0.3962056660403641]
	TIME [epoch: 1.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3103981323845518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3103981323845518 | validation: 0.3635461860225638]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400923083832464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5400923083832464 | validation: 0.2729566906109887]
	TIME [epoch: 1.4 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29921322967129144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29921322967129144 | validation: 0.47184447080684766]
	TIME [epoch: 1.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4865794059945101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4865794059945101 | validation: 0.19850012590441654]
	TIME [epoch: 1.4 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19527993921431566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19527993921431566 | validation: 0.322912500023664]
	TIME [epoch: 1.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2726677580311622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2726677580311622 | validation: 0.2235738355664478]
	TIME [epoch: 1.4 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1333394303173115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1333394303173115 | validation: 0.20632980857000538]
	TIME [epoch: 1.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16393677508284515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16393677508284515 | validation: 0.22238625682983001]
	TIME [epoch: 1.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17008749730251502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17008749730251502 | validation: 0.2750166160214046]
	TIME [epoch: 1.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18398456835292148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18398456835292148 | validation: 0.2099388884258402]
	TIME [epoch: 1.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14840467925099352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14840467925099352 | validation: 0.2111659278465494]
	TIME [epoch: 1.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13322643217790803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13322643217790803 | validation: 0.16434720602237743]
	TIME [epoch: 1.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230321618642286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12230321618642286 | validation: 0.18713085980497055]
	TIME [epoch: 1.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11507426657523576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11507426657523576 | validation: 0.17959570326272012]
	TIME [epoch: 1.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13214544041537388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13214544041537388 | validation: 0.21721049670511905]
	TIME [epoch: 1.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18637881182840332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18637881182840332 | validation: 0.2761641407295742]
	TIME [epoch: 1.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29250754635849563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29250754635849563 | validation: 0.17243236563363823]
	TIME [epoch: 1.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13223358665844687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13223358665844687 | validation: 0.16623642607462977]
	TIME [epoch: 1.4 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08857223465426202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08857223465426202 | validation: 0.12559541602816435]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10818077219077772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10818077219077772 | validation: 0.29704734097283136]
	TIME [epoch: 1.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19524702508869266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19524702508869266 | validation: 0.24031138768704696]
	TIME [epoch: 1.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2813624190273148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2813624190273148 | validation: 0.30156788839735166]
	TIME [epoch: 1.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2853730502636168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2853730502636168 | validation: 0.17359033052931025]
	TIME [epoch: 1.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10529537698345565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10529537698345565 | validation: 0.16618544230150678]
	TIME [epoch: 1.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802594536530528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08802594536530528 | validation: 0.1318652149217534]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10986245389463882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10986245389463882 | validation: 0.164288016686892]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643954797900866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643954797900866 | validation: 0.18766697387547637]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12373112592868994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12373112592868994 | validation: 0.2054868656601455]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604816036409761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2604816036409761 | validation: 0.30052827181783526]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21465984409081632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21465984409081632 | validation: 0.2226511480942485]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709642775558006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1709642775558006 | validation: 0.16614921823272574]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1271428917503928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1271428917503928 | validation: 0.1494813958347245]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054552897200256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09054552897200256 | validation: 0.10961789360207148]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862610596316469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0862610596316469 | validation: 0.21385723346311575]
	TIME [epoch: 1.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213499664652638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12213499664652638 | validation: 0.1430042286350986]
	TIME [epoch: 1.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15825788559099316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15825788559099316 | validation: 0.21823249304248066]
	TIME [epoch: 1.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447601484676848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1447601484676848 | validation: 0.16605278507713997]
	TIME [epoch: 1.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1846318899751957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1846318899751957 | validation: 0.16168584287420956]
	TIME [epoch: 1.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967027869565717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10967027869565717 | validation: 0.14291665195635692]
	TIME [epoch: 1.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08954765678092663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08954765678092663 | validation: 0.12355238229722998]
	TIME [epoch: 1.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11782647337082011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11782647337082011 | validation: 0.2194719611803759]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14569581203795004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14569581203795004 | validation: 0.22962890413886028]
	TIME [epoch: 1.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20280955455341532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20280955455341532 | validation: 0.25890519955492]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21429143248712124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21429143248712124 | validation: 0.24877016130448162]
	TIME [epoch: 1.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476890520360645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476890520360645 | validation: 0.11893488447105514]
	TIME [epoch: 186 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07809527657180876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07809527657180876 | validation: 0.09915564477897298]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802059594549753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08802059594549753 | validation: 0.14886537565777674]
	TIME [epoch: 2.76 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272847800944613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272847800944613 | validation: 0.247208769467558]
	TIME [epoch: 2.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18506189207092608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18506189207092608 | validation: 0.1583215499269296]
	TIME [epoch: 2.76 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1822218670232831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1822218670232831 | validation: 0.15543318090584357]
	TIME [epoch: 2.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205721686571739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205721686571739 | validation: 0.0973799938583765]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0582434111082853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0582434111082853 | validation: 0.10891379351661007]
	TIME [epoch: 2.76 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540438236192396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06540438236192396 | validation: 0.14869402673951734]
	TIME [epoch: 2.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003833668784324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09003833668784324 | validation: 0.1284441162425096]
	TIME [epoch: 2.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13528131031428356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13528131031428356 | validation: 0.2938326025723516]
	TIME [epoch: 2.76 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2123496886961619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2123496886961619 | validation: 0.18757708450726773]
	TIME [epoch: 2.77 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16338250792246917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16338250792246917 | validation: 0.22861101997140737]
	TIME [epoch: 2.76 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19915948313666212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19915948313666212 | validation: 0.3047265814002251]
	TIME [epoch: 2.76 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22722525986855766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22722525986855766 | validation: 0.16526344392294454]
	TIME [epoch: 2.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11794943083959498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11794943083959498 | validation: 0.11008764450452616]
	TIME [epoch: 2.77 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07517016005058462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07517016005058462 | validation: 0.14753660023915183]
	TIME [epoch: 2.76 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0692352021666294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0692352021666294 | validation: 0.06980313594441549]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984113216505129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05984113216505129 | validation: 0.1075765098591802]
	TIME [epoch: 2.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645713953671215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06645713953671215 | validation: 0.1319118228106478]
	TIME [epoch: 2.76 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11221535024689841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11221535024689841 | validation: 0.22780877309277364]
	TIME [epoch: 2.76 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19560697079091094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19560697079091094 | validation: 0.19910597795865664]
	TIME [epoch: 2.76 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18738207946569055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18738207946569055 | validation: 0.23059225680521045]
	TIME [epoch: 2.76 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19905551498348842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19905551498348842 | validation: 0.19103766978721512]
	TIME [epoch: 2.77 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802047411911522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13802047411911522 | validation: 0.14826987733410676]
	TIME [epoch: 2.78 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149182608314516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149182608314516 | validation: 0.1533666380603627]
	TIME [epoch: 2.77 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304541561570579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11304541561570579 | validation: 0.11801295741192018]
	TIME [epoch: 2.76 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545177669242896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09545177669242896 | validation: 0.12424625053194696]
	TIME [epoch: 2.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113749549023987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113749549023987 | validation: 0.09130084550588603]
	TIME [epoch: 2.76 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810077346219832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05810077346219832 | validation: 0.09111188304951058]
	TIME [epoch: 2.77 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420503504629439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05420503504629439 | validation: 0.10840956404129394]
	TIME [epoch: 2.77 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07859467006070958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07859467006070958 | validation: 0.23245300838023059]
	TIME [epoch: 2.77 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681935878197896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1681935878197896 | validation: 0.2684339970390311]
	TIME [epoch: 2.76 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26210638038004264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26210638038004264 | validation: 0.2550859019650336]
	TIME [epoch: 2.76 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2134978899509784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2134978899509784 | validation: 0.11112717415244022]
	TIME [epoch: 2.76 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810340329890948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07810340329890948 | validation: 0.11190517758208661]
	TIME [epoch: 2.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606108632648783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0606108632648783 | validation: 0.09827558596959075]
	TIME [epoch: 2.76 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337427475651885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07337427475651885 | validation: 0.1475404648401266]
	TIME [epoch: 2.76 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127990608964574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09127990608964574 | validation: 0.13004302258684441]
	TIME [epoch: 2.77 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954800287947673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10954800287947673 | validation: 0.21414945872782698]
	TIME [epoch: 2.76 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1404843354594605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1404843354594605 | validation: 0.14714624294875964]
	TIME [epoch: 2.76 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13872707023386507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13872707023386507 | validation: 0.10986011174978239]
	TIME [epoch: 2.76 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10520294963133997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10520294963133997 | validation: 0.13493184679710726]
	TIME [epoch: 2.76 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08090948762884471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08090948762884471 | validation: 0.07257618875193646]
	TIME [epoch: 2.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07429528255354083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07429528255354083 | validation: 0.1007927366213401]
	TIME [epoch: 2.77 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295747745093879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07295747745093879 | validation: 0.17116499482069808]
	TIME [epoch: 2.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1154181530243367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1154181530243367 | validation: 0.19215807845769786]
	TIME [epoch: 2.77 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17059112883172467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17059112883172467 | validation: 0.2661264629676712]
	TIME [epoch: 2.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1764071087816283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1764071087816283 | validation: 0.11942764870545566]
	TIME [epoch: 2.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537878620994416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08537878620994416 | validation: 0.05754072145054129]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057512042993723006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057512042993723006 | validation: 0.09243340896904836]
	TIME [epoch: 2.77 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476164204469158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476164204469158 | validation: 0.06601418020326164]
	TIME [epoch: 2.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264223215459662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04264223215459662 | validation: 0.08792037974082528]
	TIME [epoch: 2.76 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799062067649301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05799062067649301 | validation: 0.16826153795376178]
	TIME [epoch: 2.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13737837989826765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13737837989826765 | validation: 0.24452528221281888]
	TIME [epoch: 2.76 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24761193729370104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24761193729370104 | validation: 0.1813400339218313]
	TIME [epoch: 2.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11879977586910861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11879977586910861 | validation: 0.06830188581389293]
	TIME [epoch: 2.76 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525270942439284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0525270942439284 | validation: 0.09618199389095858]
	TIME [epoch: 2.76 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825889643859008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05825889643859008 | validation: 0.16991883983312273]
	TIME [epoch: 2.76 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12875873777570432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12875873777570432 | validation: 0.23566473049109204]
	TIME [epoch: 2.76 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2981358794937486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2981358794937486 | validation: 0.213849329701109]
	TIME [epoch: 2.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1645524011834339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645524011834339 | validation: 0.15759781006648577]
	TIME [epoch: 2.76 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11014852528009851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11014852528009851 | validation: 0.08942719348945594]
	TIME [epoch: 2.76 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936466534230643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07936466534230643 | validation: 0.10976998132466975]
	TIME [epoch: 2.76 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197259845321428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05197259845321428 | validation: 0.06361408853188842]
	TIME [epoch: 2.76 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04414021598819016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04414021598819016 | validation: 0.1146219019990791]
	TIME [epoch: 2.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05612474790864846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05612474790864846 | validation: 0.08899826821081058]
	TIME [epoch: 2.76 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114948594601894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07114948594601894 | validation: 0.15559132289102004]
	TIME [epoch: 2.76 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105773362994359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08105773362994359 | validation: 0.10554706304929327]
	TIME [epoch: 2.76 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08665269740415076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08665269740415076 | validation: 0.17655936912769732]
	TIME [epoch: 2.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15199438655364675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15199438655364675 | validation: 0.22221529334816306]
	TIME [epoch: 2.76 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23950596728610749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23950596728610749 | validation: 0.18783463953235002]
	TIME [epoch: 2.76 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849122207391811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849122207391811 | validation: 0.07317836766803576]
	TIME [epoch: 2.76 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06605786700044673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06605786700044673 | validation: 0.16195826294782767]
	TIME [epoch: 2.76 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08505035173769782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08505035173769782 | validation: 0.06974297305239098]
	TIME [epoch: 2.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06912022899809588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06912022899809588 | validation: 0.06978148507460064]
	TIME [epoch: 2.76 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05220265494860591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05220265494860591 | validation: 0.11024972378127491]
	TIME [epoch: 2.76 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099732558076088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099732558076088 | validation: 0.09064203223421219]
	TIME [epoch: 2.76 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704834351304756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704834351304756 | validation: 0.12805497269880095]
	TIME [epoch: 2.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09291535161602653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09291535161602653 | validation: 0.18551424680593512]
	TIME [epoch: 2.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13625003866313218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13625003866313218 | validation: 0.12372915825382265]
	TIME [epoch: 2.76 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11248016427709566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11248016427709566 | validation: 0.10365537799150913]
	TIME [epoch: 2.76 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507096780743243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507096780743243 | validation: 0.09537434493694828]
	TIME [epoch: 2.76 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298812088846535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298812088846535 | validation: 0.22305366669949822]
	TIME [epoch: 2.77 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410804970532628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1410804970532628 | validation: 0.19647151590936432]
	TIME [epoch: 2.77 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1865153100825922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1865153100825922 | validation: 0.15206151600114612]
	TIME [epoch: 2.77 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334005512494638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14334005512494638 | validation: 0.09378386611139894]
	TIME [epoch: 2.76 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583752613754313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05583752613754313 | validation: 0.05601705829158095]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040748746607950366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040748746607950366 | validation: 0.08958648031841186]
	TIME [epoch: 2.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477854061586732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04477854061586732 | validation: 0.06949839589871701]
	TIME [epoch: 2.77 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05874831460550967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05874831460550967 | validation: 0.1378224559659862]
	TIME [epoch: 2.76 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07755940683074199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07755940683074199 | validation: 0.10815177290912761]
	TIME [epoch: 2.76 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09820171270487262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09820171270487262 | validation: 0.11891671170326452]
	TIME [epoch: 2.76 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08237752873922918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08237752873922918 | validation: 0.11398397727524597]
	TIME [epoch: 2.76 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0773935517401836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0773935517401836 | validation: 0.12794681170893982]
	TIME [epoch: 2.76 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384065757642339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1384065757642339 | validation: 0.2023596243043472]
	TIME [epoch: 2.76 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687669478056014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687669478056014 | validation: 0.10209200404002394]
	TIME [epoch: 2.76 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755381943544997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08755381943544997 | validation: 0.06961961043546946]
	TIME [epoch: 2.76 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05133059388575666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05133059388575666 | validation: 0.15343223004742748]
	TIME [epoch: 2.76 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07613374091004117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07613374091004117 | validation: 0.05767013339406371]
	TIME [epoch: 2.76 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04848235578567024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04848235578567024 | validation: 0.07284039983596362]
	TIME [epoch: 2.76 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046498233897220465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046498233897220465 | validation: 0.20241586380762688]
	TIME [epoch: 2.76 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14485808838520708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14485808838520708 | validation: 0.18059367215165767]
	TIME [epoch: 2.76 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22416241991693575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22416241991693575 | validation: 0.13538279959755226]
	TIME [epoch: 2.76 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09805803943390026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09805803943390026 | validation: 0.10265050199222887]
	TIME [epoch: 2.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784707596765188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07784707596765188 | validation: 0.10912448425703475]
	TIME [epoch: 2.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08911355582142276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08911355582142276 | validation: 0.1273913249145385]
	TIME [epoch: 2.77 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10340869289172482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10340869289172482 | validation: 0.10384677964948935]
	TIME [epoch: 2.76 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0793962920877379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0793962920877379 | validation: 0.06908474639653321]
	TIME [epoch: 2.77 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267955255579592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05267955255579592 | validation: 0.09318302135064355]
	TIME [epoch: 2.76 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05475729668954018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05475729668954018 | validation: 0.10031864593042689]
	TIME [epoch: 2.76 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08168850893469548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08168850893469548 | validation: 0.22947589294994064]
	TIME [epoch: 2.77 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1693121734068906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1693121734068906 | validation: 0.1763489522603389]
	TIME [epoch: 2.76 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716097213196893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1716097213196893 | validation: 0.10525709569302744]
	TIME [epoch: 2.76 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11644367983671418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11644367983671418 | validation: 0.06947656052299117]
	TIME [epoch: 2.76 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04209994605490088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04209994605490088 | validation: 0.06491898937516591]
	TIME [epoch: 2.89 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044749580790195566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044749580790195566 | validation: 0.08580807557060031]
	TIME [epoch: 2.76 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059256137147107125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059256137147107125 | validation: 0.07971094667086503]
	TIME [epoch: 2.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05453036469629762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05453036469629762 | validation: 0.07777629400299321]
	TIME [epoch: 2.76 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062145531368494274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062145531368494274 | validation: 0.10586685621390242]
	TIME [epoch: 2.76 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07284189663752291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07284189663752291 | validation: 0.12130545685173054]
	TIME [epoch: 2.76 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09804826043322652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09804826043322652 | validation: 0.16106367658049728]
	TIME [epoch: 2.77 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11384678197725139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11384678197725139 | validation: 0.1390241819172647]
	TIME [epoch: 2.77 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12037031592720106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12037031592720106 | validation: 0.13891588840674723]
	TIME [epoch: 2.77 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11093340224208863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11093340224208863 | validation: 0.14348727010756576]
	TIME [epoch: 2.77 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13530672145839912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13530672145839912 | validation: 0.19164369906198866]
	TIME [epoch: 2.77 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441102377064712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1441102377064712 | validation: 0.07578182696764083]
	TIME [epoch: 2.77 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03604144795874706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03604144795874706 | validation: 0.05895721053911571]
	TIME [epoch: 2.77 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036566075838365374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036566075838365374 | validation: 0.07974898311202344]
	TIME [epoch: 2.77 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567707313550139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567707313550139 | validation: 0.09561209739975023]
	TIME [epoch: 2.77 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822791866530239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07822791866530239 | validation: 0.1376587245918057]
	TIME [epoch: 2.77 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07234332426900397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07234332426900397 | validation: 0.06345240814118866]
	TIME [epoch: 2.77 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529218329526677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05529218329526677 | validation: 0.12426030941318511]
	TIME [epoch: 2.77 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06132265212348492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06132265212348492 | validation: 0.07353069360136548]
	TIME [epoch: 2.76 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06957565123763364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06957565123763364 | validation: 0.14632679722315758]
	TIME [epoch: 2.77 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07865755224718608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07865755224718608 | validation: 0.10040052156560877]
	TIME [epoch: 2.77 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996380381465688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08996380381465688 | validation: 0.15046328345289692]
	TIME [epoch: 2.77 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15220163715640475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15220163715640475 | validation: 0.19273673002641695]
	TIME [epoch: 2.77 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389945848320955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1389945848320955 | validation: 0.08118655269420173]
	TIME [epoch: 2.77 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05439020385569736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05439020385569736 | validation: 0.0819277886989282]
	TIME [epoch: 2.77 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07210972781381676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07210972781381676 | validation: 0.25555571652056]
	TIME [epoch: 2.77 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16448973518365112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16448973518365112 | validation: 0.14066238991126281]
	TIME [epoch: 2.77 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1368307436619011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1368307436619011 | validation: 0.0882038479894683]
	TIME [epoch: 2.77 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07747154051619033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07747154051619033 | validation: 0.08266465579513653]
	TIME [epoch: 2.77 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055442404582190255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055442404582190255 | validation: 0.06119146108746999]
	TIME [epoch: 2.77 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103040013080594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04103040013080594 | validation: 0.061523535439942094]
	TIME [epoch: 2.77 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04150944187213999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04150944187213999 | validation: 0.07012060985339057]
	TIME [epoch: 2.77 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041906073226894006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041906073226894006 | validation: 0.05785810636531721]
	TIME [epoch: 2.77 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04271495491625874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04271495491625874 | validation: 0.07058004946592282]
	TIME [epoch: 2.77 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458154721884106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0458154721884106 | validation: 0.08151082940048765]
	TIME [epoch: 2.77 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055352459010658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06055352459010658 | validation: 0.10800966890717363]
	TIME [epoch: 2.77 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0809630304001698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0809630304001698 | validation: 0.1178400808170481]
	TIME [epoch: 2.77 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10141095511997211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10141095511997211 | validation: 0.12428890440843712]
	TIME [epoch: 2.77 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203738379009511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09203738379009511 | validation: 0.10031354064574326]
	TIME [epoch: 2.77 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883065109208922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0883065109208922 | validation: 0.16598244777553472]
	TIME [epoch: 2.77 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13521616519592886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13521616519592886 | validation: 0.23068527812912543]
	TIME [epoch: 2.77 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532492141012392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532492141012392 | validation: 0.18032370469816797]
	TIME [epoch: 2.77 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12180138631658331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12180138631658331 | validation: 0.05627023531568959]
	TIME [epoch: 2.77 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0351702310754456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0351702310754456 | validation: 0.07542188985698803]
	TIME [epoch: 2.77 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04950069568946887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04950069568946887 | validation: 0.06137066625142295]
	TIME [epoch: 2.77 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05888341375741824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05888341375741824 | validation: 0.08406648872926001]
	TIME [epoch: 2.77 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710858278376898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04710858278376898 | validation: 0.04250089271364354]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860929411517579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03860929411517579 | validation: 0.059428317964105165]
	TIME [epoch: 2.76 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030997329451366733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030997329451366733 | validation: 0.03699353647641285]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029296473090793033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029296473090793033 | validation: 0.06556642902030711]
	TIME [epoch: 2.77 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036827379239029685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036827379239029685 | validation: 0.05740122583641097]
	TIME [epoch: 2.77 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248269789596675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05248269789596675 | validation: 0.1905218522558235]
	TIME [epoch: 2.76 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253392705750259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1253392705750259 | validation: 0.16583686499060485]
	TIME [epoch: 2.79 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15562080690777436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15562080690777436 | validation: 0.1257457830009515]
	TIME [epoch: 2.76 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059807182508315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1059807182508315 | validation: 0.10778260685254931]
	TIME [epoch: 2.76 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057573137317297254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057573137317297254 | validation: 0.04709925877293053]
	TIME [epoch: 2.77 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031697024137451874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031697024137451874 | validation: 0.05076978862608608]
	TIME [epoch: 2.77 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02470225493349215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02470225493349215 | validation: 0.05742497905375746]
	TIME [epoch: 2.77 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047220545597960045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047220545597960045 | validation: 0.1946835267153919]
	TIME [epoch: 2.77 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18766725043110583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18766725043110583 | validation: 0.46414141290487465]
	TIME [epoch: 2.76 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36752165126853853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36752165126853853 | validation: 0.06615466415081328]
	TIME [epoch: 2.76 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046492164061257134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046492164061257134 | validation: 0.0887946230068057]
	TIME [epoch: 2.76 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508329450201013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06508329450201013 | validation: 0.13698872923785402]
	TIME [epoch: 2.77 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08670726647739137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08670726647739137 | validation: 0.06642957238588448]
	TIME [epoch: 2.76 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008720129792452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04008720129792452 | validation: 0.04045388566510627]
	TIME [epoch: 2.76 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028073292544131982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028073292544131982 | validation: 0.05852347230959594]
	TIME [epoch: 2.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470870413889471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03470870413889471 | validation: 0.06647886465451672]
	TIME [epoch: 2.77 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049957337860729416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049957337860729416 | validation: 0.09708786184360459]
	TIME [epoch: 2.77 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185563675800008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185563675800008 | validation: 0.11826913805565394]
	TIME [epoch: 2.77 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09591032636441515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09591032636441515 | validation: 0.11329114805502827]
	TIME [epoch: 2.77 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09649341257596673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09649341257596673 | validation: 0.103658293742475]
	TIME [epoch: 2.77 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06437200849477216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06437200849477216 | validation: 0.07899431941697471]
	TIME [epoch: 2.77 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413124077464866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06413124077464866 | validation: 0.14807318339241068]
	TIME [epoch: 2.77 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12819392298765311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12819392298765311 | validation: 0.1666244534665734]
	TIME [epoch: 2.76 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16400606442817173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16400606442817173 | validation: 0.09257061318960919]
	TIME [epoch: 2.76 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08498273650610479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08498273650610479 | validation: 0.04685017637122207]
	TIME [epoch: 2.76 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03347916925795649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03347916925795649 | validation: 0.06537534731315432]
	TIME [epoch: 2.76 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04884427513928295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04884427513928295 | validation: 0.09272116858978928]
	TIME [epoch: 2.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061914022603214436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061914022603214436 | validation: 0.08411415755850304]
	TIME [epoch: 2.76 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075309879941762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07075309879941762 | validation: 0.09942710402962118]
	TIME [epoch: 2.76 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0599506131901261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0599506131901261 | validation: 0.05605818783437649]
	TIME [epoch: 2.76 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03630778307342344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03630778307342344 | validation: 0.06282794914425081]
	TIME [epoch: 2.76 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031312001072969844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031312001072969844 | validation: 0.03861094230662104]
	TIME [epoch: 2.76 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030916237583367572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030916237583367572 | validation: 0.0784360358176107]
	TIME [epoch: 2.76 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053492025021951405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053492025021951405 | validation: 0.12961791508235582]
	TIME [epoch: 2.76 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14657812719164193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14657812719164193 | validation: 0.25877913259817675]
	TIME [epoch: 2.75 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23214347566458618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23214347566458618 | validation: 0.2310386435651023]
	TIME [epoch: 2.76 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1433476894020355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1433476894020355 | validation: 0.0924901937542816]
	TIME [epoch: 2.76 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048189904622138686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048189904622138686 | validation: 0.05300122205372973]
	TIME [epoch: 2.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038331372058083436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038331372058083436 | validation: 0.053713023683807364]
	TIME [epoch: 2.75 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046471181157125924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046471181157125924 | validation: 0.0872876942378277]
	TIME [epoch: 2.75 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05858871269859352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05858871269859352 | validation: 0.0819576200732155]
	TIME [epoch: 2.75 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054965226621805016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054965226621805016 | validation: 0.07204156601990552]
	TIME [epoch: 2.75 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043196398761362734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043196398761362734 | validation: 0.059466868294874214]
	TIME [epoch: 2.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042781795368970955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042781795368970955 | validation: 0.09768715401155542]
	TIME [epoch: 2.76 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728041370446826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06728041370446826 | validation: 0.10385032634067902]
	TIME [epoch: 2.76 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681040742482315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09681040742482315 | validation: 0.14195814356864564]
	TIME [epoch: 2.75 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739212546067838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11739212546067838 | validation: 0.11118424387869799]
	TIME [epoch: 2.75 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09565842750055531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09565842750055531 | validation: 0.051695478937782924]
	TIME [epoch: 2.75 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03975318163120528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03975318163120528 | validation: 0.05607856748106846]
	TIME [epoch: 2.75 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776556265772475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776556265772475 | validation: 0.15711948230600253]
	TIME [epoch: 2.76 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375338694661869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10375338694661869 | validation: 0.1457904181152979]
	TIME [epoch: 2.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14243442620459726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14243442620459726 | validation: 0.11172665589934988]
	TIME [epoch: 2.76 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220445333913505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10220445333913505 | validation: 0.07633719569402077]
	TIME [epoch: 2.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786731621757902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04786731621757902 | validation: 0.04502572682741743]
	TIME [epoch: 2.76 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168066574483744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04168066574483744 | validation: 0.07918886244042012]
	TIME [epoch: 2.75 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048821570512691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048821570512691 | validation: 0.05204791850015033]
	TIME [epoch: 2.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040150036940267406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040150036940267406 | validation: 0.043310801194044235]
	TIME [epoch: 2.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03402847690969236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03402847690969236 | validation: 0.06021774983572943]
	TIME [epoch: 2.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03726314954382209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03726314954382209 | validation: 0.05818118701787145]
	TIME [epoch: 2.75 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050679412479094736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050679412479094736 | validation: 0.10577999206934648]
	TIME [epoch: 2.75 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06593529315096529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06593529315096529 | validation: 0.07119332988219892]
	TIME [epoch: 2.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700139822203469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06700139822203469 | validation: 0.09186035362624828]
	TIME [epoch: 2.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05146654806719189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05146654806719189 | validation: 0.12006750454999193]
	TIME [epoch: 2.76 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606905419181396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0606905419181396 | validation: 0.058502210621650466]
	TIME [epoch: 2.76 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06090474724676119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06090474724676119 | validation: 0.14625335874731427]
	TIME [epoch: 2.76 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07933044693512038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07933044693512038 | validation: 0.16377445514937203]
	TIME [epoch: 2.77 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12356910716073775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12356910716073775 | validation: 0.2582313553740518]
	TIME [epoch: 2.76 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25836463632991885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25836463632991885 | validation: 0.13903190667913237]
	TIME [epoch: 2.76 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11949351152137574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11949351152137574 | validation: 0.06718709697983065]
	TIME [epoch: 2.76 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05305841081825891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05305841081825891 | validation: 0.04428465185655245]
	TIME [epoch: 2.76 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036844626679865836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036844626679865836 | validation: 0.05768012634588814]
	TIME [epoch: 2.76 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313577271295836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0313577271295836 | validation: 0.030909536283484085]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02860378496817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02860378496817 | validation: 0.045651902414664935]
	TIME [epoch: 2.76 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507280980974612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02507280980974612 | validation: 0.04230769229315182]
	TIME [epoch: 2.76 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026118948056534454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026118948056534454 | validation: 0.05421273740833963]
	TIME [epoch: 2.76 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850197839208063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03850197839208063 | validation: 0.13917688035354656]
	TIME [epoch: 2.76 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08230881571902962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08230881571902962 | validation: 0.14817637190053118]
	TIME [epoch: 2.76 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13738885910613974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13738885910613974 | validation: 0.17221182946625105]
	TIME [epoch: 2.76 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14652432186927414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14652432186927414 | validation: 0.08981983474262134]
	TIME [epoch: 2.76 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07704325966653405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07704325966653405 | validation: 0.036989878807519284]
	TIME [epoch: 2.76 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04839841514562866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04839841514562866 | validation: 0.07496592396498629]
	TIME [epoch: 2.76 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041827877098050435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041827877098050435 | validation: 0.061725300350172554]
	TIME [epoch: 2.77 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04640266496880167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04640266496880167 | validation: 0.07200204625743631]
	TIME [epoch: 2.76 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115008194397453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06115008194397453 | validation: 0.09936516781065434]
	TIME [epoch: 2.76 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175868122592696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07175868122592696 | validation: 0.07987823671027046]
	TIME [epoch: 2.76 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051488410854249315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051488410854249315 | validation: 0.05746608236690083]
	TIME [epoch: 2.76 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932796185670886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03932796185670886 | validation: 0.06835077362349992]
	TIME [epoch: 2.76 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04114865015798808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04114865015798808 | validation: 0.08177649890523386]
	TIME [epoch: 2.76 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791925752489094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0791925752489094 | validation: 0.16436220954794303]
	TIME [epoch: 2.76 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14179314346658772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14179314346658772 | validation: 0.18211249705306295]
	TIME [epoch: 2.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13340269901293833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340269901293833 | validation: 0.07839345158507754]
	TIME [epoch: 2.76 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06224391018793792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06224391018793792 | validation: 0.08744588433768416]
	TIME [epoch: 2.76 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06178965258843291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06178965258843291 | validation: 0.06575648097073783]
	TIME [epoch: 2.76 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06116432595063886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06116432595063886 | validation: 0.06081446042182053]
	TIME [epoch: 2.76 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04139453692064592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04139453692064592 | validation: 0.054191379857103594]
	TIME [epoch: 2.76 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029651711292613427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029651711292613427 | validation: 0.03813878022449527]
	TIME [epoch: 2.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019107831627359323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019107831627359323 | validation: 0.02609583769013061]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017536529901578293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017536529901578293 | validation: 0.030318763679313455]
	TIME [epoch: 2.76 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018449673085117354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018449673085117354 | validation: 0.044664735750834385]
	TIME [epoch: 2.76 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029546642636119964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029546642636119964 | validation: 0.1317808194606951]
	TIME [epoch: 2.76 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771767157092169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09771767157092169 | validation: 0.23671528924015908]
	TIME [epoch: 2.75 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23006009668122673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23006009668122673 | validation: 0.16240129936004588]
	TIME [epoch: 2.75 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15709477201850836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15709477201850836 | validation: 0.08550034678793417]
	TIME [epoch: 2.75 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06781823557452735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06781823557452735 | validation: 0.08702544107238681]
	TIME [epoch: 2.76 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05012998192934478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05012998192934478 | validation: 0.050371391486241084]
	TIME [epoch: 2.75 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04771162381945141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04771162381945141 | validation: 0.05048268151617699]
	TIME [epoch: 2.75 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03444005168809072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03444005168809072 | validation: 0.03317410565603487]
	TIME [epoch: 2.75 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021663377977093404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021663377977093404 | validation: 0.032058046905865535]
	TIME [epoch: 2.75 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01741929721840224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01741929721840224 | validation: 0.03154543756737036]
	TIME [epoch: 2.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019353356835001976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019353356835001976 | validation: 0.05238489879830193]
	TIME [epoch: 2.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035443052471436615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035443052471436615 | validation: 0.20172570685573984]
	TIME [epoch: 2.75 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11300760524474848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11300760524474848 | validation: 0.18376924149101848]
	TIME [epoch: 2.75 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17809361573386046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17809361573386046 | validation: 0.13574623184114484]
	TIME [epoch: 2.75 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11661397627802361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11661397627802361 | validation: 0.07756512313904759]
	TIME [epoch: 2.75 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06134847146879291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06134847146879291 | validation: 0.06349113179935915]
	TIME [epoch: 2.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865930831153018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06865930831153018 | validation: 0.08132469726977827]
	TIME [epoch: 2.76 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05114483512714282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05114483512714282 | validation: 0.06702375332792275]
	TIME [epoch: 2.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038615843041258056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038615843041258056 | validation: 0.0911612155769938]
	TIME [epoch: 2.75 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116689147365063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07116689147365063 | validation: 0.11869562851855796]
	TIME [epoch: 2.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264905502411935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07264905502411935 | validation: 0.055376495283152166]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04406632042481965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04406632042481965 | validation: 0.055573150584473645]
	TIME [epoch: 2.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036841565239577635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036841565239577635 | validation: 0.05700144270326884]
	TIME [epoch: 2.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401305542894261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0401305542894261 | validation: 0.047739169477079825]
	TIME [epoch: 2.75 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039524914510205844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039524914510205844 | validation: 0.056404552281655695]
	TIME [epoch: 2.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042845600092926935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042845600092926935 | validation: 0.09386667730832542]
	TIME [epoch: 2.75 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409374653056855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07409374653056855 | validation: 0.18340314895355547]
	TIME [epoch: 2.75 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15038036151994613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15038036151994613 | validation: 0.26026774624535404]
	TIME [epoch: 2.76 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18480193674873743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18480193674873743 | validation: 0.05390756049406756]
	TIME [epoch: 2.76 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719379314375214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03719379314375214 | validation: 0.05841741250563193]
	TIME [epoch: 2.76 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032325417781958604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032325417781958604 | validation: 0.09067314878698134]
	TIME [epoch: 2.76 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058145921450852345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058145921450852345 | validation: 0.08619616255816848]
	TIME [epoch: 2.75 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04910349547147757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04910349547147757 | validation: 0.06130392158122641]
	TIME [epoch: 2.75 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03930817997078356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03930817997078356 | validation: 0.04818663923907479]
	TIME [epoch: 2.75 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03480913275178946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03480913275178946 | validation: 0.06630125521457518]
	TIME [epoch: 2.76 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044700497543339886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044700497543339886 | validation: 0.07778723242152671]
	TIME [epoch: 2.76 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07219439940307591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07219439940307591 | validation: 0.09818674790428956]
	TIME [epoch: 2.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08669449993738317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08669449993738317 | validation: 0.0778457868760997]
	TIME [epoch: 2.75 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06587545202476672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06587545202476672 | validation: 0.06649794267018916]
	TIME [epoch: 2.76 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04274142959567114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04274142959567114 | validation: 0.03870342445503248]
	TIME [epoch: 2.76 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449060359496193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449060359496193 | validation: 0.06459872075979776]
	TIME [epoch: 2.76 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055688089831221886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055688089831221886 | validation: 0.1368679609937671]
	TIME [epoch: 2.76 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12664193104538074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12664193104538074 | validation: 0.15064959733041802]
	TIME [epoch: 2.76 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12679276109774099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12679276109774099 | validation: 0.08240173730984396]
	TIME [epoch: 2.76 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478346762044832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478346762044832 | validation: 0.05440612573647389]
	TIME [epoch: 2.76 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449044066554068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449044066554068 | validation: 0.056100301061996816]
	TIME [epoch: 2.81 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030121503364922628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030121503364922628 | validation: 0.026879908313021596]
	TIME [epoch: 2.76 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020166007515605378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020166007515605378 | validation: 0.040794121954343454]
	TIME [epoch: 2.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018767527156192038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018767527156192038 | validation: 0.024253935002498495]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02134247580978597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02134247580978597 | validation: 0.05145835963288793]
	TIME [epoch: 2.75 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02761472355548386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02761472355548386 | validation: 0.03706345940772853]
	TIME [epoch: 2.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03664681908933521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03664681908933521 | validation: 0.14636748387538004]
	TIME [epoch: 2.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06625884770749714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06625884770749714 | validation: 0.10573706055492203]
	TIME [epoch: 2.76 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08375151474197372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08375151474197372 | validation: 0.22352601208616218]
	TIME [epoch: 2.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17318279908842435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17318279908842435 | validation: 0.12038117664276991]
	TIME [epoch: 2.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805196472232148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11805196472232148 | validation: 0.08090524299951594]
	TIME [epoch: 2.76 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08578419933991441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08578419933991441 | validation: 0.07260509190854988]
	TIME [epoch: 2.76 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071822526355372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07071822526355372 | validation: 0.141302323219041]
	TIME [epoch: 2.75 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211494697928116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09211494697928116 | validation: 0.08431444539961312]
	TIME [epoch: 2.76 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07361438130386154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07361438130386154 | validation: 0.049850005970991075]
	TIME [epoch: 2.76 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030641149383662195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030641149383662195 | validation: 0.029100479780604795]
	TIME [epoch: 2.76 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018277981706537872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018277981706537872 | validation: 0.028231232037681565]
	TIME [epoch: 2.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016745743912096347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016745743912096347 | validation: 0.035553735377420195]
	TIME [epoch: 2.76 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01569949726655017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01569949726655017 | validation: 0.025185391867319773]
	TIME [epoch: 2.75 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01595344605675861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01595344605675861 | validation: 0.03716996242596148]
	TIME [epoch: 2.76 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018293412746756794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018293412746756794 | validation: 0.05814528902773998]
	TIME [epoch: 2.75 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035885447521661076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035885447521661076 | validation: 0.13095964880439687]
	TIME [epoch: 2.76 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10922376820940677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10922376820940677 | validation: 0.328954868299969]
	TIME [epoch: 2.75 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2120262697386778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2120262697386778 | validation: 0.08957809280420041]
	TIME [epoch: 2.76 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269732334912211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269732334912211 | validation: 0.08615051271521397]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07283497107310793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07283497107310793 | validation: 0.12677935437477683]
	TIME [epoch: 2.76 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12091556205197268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12091556205197268 | validation: 0.09546590424303014]
	TIME [epoch: 2.75 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439586032121099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06439586032121099 | validation: 0.053727947360220386]
	TIME [epoch: 2.76 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031891421070291845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031891421070291845 | validation: 0.05878106274831857]
	TIME [epoch: 2.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029525363936225627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029525363936225627 | validation: 0.031656346898083214]
	TIME [epoch: 2.76 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019996392896228864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019996392896228864 | validation: 0.027932966112895752]
	TIME [epoch: 2.75 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017179927829007745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017179927829007745 | validation: 0.03330338982224475]
	TIME [epoch: 2.76 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016708394100498247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016708394100498247 | validation: 0.022172067169366372]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019712258223569618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019712258223569618 | validation: 0.054220404970699534]
	TIME [epoch: 2.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037381239917038574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037381239917038574 | validation: 0.1334964092605477]
	TIME [epoch: 2.76 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12719070320819176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12719070320819176 | validation: 0.22504385078329037]
	TIME [epoch: 2.76 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20916284313265882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20916284313265882 | validation: 0.12563603115147096]
	TIME [epoch: 2.76 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10242065464500741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10242065464500741 | validation: 0.05516498397037331]
	TIME [epoch: 2.76 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03838213132627988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03838213132627988 | validation: 0.056098730500836594]
	TIME [epoch: 2.76 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04314964183273445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04314964183273445 | validation: 0.04750149751769717]
	TIME [epoch: 2.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041271706373297676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041271706373297676 | validation: 0.05888535340440337]
	TIME [epoch: 2.76 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03805415409709651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03805415409709651 | validation: 0.045395318233777876]
	TIME [epoch: 2.75 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023543793216331856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023543793216331856 | validation: 0.03571861320049498]
	TIME [epoch: 2.76 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02470017030936206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02470017030936206 | validation: 0.04443122607599207]
	TIME [epoch: 2.76 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040488712397175164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040488712397175164 | validation: 0.09067622955236981]
	TIME [epoch: 2.76 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727383622588455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0727383622588455 | validation: 0.1338984780974928]
	TIME [epoch: 2.76 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11590727700014433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11590727700014433 | validation: 0.11289053716203692]
	TIME [epoch: 2.76 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125360772096248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08125360772096248 | validation: 0.04332498124596327]
	TIME [epoch: 2.75 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0348241382871127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0348241382871127 | validation: 0.07202115611469755]
	TIME [epoch: 2.75 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034501264731850556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034501264731850556 | validation: 0.07203215679227275]
	TIME [epoch: 2.76 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06150528080413561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06150528080413561 | validation: 0.16284775337688734]
	TIME [epoch: 2.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10940512699293847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10940512699293847 | validation: 0.09135841094552374]
	TIME [epoch: 2.76 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08168632584050319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08168632584050319 | validation: 0.062386943506837295]
	TIME [epoch: 2.75 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053287261599181214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053287261599181214 | validation: 0.05155319571007014]
	TIME [epoch: 2.76 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980497907161133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980497907161133 | validation: 0.03672584981247915]
	TIME [epoch: 2.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04629204447405547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04629204447405547 | validation: 0.05640000638615533]
	TIME [epoch: 2.76 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04625190343830095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04625190343830095 | validation: 0.05915445406763895]
	TIME [epoch: 2.76 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750158313478828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04750158313478828 | validation: 0.0725123262912735]
	TIME [epoch: 2.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587959876558627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05587959876558627 | validation: 0.10263875842554732]
	TIME [epoch: 2.76 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514423182244547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06514423182244547 | validation: 0.07169402496185222]
	TIME [epoch: 2.75 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053809584747850074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053809584747850074 | validation: 0.09871482879178162]
	TIME [epoch: 2.76 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0665569559630237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0665569559630237 | validation: 0.08518121475814633]
	TIME [epoch: 2.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07005447048117383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07005447048117383 | validation: 0.060750542295906776]
	TIME [epoch: 2.76 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06438934903581853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06438934903581853 | validation: 0.07053160722128239]
	TIME [epoch: 2.76 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327150181633268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04327150181633268 | validation: 0.03377737535054328]
	TIME [epoch: 2.76 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0251315172784193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0251315172784193 | validation: 0.03573812804361013]
	TIME [epoch: 2.76 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018946560208709464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018946560208709464 | validation: 0.022844673652893867]
	TIME [epoch: 2.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014632526011567224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014632526011567224 | validation: 0.030291243266622372]
	TIME [epoch: 2.76 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01367158125747212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01367158125747212 | validation: 0.026177560402441926]
	TIME [epoch: 2.76 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01642282255592812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01642282255592812 | validation: 0.0559979494238233]
	TIME [epoch: 2.76 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429004608692589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03429004608692589 | validation: 0.12075504450708795]
	TIME [epoch: 2.76 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11721693743151412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11721693743151412 | validation: 0.2928523330351085]
	TIME [epoch: 2.76 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20321556624592765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20321556624592765 | validation: 0.09091918686057625]
	TIME [epoch: 2.76 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06417845847746059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06417845847746059 | validation: 0.10905458467934218]
	TIME [epoch: 2.76 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701449462481758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06701449462481758 | validation: 0.14987335152817746]
	TIME [epoch: 2.76 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07374269204047133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07374269204047133 | validation: 0.073064954556305]
	TIME [epoch: 2.76 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042188413144636484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042188413144636484 | validation: 0.07593642118115779]
	TIME [epoch: 2.87 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0873600381253037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0873600381253037 | validation: 0.08793071185069007]
	TIME [epoch: 2.76 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11719075066180519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11719075066180519 | validation: 0.1360808502161887]
	TIME [epoch: 2.76 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09502289652176643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09502289652176643 | validation: 0.07568233612888738]
	TIME [epoch: 2.76 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818723421828407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04818723421828407 | validation: 0.03730098361570385]
	TIME [epoch: 2.76 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021805210011676044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021805210011676044 | validation: 0.03845316466082645]
	TIME [epoch: 2.76 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01687488651929616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01687488651929616 | validation: 0.01673198698097732]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01536575774592384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01536575774592384 | validation: 0.02895526194049333]
	TIME [epoch: 2.76 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017977826741574214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017977826741574214 | validation: 0.048422221300585655]
	TIME [epoch: 2.76 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027628856696612462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027628856696612462 | validation: 0.0783515868158657]
	TIME [epoch: 2.76 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0504166090966825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0504166090966825 | validation: 0.09921444180229899]
	TIME [epoch: 2.76 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640395924073879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08640395924073879 | validation: 0.12253195396281145]
	TIME [epoch: 2.76 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08947321105679013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08947321105679013 | validation: 0.062489840719720115]
	TIME [epoch: 2.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051931263041947846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051931263041947846 | validation: 0.06129795372213101]
	TIME [epoch: 2.75 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038089430632302294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038089430632302294 | validation: 0.10572587675315953]
	TIME [epoch: 2.77 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07144744956282705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07144744956282705 | validation: 0.1461354644653342]
	TIME [epoch: 2.77 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13596858285580754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13596858285580754 | validation: 0.10048316992859344]
	TIME [epoch: 2.75 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11741613990256773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11741613990256773 | validation: 0.04950800477224866]
	TIME [epoch: 2.76 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03324641905919787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03324641905919787 | validation: 0.018855240976189916]
	TIME [epoch: 2.75 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017175205975313954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017175205975313954 | validation: 0.0322834292349593]
	TIME [epoch: 2.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02149658505170004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02149658505170004 | validation: 0.0323902547870033]
	TIME [epoch: 2.75 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02572496846895029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02572496846895029 | validation: 0.043069366561145864]
	TIME [epoch: 2.76 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875824467959609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02875824467959609 | validation: 0.043330164932993964]
	TIME [epoch: 2.76 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03515549608335672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03515549608335672 | validation: 0.06132974655716866]
	TIME [epoch: 2.76 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04399310204433848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04399310204433848 | validation: 0.06098322693073806]
	TIME [epoch: 2.75 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050351217221087624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050351217221087624 | validation: 0.08928599975983313]
	TIME [epoch: 2.76 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729296480589963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05729296480589963 | validation: 0.07013058388677977]
	TIME [epoch: 2.76 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0618329011959827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0618329011959827 | validation: 0.08564521485030871]
	TIME [epoch: 2.76 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06430989381465241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06430989381465241 | validation: 0.08736134994445592]
	TIME [epoch: 2.76 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08643659374238215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08643659374238215 | validation: 0.15510892888368175]
	TIME [epoch: 2.76 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12947867668210641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12947867668210641 | validation: 0.17934025328522157]
	TIME [epoch: 2.77 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261375253600107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261375253600107 | validation: 0.05509984973784304]
	TIME [epoch: 2.77 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03446414523891049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03446414523891049 | validation: 0.026211690564741777]
	TIME [epoch: 2.76 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025015516887030388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025015516887030388 | validation: 0.06623488660185733]
	TIME [epoch: 2.76 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03132783021202626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03132783021202626 | validation: 0.03567875015843216]
	TIME [epoch: 2.77 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024028853917512222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024028853917512222 | validation: 0.03289880337773928]
	TIME [epoch: 2.76 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017716926908004554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017716926908004554 | validation: 0.030891538714498647]
	TIME [epoch: 2.77 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01542912345335473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01542912345335473 | validation: 0.02865332693219144]
	TIME [epoch: 2.76 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019882395318207683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019882395318207683 | validation: 0.06907389383617327]
	TIME [epoch: 2.76 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340467898130625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340467898130625 | validation: 0.1066218561787402]
	TIME [epoch: 2.77 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576551650259787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09576551650259787 | validation: 0.14087199774299375]
	TIME [epoch: 2.76 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12871234334497572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12871234334497572 | validation: 0.09421916071962444]
	TIME [epoch: 2.77 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347777786064454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07347777786064454 | validation: 0.08535192889927137]
	TIME [epoch: 2.77 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185474655453733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185474655453733 | validation: 0.12432718926017201]
	TIME [epoch: 2.77 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775580175949635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11775580175949635 | validation: 0.0710044038101209]
	TIME [epoch: 2.76 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05634845581393853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05634845581393853 | validation: 0.04474781745618888]
	TIME [epoch: 2.76 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019958528757018763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019958528757018763 | validation: 0.026489683312730064]
	TIME [epoch: 2.76 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022190570985252386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022190570985252386 | validation: 0.04988830497158722]
	TIME [epoch: 2.76 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02561933888768941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02561933888768941 | validation: 0.02559095899600046]
	TIME [epoch: 2.76 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021284966159770262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021284966159770262 | validation: 0.03884880044536722]
	TIME [epoch: 2.76 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022173786466727234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022173786466727234 | validation: 0.04059436780607158]
	TIME [epoch: 2.76 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106817429420883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04106817429420883 | validation: 0.12391981874777347]
	TIME [epoch: 2.76 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10717636134125741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10717636134125741 | validation: 0.18631465960321508]
	TIME [epoch: 2.76 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15568494096712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15568494096712 | validation: 0.08031303171323871]
	TIME [epoch: 2.76 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0515404084653875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0515404084653875 | validation: 0.05884355522413287]
	TIME [epoch: 2.76 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03096042051634786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03096042051634786 | validation: 0.04788551655998187]
	TIME [epoch: 2.75 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04635054251286035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04635054251286035 | validation: 0.08359104095517739]
	TIME [epoch: 2.75 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049780141027397085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049780141027397085 | validation: 0.0624432756861579]
	TIME [epoch: 2.75 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04154313351352295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04154313351352295 | validation: 0.049620258133904874]
	TIME [epoch: 2.76 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0397976634519681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0397976634519681 | validation: 0.04090640285318292]
	TIME [epoch: 2.76 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04184731372917689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04184731372917689 | validation: 0.05208044593122958]
	TIME [epoch: 2.76 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03889870354298234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03889870354298234 | validation: 0.05178903924475187]
	TIME [epoch: 2.76 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04371867168768569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04371867168768569 | validation: 0.07935843357582317]
	TIME [epoch: 2.76 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054877131287656006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054877131287656006 | validation: 0.08818656539322058]
	TIME [epoch: 2.76 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852734255623513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06852734255623513 | validation: 0.10734561808911042]
	TIME [epoch: 2.76 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359716035723595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06359716035723595 | validation: 0.06994960268486801]
	TIME [epoch: 2.76 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05057311613718147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05057311613718147 | validation: 0.07207238186493585]
	TIME [epoch: 2.76 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388823296338242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06388823296338242 | validation: 0.07680927940347249]
	TIME [epoch: 2.78 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577284990594781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0577284990594781 | validation: 0.0556028702441579]
	TIME [epoch: 2.76 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04398155353538504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04398155353538504 | validation: 0.045212891735449734]
	TIME [epoch: 2.77 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028492417164432703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028492417164432703 | validation: 0.03124909695599222]
	TIME [epoch: 2.76 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023100831447060842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023100831447060842 | validation: 0.03837021166910214]
	TIME [epoch: 2.76 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026619602553542548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026619602553542548 | validation: 0.07344460442957648]
	TIME [epoch: 2.76 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143784683290654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05143784683290654 | validation: 0.09399928843741034]
	TIME [epoch: 2.77 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752625413395169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752625413395169 | validation: 0.09580937934872023]
	TIME [epoch: 2.76 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0804490134614698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0804490134614698 | validation: 0.07746969924704032]
	TIME [epoch: 2.76 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05080101216780318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05080101216780318 | validation: 0.09122473594655714]
	TIME [epoch: 2.76 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06722359706029808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06722359706029808 | validation: 0.2485462020788941]
	TIME [epoch: 2.76 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13639383460854188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13639383460854188 | validation: 0.08791007209851388]
	TIME [epoch: 2.76 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442179822193836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08442179822193836 | validation: 0.04922159922365657]
	TIME [epoch: 2.76 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051524459677490614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051524459677490614 | validation: 0.05484979394044873]
	TIME [epoch: 2.76 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030367119188376774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030367119188376774 | validation: 0.025348412692690572]
	TIME [epoch: 2.76 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021169298200988437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021169298200988437 | validation: 0.03215392893623319]
	TIME [epoch: 2.76 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02214855790915404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02214855790915404 | validation: 0.04479627709620039]
	TIME [epoch: 2.76 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024822935990789555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024822935990789555 | validation: 0.04300397377974718]
	TIME [epoch: 2.76 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031040600752676797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031040600752676797 | validation: 0.05424529442613072]
	TIME [epoch: 2.76 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041667641338172016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041667641338172016 | validation: 0.0599644477791648]
	TIME [epoch: 2.76 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519693827435746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06519693827435746 | validation: 0.11862817719641412]
	TIME [epoch: 2.76 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10703684188941306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10703684188941306 | validation: 0.14331783959096167]
	TIME [epoch: 2.76 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09581505562858769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09581505562858769 | validation: 0.03541758376378872]
	TIME [epoch: 2.76 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02645957459464832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02645957459464832 | validation: 0.028873761420002798]
	TIME [epoch: 2.76 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014171020289174345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014171020289174345 | validation: 0.025951870877610386]
	TIME [epoch: 2.76 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01844491132786978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01844491132786978 | validation: 0.04010266094790147]
	TIME [epoch: 2.76 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026311159750696596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026311159750696596 | validation: 0.06933999517911753]
	TIME [epoch: 2.76 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048229432915688354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048229432915688354 | validation: 0.12630437656386426]
	TIME [epoch: 2.76 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147454674412218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10147454674412218 | validation: 0.1336116302852817]
	TIME [epoch: 2.76 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0910401003985178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0910401003985178 | validation: 0.06370972356308377]
	TIME [epoch: 2.76 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590570846316796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04590570846316796 | validation: 0.06253859452087819]
	TIME [epoch: 2.76 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043159176971079606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043159176971079606 | validation: 0.0999188399033898]
	TIME [epoch: 2.76 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06504417685615878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06504417685615878 | validation: 0.10456318485590921]
	TIME [epoch: 2.76 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09891594090918115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09891594090918115 | validation: 0.05860023080620929]
	TIME [epoch: 2.76 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078194405031896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06078194405031896 | validation: 0.03685062283536977]
	TIME [epoch: 2.76 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02485238974796082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02485238974796082 | validation: 0.018122133116429785]
	TIME [epoch: 2.76 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01889274087835689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01889274087835689 | validation: 0.04033619006388724]
	TIME [epoch: 2.76 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019882598627482845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019882598627482845 | validation: 0.03071371855573416]
	TIME [epoch: 2.76 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026246950983848768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026246950983848768 | validation: 0.05817423445534057]
	TIME [epoch: 2.76 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032531457217844714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032531457217844714 | validation: 0.036002150415905855]
	TIME [epoch: 2.76 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397504950735541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03397504950735541 | validation: 0.06793958021826135]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241113_145920/states/model_phi1_4a_v_mmd2_994.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2560.497 seconds.
