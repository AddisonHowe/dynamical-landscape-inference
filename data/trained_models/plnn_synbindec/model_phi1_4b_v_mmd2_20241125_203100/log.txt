Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1874013902

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.673719832951238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.673719832951238 | validation: 4.492766926795588]
	TIME [epoch: 182 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.826722502360232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.826722502360232 | validation: 3.8599612295641323]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8574537343318323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8574537343318323 | validation: 3.351500565446553]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4979468399003846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4979468399003846 | validation: 4.33707210131134]
	TIME [epoch: 1.32 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3571083213476682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3571083213476682 | validation: 3.2792465052815944]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5942502388711115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5942502388711115 | validation: 2.897170740951722]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.099504498978647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.099504498978647 | validation: 2.433159392553665]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7833317177891121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7833317177891121 | validation: 2.20704493157235]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5985511250756628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5985511250756628 | validation: 2.5111289362033347]
	TIME [epoch: 1.31 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8415266229951675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8415266229951675 | validation: 2.0692393855543316]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.623075905109866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.623075905109866 | validation: 1.7954459610270548]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3303604619062508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3303604619062508 | validation: 1.8446663139642696]
	TIME [epoch: 1.3 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2130070823731745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2130070823731745 | validation: 1.7549383248842823]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225647336155072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.225647336155072 | validation: 1.7942649005061861]
	TIME [epoch: 1.31 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2381856530569055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2381856530569055 | validation: 1.6681261083902603]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1948039111124928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1948039111124928 | validation: 1.735201168482066]
	TIME [epoch: 1.31 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1515023342283621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1515023342283621 | validation: 1.5557388435557595]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163699971959784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1163699971959784 | validation: 1.6034361428450887]
	TIME [epoch: 1.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0867908894045524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0867908894045524 | validation: 1.4567590183893548]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.076960653967116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.076960653967116 | validation: 1.5243190140038363]
	TIME [epoch: 1.31 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1097087643047312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1097087643047312 | validation: 1.479022019213143]
	TIME [epoch: 1.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1500672500873237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1500672500873237 | validation: 1.4965909388655758]
	TIME [epoch: 1.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0933625946892163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0933625946892163 | validation: 1.3049362658056538]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9710761370402641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9710761370402641 | validation: 1.2737488774390784]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492624567043724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9492624567043724 | validation: 1.2431565003656553]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9369846142487892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9369846142487892 | validation: 1.2528387068639277]
	TIME [epoch: 1.31 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.961143208278115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.961143208278115 | validation: 1.1914583216198584]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940564173945933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9940564173945933 | validation: 1.3934110734046765]
	TIME [epoch: 1.31 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0481013839986293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0481013839986293 | validation: 1.1645067345781979]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702552543568753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9702552543568753 | validation: 1.3174154911346132]
	TIME [epoch: 1.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0982085485025312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0982085485025312 | validation: 1.1996980390343657]
	TIME [epoch: 1.34 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096760271961774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9096760271961774 | validation: 1.1129713298328927]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610346379976455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8610346379976455 | validation: 1.03117548864142]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578792661946978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8578792661946978 | validation: 1.1036260978252312]
	TIME [epoch: 1.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589375913989497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589375913989497 | validation: 0.9980560942882744]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678427700985577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8678427700985577 | validation: 1.1544751797708475]
	TIME [epoch: 1.31 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788268416174612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8788268416174612 | validation: 1.042116922843703]
	TIME [epoch: 1.31 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877204627357297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.877204627357297 | validation: 1.104774259285448]
	TIME [epoch: 1.31 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593575568793873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9593575568793873 | validation: 1.30363968723167]
	TIME [epoch: 1.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1340976795544118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1340976795544118 | validation: 0.9980485602514059]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029917955745179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9029917955745179 | validation: 1.064614593571889]
	TIME [epoch: 1.31 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202637827866295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202637827866295 | validation: 0.9689139917448041]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041657726059617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8041657726059617 | validation: 1.0048162222855348]
	TIME [epoch: 1.31 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257495608012139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8257495608012139 | validation: 0.9679123199390042]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121660197441918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121660197441918 | validation: 1.0279050008454063]
	TIME [epoch: 1.31 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190811186296952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190811186296952 | validation: 0.961248547476844]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207986452296007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207986452296007 | validation: 1.109256845316797]
	TIME [epoch: 1.31 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301447134247789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8301447134247789 | validation: 0.9269239259648239]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976933396161487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976933396161487 | validation: 0.9843776317279218]
	TIME [epoch: 1.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774581717958003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774581717958003 | validation: 0.8844837409457758]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874964049393373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7874964049393373 | validation: 1.1435398800389944]
	TIME [epoch: 1.31 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312173044548246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8312173044548246 | validation: 0.9053929250824521]
	TIME [epoch: 1.31 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649501928539692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649501928539692 | validation: 1.2890123718589943]
	TIME [epoch: 1.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025866059109521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.025866059109521 | validation: 1.2199564105798144]
	TIME [epoch: 1.31 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890902983639569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8890902983639569 | validation: 0.9538406696101345]
	TIME [epoch: 1.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587678070064396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8587678070064396 | validation: 1.0820417969731164]
	TIME [epoch: 1.31 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089787770686873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089787770686873 | validation: 0.9929712527650788]
	TIME [epoch: 1.3 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932726942380847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932726942380847 | validation: 0.9200380279930326]
	TIME [epoch: 1.31 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103770325382115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103770325382115 | validation: 0.9562759182146157]
	TIME [epoch: 1.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665352379770164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7665352379770164 | validation: 0.9352034058691718]
	TIME [epoch: 1.31 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597870109243258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597870109243258 | validation: 0.9007595712803393]
	TIME [epoch: 1.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613335482417017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7613335482417017 | validation: 0.9368817870078354]
	TIME [epoch: 1.31 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556150097143514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556150097143514 | validation: 0.9042687793151162]
	TIME [epoch: 1.31 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7493474714644555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7493474714644555 | validation: 0.9538809164591966]
	TIME [epoch: 1.31 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760237638614389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760237638614389 | validation: 1.0258940706868234]
	TIME [epoch: 1.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589442504357195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589442504357195 | validation: 1.1960843763784212]
	TIME [epoch: 1.3 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986350877494377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9986350877494377 | validation: 0.9397560847942281]
	TIME [epoch: 1.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858176812781394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858176812781394 | validation: 0.8733969126547063]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637955344488342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637955344488342 | validation: 1.0324610932766727]
	TIME [epoch: 1.31 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860523955081486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7860523955081486 | validation: 0.8859091440901956]
	TIME [epoch: 1.31 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803922161573641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803922161573641 | validation: 0.996093067942566]
	TIME [epoch: 1.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824894534843391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824894534843391 | validation: 0.8840811422259702]
	TIME [epoch: 1.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772991874053063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.772991874053063 | validation: 0.9629458709190086]
	TIME [epoch: 1.31 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711663260538254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711663260538254 | validation: 0.9130678754547547]
	TIME [epoch: 1.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711604373644337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711604373644337 | validation: 0.930283599613634]
	TIME [epoch: 1.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827003277536875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827003277536875 | validation: 1.0520090083817462]
	TIME [epoch: 1.31 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8123605958793115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8123605958793115 | validation: 0.9929377464412679]
	TIME [epoch: 1.3 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850078786787708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8850078786787708 | validation: 1.0076461907974716]
	TIME [epoch: 1.31 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794589105668333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794589105668333 | validation: 0.8601070128988977]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705280325561489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705280325561489 | validation: 0.995635585172011]
	TIME [epoch: 1.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7716492260677388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7716492260677388 | validation: 0.8745936065822577]
	TIME [epoch: 1.31 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965371056499523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965371056499523 | validation: 1.1169966271666976]
	TIME [epoch: 1.31 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8627502748534522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8627502748534522 | validation: 0.9483639124445022]
	TIME [epoch: 1.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740784631151644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740784631151644 | validation: 0.8904852077206219]
	TIME [epoch: 1.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919386915524104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7919386915524104 | validation: 1.0457362123566118]
	TIME [epoch: 1.31 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912712664850574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7912712664850574 | validation: 0.8721004149933171]
	TIME [epoch: 1.31 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478513085830645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478513085830645 | validation: 0.9003158445831538]
	TIME [epoch: 1.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427299880890758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7427299880890758 | validation: 0.9380274594578257]
	TIME [epoch: 1.31 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566078586781325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566078586781325 | validation: 0.9089999808661289]
	TIME [epoch: 1.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841667073431916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841667073431916 | validation: 0.9882370499204329]
	TIME [epoch: 1.31 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8255019655977806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8255019655977806 | validation: 1.0975388488235989]
	TIME [epoch: 1.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357696912352674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9357696912352674 | validation: 0.9034392216369336]
	TIME [epoch: 1.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239828504520693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8239828504520693 | validation: 1.0273622116717853]
	TIME [epoch: 1.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835212132821869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7835212132821869 | validation: 0.8712402294586579]
	TIME [epoch: 1.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532024429794407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532024429794407 | validation: 0.8535959634913168]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586453729610794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586453729610794 | validation: 0.9771713588794555]
	TIME [epoch: 1.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671226480414385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7671226480414385 | validation: 0.8866745500117799]
	TIME [epoch: 1.31 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547143939702832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547143939702832 | validation: 0.8923740680937494]
	TIME [epoch: 1.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569834702709533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7569834702709533 | validation: 1.067851295726694]
	TIME [epoch: 1.31 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163255035217515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163255035217515 | validation: 0.9539456028245973]
	TIME [epoch: 1.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613594251813922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8613594251813922 | validation: 0.9246304904020128]
	TIME [epoch: 1.31 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783280350501746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783280350501746 | validation: 0.990373721202813]
	TIME [epoch: 1.31 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77698122571075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77698122571075 | validation: 0.8648088868728897]
	TIME [epoch: 1.31 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701911803369538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7701911803369538 | validation: 1.006570604788913]
	TIME [epoch: 1.32 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970062008538166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7970062008538166 | validation: 0.8960773238024924]
	TIME [epoch: 1.31 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904509504488173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904509504488173 | validation: 0.9925088288966131]
	TIME [epoch: 1.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8073248760712847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8073248760712847 | validation: 0.9291488775300376]
	TIME [epoch: 1.31 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691527779131011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691527779131011 | validation: 0.9024119982285859]
	TIME [epoch: 1.31 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7727317221290193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7727317221290193 | validation: 0.9918518848203819]
	TIME [epoch: 1.31 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621242441210686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7621242441210686 | validation: 0.8647547873441592]
	TIME [epoch: 1.31 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764609745241501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764609745241501 | validation: 0.9521733328220435]
	TIME [epoch: 1.31 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485396133218872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485396133218872 | validation: 0.8462899569708233]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576832559896886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576832559896886 | validation: 1.0247937814340042]
	TIME [epoch: 1.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658100987245371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7658100987245371 | validation: 0.8311567146222676]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708115168348278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708115168348278 | validation: 1.0806245981911091]
	TIME [epoch: 1.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813339542946824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813339542946824 | validation: 0.9337434593721282]
	TIME [epoch: 1.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8124602967892258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8124602967892258 | validation: 1.0372698708590524]
	TIME [epoch: 1.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973800087895327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973800087895327 | validation: 1.0044817043553909]
	TIME [epoch: 1.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050863754171775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8050863754171775 | validation: 0.8727262518829808]
	TIME [epoch: 1.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576017891965536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576017891965536 | validation: 0.9217194982417092]
	TIME [epoch: 1.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353360803814809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353360803814809 | validation: 0.870035795942075]
	TIME [epoch: 1.31 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452065563111891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452065563111891 | validation: 0.9221462388722195]
	TIME [epoch: 1.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601451599617915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601451599617915 | validation: 0.8546670353784205]
	TIME [epoch: 1.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705758664025482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705758664025482 | validation: 1.0168802457183583]
	TIME [epoch: 1.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001618932955827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8001618932955827 | validation: 0.9537983246036783]
	TIME [epoch: 1.31 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747285986463108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747285986463108 | validation: 0.9134119060334371]
	TIME [epoch: 1.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878265970160573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7878265970160573 | validation: 1.0161130720340699]
	TIME [epoch: 1.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815290092399662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7815290092399662 | validation: 0.9015852482601915]
	TIME [epoch: 1.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731354733004662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731354733004662 | validation: 0.9185014741444537]
	TIME [epoch: 1.31 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495221015180635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495221015180635 | validation: 0.9324160832302777]
	TIME [epoch: 1.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750722951040383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750722951040383 | validation: 0.8525598242238148]
	TIME [epoch: 1.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643588953161986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643588953161986 | validation: 1.1374099468154946]
	TIME [epoch: 1.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842063431065683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.842063431065683 | validation: 0.8608926300248971]
	TIME [epoch: 1.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468511297846095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7468511297846095 | validation: 0.8477190905051967]
	TIME [epoch: 1.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504260546747807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504260546747807 | validation: 1.0593694926586952]
	TIME [epoch: 1.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822418956400304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822418956400304 | validation: 0.8950408595134433]
	TIME [epoch: 1.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041393844092991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8041393844092991 | validation: 0.9221319482936559]
	TIME [epoch: 1.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587805567070746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7587805567070746 | validation: 1.0279151466194045]
	TIME [epoch: 1.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978621874009838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978621874009838 | validation: 0.9084899341348692]
	TIME [epoch: 1.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109587863646601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109587863646601 | validation: 1.059273943239522]
	TIME [epoch: 1.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499203966675651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499203966675651 | validation: 0.9383554908141373]
	TIME [epoch: 1.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480281323335379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7480281323335379 | validation: 0.8441920986382145]
	TIME [epoch: 1.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473896139230725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473896139230725 | validation: 0.9002910998599518]
	TIME [epoch: 1.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285933685226732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7285933685226732 | validation: 0.8933433219288038]
	TIME [epoch: 1.31 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7294370827214622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7294370827214622 | validation: 0.8680529683010753]
	TIME [epoch: 1.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730009805130521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730009805130521 | validation: 0.8947750444719742]
	TIME [epoch: 1.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733543337213519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733543337213519 | validation: 0.9406579971686052]
	TIME [epoch: 1.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655813974738139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655813974738139 | validation: 1.0294990012127945]
	TIME [epoch: 1.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545761847221458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545761847221458 | validation: 1.0954474518293844]
	TIME [epoch: 1.31 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438317833223222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9438317833223222 | validation: 0.8961496232408388]
	TIME [epoch: 1.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357601594639688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357601594639688 | validation: 0.9215515266077423]
	TIME [epoch: 1.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350678442511591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350678442511591 | validation: 0.900465114157293]
	TIME [epoch: 1.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668910494351227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668910494351227 | validation: 0.9578723022123482]
	TIME [epoch: 1.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748155015767969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.748155015767969 | validation: 0.8651450859007443]
	TIME [epoch: 1.31 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400016344728536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7400016344728536 | validation: 0.9272232996963994]
	TIME [epoch: 1.31 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173108612960073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173108612960073 | validation: 0.8865384956428782]
	TIME [epoch: 1.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236252947830025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236252947830025 | validation: 0.9459857389255562]
	TIME [epoch: 1.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252617445495608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7252617445495608 | validation: 0.8404019694466666]
	TIME [epoch: 1.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7381012078921816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7381012078921816 | validation: 1.0837688840722366]
	TIME [epoch: 1.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109253083498992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109253083498992 | validation: 0.9362232377893074]
	TIME [epoch: 1.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220798543979178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8220798543979178 | validation: 1.038709632579352]
	TIME [epoch: 1.3 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8699124301989568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699124301989568 | validation: 1.001025772037235]
	TIME [epoch: 1.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576357606123456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576357606123456 | validation: 0.8287976392723702]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226815972189954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226815972189954 | validation: 0.9135768473117225]
	TIME [epoch: 1.31 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198976243845558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7198976243845558 | validation: 0.8852735792909983]
	TIME [epoch: 1.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142729431721219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142729431721219 | validation: 0.8562056956225046]
	TIME [epoch: 1.31 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353055476830107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353055476830107 | validation: 0.9265564270502478]
	TIME [epoch: 1.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7315430971642352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7315430971642352 | validation: 0.8920676559174157]
	TIME [epoch: 1.31 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481313941871764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481313941871764 | validation: 0.9264684522952884]
	TIME [epoch: 1.3 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7825453846466617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7825453846466617 | validation: 0.992585739760671]
	TIME [epoch: 1.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215649861600522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8215649861600522 | validation: 0.8972793579260012]
	TIME [epoch: 1.31 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468110762510909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7468110762510909 | validation: 0.8578918136135054]
	TIME [epoch: 1.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201667183715006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201667183715006 | validation: 0.8970381596214729]
	TIME [epoch: 1.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003249804445794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003249804445794 | validation: 0.8176551420995694]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699318447225026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699318447225026 | validation: 0.9488036094203118]
	TIME [epoch: 1.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053016069186987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053016069186987 | validation: 0.8133545644945663]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069366483696388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069366483696388 | validation: 0.9699435059825874]
	TIME [epoch: 1.31 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151178968861345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151178968861345 | validation: 0.8008401790425625]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328273347747939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7328273347747939 | validation: 1.0745287589577615]
	TIME [epoch: 1.31 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638157028135865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638157028135865 | validation: 0.800809430560486]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7429257909584817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7429257909584817 | validation: 0.9988916907045088]
	TIME [epoch: 1.31 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126860813044511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126860813044511 | validation: 1.339420594780226]
	TIME [epoch: 1.31 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9637117836854853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9637117836854853 | validation: 0.9225166165494042]
	TIME [epoch: 1.31 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975420360128649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975420360128649 | validation: 0.8023786696175605]
	TIME [epoch: 1.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001590882221701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001590882221701 | validation: 1.026170979659728]
	TIME [epoch: 1.31 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331352029955721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331352029955721 | validation: 0.8404575718391375]
	TIME [epoch: 1.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115052855585313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7115052855585313 | validation: 0.7911907054622858]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083163493039513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083163493039513 | validation: 0.9646779796603016]
	TIME [epoch: 1.31 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253489041770468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253489041770468 | validation: 0.8261797037516669]
	TIME [epoch: 1.31 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006203922623532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006203922623532 | validation: 0.8778983785578828]
	TIME [epoch: 1.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083775784087558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083775784087558 | validation: 0.9191017888724615]
	TIME [epoch: 1.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052269602672533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052269602672533 | validation: 0.8786609787981949]
	TIME [epoch: 1.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305244433519391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305244433519391 | validation: 0.8718543588935748]
	TIME [epoch: 1.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395424918923785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7395424918923785 | validation: 0.9566352946803871]
	TIME [epoch: 1.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632784458453494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7632784458453494 | validation: 0.8939821646704842]
	TIME [epoch: 1.31 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119886726889605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119886726889605 | validation: 0.8535544110952883]
	TIME [epoch: 1.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951661974209478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951661974209478 | validation: 0.9149704535567287]
	TIME [epoch: 1.31 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862196648353743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6862196648353743 | validation: 0.8503691943269882]
	TIME [epoch: 1.31 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974526395930506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974526395930506 | validation: 0.8588787419779833]
	TIME [epoch: 1.31 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101056456134655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101056456134655 | validation: 0.9293056787684958]
	TIME [epoch: 1.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760771167332581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760771167332581 | validation: 0.9126570222397471]
	TIME [epoch: 189 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412282324064648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412282324064648 | validation: 0.9022068571003534]
	TIME [epoch: 2.62 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219858981307496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219858981307496 | validation: 0.9053426603786021]
	TIME [epoch: 2.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778117668106338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6778117668106338 | validation: 0.8093255521009752]
	TIME [epoch: 2.59 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608049472773453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608049472773453 | validation: 0.8687595310582332]
	TIME [epoch: 2.59 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599513556604103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6599513556604103 | validation: 0.8230987407541172]
	TIME [epoch: 2.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668902807480539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668902807480539 | validation: 0.875068024965655]
	TIME [epoch: 2.6 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471166944892426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471166944892426 | validation: 1.0728619645171331]
	TIME [epoch: 2.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87377852886187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87377852886187 | validation: 0.9423981077207328]
	TIME [epoch: 2.59 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.689662405413339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.689662405413339 | validation: 0.8307487609656866]
	TIME [epoch: 2.6 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305556037298376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305556037298376 | validation: 0.9922036779445951]
	TIME [epoch: 2.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825426154178799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825426154178799 | validation: 0.8306530158087235]
	TIME [epoch: 2.59 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6638053798901693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6638053798901693 | validation: 0.7992728323535279]
	TIME [epoch: 2.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571679771499634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571679771499634 | validation: 0.8416207287523374]
	TIME [epoch: 2.59 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612575944957007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612575944957007 | validation: 0.8526594526086101]
	TIME [epoch: 2.6 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747836856675992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6747836856675992 | validation: 0.8728078512279844]
	TIME [epoch: 2.6 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193325799990615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193325799990615 | validation: 0.9538614461534506]
	TIME [epoch: 2.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086083929936889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7086083929936889 | validation: 0.8446793748173236]
	TIME [epoch: 2.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915334725111013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915334725111013 | validation: 0.8185177228616999]
	TIME [epoch: 2.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384801069586347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6384801069586347 | validation: 0.7874363121594953]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6228617078463154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6228617078463154 | validation: 0.7653003992440408]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097893854252633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097893854252633 | validation: 0.7697356260832264]
	TIME [epoch: 2.59 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6119342457119542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6119342457119542 | validation: 0.7487894414154556]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6144002998503274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6144002998503274 | validation: 0.9021345912384513]
	TIME [epoch: 2.59 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260285497704793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7260285497704793 | validation: 1.0425097967995605]
	TIME [epoch: 2.59 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723421854981206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723421854981206 | validation: 0.9614805184053227]
	TIME [epoch: 2.59 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199332330707122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199332330707122 | validation: 0.8827407428046936]
	TIME [epoch: 2.59 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139295997261359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139295997261359 | validation: 0.904959024330752]
	TIME [epoch: 2.58 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287113265230547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6287113265230547 | validation: 0.8114473585576877]
	TIME [epoch: 2.6 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358828926685001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358828926685001 | validation: 0.797034751500047]
	TIME [epoch: 2.59 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215517965920403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6215517965920403 | validation: 0.7511954557439622]
	TIME [epoch: 2.59 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6038495594414645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6038495594414645 | validation: 0.7985959000546949]
	TIME [epoch: 2.59 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5971146163960894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971146163960894 | validation: 0.7520732391999614]
	TIME [epoch: 2.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5946952713723008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5946952713723008 | validation: 0.8217124866325559]
	TIME [epoch: 2.59 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360308674025903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360308674025903 | validation: 0.8668378879842241]
	TIME [epoch: 2.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254644185331782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254644185331782 | validation: 0.9487678458888212]
	TIME [epoch: 2.59 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6892944837494162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6892944837494162 | validation: 0.7847403180062872]
	TIME [epoch: 2.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6172255146687418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6172255146687418 | validation: 0.764575524828779]
	TIME [epoch: 2.58 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693655996592231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5693655996592231 | validation: 0.7945405049151659]
	TIME [epoch: 2.59 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5603186851517877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5603186851517877 | validation: 0.6896628181486077]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752305419237338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752305419237338 | validation: 0.8482696310824542]
	TIME [epoch: 2.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973326831396607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5973326831396607 | validation: 0.7093338329808218]
	TIME [epoch: 2.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525585928189096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525585928189096 | validation: 0.885517391008325]
	TIME [epoch: 2.61 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139139638467941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139139638467941 | validation: 0.7782775333144627]
	TIME [epoch: 2.59 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229352279370218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229352279370218 | validation: 0.7406303441820543]
	TIME [epoch: 2.59 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569565862125265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569565862125265 | validation: 0.8531370394477835]
	TIME [epoch: 2.59 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443544978525696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6443544978525696 | validation: 0.760223472976298]
	TIME [epoch: 2.59 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786509638105584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786509638105584 | validation: 0.6902068355510587]
	TIME [epoch: 2.59 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581778119543382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5581778119543382 | validation: 0.7382144768832574]
	TIME [epoch: 2.59 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314500741547816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5314500741547816 | validation: 0.6953542975026149]
	TIME [epoch: 2.59 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5136273736156826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5136273736156826 | validation: 0.6073887856351998]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5097155540011117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5097155540011117 | validation: 0.6926330895545939]
	TIME [epoch: 2.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251237585180537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251237585180537 | validation: 0.7024292065779569]
	TIME [epoch: 2.61 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6126978634739393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6126978634739393 | validation: 1.0289133274099667]
	TIME [epoch: 2.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111242226066463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9111242226066463 | validation: 1.1861717524322861]
	TIME [epoch: 2.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474832980076834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474832980076834 | validation: 0.7030789043260026]
	TIME [epoch: 2.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5577242102945963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5577242102945963 | validation: 0.7248987482602137]
	TIME [epoch: 2.59 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5338979811562533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5338979811562533 | validation: 0.8205325222284018]
	TIME [epoch: 2.59 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400837440525151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5400837440525151 | validation: 0.6697377490081219]
	TIME [epoch: 2.59 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5088081302056017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5088081302056017 | validation: 0.6595211586190532]
	TIME [epoch: 2.59 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4887073229005377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4887073229005377 | validation: 0.6595253091608384]
	TIME [epoch: 2.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4838178491792043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4838178491792043 | validation: 0.62622580430207]
	TIME [epoch: 2.59 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4812570087833338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4812570087833338 | validation: 0.6703560331793996]
	TIME [epoch: 2.59 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49730747213549065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49730747213549065 | validation: 0.7928742471818571]
	TIME [epoch: 2.59 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755963088327632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5755963088327632 | validation: 0.8117807486506671]
	TIME [epoch: 2.59 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863853639072113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6863853639072113 | validation: 0.885224753320136]
	TIME [epoch: 2.59 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564242449170052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.564242449170052 | validation: 0.6716530188895896]
	TIME [epoch: 2.59 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49622973235712176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49622973235712176 | validation: 0.788638926471999]
	TIME [epoch: 2.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4857090614902164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4857090614902164 | validation: 0.5903030757928679]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803063259771743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4803063259771743 | validation: 0.6280630666275402]
	TIME [epoch: 2.61 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4333323391181404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4333323391181404 | validation: 0.5882680417873223]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4247278289096106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4247278289096106 | validation: 0.5756958877771429]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241796601773129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4241796601773129 | validation: 0.7102038715501487]
	TIME [epoch: 2.6 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44752746013921324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44752746013921324 | validation: 0.820299654376031]
	TIME [epoch: 2.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810327080240006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810327080240006 | validation: 1.0183375849099372]
	TIME [epoch: 2.61 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641545227087378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641545227087378 | validation: 0.7439543289036581]
	TIME [epoch: 2.59 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4452043135557554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4452043135557554 | validation: 0.6338076733801928]
	TIME [epoch: 2.6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43922682233169946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43922682233169946 | validation: 0.6860244025243545]
	TIME [epoch: 2.58 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4366885150544966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4366885150544966 | validation: 0.5984719957828196]
	TIME [epoch: 2.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4313889868376849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4313889868376849 | validation: 0.6340137964922158]
	TIME [epoch: 2.58 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4297914804910174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4297914804910174 | validation: 0.633184553021889]
	TIME [epoch: 2.59 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.472303056961092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.472303056961092 | validation: 0.7925668760691796]
	TIME [epoch: 2.59 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140289814153372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6140289814153372 | validation: 0.6475016262024536]
	TIME [epoch: 2.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41776139954026326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41776139954026326 | validation: 0.583728404473335]
	TIME [epoch: 2.58 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37414415958139663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37414415958139663 | validation: 0.5258452817914797]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953081964241251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3953081964241251 | validation: 0.6576942070191365]
	TIME [epoch: 2.58 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4554415781663399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4554415781663399 | validation: 0.5364517603041777]
	TIME [epoch: 2.58 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47565677115746563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47565677115746563 | validation: 0.621096565142603]
	TIME [epoch: 2.57 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3842643233052992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3842643233052992 | validation: 0.6588336927387655]
	TIME [epoch: 2.57 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3903626191485762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3903626191485762 | validation: 0.5727285107040411]
	TIME [epoch: 2.58 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957890566551796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5957890566551796 | validation: 1.077123596896381]
	TIME [epoch: 2.59 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655125422495521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655125422495521 | validation: 0.5131572847739546]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3694337149474461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3694337149474461 | validation: 0.49930619342368526]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3613320174198735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3613320174198735 | validation: 0.6244658113577942]
	TIME [epoch: 2.6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774086892953234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3774086892953234 | validation: 0.5411161423027571]
	TIME [epoch: 2.6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38021458366848104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38021458366848104 | validation: 0.6407739653943569]
	TIME [epoch: 2.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41558359135431505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41558359135431505 | validation: 0.5876260861821895]
	TIME [epoch: 2.59 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.405478688836118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.405478688836118 | validation: 0.6448148411590771]
	TIME [epoch: 2.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40766301735695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40766301735695 | validation: 0.5439082641731235]
	TIME [epoch: 2.59 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3383557510715804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3383557510715804 | validation: 0.5097961123537206]
	TIME [epoch: 2.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3103160173305128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3103160173305128 | validation: 0.4831315691792005]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025261669569319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3025261669569319 | validation: 0.6656403096956831]
	TIME [epoch: 2.6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3759370074281275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3759370074281275 | validation: 0.5889913102749366]
	TIME [epoch: 2.59 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4904559126428144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4904559126428144 | validation: 1.0577515770342147]
	TIME [epoch: 2.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028659879682033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6028659879682033 | validation: 0.5256513693479621]
	TIME [epoch: 2.6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478425128957854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5478425128957854 | validation: 0.5294125603593444]
	TIME [epoch: 2.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3617229652756859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3617229652756859 | validation: 0.6428186188286292]
	TIME [epoch: 2.6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3727090687209736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3727090687209736 | validation: 0.4609304332618347]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3078120614300693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3078120614300693 | validation: 0.4859647565161802]
	TIME [epoch: 2.59 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30327804254314106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30327804254314106 | validation: 0.5320564396440736]
	TIME [epoch: 2.6 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2942140244547092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2942140244547092 | validation: 0.46061428078711286]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2897480529385742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2897480529385742 | validation: 0.5527123652414676]
	TIME [epoch: 2.6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2987349230684748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2987349230684748 | validation: 0.41181443249616384]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34587315179431277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34587315179431277 | validation: 0.7098873874098196]
	TIME [epoch: 2.59 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41060141543181145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41060141543181145 | validation: 0.4057668242650822]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3272735404950825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3272735404950825 | validation: 0.8051577913651976]
	TIME [epoch: 2.59 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4257671527963284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4257671527963284 | validation: 0.5304866933645848]
	TIME [epoch: 2.59 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37251755838790035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37251755838790035 | validation: 0.6041259616128862]
	TIME [epoch: 2.59 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30032042254768687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30032042254768687 | validation: 0.44721096389145676]
	TIME [epoch: 2.58 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25234989247881645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25234989247881645 | validation: 0.4670853209546686]
	TIME [epoch: 2.59 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23552003080268652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23552003080268652 | validation: 0.41956273155959045]
	TIME [epoch: 2.59 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23237857646602336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23237857646602336 | validation: 0.6082726479902765]
	TIME [epoch: 2.59 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29222794895931486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29222794895931486 | validation: 0.5082875524021142]
	TIME [epoch: 2.58 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397842823995943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6397842823995943 | validation: 0.7403091949042891]
	TIME [epoch: 2.59 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36272194051207257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36272194051207257 | validation: 0.48209370646317967]
	TIME [epoch: 2.59 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26959075503025665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26959075503025665 | validation: 0.4170104506272745]
	TIME [epoch: 2.58 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810657210178517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2810657210178517 | validation: 0.47968364536241936]
	TIME [epoch: 2.58 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24703467325267908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24703467325267908 | validation: 0.5111179828165253]
	TIME [epoch: 2.58 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24467356672044396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24467356672044396 | validation: 0.4505699680333855]
	TIME [epoch: 2.59 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2949376538318552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2949376538318552 | validation: 0.7730898743851927]
	TIME [epoch: 2.58 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.405161877216498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.405161877216498 | validation: 0.4696013978520057]
	TIME [epoch: 2.59 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26398824267544174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26398824267544174 | validation: 0.4500857561835301]
	TIME [epoch: 2.59 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22268299074927403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22268299074927403 | validation: 0.38248759320003717]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2278342930440595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2278342930440595 | validation: 0.6010648491208789]
	TIME [epoch: 2.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3078575072352447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3078575072352447 | validation: 0.36420937365857453]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.349427256455776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.349427256455776 | validation: 0.688682197126545]
	TIME [epoch: 2.61 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051383779252313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3051383779252313 | validation: 0.4097936261368491]
	TIME [epoch: 2.59 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2524022860564354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2524022860564354 | validation: 0.6448615240100029]
	TIME [epoch: 2.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34452813725649273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34452813725649273 | validation: 0.4637290633844436]
	TIME [epoch: 2.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25723863671339914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25723863671339914 | validation: 0.5065831102471812]
	TIME [epoch: 2.59 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21000922901871305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21000922901871305 | validation: 0.36867281715347655]
	TIME [epoch: 2.59 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2184648639011572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2184648639011572 | validation: 0.643830366761916]
	TIME [epoch: 2.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28242906173720156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28242906173720156 | validation: 0.3705529624622902]
	TIME [epoch: 2.59 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3264133098798241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3264133098798241 | validation: 0.6437526268206679]
	TIME [epoch: 2.6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26390667754343056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26390667754343056 | validation: 0.4172275593582977]
	TIME [epoch: 2.58 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20460025999573758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20460025999573758 | validation: 0.40890584572385374]
	TIME [epoch: 2.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20102198592483903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20102198592483903 | validation: 0.44796915439377155]
	TIME [epoch: 2.6 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22005983087358796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22005983087358796 | validation: 0.46398803328356425]
	TIME [epoch: 2.58 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2677799205573485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2677799205573485 | validation: 0.44963470861997834]
	TIME [epoch: 2.59 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19655754378931334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19655754378931334 | validation: 0.39024689628022785]
	TIME [epoch: 2.58 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18526521892451747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18526521892451747 | validation: 0.439875788989561]
	TIME [epoch: 2.59 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1682689870077605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1682689870077605 | validation: 0.3597465734298189]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20695174602165153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20695174602165153 | validation: 1.0606571644164358]
	TIME [epoch: 2.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817177791051341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817177791051341 | validation: 0.3931530558281944]
	TIME [epoch: 2.61 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44588753589623836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44588753589623836 | validation: 0.41980194840668467]
	TIME [epoch: 2.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21867882227647692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21867882227647692 | validation: 0.6213009537679102]
	TIME [epoch: 2.59 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716884103938462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2716884103938462 | validation: 0.3941820622124418]
	TIME [epoch: 2.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22313773436588527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22313773436588527 | validation: 0.4799902190653684]
	TIME [epoch: 2.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1977963921883795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1977963921883795 | validation: 0.42454164322055105]
	TIME [epoch: 2.59 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17203208676734355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17203208676734355 | validation: 0.3875865729859327]
	TIME [epoch: 2.59 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16312851709205872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16312851709205872 | validation: 0.42337418490723977]
	TIME [epoch: 2.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601228268568965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601228268568965 | validation: 0.3594041010215349]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15322555156202494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15322555156202494 | validation: 0.4698390018330515]
	TIME [epoch: 2.58 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17474795084893618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17474795084893618 | validation: 0.3165742539854618]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3022001385399581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3022001385399581 | validation: 0.8561821733514086]
	TIME [epoch: 2.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47677991432856937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47677991432856937 | validation: 0.3719679077403846]
	TIME [epoch: 2.6 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19154925955285712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19154925955285712 | validation: 0.5104072488711197]
	TIME [epoch: 2.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3268591890268174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3268591890268174 | validation: 0.4148010709542727]
	TIME [epoch: 2.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20570549058059068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20570549058059068 | validation: 0.4567919716776012]
	TIME [epoch: 2.6 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16907902591889823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16907902591889823 | validation: 0.3710742797815258]
	TIME [epoch: 2.6 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17179920740143703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17179920740143703 | validation: 0.4531249837072022]
	TIME [epoch: 2.59 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15769070102045554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15769070102045554 | validation: 0.36856311307468403]
	TIME [epoch: 2.6 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15935652593908045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15935652593908045 | validation: 0.5279843226996714]
	TIME [epoch: 2.59 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18766402260054782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18766402260054782 | validation: 0.39201866269425933]
	TIME [epoch: 2.6 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24984184526176592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24984184526176592 | validation: 0.7205675764963236]
	TIME [epoch: 2.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197830831296211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3197830831296211 | validation: 0.40466888016787306]
	TIME [epoch: 2.6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16903468871962513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16903468871962513 | validation: 0.4054999584815698]
	TIME [epoch: 2.59 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24093838807028967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24093838807028967 | validation: 0.6277760435506023]
	TIME [epoch: 2.59 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23920564800943667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23920564800943667 | validation: 0.39916394924827703]
	TIME [epoch: 2.59 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2020588016000169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2020588016000169 | validation: 0.4582802018996461]
	TIME [epoch: 2.59 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18269771861540876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18269771861540876 | validation: 0.3444622664071342]
	TIME [epoch: 2.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819697479044831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1819697479044831 | validation: 0.5736586761721607]
	TIME [epoch: 2.6 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20421701532889963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20421701532889963 | validation: 0.3160560924499824]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17622889174199194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17622889174199194 | validation: 0.4915243706867406]
	TIME [epoch: 2.59 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16995331948144832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16995331948144832 | validation: 0.3221761747491964]
	TIME [epoch: 2.59 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18708569042289802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18708569042289802 | validation: 0.6332846385601343]
	TIME [epoch: 2.58 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22146141607612704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22146141607612704 | validation: 0.3129991785143199]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21893222863736408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21893222863736408 | validation: 0.5378834946248555]
	TIME [epoch: 2.59 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18368504386392306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18368504386392306 | validation: 0.328572512157517]
	TIME [epoch: 2.6 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1747463802052814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1747463802052814 | validation: 0.4633409569551752]
	TIME [epoch: 2.59 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14808910719655216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14808910719655216 | validation: 0.36918530756003076]
	TIME [epoch: 2.6 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247642981034474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247642981034474 | validation: 0.33705810185889384]
	TIME [epoch: 2.58 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14131640933878686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14131640933878686 | validation: 0.49891754481730505]
	TIME [epoch: 2.59 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16255286549008158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16255286549008158 | validation: 0.31097365819999007]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1963279483370424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1963279483370424 | validation: 0.7547151839386397]
	TIME [epoch: 2.6 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29879061569537935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29879061569537935 | validation: 0.30319756327314573]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30155450062079875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30155450062079875 | validation: 0.3746972114792529]
	TIME [epoch: 2.58 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19719695243517094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19719695243517094 | validation: 0.6241267109406776]
	TIME [epoch: 2.58 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22935517870370867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22935517870370867 | validation: 0.39786081177109234]
	TIME [epoch: 2.59 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17205697007701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17205697007701 | validation: 0.343689601320136]
	TIME [epoch: 2.59 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14529763147269578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14529763147269578 | validation: 0.43519755715057534]
	TIME [epoch: 2.58 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14872538089988646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14872538089988646 | validation: 0.33062209319690744]
	TIME [epoch: 2.59 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568925403739541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1568925403739541 | validation: 0.5641196251159631]
	TIME [epoch: 2.58 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916533826466306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1916533826466306 | validation: 0.3561231871256499]
	TIME [epoch: 2.59 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12163834017541675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12163834017541675 | validation: 0.3293630974442313]
	TIME [epoch: 2.58 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15597427199758404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15597427199758404 | validation: 0.6542616349822323]
	TIME [epoch: 2.59 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23763745670933023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23763745670933023 | validation: 0.5418033579929292]
	TIME [epoch: 2.58 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5777784074457325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5777784074457325 | validation: 0.32512337866711616]
	TIME [epoch: 2.59 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15898048874234583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15898048874234583 | validation: 0.7102531026227942]
	TIME [epoch: 2.58 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2642003578228297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2642003578228297 | validation: 0.46218703826447016]
	TIME [epoch: 2.58 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17569861550161542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17569861550161542 | validation: 0.32705673974263133]
	TIME [epoch: 2.58 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16759263197365706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16759263197365706 | validation: 0.3681959434842405]
	TIME [epoch: 2.59 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14057272729391287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14057272729391287 | validation: 0.3917688857172343]
	TIME [epoch: 2.58 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13052729772962193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13052729772962193 | validation: 0.3755683486528254]
	TIME [epoch: 2.59 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261895728182588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261895728182588 | validation: 0.3531786328694205]
	TIME [epoch: 2.58 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12019327324778235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12019327324778235 | validation: 0.35632035516887617]
	TIME [epoch: 2.58 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141315704138808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1141315704138808 | validation: 0.3591148017502686]
	TIME [epoch: 2.58 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944953315391985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10944953315391985 | validation: 0.34604546520223534]
	TIME [epoch: 2.58 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11046756958220526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11046756958220526 | validation: 0.33112716916712037]
	TIME [epoch: 2.58 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13577295029393735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13577295029393735 | validation: 0.5665805208592367]
	TIME [epoch: 2.58 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34039755229341806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34039755229341806 | validation: 0.34044880518328596]
	TIME [epoch: 2.58 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18467234885503406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18467234885503406 | validation: 0.4278476008313418]
	TIME [epoch: 2.58 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394574146650773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12394574146650773 | validation: 0.3213887714217906]
	TIME [epoch: 2.58 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12958743886637816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12958743886637816 | validation: 0.535239594517697]
	TIME [epoch: 2.58 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16589553098164941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16589553098164941 | validation: 0.2882026584706174]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13506593860654445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13506593860654445 | validation: 0.5011500637026363]
	TIME [epoch: 2.59 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14685830124776078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14685830124776078 | validation: 0.2707915953500777]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14538302594538663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14538302594538663 | validation: 0.6311854091599657]
	TIME [epoch: 2.58 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20328111281826935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20328111281826935 | validation: 0.2949345162737981]
	TIME [epoch: 2.59 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12509746049381815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12509746049381815 | validation: 0.34304843613522157]
	TIME [epoch: 2.59 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10780731635624528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10780731635624528 | validation: 0.3604982060051831]
	TIME [epoch: 2.59 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10857986276551561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10857986276551561 | validation: 0.29339442096429674]
	TIME [epoch: 2.59 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11112280706850246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11112280706850246 | validation: 0.45815417357879973]
	TIME [epoch: 2.59 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13412606454066822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13412606454066822 | validation: 0.27850183701138426]
	TIME [epoch: 2.58 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20839763239879724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20839763239879724 | validation: 0.719331081808609]
	TIME [epoch: 2.6 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27387538813270995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27387538813270995 | validation: 0.36711506694606555]
	TIME [epoch: 2.58 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397299322598553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12397299322598553 | validation: 0.31683465534599176]
	TIME [epoch: 2.59 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16693497210995184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16693497210995184 | validation: 0.582596927288151]
	TIME [epoch: 2.58 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18769144077479255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18769144077479255 | validation: 0.44649862894096953]
	TIME [epoch: 2.59 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14740798795851254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14740798795851254 | validation: 0.29826827481993795]
	TIME [epoch: 2.58 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14655688802701142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14655688802701142 | validation: 0.5437170825302696]
	TIME [epoch: 2.59 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1730890917061675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1730890917061675 | validation: 0.35375519737789285]
	TIME [epoch: 2.58 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11179293525468226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11179293525468226 | validation: 0.2954810538273501]
	TIME [epoch: 2.59 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384345800782255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1384345800782255 | validation: 0.5160371149822734]
	TIME [epoch: 2.58 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15446139105845408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15446139105845408 | validation: 0.4077812913783751]
	TIME [epoch: 2.59 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11242194125970265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11242194125970265 | validation: 0.30552833113011413]
	TIME [epoch: 2.58 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15850603977992803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15850603977992803 | validation: 0.5374678883527809]
	TIME [epoch: 2.59 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16909925255800393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16909925255800393 | validation: 0.5104089931536943]
	TIME [epoch: 2.58 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5321972920688368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5321972920688368 | validation: 0.30502513614843546]
	TIME [epoch: 2.59 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16829281108319782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16829281108319782 | validation: 0.6787238054263721]
	TIME [epoch: 2.58 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23911103801394645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23911103801394645 | validation: 0.41244300293189246]
	TIME [epoch: 2.59 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12119913376388866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12119913376388866 | validation: 0.3220621224595722]
	TIME [epoch: 2.59 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13774100926984467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13774100926984467 | validation: 0.3602442409018148]
	TIME [epoch: 2.59 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13293210463364102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13293210463364102 | validation: 0.42320673475282045]
	TIME [epoch: 2.58 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428024446742021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428024446742021 | validation: 0.3533453839583677]
	TIME [epoch: 2.59 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989438572543812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11989438572543812 | validation: 0.32659669022107946]
	TIME [epoch: 2.58 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09806536663879924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09806536663879924 | validation: 0.31122421201935346]
	TIME [epoch: 2.59 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09952052786832244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09952052786832244 | validation: 0.344819659698302]
	TIME [epoch: 2.58 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020925826802215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1020925826802215 | validation: 0.30905072950187196]
	TIME [epoch: 2.59 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615713851124276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09615713851124276 | validation: 0.3253418000577731]
	TIME [epoch: 2.58 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09951988075119644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09951988075119644 | validation: 0.2895758626569908]
	TIME [epoch: 2.59 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1065884292063343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1065884292063343 | validation: 0.39934271481806993]
	TIME [epoch: 2.58 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16131691214203833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16131691214203833 | validation: 0.2777738057691887]
	TIME [epoch: 2.59 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424616395634895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1424616395634895 | validation: 0.3349980005443035]
	TIME [epoch: 2.59 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095949962936476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1095949962936476 | validation: 0.2846208865542333]
	TIME [epoch: 2.59 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940745899105493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940745899105493 | validation: 0.4188272642502602]
	TIME [epoch: 2.58 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11380213474987631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11380213474987631 | validation: 0.29275595538017507]
	TIME [epoch: 2.59 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1043178327354229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1043178327354229 | validation: 0.3732282947229959]
	TIME [epoch: 2.59 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10367985419968419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10367985419968419 | validation: 0.27841707669858246]
	TIME [epoch: 2.59 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10130829272559698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10130829272559698 | validation: 0.4527696662939]
	TIME [epoch: 2.58 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15403482249185038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15403482249185038 | validation: 0.24332783843557812]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16016984918039903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16016984918039903 | validation: 0.5693370426504405]
	TIME [epoch: 2.59 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638127177481422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638127177481422 | validation: 0.28862549242790164]
	TIME [epoch: 2.59 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325018160356073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09325018160356073 | validation: 0.27051607982224496]
	TIME [epoch: 2.58 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14020793739478674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14020793739478674 | validation: 0.6539438600005826]
	TIME [epoch: 2.59 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23293296219244597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23293296219244597 | validation: 0.26469352208001934]
	TIME [epoch: 2.59 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583857249644585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10583857249644585 | validation: 0.28059579209754076]
	TIME [epoch: 2.59 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09697019014195063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09697019014195063 | validation: 0.3265876835753032]
	TIME [epoch: 2.59 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09747821140586473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09747821140586473 | validation: 0.25941559972052836]
	TIME [epoch: 2.59 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08822976402119352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08822976402119352 | validation: 0.24760877393121794]
	TIME [epoch: 2.59 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08973350210084192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08973350210084192 | validation: 0.27626336422456205]
	TIME [epoch: 2.59 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10235416665749783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10235416665749783 | validation: 0.3415808541211771]
	TIME [epoch: 2.59 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15824206362136925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15824206362136925 | validation: 0.24467731835731554]
	TIME [epoch: 2.59 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13490173406869227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13490173406869227 | validation: 0.3200093001270258]
	TIME [epoch: 2.59 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10611894838076186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10611894838076186 | validation: 0.25471096504682544]
	TIME [epoch: 2.59 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08506692376697628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08506692376697628 | validation: 0.3507206920451287]
	TIME [epoch: 2.58 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126960464577007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.126960464577007 | validation: 0.296828763349693]
	TIME [epoch: 2.6 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09622365646437064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09622365646437064 | validation: 0.24143398281589865]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10225102023279822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10225102023279822 | validation: 0.6260802842259436]
	TIME [epoch: 2.59 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18415435282766635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18415435282766635 | validation: 0.22586464790352556]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710490364578713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08710490364578713 | validation: 0.26864319145837556]
	TIME [epoch: 2.59 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858749986231445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09858749986231445 | validation: 0.2957943042839943]
	TIME [epoch: 2.59 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09713331975191306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09713331975191306 | validation: 0.2800836519060847]
	TIME [epoch: 2.59 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1016652231076356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1016652231076356 | validation: 0.2590556344836342]
	TIME [epoch: 2.59 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10717932706761316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10717932706761316 | validation: 0.44876501904930005]
	TIME [epoch: 2.59 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379063326021484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1379063326021484 | validation: 0.2432471781542375]
	TIME [epoch: 2.59 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161241327524262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10161241327524262 | validation: 0.40302902189289336]
	TIME [epoch: 2.59 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09971516363520486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09971516363520486 | validation: 0.233872946810057]
	TIME [epoch: 2.59 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08655218288773113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08655218288773113 | validation: 0.41770238636132695]
	TIME [epoch: 2.58 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10409654584427912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10409654584427912 | validation: 0.2187474464216126]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1183809812628671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1183809812628671 | validation: 0.3213673118534469]
	TIME [epoch: 194 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15059115910344104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15059115910344104 | validation: 0.2527785754087093]
	TIME [epoch: 5.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09480497429791786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09480497429791786 | validation: 0.2909746408348724]
	TIME [epoch: 5.57 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08494163348943923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08494163348943923 | validation: 0.23387217771472724]
	TIME [epoch: 5.57 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867261402330206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0867261402330206 | validation: 0.4498586038011501]
	TIME [epoch: 5.59 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10510337019572491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10510337019572491 | validation: 0.24485611690781345]
	TIME [epoch: 5.57 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131790391870895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1131790391870895 | validation: 0.5703169750327055]
	TIME [epoch: 5.59 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511055558804133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511055558804133 | validation: 0.25697688318623735]
	TIME [epoch: 5.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14132251037978188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14132251037978188 | validation: 0.40236791106444114]
	TIME [epoch: 5.59 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810042773633072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09810042773633072 | validation: 0.28158612331872285]
	TIME [epoch: 5.58 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08078958938635854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08078958938635854 | validation: 0.2177636566113179]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07734087387527118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07734087387527118 | validation: 0.30924805600043204]
	TIME [epoch: 5.59 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467647181654148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08467647181654148 | validation: 0.2283517545909784]
	TIME [epoch: 5.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727262172890025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07727262172890025 | validation: 0.23315819930053544]
	TIME [epoch: 5.61 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08698118226875512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08698118226875512 | validation: 0.3182151092171303]
	TIME [epoch: 5.58 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09566170990186784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09566170990186784 | validation: 0.2182403703242478]
	TIME [epoch: 5.59 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192596997180335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192596997180335 | validation: 0.23313048206653111]
	TIME [epoch: 5.58 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514347781241712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06514347781241712 | validation: 0.20821359040513363]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264902173002912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06264902173002912 | validation: 0.21392157717245508]
	TIME [epoch: 5.61 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825501532364488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825501532364488 | validation: 0.28337340037584674]
	TIME [epoch: 5.59 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20359891601559413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20359891601559413 | validation: 0.41328820740456096]
	TIME [epoch: 5.59 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23780616899977497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23780616899977497 | validation: 0.25485729287508446]
	TIME [epoch: 5.59 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10525390189984712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10525390189984712 | validation: 0.6311058588803532]
	TIME [epoch: 5.58 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25143098667716024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25143098667716024 | validation: 0.3323565620084987]
	TIME [epoch: 5.59 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13036157983068536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13036157983068536 | validation: 0.320761916686879]
	TIME [epoch: 5.58 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696563200605464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1696563200605464 | validation: 0.5421646028545236]
	TIME [epoch: 5.58 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14370985617619736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14370985617619736 | validation: 0.24543042652293298]
	TIME [epoch: 5.59 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109433909724152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.109433909724152 | validation: 0.2436194315971447]
	TIME [epoch: 5.58 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115599019138497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09115599019138497 | validation: 0.3261025699159162]
	TIME [epoch: 5.58 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08140070465403962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08140070465403962 | validation: 0.314441897675959]
	TIME [epoch: 5.58 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867327047801384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07867327047801384 | validation: 0.24562827933786227]
	TIME [epoch: 5.58 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06770065846518719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06770065846518719 | validation: 0.2698222151524803]
	TIME [epoch: 5.58 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06602225347312729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06602225347312729 | validation: 0.24776565207157067]
	TIME [epoch: 5.59 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455659233918627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455659233918627 | validation: 0.23941616640401522]
	TIME [epoch: 5.59 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06048557216842995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06048557216842995 | validation: 0.21804361625946408]
	TIME [epoch: 5.58 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0592223031955506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0592223031955506 | validation: 0.2751439741516416]
	TIME [epoch: 5.58 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631805739739709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631805739739709 | validation: 0.1934838773485802]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140403836107066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07140403836107066 | validation: 0.28023677475136094]
	TIME [epoch: 5.58 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15102244207772725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15102244207772725 | validation: 0.3300291891746656]
	TIME [epoch: 5.59 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14863057609794747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14863057609794747 | validation: 0.3250979791551971]
	TIME [epoch: 5.57 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11761349582281148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11761349582281148 | validation: 0.3164791211690283]
	TIME [epoch: 5.59 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18737286097827124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18737286097827124 | validation: 0.5804857012944499]
	TIME [epoch: 5.64 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14908059220758038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14908059220758038 | validation: 0.24217865113092354]
	TIME [epoch: 5.58 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302385957895794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09302385957895794 | validation: 0.22957786926289675]
	TIME [epoch: 5.59 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10052049959770862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10052049959770862 | validation: 0.2889114091589932]
	TIME [epoch: 5.58 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495849294770549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07495849294770549 | validation: 0.2467458111562352]
	TIME [epoch: 5.57 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125584106720409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07125584106720409 | validation: 0.2001019551435106]
	TIME [epoch: 5.59 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648367621409016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648367621409016 | validation: 0.20338379497119058]
	TIME [epoch: 5.58 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06062483012652891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06062483012652891 | validation: 0.21114918710762845]
	TIME [epoch: 5.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691822583675176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05691822583675176 | validation: 0.2030903734111903]
	TIME [epoch: 5.58 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05989958319716683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05989958319716683 | validation: 0.20750053152817977]
	TIME [epoch: 5.59 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023915319802596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07023915319802596 | validation: 0.28632953445162423]
	TIME [epoch: 5.59 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10021693488739995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10021693488739995 | validation: 0.25749453582529913]
	TIME [epoch: 5.59 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12873839519981561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12873839519981561 | validation: 0.37215053882802746]
	TIME [epoch: 5.57 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09792987557522236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09792987557522236 | validation: 0.2412248256147359]
	TIME [epoch: 5.59 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774286024860715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774286024860715 | validation: 0.1798234604449543]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530260986966993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07530260986966993 | validation: 0.5977390949219181]
	TIME [epoch: 5.59 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16016755716914519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16016755716914519 | validation: 0.19777919185821446]
	TIME [epoch: 5.58 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10998265362788344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10998265362788344 | validation: 0.2303095450121283]
	TIME [epoch: 5.59 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10283240901443057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10283240901443057 | validation: 0.32227731367505247]
	TIME [epoch: 5.57 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557343404885213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557343404885213 | validation: 0.22269733130942113]
	TIME [epoch: 5.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060687316406229444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060687316406229444 | validation: 0.20111299384870365]
	TIME [epoch: 5.57 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06556562407475933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06556562407475933 | validation: 0.2734739512552571]
	TIME [epoch: 5.59 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684270288070164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684270288070164 | validation: 0.17705460146654148]
	TIME [epoch: 5.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303922627761374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06303922627761374 | validation: 0.17650984900557468]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711376263140519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0711376263140519 | validation: 0.24137864368331644]
	TIME [epoch: 5.59 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959729211633018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08959729211633018 | validation: 0.1734423131690692]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08446339246180966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08446339246180966 | validation: 0.390208595112024]
	TIME [epoch: 5.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11254787513474866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11254787513474866 | validation: 0.19021309333088715]
	TIME [epoch: 5.56 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942184544290538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0942184544290538 | validation: 0.2639411632937498]
	TIME [epoch: 5.59 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990595367924116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06990595367924116 | validation: 0.16997625411570547]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356189291772601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05356189291772601 | validation: 0.23443330569211504]
	TIME [epoch: 5.59 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239295202914769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239295202914769 | validation: 0.1784184729483408]
	TIME [epoch: 5.58 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0471772492755599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0471772492755599 | validation: 0.18525977956323933]
	TIME [epoch: 5.59 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052845925729513445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052845925729513445 | validation: 0.35455429586955545]
	TIME [epoch: 5.57 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09737554433570267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09737554433570267 | validation: 0.2195590291686041]
	TIME [epoch: 5.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11439237937806356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11439237937806356 | validation: 0.3640087879718312]
	TIME [epoch: 5.59 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11835196311858467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11835196311858467 | validation: 0.28381940793931143]
	TIME [epoch: 5.58 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09057634695161458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09057634695161458 | validation: 0.23262258320638823]
	TIME [epoch: 5.58 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11318875162368511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11318875162368511 | validation: 0.5148878402892544]
	TIME [epoch: 5.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12331715565408427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12331715565408427 | validation: 0.2662610235946238]
	TIME [epoch: 5.57 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510798358274041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1510798358274041 | validation: 0.34018134060536537]
	TIME [epoch: 5.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07455632922759435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07455632922759435 | validation: 0.27315362749439126]
	TIME [epoch: 5.58 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06517100902530956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06517100902530956 | validation: 0.17011533358843675]
	TIME [epoch: 5.58 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671367691407254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0671367691407254 | validation: 0.1923599778546139]
	TIME [epoch: 5.58 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500316606136086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0500316606136086 | validation: 0.1999221900592626]
	TIME [epoch: 5.61 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05016776977745279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05016776977745279 | validation: 0.16827851355033138]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04840588645204893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04840588645204893 | validation: 0.15955619083878708]
	TIME [epoch: 5.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044298765361471155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044298765361471155 | validation: 0.15276382615899667]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04756752508436995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04756752508436995 | validation: 0.1947384009650443]
	TIME [epoch: 5.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515582083628386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07515582083628386 | validation: 0.18947420625735797]
	TIME [epoch: 5.59 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13167917776000132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13167917776000132 | validation: 0.2516956920532732]
	TIME [epoch: 5.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373085278182771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373085278182771 | validation: 0.17891808781295176]
	TIME [epoch: 5.59 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04943933989147775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04943933989147775 | validation: 0.15600854867862723]
	TIME [epoch: 5.57 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05840290663069364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05840290663069364 | validation: 0.1691362241966183]
	TIME [epoch: 5.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056678515297271775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056678515297271775 | validation: 0.1524062955474209]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04919605317119661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04919605317119661 | validation: 0.19831200972213808]
	TIME [epoch: 5.59 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826899932652686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04826899932652686 | validation: 0.13732643285273902]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05105857290947197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05105857290947197 | validation: 0.16216215716513957]
	TIME [epoch: 5.59 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051164347808137896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051164347808137896 | validation: 0.24754799195064311]
	TIME [epoch: 5.58 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06894237524402624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06894237524402624 | validation: 0.2567310619369497]
	TIME [epoch: 5.59 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10525369682138942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10525369682138942 | validation: 0.608740434941358]
	TIME [epoch: 5.58 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16543380505927766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16543380505927766 | validation: 0.15713274003309763]
	TIME [epoch: 5.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952509678740288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952509678740288 | validation: 0.17818406009342147]
	TIME [epoch: 5.57 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926779597821408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05926779597821408 | validation: 0.20674792768696154]
	TIME [epoch: 5.59 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05726994066318511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05726994066318511 | validation: 0.1838942394674251]
	TIME [epoch: 5.57 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057127258439232914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057127258439232914 | validation: 0.17154604690346786]
	TIME [epoch: 5.61 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534612879048259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534612879048259 | validation: 0.16536361966156865]
	TIME [epoch: 5.58 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874602718495856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874602718495856 | validation: 0.17434963607166087]
	TIME [epoch: 5.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09029779502526691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09029779502526691 | validation: 0.21993850826387576]
	TIME [epoch: 5.59 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09993380651782234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09993380651782234 | validation: 0.19769451040646094]
	TIME [epoch: 5.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476226356789017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10476226356789017 | validation: 0.3970425675204285]
	TIME [epoch: 5.58 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08060862346877694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08060862346877694 | validation: 0.2091741605799851]
	TIME [epoch: 5.59 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05479944195277254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05479944195277254 | validation: 0.14290142242978496]
	TIME [epoch: 5.58 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945954218199395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04945954218199395 | validation: 0.14558861499367712]
	TIME [epoch: 5.61 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04213623460700778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04213623460700778 | validation: 0.24368179740804138]
	TIME [epoch: 5.58 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04667311739424155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04667311739424155 | validation: 0.1379170230417112]
	TIME [epoch: 5.61 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03919038619116857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03919038619116857 | validation: 0.14489572096155925]
	TIME [epoch: 5.58 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345462314256056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0345462314256056 | validation: 0.1277597397636257]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033188104917748686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033188104917748686 | validation: 0.1416621472713795]
	TIME [epoch: 5.58 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040853662558743585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040853662558743585 | validation: 0.16859126289072146]
	TIME [epoch: 5.59 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127288406054365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1127288406054365 | validation: 0.2908449134600598]
	TIME [epoch: 5.59 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14197613236623893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14197613236623893 | validation: 0.4509415118858391]
	TIME [epoch: 5.61 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25809345635685593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25809345635685593 | validation: 0.435099115533499]
	TIME [epoch: 5.59 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09377058121201136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09377058121201136 | validation: 0.19695780900765067]
	TIME [epoch: 5.59 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1158038600776197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1158038600776197 | validation: 0.3229840166652957]
	TIME [epoch: 5.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930805215039978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07930805215039978 | validation: 0.19485771492796633]
	TIME [epoch: 5.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271380454497126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08271380454497126 | validation: 0.2930669475592444]
	TIME [epoch: 5.59 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07429122769351422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07429122769351422 | validation: 0.16139937289242975]
	TIME [epoch: 5.58 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040894312533917246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040894312533917246 | validation: 0.16910955866931254]
	TIME [epoch: 5.59 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040871127742985965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040871127742985965 | validation: 0.20752314753441803]
	TIME [epoch: 5.59 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440354415491962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0440354415491962 | validation: 0.13407075567375393]
	TIME [epoch: 5.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04029658718926628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04029658718926628 | validation: 0.2261859903348632]
	TIME [epoch: 5.59 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04355676433988728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04355676433988728 | validation: 0.13577069950287549]
	TIME [epoch: 5.59 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093294300181681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04093294300181681 | validation: 0.3085518167422621]
	TIME [epoch: 5.58 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729887001528906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05729887001528906 | validation: 0.20422881357287437]
	TIME [epoch: 5.59 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732499309444361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07732499309444361 | validation: 0.6422271517942684]
	TIME [epoch: 5.58 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1948733594885388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1948733594885388 | validation: 0.34015798525258034]
	TIME [epoch: 5.58 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083487101476152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.083487101476152 | validation: 0.20176394719173707]
	TIME [epoch: 5.58 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07438593931755022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07438593931755022 | validation: 0.17033699002630698]
	TIME [epoch: 5.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052152469194973225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052152469194973225 | validation: 0.15471096172476925]
	TIME [epoch: 5.58 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788267314213929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04788267314213929 | validation: 0.12209687790902941]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938183698178734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03938183698178734 | validation: 0.1277543342227837]
	TIME [epoch: 5.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03935130883386421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03935130883386421 | validation: 0.12649947304656758]
	TIME [epoch: 5.58 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577324201633478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03577324201633478 | validation: 0.1481195015967226]
	TIME [epoch: 5.59 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054661781159519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054661781159519 | validation: 0.11522069534177369]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360620356389082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06360620356389082 | validation: 0.1965519354711317]
	TIME [epoch: 5.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1647777663532357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1647777663532357 | validation: 0.1477955550905612]
	TIME [epoch: 5.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061365953305815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061365953305815 | validation: 0.1263340118477614]
	TIME [epoch: 5.58 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039350357742688476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039350357742688476 | validation: 0.1643156349861243]
	TIME [epoch: 5.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041002659848799486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041002659848799486 | validation: 0.11676689401257012]
	TIME [epoch: 5.58 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045801384954463516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045801384954463516 | validation: 0.2662976627497477]
	TIME [epoch: 5.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05979623825986929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05979623825986929 | validation: 0.1218384367806734]
	TIME [epoch: 5.58 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863735399958207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04863735399958207 | validation: 0.12074605944182096]
	TIME [epoch: 5.59 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047859382745650374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047859382745650374 | validation: 0.12959221411625774]
	TIME [epoch: 5.58 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04122852959754158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04122852959754158 | validation: 0.12423207338605483]
	TIME [epoch: 5.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03318017854631406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03318017854631406 | validation: 0.11569951663579199]
	TIME [epoch: 5.58 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254874707430993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03254874707430993 | validation: 0.11495548678771668]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044008107202857805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044008107202857805 | validation: 0.2845652479365838]
	TIME [epoch: 5.55 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868680490076962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06868680490076962 | validation: 0.20787420493563635]
	TIME [epoch: 5.58 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10439929972943728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10439929972943728 | validation: 0.40624313893750313]
	TIME [epoch: 5.55 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08841241073932288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08841241073932288 | validation: 0.12964910590996365]
	TIME [epoch: 5.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04650656469085947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04650656469085947 | validation: 0.13958559495728934]
	TIME [epoch: 5.56 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046701252496844675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046701252496844675 | validation: 0.30533567150325996]
	TIME [epoch: 5.57 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885662603687921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885662603687921 | validation: 0.11781511965230691]
	TIME [epoch: 5.57 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04226386721702885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04226386721702885 | validation: 0.14134875408706535]
	TIME [epoch: 5.56 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05636643717511859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05636643717511859 | validation: 0.12104824674015402]
	TIME [epoch: 5.58 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05864459744688369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05864459744688369 | validation: 0.2423803072526854]
	TIME [epoch: 5.59 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827186901109955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0827186901109955 | validation: 0.10083837458095522]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037119644278392865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037119644278392865 | validation: 0.11348387572313823]
	TIME [epoch: 5.57 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030846299977203222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030846299977203222 | validation: 0.0952647413337108]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026475547123358117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026475547123358117 | validation: 0.10631184811162282]
	TIME [epoch: 5.55 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027473279824246095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027473279824246095 | validation: 0.11356362139029808]
	TIME [epoch: 5.55 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05494314574720131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05494314574720131 | validation: 0.21842756067783142]
	TIME [epoch: 5.53 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12906127783991528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12906127783991528 | validation: 0.11994186302013221]
	TIME [epoch: 5.56 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400481329125561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400481329125561 | validation: 0.18566202484180525]
	TIME [epoch: 5.53 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050316900839766594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050316900839766594 | validation: 0.10608414153909902]
	TIME [epoch: 5.55 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043059870003592834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043059870003592834 | validation: 0.30048137054385543]
	TIME [epoch: 5.53 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483704755785217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0483704755785217 | validation: 0.1618125797510538]
	TIME [epoch: 5.54 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644469572392996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0644469572392996 | validation: 0.3613740759092552]
	TIME [epoch: 5.55 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10834376586531554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10834376586531554 | validation: 0.269278983549168]
	TIME [epoch: 5.56 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335222383771758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1335222383771758 | validation: 0.29825436547838463]
	TIME [epoch: 5.56 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366013961229364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366013961229364 | validation: 0.19194916819427338]
	TIME [epoch: 5.56 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06208718171623483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06208718171623483 | validation: 0.12856169646104793]
	TIME [epoch: 5.54 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042666212269375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042666212269375 | validation: 0.2031947462126451]
	TIME [epoch: 5.56 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092598901980124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04092598901980124 | validation: 0.13238667444520943]
	TIME [epoch: 5.55 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029670210866133057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029670210866133057 | validation: 0.13519635229681756]
	TIME [epoch: 5.54 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04603434920475891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04603434920475891 | validation: 0.2356939524074762]
	TIME [epoch: 5.54 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754238138932487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03754238138932487 | validation: 0.10824828896523289]
	TIME [epoch: 5.56 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030678719439874512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030678719439874512 | validation: 0.1259249401139145]
	TIME [epoch: 5.54 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027803496694032023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027803496694032023 | validation: 0.11606956445851258]
	TIME [epoch: 5.57 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644024225806026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03644024225806026 | validation: 0.24238391163592354]
	TIME [epoch: 5.54 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905198407078371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04905198407078371 | validation: 0.10350398447474332]
	TIME [epoch: 5.55 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146917803183546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04146917803183546 | validation: 0.25465393578760714]
	TIME [epoch: 5.55 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045483115647731644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045483115647731644 | validation: 0.10519334384730783]
	TIME [epoch: 5.54 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961207180001391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961207180001391 | validation: 0.20174669161266456]
	TIME [epoch: 5.54 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05162696852631321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05162696852631321 | validation: 0.1317835848064052]
	TIME [epoch: 5.56 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688160861836539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0688160861836539 | validation: 0.25164163813818735]
	TIME [epoch: 5.56 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936116357215131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0936116357215131 | validation: 0.13885882935609392]
	TIME [epoch: 5.56 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040103473914776915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040103473914776915 | validation: 0.1297172755507999]
	TIME [epoch: 5.54 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140924991281238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07140924991281238 | validation: 0.2133572427585446]
	TIME [epoch: 5.61 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046374753231609674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046374753231609674 | validation: 0.11335126063115282]
	TIME [epoch: 5.62 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692215375123015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03692215375123015 | validation: 0.12565446177765557]
	TIME [epoch: 5.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04959927333894892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04959927333894892 | validation: 0.2790485993106879]
	TIME [epoch: 5.55 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058432822167110765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058432822167110765 | validation: 0.12486396344124884]
	TIME [epoch: 5.55 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050696302481225926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050696302481225926 | validation: 0.15129077270927999]
	TIME [epoch: 5.57 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053449591740373614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053449591740373614 | validation: 0.11697008193405173]
	TIME [epoch: 5.57 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047092407844268824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047092407844268824 | validation: 0.17704142400591014]
	TIME [epoch: 5.57 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380220404642039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0380220404642039 | validation: 0.11821074439502621]
	TIME [epoch: 5.56 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0463430388480925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0463430388480925 | validation: 0.17586745959762215]
	TIME [epoch: 5.57 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115102284368197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05115102284368197 | validation: 0.12827592872500043]
	TIME [epoch: 5.57 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08149613357978375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08149613357978375 | validation: 0.24170236699738146]
	TIME [epoch: 5.57 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06333718218236858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06333718218236858 | validation: 0.14940659258413982]
	TIME [epoch: 5.58 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157153459161606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06157153459161606 | validation: 0.10093260415718062]
	TIME [epoch: 5.57 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142752403859045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03142752403859045 | validation: 0.09019679214709447]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026023136172769936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026023136172769936 | validation: 0.08891607167939411]
	TIME [epoch: 5.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03185314512995625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03185314512995625 | validation: 0.38195466144628387]
	TIME [epoch: 5.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741311247504945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0741311247504945 | validation: 0.11155075591342355]
	TIME [epoch: 5.58 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573150076432953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0573150076432953 | validation: 0.1185635790740479]
	TIME [epoch: 5.59 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059113344927768624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059113344927768624 | validation: 0.21040405419111874]
	TIME [epoch: 5.59 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058547684905481845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058547684905481845 | validation: 0.0777265965818203]
	TIME [epoch: 5.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029158223778273488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029158223778273488 | validation: 0.09185532275797748]
	TIME [epoch: 5.55 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027264734192386352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027264734192386352 | validation: 0.1641397551744641]
	TIME [epoch: 5.58 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030338667522087324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030338667522087324 | validation: 0.10459201572184547]
	TIME [epoch: 5.57 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031929598040731415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031929598040731415 | validation: 0.2179970495337448]
	TIME [epoch: 5.58 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04167939281383564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04167939281383564 | validation: 0.08766922572151602]
	TIME [epoch: 5.59 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02557320321448997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02557320321448997 | validation: 0.12778716993906178]
	TIME [epoch: 5.58 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04320587211430849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04320587211430849 | validation: 0.24422152422696097]
	TIME [epoch: 5.56 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09862120927064455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09862120927064455 | validation: 0.1563892377414264]
	TIME [epoch: 5.61 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1198879690232043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1198879690232043 | validation: 0.11422351119398116]
	TIME [epoch: 5.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030219827622708726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030219827622708726 | validation: 0.10893501479320133]
	TIME [epoch: 5.59 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441445326288304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04441445326288304 | validation: 0.13855195753571695]
	TIME [epoch: 5.59 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04091446060334735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04091446060334735 | validation: 0.10242092972105438]
	TIME [epoch: 5.58 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034100724023937094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034100724023937094 | validation: 0.12090260774428303]
	TIME [epoch: 5.58 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03325551476859094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03325551476859094 | validation: 0.08479023111269163]
	TIME [epoch: 5.58 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03247984433289314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03247984433289314 | validation: 0.16294656245140127]
	TIME [epoch: 5.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479202367266586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03479202367266586 | validation: 0.13133570598739888]
	TIME [epoch: 5.59 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04350413165235741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04350413165235741 | validation: 0.16939228994073938]
	TIME [epoch: 5.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03336464140552344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03336464140552344 | validation: 0.0868480495713637]
	TIME [epoch: 5.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885594329882877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02885594329882877 | validation: 0.12001857530048068]
	TIME [epoch: 5.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440094942795976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0440094942795976 | validation: 0.13558663686678687]
	TIME [epoch: 5.59 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833648930166296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06833648930166296 | validation: 0.13454492095270468]
	TIME [epoch: 5.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11785874925311746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11785874925311746 | validation: 0.19167179611940208]
	TIME [epoch: 5.59 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17010887210910583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17010887210910583 | validation: 0.24038627974656285]
	TIME [epoch: 5.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14521720420812945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14521720420812945 | validation: 0.24321271695263375]
	TIME [epoch: 5.59 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12511281271308108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12511281271308108 | validation: 0.1402306464387301]
	TIME [epoch: 5.58 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728631541643364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0728631541643364 | validation: 0.14186592661126393]
	TIME [epoch: 5.58 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05931068519011285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05931068519011285 | validation: 0.11113030080497517]
	TIME [epoch: 5.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560161893892883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04560161893892883 | validation: 0.12447648548638811]
	TIME [epoch: 5.61 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032810732022987885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032810732022987885 | validation: 0.13070337760415823]
	TIME [epoch: 5.61 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02715123141754801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02715123141754801 | validation: 0.10390348449927078]
	TIME [epoch: 5.59 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031279672079980864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031279672079980864 | validation: 0.17996308892989699]
	TIME [epoch: 5.58 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02934115736320651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02934115736320651 | validation: 0.08706133957795588]
	TIME [epoch: 5.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02384834396517534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02384834396517534 | validation: 0.09685309575523364]
	TIME [epoch: 5.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026911264081167622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026911264081167622 | validation: 0.19654404774848686]
	TIME [epoch: 5.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032502724368644056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032502724368644056 | validation: 0.09168460178058796]
	TIME [epoch: 5.59 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875308143055493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02875308143055493 | validation: 0.20479787644373132]
	TIME [epoch: 5.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05580582006652906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05580582006652906 | validation: 0.1253765575286144]
	TIME [epoch: 5.59 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057690063575825255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057690063575825255 | validation: 0.1190747729786823]
	TIME [epoch: 5.58 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040989341877267396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040989341877267396 | validation: 0.18311596066649596]
	TIME [epoch: 5.59 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04315192240302954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04315192240302954 | validation: 0.08579873717720828]
	TIME [epoch: 5.59 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028503581303600582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028503581303600582 | validation: 0.12080543035460099]
	TIME [epoch: 5.59 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024301636447416387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024301636447416387 | validation: 0.0928353969054926]
	TIME [epoch: 5.59 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02449412251448464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02449412251448464 | validation: 0.11651330160385327]
	TIME [epoch: 5.58 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311051715037551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0311051715037551 | validation: 0.10832404287202793]
	TIME [epoch: 5.59 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0336612062437635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0336612062437635 | validation: 0.24261005323203025]
	TIME [epoch: 5.58 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804138504796267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804138504796267 | validation: 0.07719471947840978]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020084366670922085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020084366670922085 | validation: 0.08072470808839843]
	TIME [epoch: 5.56 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02733077151564104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02733077151564104 | validation: 0.10381554033416003]
	TIME [epoch: 5.56 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882105555285844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03882105555285844 | validation: 0.2936185962929814]
	TIME [epoch: 5.57 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508599951677868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07508599951677868 | validation: 0.12466842336457824]
	TIME [epoch: 5.54 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06632073908278209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06632073908278209 | validation: 0.07269267581223496]
	TIME [epoch: 5.56 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022387520248443628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022387520248443628 | validation: 0.1016064692416467]
	TIME [epoch: 5.58 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02979038234434637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02979038234434637 | validation: 0.08710253157287465]
	TIME [epoch: 5.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037565921407865084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037565921407865084 | validation: 0.15060341698767812]
	TIME [epoch: 5.53 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058523917470479435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058523917470479435 | validation: 0.12202768627821695]
	TIME [epoch: 5.56 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05523450485242787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05523450485242787 | validation: 0.2618672682179893]
	TIME [epoch: 5.54 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0541343911925763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0541343911925763 | validation: 0.09551062502166549]
	TIME [epoch: 5.56 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028768355448992644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028768355448992644 | validation: 0.07727146412963709]
	TIME [epoch: 5.55 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028496797194237377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028496797194237377 | validation: 0.1953135520850017]
	TIME [epoch: 5.56 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03650465614526195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03650465614526195 | validation: 0.0694961487688534]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04294376194551824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04294376194551824 | validation: 0.11561809121338529]
	TIME [epoch: 5.61 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565793088804591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04565793088804591 | validation: 0.1572347009513104]
	TIME [epoch: 5.58 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808388389363321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0808388389363321 | validation: 0.1321387055231328]
	TIME [epoch: 5.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133644724431413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08133644724431413 | validation: 0.2368210267931921]
	TIME [epoch: 5.61 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020634783013977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1020634783013977 | validation: 0.11974605377081911]
	TIME [epoch: 5.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511075595780256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0511075595780256 | validation: 0.1638420505117935]
	TIME [epoch: 5.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035925635987820954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035925635987820954 | validation: 0.11703697044828615]
	TIME [epoch: 5.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066927267468309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04066927267468309 | validation: 0.13103458000822088]
	TIME [epoch: 5.57 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034600585924945776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034600585924945776 | validation: 0.08120124811603757]
	TIME [epoch: 5.58 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022728137033418767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022728137033418767 | validation: 0.07354638763027492]
	TIME [epoch: 5.57 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021222762465911097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021222762465911097 | validation: 0.19344764029821837]
	TIME [epoch: 5.59 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030852993741270367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030852993741270367 | validation: 0.06216827333442883]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020895369341050404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020895369341050404 | validation: 0.08418483278082144]
	TIME [epoch: 5.61 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753383542055195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02753383542055195 | validation: 0.07392126852748646]
	TIME [epoch: 5.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02887921055600896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02887921055600896 | validation: 0.1219618459173399]
	TIME [epoch: 5.59 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115791208757958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04115791208757958 | validation: 0.08858127246496272]
	TIME [epoch: 5.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692079766743762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04692079766743762 | validation: 0.2141895847527696]
	TIME [epoch: 5.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050353683789900856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050353683789900856 | validation: 0.08745542467533961]
	TIME [epoch: 5.61 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03057532612581967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03057532612581967 | validation: 0.14719785367560034]
	TIME [epoch: 5.61 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043213627776801024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043213627776801024 | validation: 0.08357535229471595]
	TIME [epoch: 5.61 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030608588772378934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030608588772378934 | validation: 0.08327962308846791]
	TIME [epoch: 5.61 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01988465062491525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01988465062491525 | validation: 0.10659335375430082]
	TIME [epoch: 5.61 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020131387940944868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020131387940944868 | validation: 0.054720594211769086]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021458177238428338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021458177238428338 | validation: 0.15096586757460978]
	TIME [epoch: 5.61 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040567368714479156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040567368714479156 | validation: 0.1213930306875259]
	TIME [epoch: 5.58 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06278065839210387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06278065839210387 | validation: 0.2162660568345725]
	TIME [epoch: 5.61 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11408478092752997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11408478092752997 | validation: 0.10162306201977754]
	TIME [epoch: 5.59 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07281290137191483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07281290137191483 | validation: 0.0978579991854726]
	TIME [epoch: 5.61 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03316795698979806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03316795698979806 | validation: 0.2995423089576872]
	TIME [epoch: 5.59 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624057778027174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624057778027174 | validation: 0.09660315102589999]
	TIME [epoch: 5.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030170960776669187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030170960776669187 | validation: 0.08705149494969698]
	TIME [epoch: 5.58 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02460871663296033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02460871663296033 | validation: 0.07912885530340433]
	TIME [epoch: 5.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021732029165410668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021732029165410668 | validation: 0.05160010895140822]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017938285792915055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017938285792915055 | validation: 0.04871711972525253]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014466064597277338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014466064597277338 | validation: 0.0552330058339127]
	TIME [epoch: 5.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01564388190150236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01564388190150236 | validation: 0.05535934524714811]
	TIME [epoch: 5.61 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01636463977319674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01636463977319674 | validation: 0.07038751766791457]
	TIME [epoch: 5.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016077879728060036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016077879728060036 | validation: 0.07360534922038657]
	TIME [epoch: 5.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016233223190555732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016233223190555732 | validation: 0.06864784207246763]
	TIME [epoch: 5.59 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021767639249261058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021767639249261058 | validation: 0.14655140258322258]
	TIME [epoch: 5.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929673693223419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0929673693223419 | validation: 0.23082431116468805]
	TIME [epoch: 5.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644173025348195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08644173025348195 | validation: 0.17218419345652392]
	TIME [epoch: 5.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719718557219194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06719718557219194 | validation: 0.19621900655374325]
	TIME [epoch: 5.57 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13128701312291352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13128701312291352 | validation: 0.2287698918305738]
	TIME [epoch: 5.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710501216105262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710501216105262 | validation: 0.12698120945687333]
	TIME [epoch: 5.58 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054231371218124796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054231371218124796 | validation: 0.13041260757502546]
	TIME [epoch: 5.61 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029156015911439636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029156015911439636 | validation: 0.08739443191091126]
	TIME [epoch: 5.58 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024864101642179268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024864101642179268 | validation: 0.07398163524794443]
	TIME [epoch: 5.62 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022885433457927453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022885433457927453 | validation: 0.07270852296499833]
	TIME [epoch: 5.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020392770559832147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020392770559832147 | validation: 0.07020232397461847]
	TIME [epoch: 5.61 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018470128841471777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018470128841471777 | validation: 0.0713501574936815]
	TIME [epoch: 5.59 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018965927988134575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018965927988134575 | validation: 0.07994551044753867]
	TIME [epoch: 5.62 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018121211666495433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018121211666495433 | validation: 0.08586248076926177]
	TIME [epoch: 5.59 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02095987110670088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02095987110670088 | validation: 0.07083619182403185]
	TIME [epoch: 5.61 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023076337646371292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023076337646371292 | validation: 0.09334288327021195]
	TIME [epoch: 5.58 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027156722318755683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027156722318755683 | validation: 0.12249975212227832]
	TIME [epoch: 5.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040526930742861574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040526930742861574 | validation: 0.16719269487402386]
	TIME [epoch: 5.61 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352891385749348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05352891385749348 | validation: 0.12121571070655582]
	TIME [epoch: 5.61 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352640663839198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0352640663839198 | validation: 0.10818467496655584]
	TIME [epoch: 5.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330738534836529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04330738534836529 | validation: 0.12504448858839706]
	TIME [epoch: 5.61 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06164555861031646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06164555861031646 | validation: 0.23654602097917554]
	TIME [epoch: 5.61 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990591323502553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0990591323502553 | validation: 0.11248662522892441]
	TIME [epoch: 5.62 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682089798264208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07682089798264208 | validation: 0.07150673208339414]
	TIME [epoch: 5.59 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961937843971293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01961937843971293 | validation: 0.18591861452582395]
	TIME [epoch: 5.61 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644671573506022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05644671573506022 | validation: 0.3161269550984094]
	TIME [epoch: 5.59 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06649206126933067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06649206126933067 | validation: 0.09380486531942872]
	TIME [epoch: 5.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05596973760110196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05596973760110196 | validation: 0.08924040956312507]
	TIME [epoch: 5.58 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321283728110128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03321283728110128 | validation: 0.0676674241138089]
	TIME [epoch: 5.61 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016033168181045516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016033168181045516 | validation: 0.0688711867311149]
	TIME [epoch: 5.59 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019531867185304686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019531867185304686 | validation: 0.07580147284758897]
	TIME [epoch: 5.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01756409271994012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01756409271994012 | validation: 0.07067217845921385]
	TIME [epoch: 5.59 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012527132085718653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012527132085718653 | validation: 0.07596217938106711]
	TIME [epoch: 5.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014033032660914695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014033032660914695 | validation: 0.04957620367941112]
	TIME [epoch: 5.59 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014680286995824037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014680286995824037 | validation: 0.0714644533801894]
	TIME [epoch: 5.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024609051283471725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024609051283471725 | validation: 0.10554576934955495]
	TIME [epoch: 5.61 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726442396961229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0726442396961229 | validation: 0.062103075262001865]
	TIME [epoch: 5.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020522413122389213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020522413122389213 | validation: 0.05608636400519108]
	TIME [epoch: 5.59 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01640132006200117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01640132006200117 | validation: 0.07138272273709176]
	TIME [epoch: 5.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018515328428418795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018515328428418795 | validation: 0.1399791633663656]
	TIME [epoch: 5.59 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435449945280858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05435449945280858 | validation: 0.12813792843958227]
	TIME [epoch: 5.58 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09223458463053184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09223458463053184 | validation: 0.20063501913874937]
	TIME [epoch: 5.57 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12056844712605305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12056844712605305 | validation: 0.18305062820661494]
	TIME [epoch: 5.61 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04639258866643317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04639258866643317 | validation: 0.1050136139551753]
	TIME [epoch: 5.61 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06344410434690013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06344410434690013 | validation: 0.12918053080163103]
	TIME [epoch: 5.61 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352431759327268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05352431759327268 | validation: 0.08831725027426923]
	TIME [epoch: 5.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03442552072750442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03442552072750442 | validation: 0.05607450105630682]
	TIME [epoch: 5.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029463049665725036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029463049665725036 | validation: 0.08448540958258584]
	TIME [epoch: 5.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017925397234913532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017925397234913532 | validation: 0.05845232221930128]
	TIME [epoch: 5.62 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015597337263588407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015597337263588407 | validation: 0.15152673338286232]
	TIME [epoch: 5.58 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02398035854615226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02398035854615226 | validation: 0.10123170207608685]
	TIME [epoch: 5.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029204931568734285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029204931568734285 | validation: 0.07124378219868287]
	TIME [epoch: 5.61 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020190736174732573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020190736174732573 | validation: 0.06368373097837231]
	TIME [epoch: 5.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01602663724159668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01602663724159668 | validation: 0.055938101603217216]
	TIME [epoch: 5.61 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012984806537277073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012984806537277073 | validation: 0.05549019163545778]
	TIME [epoch: 5.62 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018622066451185013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018622066451185013 | validation: 0.07357234161604748]
	TIME [epoch: 5.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030125090075078395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030125090075078395 | validation: 0.28071648659593723]
	TIME [epoch: 5.59 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266256940875698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07266256940875698 | validation: 0.06464402884373742]
	TIME [epoch: 5.59 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881079159926757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03881079159926757 | validation: 0.07894294159332466]
	TIME [epoch: 5.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03107601675406647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03107601675406647 | validation: 0.1600893619739022]
	TIME [epoch: 5.59 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993018582575248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03993018582575248 | validation: 0.0965149126984611]
	TIME [epoch: 5.61 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321314987188051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04321314987188051 | validation: 0.216450904762603]
	TIME [epoch: 5.59 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111449397566799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05111449397566799 | validation: 0.10854100187804919]
	TIME [epoch: 5.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045522400874203124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045522400874203124 | validation: 0.09696146952880377]
	TIME [epoch: 5.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024495560229681242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024495560229681242 | validation: 0.08532970388419509]
	TIME [epoch: 5.62 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034223376257262086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034223376257262086 | validation: 0.06724300360892224]
	TIME [epoch: 5.58 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460081901253365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0460081901253365 | validation: 0.22433868512449245]
	TIME [epoch: 5.63 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0681974763726407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0681974763726407 | validation: 0.10670711927602126]
	TIME [epoch: 5.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05552054200415617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05552054200415617 | validation: 0.15309678250307168]
	TIME [epoch: 5.61 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729751643643691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03729751643643691 | validation: 0.11069669504758406]
	TIME [epoch: 5.59 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885980526373344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02885980526373344 | validation: 0.061493966723391094]
	TIME [epoch: 5.59 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022517459554511544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022517459554511544 | validation: 0.057031875131006396]
	TIME [epoch: 5.61 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015349043065470998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015349043065470998 | validation: 0.05120790042638033]
	TIME [epoch: 5.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018393419616037902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018393419616037902 | validation: 0.05850821327294688]
	TIME [epoch: 5.59 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01966821867466872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01966821867466872 | validation: 0.07017321466079295]
	TIME [epoch: 5.61 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029579589977564363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029579589977564363 | validation: 0.06907190201683898]
	TIME [epoch: 5.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041701865012926734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041701865012926734 | validation: 0.0781908454013876]
	TIME [epoch: 5.61 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050634965985271826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050634965985271826 | validation: 0.05795646491802983]
	TIME [epoch: 5.59 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04867457709076213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04867457709076213 | validation: 0.08135816675987968]
	TIME [epoch: 5.62 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021629035265430276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021629035265430276 | validation: 0.07827274649790815]
	TIME [epoch: 5.59 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018536222639523297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018536222639523297 | validation: 0.05698818570230442]
	TIME [epoch: 5.61 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018305445641606292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018305445641606292 | validation: 0.048098524617195916]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01729485769261399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01729485769261399 | validation: 0.16596545726183104]
	TIME [epoch: 5.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03561917825479791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03561917825479791 | validation: 0.056825945964821135]
	TIME [epoch: 5.62 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03553461627350954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03553461627350954 | validation: 0.05762143718591772]
	TIME [epoch: 5.59 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03493348058661532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03493348058661532 | validation: 0.06836999369773082]
	TIME [epoch: 5.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447268991814415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04447268991814415 | validation: 0.12174556094255734]
	TIME [epoch: 5.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04775639909900844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04775639909900844 | validation: 0.08022822869886062]
	TIME [epoch: 5.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046892827274717136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046892827274717136 | validation: 0.13548048086209685]
	TIME [epoch: 5.59 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12105090226703244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12105090226703244 | validation: 0.29617058976281285]
	TIME [epoch: 5.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715522781597577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06715522781597577 | validation: 0.1384269159760757]
	TIME [epoch: 5.59 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870962267151409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06870962267151409 | validation: 0.06818144121251758]
	TIME [epoch: 5.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595753400335993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03595753400335993 | validation: 0.06643657031671556]
	TIME [epoch: 5.59 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018311970188181877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018311970188181877 | validation: 0.0864916648032891]
	TIME [epoch: 5.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020900574502396997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020900574502396997 | validation: 0.04903734522900212]
	TIME [epoch: 5.59 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013456263942087196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013456263942087196 | validation: 0.037850827562643255]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015111212742520442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015111212742520442 | validation: 0.04371988707322069]
	TIME [epoch: 5.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015107731225560595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015107731225560595 | validation: 0.044304998966677334]
	TIME [epoch: 5.59 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017070216134983204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017070216134983204 | validation: 0.05757467433912575]
	TIME [epoch: 5.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023406916327884497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023406916327884497 | validation: 0.056552166750533206]
	TIME [epoch: 5.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583144253041918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03583144253041918 | validation: 0.07098731742959317]
	TIME [epoch: 5.59 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038581744602116455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038581744602116455 | validation: 0.04746937120715573]
	TIME [epoch: 5.61 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025585110939429282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025585110939429282 | validation: 0.06793972096604022]
	TIME [epoch: 5.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027404576698672127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027404576698672127 | validation: 0.1936769529246387]
	TIME [epoch: 5.61 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396276184790965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396276184790965 | validation: 0.08898261022671816]
	TIME [epoch: 5.62 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624805803423393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05624805803423393 | validation: 0.09457910212264531]
	TIME [epoch: 5.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061064202378280995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061064202378280995 | validation: 0.07048360667522394]
	TIME [epoch: 5.59 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04223575162950257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04223575162950257 | validation: 0.05632522801335134]
	TIME [epoch: 5.59 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491408145791161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03491408145791161 | validation: 0.0857857135758924]
	TIME [epoch: 5.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039450514044194245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039450514044194245 | validation: 0.043754601256184814]
	TIME [epoch: 5.61 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021391103956596207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021391103956596207 | validation: 0.0631710645110043]
	TIME [epoch: 5.61 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017782212098619934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017782212098619934 | validation: 0.09030291041516851]
	TIME [epoch: 5.61 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01940651713787733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01940651713787733 | validation: 0.04846335924234298]
	TIME [epoch: 5.61 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01691794914051765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01691794914051765 | validation: 0.06869863292813323]
	TIME [epoch: 5.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01586270517676903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01586270517676903 | validation: 0.03812119487564906]
	TIME [epoch: 5.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012551017639369234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012551017639369234 | validation: 0.05285203578625125]
	TIME [epoch: 5.61 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011078759735517982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011078759735517982 | validation: 0.04841748668494247]
	TIME [epoch: 5.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016735328412051255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016735328412051255 | validation: 0.06872915883626361]
	TIME [epoch: 5.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03737301759182843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03737301759182843 | validation: 0.23892677808224622]
	TIME [epoch: 5.61 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377148241540909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08377148241540909 | validation: 0.07539439283174944]
	TIME [epoch: 5.59 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026669122338634815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026669122338634815 | validation: 0.10914238735067748]
	TIME [epoch: 5.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035900919017553615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035900919017553615 | validation: 0.417184917093627]
	TIME [epoch: 5.62 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20278976868533805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20278976868533805 | validation: 0.07871057696559638]
	TIME [epoch: 5.59 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050973217487672556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050973217487672556 | validation: 0.20426141821565624]
	TIME [epoch: 5.61 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0501907536284666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0501907536284666 | validation: 0.050766155772509175]
	TIME [epoch: 5.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021261916298177672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021261916298177672 | validation: 0.06662085544175107]
	TIME [epoch: 5.61 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021034502748147825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021034502748147825 | validation: 0.06924266347882502]
	TIME [epoch: 5.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018725362775699025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018725362775699025 | validation: 0.04745044577342681]
	TIME [epoch: 5.59 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019571004337032197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019571004337032197 | validation: 0.09597233638293587]
	TIME [epoch: 5.59 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027510829261362037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027510829261362037 | validation: 0.09475537948459538]
	TIME [epoch: 5.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028121138996052987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028121138996052987 | validation: 0.11637453172656152]
	TIME [epoch: 5.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043204033807549155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043204033807549155 | validation: 0.12176877795381956]
	TIME [epoch: 5.59 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03573059278496052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03573059278496052 | validation: 0.05591672332813974]
	TIME [epoch: 5.58 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02057475302140377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02057475302140377 | validation: 0.050597568569332285]
	TIME [epoch: 5.58 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021700794989141695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021700794989141695 | validation: 0.08047780858217844]
	TIME [epoch: 5.58 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025975136466362597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025975136466362597 | validation: 0.08580107209318466]
	TIME [epoch: 5.59 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029853965256914163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029853965256914163 | validation: 0.06808665017808048]
	TIME [epoch: 5.58 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031062697477901847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031062697477901847 | validation: 0.049006422836207944]
	TIME [epoch: 5.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022517500764031997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022517500764031997 | validation: 0.13931211506912713]
	TIME [epoch: 5.57 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02197996798541245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02197996798541245 | validation: 0.06670970866063645]
	TIME [epoch: 5.59 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017575212488965368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017575212488965368 | validation: 0.04858107708993074]
	TIME [epoch: 5.56 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017938088712455672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017938088712455672 | validation: 0.041997182002703075]
	TIME [epoch: 5.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01547397646631861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01547397646631861 | validation: 0.037150098138704506]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016589132617922768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016589132617922768 | validation: 0.06028600554772689]
	TIME [epoch: 5.58 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028107312922240415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028107312922240415 | validation: 0.06198405564321059]
	TIME [epoch: 5.58 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885823300658715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03885823300658715 | validation: 0.08346995981228485]
	TIME [epoch: 5.57 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042589877267997144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042589877267997144 | validation: 0.05700858655736582]
	TIME [epoch: 5.59 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030745884300175587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030745884300175587 | validation: 0.07021634064449014]
	TIME [epoch: 5.58 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02553505734755979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02553505734755979 | validation: 0.051336338575515374]
	TIME [epoch: 5.58 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01517224645349734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01517224645349734 | validation: 0.06666061413603773]
	TIME [epoch: 5.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040502601945306305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040502601945306305 | validation: 0.07812680858536619]
	TIME [epoch: 5.58 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029551000415913995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029551000415913995 | validation: 0.07444412993846267]
	TIME [epoch: 5.58 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026557262663183985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026557262663183985 | validation: 0.06257786937242457]
	TIME [epoch: 5.59 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01979500889893219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01979500889893219 | validation: 0.09532150691044304]
	TIME [epoch: 5.58 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03659817115118231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03659817115118231 | validation: 0.058861766435354304]
	TIME [epoch: 5.59 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778626279776594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03778626279776594 | validation: 0.05683928911276647]
	TIME [epoch: 5.58 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019683667490298463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019683667490298463 | validation: 0.04952861941729954]
	TIME [epoch: 5.58 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011568153878376728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011568153878376728 | validation: 0.04200084182125977]
	TIME [epoch: 5.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016822809881332636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016822809881332636 | validation: 0.05613711020869149]
	TIME [epoch: 5.59 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02769616256886704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02769616256886704 | validation: 0.13567354922391006]
	TIME [epoch: 5.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06622183056767207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06622183056767207 | validation: 0.1302533087000393]
	TIME [epoch: 5.58 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062390538570621365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062390538570621365 | validation: 0.21503978412582594]
	TIME [epoch: 5.59 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11552674260863842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11552674260863842 | validation: 0.1975448420234177]
	TIME [epoch: 5.58 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807105517971775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807105517971775 | validation: 0.32271517403824584]
	TIME [epoch: 5.59 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11976388592729692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11976388592729692 | validation: 0.18868453678956168]
	TIME [epoch: 5.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049057266986933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049057266986933 | validation: 0.12304163943276232]
	TIME [epoch: 5.59 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378915003697503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0378915003697503 | validation: 0.08303450026321241]
	TIME [epoch: 5.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03232348668949652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03232348668949652 | validation: 0.06194137961065615]
	TIME [epoch: 5.58 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021169951898132686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021169951898132686 | validation: 0.155074200449876]
	TIME [epoch: 5.59 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021957313520806748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021957313520806748 | validation: 0.07314291464002913]
	TIME [epoch: 5.58 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01679972454535294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01679972454535294 | validation: 0.044127448041295064]
	TIME [epoch: 5.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014575730432895614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014575730432895614 | validation: 0.10662199880133644]
	TIME [epoch: 5.59 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026612578951004988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026612578951004988 | validation: 0.07936436784149958]
	TIME [epoch: 5.58 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02406742419306421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02406742419306421 | validation: 0.07913274954220013]
	TIME [epoch: 5.58 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035699581184354616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035699581184354616 | validation: 0.08597735107377982]
	TIME [epoch: 5.58 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827553790509466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06827553790509466 | validation: 0.0775782540370906]
	TIME [epoch: 5.59 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04077795342153657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04077795342153657 | validation: 0.07221316698384853]
	TIME [epoch: 5.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030708537580176467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030708537580176467 | validation: 0.05855680631001823]
	TIME [epoch: 5.61 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027013917415849412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027013917415849412 | validation: 0.047499333963801564]
	TIME [epoch: 5.59 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018897998814266907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018897998814266907 | validation: 0.03572166936386576]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01353507938076315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01353507938076315 | validation: 0.046442699486642536]
	TIME [epoch: 202 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00921697932362169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00921697932362169 | validation: 0.045159653093957225]
	TIME [epoch: 11.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012049539986301405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012049539986301405 | validation: 0.05783469365044034]
	TIME [epoch: 11.9 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011834742809307597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011834742809307597 | validation: 0.046207402737996464]
	TIME [epoch: 11.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015214749081690484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015214749081690484 | validation: 0.05208474796245506]
	TIME [epoch: 11.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211985542986024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0211985542986024 | validation: 0.05472985404157356]
	TIME [epoch: 11.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02919624296112058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02919624296112058 | validation: 0.045531443717993815]
	TIME [epoch: 11.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029105883344343306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029105883344343306 | validation: 0.05577150429880972]
	TIME [epoch: 11.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02172473900305773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02172473900305773 | validation: 0.1504183200032374]
	TIME [epoch: 11.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031459455506006664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031459455506006664 | validation: 0.08476336520717984]
	TIME [epoch: 11.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03291126313612113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03291126313612113 | validation: 0.0895582619409595]
	TIME [epoch: 11.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027626479397158177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027626479397158177 | validation: 0.06402969214941806]
	TIME [epoch: 11.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023067530413051422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023067530413051422 | validation: 0.2055113288955763]
	TIME [epoch: 11.9 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038282531016529425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038282531016529425 | validation: 0.09899049995524252]
	TIME [epoch: 11.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03997498862642149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03997498862642149 | validation: 0.0630873336525528]
	TIME [epoch: 11.9 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024051853481122522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024051853481122522 | validation: 0.03412197680208558]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1016.pth
	Model improved!!!
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021689592157355345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021689592157355345 | validation: 0.04170485407025622]
	TIME [epoch: 11.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015619857966065022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015619857966065022 | validation: 0.040442481098097144]
	TIME [epoch: 11.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02355513810281716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02355513810281716 | validation: 0.04591406338053841]
	TIME [epoch: 11.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024880765313248565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024880765313248565 | validation: 0.060226664850284196]
	TIME [epoch: 11.9 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025528165417517075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025528165417517075 | validation: 0.04112645314087204]
	TIME [epoch: 11.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015460550783484066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015460550783484066 | validation: 0.08067919490211665]
	TIME [epoch: 11.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02379577440804291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02379577440804291 | validation: 0.06810779523809334]
	TIME [epoch: 11.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169583955434569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06169583955434569 | validation: 0.08865303964346995]
	TIME [epoch: 11.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07379656737732895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07379656737732895 | validation: 0.06539406856159412]
	TIME [epoch: 11.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0258486753984226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0258486753984226 | validation: 0.07272376461248704]
	TIME [epoch: 11.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053202746972068955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053202746972068955 | validation: 0.05933507264445122]
	TIME [epoch: 11.9 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04145302886411034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04145302886411034 | validation: 0.11610608468811563]
	TIME [epoch: 11.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04982443645547633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04982443645547633 | validation: 0.10079582909316094]
	TIME [epoch: 11.9 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034988060484718045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034988060484718045 | validation: 0.08738606579975447]
	TIME [epoch: 11.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03814373141290745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03814373141290745 | validation: 0.04151650569019205]
	TIME [epoch: 11.9 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0225703122504953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0225703122504953 | validation: 0.048277901065665596]
	TIME [epoch: 11.9 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015121126399242327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015121126399242327 | validation: 0.07537845168856105]
	TIME [epoch: 11.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01892198000335047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01892198000335047 | validation: 0.03887116459856266]
	TIME [epoch: 11.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012885192343203946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012885192343203946 | validation: 0.06714245266932022]
	TIME [epoch: 11.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016118775408873545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016118775408873545 | validation: 0.040123612997668846]
	TIME [epoch: 11.9 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010485594859448208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010485594859448208 | validation: 0.03393026433642612]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1037.pth
	Model improved!!!
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008969416299331486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008969416299331486 | validation: 0.03854101523200783]
	TIME [epoch: 11.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009069866700140625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009069866700140625 | validation: 0.05412661825167961]
	TIME [epoch: 11.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01176947997561878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01176947997561878 | validation: 0.04310441292639222]
	TIME [epoch: 11.9 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01443398949043672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01443398949043672 | validation: 0.055188171563801125]
	TIME [epoch: 11.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024434358673763503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024434358673763503 | validation: 0.15696441883988732]
	TIME [epoch: 11.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213920677266568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05213920677266568 | validation: 0.051359122002837654]
	TIME [epoch: 11.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054021427124755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054021427124755 | validation: 0.06764064916959649]
	TIME [epoch: 11.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032462136601207774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032462136601207774 | validation: 0.03949435402398216]
	TIME [epoch: 11.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02145977314773654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02145977314773654 | validation: 0.1048417443813408]
	TIME [epoch: 11.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03403377189286001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03403377189286001 | validation: 0.09171946318397768]
	TIME [epoch: 11.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040208205707497474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040208205707497474 | validation: 0.0641059044966604]
	TIME [epoch: 11.9 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914667076142034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03914667076142034 | validation: 0.09203051285268049]
	TIME [epoch: 11.8 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05302602012407784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05302602012407784 | validation: 0.049484976834774166]
	TIME [epoch: 11.9 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030943626642105447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030943626642105447 | validation: 0.04750992995460285]
	TIME [epoch: 11.9 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03390660125832414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03390660125832414 | validation: 0.07881181022399236]
	TIME [epoch: 11.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047867083638392635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047867083638392635 | validation: 0.07569016600255152]
	TIME [epoch: 11.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046224569803951294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046224569803951294 | validation: 0.0961961309945615]
	TIME [epoch: 11.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281417917111649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04281417917111649 | validation: 0.1274070678008694]
	TIME [epoch: 11.9 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0357647942040863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0357647942040863 | validation: 0.051871165513647705]
	TIME [epoch: 11.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062967565591497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02062967565591497 | validation: 0.07747633103734275]
	TIME [epoch: 11.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02138506479346118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02138506479346118 | validation: 0.044877186434671215]
	TIME [epoch: 11.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01289377362568011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01289377362568011 | validation: 0.037098878247164235]
	TIME [epoch: 11.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008776460256008908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008776460256008908 | validation: 0.03701327474021308]
	TIME [epoch: 11.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010980550155446984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010980550155446984 | validation: 0.03467656500611776]
	TIME [epoch: 11.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00866633254722454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00866633254722454 | validation: 0.03450146269655335]
	TIME [epoch: 11.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075071600368459565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0075071600368459565 | validation: 0.03576914287634827]
	TIME [epoch: 11.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009546087682196504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009546087682196504 | validation: 0.032231313352910976]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011239253700512724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011239253700512724 | validation: 0.058876874939089066]
	TIME [epoch: 11.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029176478287116732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029176478287116732 | validation: 0.07187768099613394]
	TIME [epoch: 11.8 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059321703610562826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059321703610562826 | validation: 0.09166726798499543]
	TIME [epoch: 11.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641977021373129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05641977021373129 | validation: 0.06633833040555602]
	TIME [epoch: 11.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02321516724068035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02321516724068035 | validation: 0.05087248501990898]
	TIME [epoch: 11.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02661916466924834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02661916466924834 | validation: 0.04382835092676727]
	TIME [epoch: 11.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025485392710274495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025485392710274495 | validation: 0.036255781209923475]
	TIME [epoch: 11.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01305853717384047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01305853717384047 | validation: 0.03658177090831467]
	TIME [epoch: 11.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010950537974479117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010950537974479117 | validation: 0.0999881765053422]
	TIME [epoch: 11.9 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01825231540702322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01825231540702322 | validation: 0.08600999906857895]
	TIME [epoch: 11.9 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242335909759214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03242335909759214 | validation: 0.06969252854822787]
	TIME [epoch: 11.9 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024575378625522212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024575378625522212 | validation: 0.05967354356140381]
	TIME [epoch: 11.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024295434277756406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024295434277756406 | validation: 0.05609793241963146]
	TIME [epoch: 11.9 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02318311669721028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02318311669721028 | validation: 0.03659543069205914]
	TIME [epoch: 11.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015897203066019907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015897203066019907 | validation: 0.0419232793572582]
	TIME [epoch: 11.9 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013660175706797922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013660175706797922 | validation: 0.03852023269160894]
	TIME [epoch: 11.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015780701429010828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015780701429010828 | validation: 0.04369017946218509]
	TIME [epoch: 11.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018709257024965873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018709257024965873 | validation: 0.05250615112082141]
	TIME [epoch: 11.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025788048612803465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025788048612803465 | validation: 0.08533366464888677]
	TIME [epoch: 11.9 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035589808628296096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035589808628296096 | validation: 0.25534359188521605]
	TIME [epoch: 11.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0439626052845382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0439626052845382 | validation: 0.030572689668237674]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01904248061429545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01904248061429545 | validation: 0.04292788456292275]
	TIME [epoch: 11.9 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03511382988840271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03511382988840271 | validation: 0.1242564594190915]
	TIME [epoch: 11.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303295120985153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04303295120985153 | validation: 0.09537327960690746]
	TIME [epoch: 11.9 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583932912000989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05583932912000989 | validation: 0.29084939161675966]
	TIME [epoch: 11.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15944052434061687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15944052434061687 | validation: 0.1483863788095745]
	TIME [epoch: 11.8 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12454556219944596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12454556219944596 | validation: 0.7426877650210127]
	TIME [epoch: 11.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364902917056194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364902917056194 | validation: 0.6307588793824243]
	TIME [epoch: 11.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2025051748514207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2025051748514207 | validation: 0.17804433276309992]
	TIME [epoch: 11.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635403821765314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1635403821765314 | validation: 0.3529952954974575]
	TIME [epoch: 11.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11337575562406879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11337575562406879 | validation: 0.11391979753472509]
	TIME [epoch: 11.9 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0615769900478681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0615769900478681 | validation: 0.10202764043760504]
	TIME [epoch: 11.9 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731332687860935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04731332687860935 | validation: 0.07175924953962168]
	TIME [epoch: 11.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030181616083403782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030181616083403782 | validation: 0.06579551874211237]
	TIME [epoch: 11.9 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024110772787083744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024110772787083744 | validation: 0.0569818734203259]
	TIME [epoch: 11.9 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01672791379137458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01672791379137458 | validation: 0.044193308314206106]
	TIME [epoch: 11.9 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01575171278323845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01575171278323845 | validation: 0.037411952497033406]
	TIME [epoch: 11.9 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014249187819467532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014249187819467532 | validation: 0.042977793633697386]
	TIME [epoch: 11.9 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013512117647693197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013512117647693197 | validation: 0.03644037135910584]
	TIME [epoch: 11.9 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012863736146734948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012863736146734948 | validation: 0.03376802553101406]
	TIME [epoch: 11.9 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011184755649000297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011184755649000297 | validation: 0.03602912917438864]
	TIME [epoch: 11.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010942280136248721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010942280136248721 | validation: 0.04005926135139781]
	TIME [epoch: 11.9 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010739894856177082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010739894856177082 | validation: 0.03822629792451374]
	TIME [epoch: 11.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009870972825036444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009870972825036444 | validation: 0.036094919194712714]
	TIME [epoch: 11.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009748702157143019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009748702157143019 | validation: 0.034766206952555334]
	TIME [epoch: 11.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015638694183419993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015638694183419993 | validation: 0.06366013107479683]
	TIME [epoch: 11.9 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032177330151806266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032177330151806266 | validation: 0.04679727826774186]
	TIME [epoch: 11.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029430286158590883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029430286158590883 | validation: 0.04895625588728714]
	TIME [epoch: 11.9 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018544629040537348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018544629040537348 | validation: 0.033083808159313464]
	TIME [epoch: 11.9 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00922309241429123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00922309241429123 | validation: 0.027009074615560527]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008657205310472593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008657205310472593 | validation: 0.03422343136657196]
	TIME [epoch: 11.9 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01479500554172471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01479500554172471 | validation: 0.07693873441300916]
	TIME [epoch: 11.9 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029302845825120577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029302845825120577 | validation: 0.05177524997453489]
	TIME [epoch: 11.9 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029937088901888193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029937088901888193 | validation: 0.04819162706641022]
	TIME [epoch: 11.9 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013906472860079991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013906472860079991 | validation: 0.03588820627828123]
	TIME [epoch: 11.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011739961029534176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011739961029534176 | validation: 0.04250861018884299]
	TIME [epoch: 11.9 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017559853537493855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017559853537493855 | validation: 0.07209636559724161]
	TIME [epoch: 11.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029101060195708888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029101060195708888 | validation: 0.05380675496543128]
	TIME [epoch: 11.9 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032735399752499554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032735399752499554 | validation: 0.055720274314071894]
	TIME [epoch: 11.9 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03415542437197847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03415542437197847 | validation: 0.07276258256680038]
	TIME [epoch: 11.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030704258067879873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030704258067879873 | validation: 0.059459557477021176]
	TIME [epoch: 11.9 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0290992303641499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0290992303641499 | validation: 0.05040769570787104]
	TIME [epoch: 11.9 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02180400552786554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02180400552786554 | validation: 0.03864855316297509]
	TIME [epoch: 11.9 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018550856492406294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018550856492406294 | validation: 0.05950466049916956]
	TIME [epoch: 11.9 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02652072970485645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02652072970485645 | validation: 0.07871706150302613]
	TIME [epoch: 11.9 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913730075073299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03913730075073299 | validation: 0.04679011054997924]
	TIME [epoch: 11.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024755070648100928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024755070648100928 | validation: 0.05350384721001803]
	TIME [epoch: 11.9 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02157570012698093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02157570012698093 | validation: 0.07914639464087286]
	TIME [epoch: 11.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034871960363599686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034871960363599686 | validation: 0.11675404757615866]
	TIME [epoch: 11.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03226981116372478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03226981116372478 | validation: 0.07247845530132986]
	TIME [epoch: 11.9 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0480010839849266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0480010839849266 | validation: 0.07975827438047266]
	TIME [epoch: 11.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033333217157488354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033333217157488354 | validation: 0.04305127187275766]
	TIME [epoch: 11.9 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016062014720212354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016062014720212354 | validation: 0.10835495544007459]
	TIME [epoch: 11.9 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579759931811984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03579759931811984 | validation: 0.09764489305537763]
	TIME [epoch: 11.9 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039734375106840795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039734375106840795 | validation: 0.05995057792041469]
	TIME [epoch: 11.9 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03348970195969316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03348970195969316 | validation: 0.09143722954051249]
	TIME [epoch: 11.9 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025193873482024013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025193873482024013 | validation: 0.06534655088864556]
	TIME [epoch: 11.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026966333551126622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026966333551126622 | validation: 0.04426235850716518]
	TIME [epoch: 11.9 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02833001951140857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02833001951140857 | validation: 0.04272653523121646]
	TIME [epoch: 11.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02093866540816618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02093866540816618 | validation: 0.037646378419639914]
	TIME [epoch: 11.9 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014668593764953306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014668593764953306 | validation: 0.02775155130818864]
	TIME [epoch: 11.9 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008478591514219461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008478591514219461 | validation: 0.025577012595590577]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1146.pth
	Model improved!!!
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008956780119843518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008956780119843518 | validation: 0.03163818909490781]
	TIME [epoch: 11.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010534379634125268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010534379634125268 | validation: 0.07599405941862586]
	TIME [epoch: 11.8 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013712523128698926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013712523128698926 | validation: 0.029474007900688926]
	TIME [epoch: 11.9 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011296997274106043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011296997274106043 | validation: 0.05463113633500854]
	TIME [epoch: 11.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01462242763480629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01462242763480629 | validation: 0.038522816282297236]
	TIME [epoch: 11.9 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019400356849419355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019400356849419355 | validation: 0.04896773903414994]
	TIME [epoch: 11.8 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032513985984231546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032513985984231546 | validation: 0.050799288667443314]
	TIME [epoch: 11.9 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02866301553787731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02866301553787731 | validation: 0.06930313822163663]
	TIME [epoch: 11.8 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030753173343202246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030753173343202246 | validation: 0.039311537374519104]
	TIME [epoch: 11.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02248275273213864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02248275273213864 | validation: 0.04973527901400287]
	TIME [epoch: 11.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019835025553547773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019835025553547773 | validation: 0.041353147989288234]
	TIME [epoch: 11.8 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015344953470879228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015344953470879228 | validation: 0.1641300498611898]
	TIME [epoch: 11.9 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344110399517193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0344110399517193 | validation: 0.07686269442690846]
	TIME [epoch: 11.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943807050748677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03943807050748677 | validation: 0.06317713286010611]
	TIME [epoch: 11.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028535509467726247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028535509467726247 | validation: 0.08770096300874497]
	TIME [epoch: 11.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022396059483978395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022396059483978395 | validation: 0.04923734062675944]
	TIME [epoch: 11.8 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017762149335204026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017762149335204026 | validation: 0.04409324326436437]
	TIME [epoch: 11.8 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02374617390636267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02374617390636267 | validation: 0.06661307143219006]
	TIME [epoch: 11.8 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384310415782296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04384310415782296 | validation: 0.0716306471490877]
	TIME [epoch: 11.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03572949693474052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03572949693474052 | validation: 0.0661462554041297]
	TIME [epoch: 11.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02537491551809023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02537491551809023 | validation: 0.05338629925597132]
	TIME [epoch: 11.8 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024969982507199208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024969982507199208 | validation: 0.09471538050476422]
	TIME [epoch: 11.8 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047186084766445036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047186084766445036 | validation: 0.12043941535753154]
	TIME [epoch: 11.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033712350231911055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033712350231911055 | validation: 0.05421560434481973]
	TIME [epoch: 11.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400193811844764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400193811844764 | validation: 0.06283381063327494]
	TIME [epoch: 11.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046646322537133876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046646322537133876 | validation: 0.06930718531800548]
	TIME [epoch: 11.8 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03575826987172362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03575826987172362 | validation: 0.029912350208209093]
	TIME [epoch: 11.8 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015077140543082203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015077140543082203 | validation: 0.07271564828896399]
	TIME [epoch: 11.8 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018717920868317816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018717920868317816 | validation: 0.06056943144616465]
	TIME [epoch: 11.9 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022968087355707247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022968087355707247 | validation: 0.04633216227927215]
	TIME [epoch: 11.8 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02524600433430452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02524600433430452 | validation: 0.04267511691067262]
	TIME [epoch: 11.8 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018391303798862398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018391303798862398 | validation: 0.029751743606893158]
	TIME [epoch: 11.8 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010089236558388495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010089236558388495 | validation: 0.024862156372503264]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1179.pth
	Model improved!!!
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007447089375542655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007447089375542655 | validation: 0.033888425852138596]
	TIME [epoch: 11.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008939860159767925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008939860159767925 | validation: 0.022605128687284582]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1181.pth
	Model improved!!!
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009301470852013844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009301470852013844 | validation: 0.033027980908361856]
	TIME [epoch: 11.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008689825502926933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008689825502926933 | validation: 0.04446882582619976]
	TIME [epoch: 11.9 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015277452413660684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015277452413660684 | validation: 0.11298319609479371]
	TIME [epoch: 11.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439606133509186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04439606133509186 | validation: 0.07926761950601642]
	TIME [epoch: 11.9 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045736598712723946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045736598712723946 | validation: 0.03377034813270446]
	TIME [epoch: 11.8 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013860467133092996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013860467133092996 | validation: 0.04186666581558828]
	TIME [epoch: 11.9 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014523759031174508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014523759031174508 | validation: 0.04096539242223292]
	TIME [epoch: 11.8 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010535900694766245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010535900694766245 | validation: 0.035422069237058464]
	TIME [epoch: 11.9 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009972191046005953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009972191046005953 | validation: 0.03176182730927858]
	TIME [epoch: 11.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01688248293989227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01688248293989227 | validation: 0.07290800891366309]
	TIME [epoch: 11.9 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626863029197051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626863029197051 | validation: 0.07608865564290823]
	TIME [epoch: 11.8 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06308040161632002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06308040161632002 | validation: 0.04765600347660187]
	TIME [epoch: 11.9 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02654949532894186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02654949532894186 | validation: 0.057164416540894825]
	TIME [epoch: 11.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04659334694534754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04659334694534754 | validation: 0.044346767680980226]
	TIME [epoch: 11.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020270280017794927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020270280017794927 | validation: 0.03304454218020336]
	TIME [epoch: 11.9 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009664672190410718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009664672190410718 | validation: 0.04496401502586786]
	TIME [epoch: 11.9 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011587270439818158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011587270439818158 | validation: 0.043085069711707016]
	TIME [epoch: 11.9 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013243411379493437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013243411379493437 | validation: 0.04677067939850582]
	TIME [epoch: 11.9 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014215559432455354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014215559432455354 | validation: 0.03454653781272057]
	TIME [epoch: 11.9 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020530970226610563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020530970226610563 | validation: 0.04219495379882557]
	TIME [epoch: 11.9 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01828621044945854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01828621044945854 | validation: 0.05001302322075053]
	TIME [epoch: 11.9 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011715676711138566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011715676711138566 | validation: 0.03626684845297281]
	TIME [epoch: 12 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009015775438703348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009015775438703348 | validation: 0.03178768553065763]
	TIME [epoch: 11.9 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012174024951443577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012174024951443577 | validation: 0.046700934791358596]
	TIME [epoch: 12 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039513542614831774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039513542614831774 | validation: 0.05977489543013518]
	TIME [epoch: 11.8 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04758358983271358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04758358983271358 | validation: 0.07253511227710041]
	TIME [epoch: 11.9 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028971975951381887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028971975951381887 | validation: 0.06141088315300017]
	TIME [epoch: 11.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02818204613651197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02818204613651197 | validation: 0.06170715464276025]
	TIME [epoch: 11.9 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024216795235401715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024216795235401715 | validation: 0.0360492756836871]
	TIME [epoch: 11.8 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017808525451125972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017808525451125972 | validation: 0.0456289649255821]
	TIME [epoch: 11.9 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02271077557296249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02271077557296249 | validation: 0.08502110476460444]
	TIME [epoch: 11.9 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029921477682105426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029921477682105426 | validation: 0.03907871609154265]
	TIME [epoch: 11.9 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015499557641610739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015499557641610739 | validation: 0.045830096545634635]
	TIME [epoch: 11.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0165174943135812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0165174943135812 | validation: 0.05307795664346156]
	TIME [epoch: 11.9 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017225537731362543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017225537731362543 | validation: 0.05771972428918293]
	TIME [epoch: 11.9 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023645249739457596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023645249739457596 | validation: 0.06134445973428129]
	TIME [epoch: 11.9 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473027795215399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03473027795215399 | validation: 0.06361405426092835]
	TIME [epoch: 11.9 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033102771125724725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033102771125724725 | validation: 0.06841494899178717]
	TIME [epoch: 11.9 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025262993683803865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025262993683803865 | validation: 0.060309702764833543]
	TIME [epoch: 12 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410949728811819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03410949728811819 | validation: 0.04711564261410588]
	TIME [epoch: 11.9 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01997790073326625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01997790073326625 | validation: 0.035491619942809104]
	TIME [epoch: 11.9 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00793363161099187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00793363161099187 | validation: 0.040594012225028425]
	TIME [epoch: 11.9 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014100635824456398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014100635824456398 | validation: 0.04595944625006615]
	TIME [epoch: 11.9 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015701504389912625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015701504389912625 | validation: 0.03868783161605274]
	TIME [epoch: 11.9 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020143901387567754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020143901387567754 | validation: 0.04542052927401235]
	TIME [epoch: 11.9 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022214080929639453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022214080929639453 | validation: 0.048838158231739086]
	TIME [epoch: 11.9 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022804250017323495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022804250017323495 | validation: 0.03831884793742968]
	TIME [epoch: 11.8 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02806137238751735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02806137238751735 | validation: 0.05133327808349906]
	TIME [epoch: 11.9 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026677207517068015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026677207517068015 | validation: 0.05830715618480747]
	TIME [epoch: 11.9 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01587056293916348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01587056293916348 | validation: 0.039399437731723976]
	TIME [epoch: 11.9 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008970054957639544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008970054957639544 | validation: 0.034863834457468344]
	TIME [epoch: 11.9 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009796233268178115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009796233268178115 | validation: 0.03457085132320566]
	TIME [epoch: 11.9 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010075747893092136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010075747893092136 | validation: 0.055276291930814694]
	TIME [epoch: 11.9 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025448121618091776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025448121618091776 | validation: 0.32864696214314143]
	TIME [epoch: 11.9 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684385580389131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684385580389131 | validation: 0.10053719213213196]
	TIME [epoch: 11.9 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040486360666668254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040486360666668254 | validation: 0.16360291735739843]
	TIME [epoch: 11.9 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420022816033361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420022816033361 | validation: 0.05630063361014871]
	TIME [epoch: 11.9 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050029280026411986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050029280026411986 | validation: 0.03486778650522713]
	TIME [epoch: 11.9 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027402527161350467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027402527161350467 | validation: 0.042090912490112]
	TIME [epoch: 11.9 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018178151201136965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018178151201136965 | validation: 0.045212718680365166]
	TIME [epoch: 11.9 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0161058457547322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0161058457547322 | validation: 0.034109230261408545]
	TIME [epoch: 11.9 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013713678253574342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013713678253574342 | validation: 0.020525101071467967]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1243.pth
	Model improved!!!
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007039872335558397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007039872335558397 | validation: 0.046540910521672356]
	TIME [epoch: 11.9 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009313069042348568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009313069042348568 | validation: 0.03330085060248161]
	TIME [epoch: 11.9 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012133237343216209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012133237343216209 | validation: 0.033304427235720795]
	TIME [epoch: 11.9 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014412445996582661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014412445996582661 | validation: 0.033436576085337504]
	TIME [epoch: 11.9 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01676664431628176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01676664431628176 | validation: 0.04612258810176334]
	TIME [epoch: 11.9 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02058002573063262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02058002573063262 | validation: 0.04054831242946741]
	TIME [epoch: 11.9 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022361454331588107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022361454331588107 | validation: 0.0313117678113465]
	TIME [epoch: 11.9 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020491930511307564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020491930511307564 | validation: 0.029883227810391235]
	TIME [epoch: 11.9 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009875893296668476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009875893296668476 | validation: 0.03281749963529168]
	TIME [epoch: 11.9 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008136690218324347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008136690218324347 | validation: 0.028498412278615316]
	TIME [epoch: 11.9 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00932505008958785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00932505008958785 | validation: 0.034411870013722146]
	TIME [epoch: 12 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016276965588840252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016276965588840252 | validation: 0.050596002818501976]
	TIME [epoch: 11.9 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02688625970487636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02688625970487636 | validation: 0.04922352166373706]
	TIME [epoch: 12 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030596709059774643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030596709059774643 | validation: 0.04310927779641071]
	TIME [epoch: 11.9 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02362341916555667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02362341916555667 | validation: 0.06520760313228352]
	TIME [epoch: 12 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02934986761121567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02934986761121567 | validation: 0.03927837509139839]
	TIME [epoch: 11.9 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016002431776209542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016002431776209542 | validation: 0.04084676354796893]
	TIME [epoch: 12 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015591161499442636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015591161499442636 | validation: 0.08441272690464709]
	TIME [epoch: 11.9 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026891500405275163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026891500405275163 | validation: 0.16678432973183407]
	TIME [epoch: 12 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0613830136450575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0613830136450575 | validation: 0.12461367243016414]
	TIME [epoch: 11.9 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901442698694146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04901442698694146 | validation: 0.09001537217479025]
	TIME [epoch: 11.9 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05305180371150499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05305180371150499 | validation: 0.16582464183738566]
	TIME [epoch: 11.9 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421514984167875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421514984167875 | validation: 0.06188059390876499]
	TIME [epoch: 11.9 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04457526936474987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04457526936474987 | validation: 0.08180133149194897]
	TIME [epoch: 11.9 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04393088446887617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04393088446887617 | validation: 0.1199020494450259]
	TIME [epoch: 11.9 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564721518498784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04564721518498784 | validation: 0.04751635681361369]
	TIME [epoch: 11.9 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012604026292919544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012604026292919544 | validation: 0.05310843025745324]
	TIME [epoch: 11.9 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01838293729727188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01838293729727188 | validation: 0.048233996029596896]
	TIME [epoch: 12 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011710514600601594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011710514600601594 | validation: 0.061886140157897146]
	TIME [epoch: 11.9 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013232703956582298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013232703956582298 | validation: 0.028382077277076558]
	TIME [epoch: 12 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00983111790918354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00983111790918354 | validation: 0.03528706163266842]
	TIME [epoch: 11.9 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010552938303408019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010552938303408019 | validation: 0.035402870592591874]
	TIME [epoch: 11.9 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010159025248295579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010159025248295579 | validation: 0.036652060390739885]
	TIME [epoch: 11.9 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013804958853619898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013804958853619898 | validation: 0.03754573224758693]
	TIME [epoch: 11.9 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020500955649434847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020500955649434847 | validation: 0.061492250573358956]
	TIME [epoch: 11.9 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027238191844613242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027238191844613242 | validation: 0.04298865218953338]
	TIME [epoch: 12 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022843885803701204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022843885803701204 | validation: 0.03934825340943603]
	TIME [epoch: 11.9 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011286831148830605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011286831148830605 | validation: 0.0395948339789256]
	TIME [epoch: 11.9 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00620420905137149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00620420905137149 | validation: 0.032085432432304974]
	TIME [epoch: 11.9 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007201978675114415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007201978675114415 | validation: 0.02948440527796784]
	TIME [epoch: 11.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008021322339854108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008021322339854108 | validation: 0.029466824713993523]
	TIME [epoch: 11.9 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009736027497945752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009736027497945752 | validation: 0.03222103041808068]
	TIME [epoch: 11.9 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011894771053411982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011894771053411982 | validation: 0.03402318056673953]
	TIME [epoch: 11.8 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019465714665613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019465714665613 | validation: 0.07190349086325316]
	TIME [epoch: 11.8 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041429240427193995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041429240427193995 | validation: 0.08855459504811175]
	TIME [epoch: 11.8 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049301383928009666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049301383928009666 | validation: 0.0954280683088764]
	TIME [epoch: 11.8 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053346042088299836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053346042088299836 | validation: 0.12900522930769653]
	TIME [epoch: 11.8 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454505899715811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454505899715811 | validation: 0.6712613651526262]
	TIME [epoch: 11.8 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780543315542318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780543315542318 | validation: 0.36809930606698926]
	TIME [epoch: 11.8 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08463583060748396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08463583060748396 | validation: 0.1690619933432452]
	TIME [epoch: 11.9 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057594774903779254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057594774903779254 | validation: 0.08457930274614234]
	TIME [epoch: 11.9 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04209613563033855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04209613563033855 | validation: 0.04583930660278362]
	TIME [epoch: 11.9 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03328461541903217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03328461541903217 | validation: 0.03175948070336187]
	TIME [epoch: 11.9 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018170187291692456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018170187291692456 | validation: 0.02845037324365778]
	TIME [epoch: 11.9 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012796681857450422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012796681857450422 | validation: 0.0703159231794733]
	TIME [epoch: 11.9 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017595436018023883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017595436018023883 | validation: 0.13897897991104016]
	TIME [epoch: 11.9 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672974668719964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672974668719964 | validation: 0.053861450288764025]
	TIME [epoch: 11.9 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022305889216324908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022305889216324908 | validation: 0.03800131139280558]
	TIME [epoch: 11.9 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012001890569422025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012001890569422025 | validation: 0.026425504326171023]
	TIME [epoch: 12 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010505905053697891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010505905053697891 | validation: 0.027431429787502782]
	TIME [epoch: 11.9 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007651298651708185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007651298651708185 | validation: 0.0208118587854806]
	TIME [epoch: 11.9 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010248985871915885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010248985871915885 | validation: 0.026242863919470106]
	TIME [epoch: 11.9 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011411436615864445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011411436615864445 | validation: 0.03372702492044862]
	TIME [epoch: 11.9 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01737396078955774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01737396078955774 | validation: 0.27597325807004697]
	TIME [epoch: 11.8 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811070851167419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07811070851167419 | validation: 0.14392020391309296]
	TIME [epoch: 11.9 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050801287449144875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050801287449144875 | validation: 0.17412077705154203]
	TIME [epoch: 11.9 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053670426353771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053670426353771 | validation: 0.02463944776348566]
	TIME [epoch: 11.9 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009387218415811423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009387218415811423 | validation: 0.07777688464237971]
	TIME [epoch: 11.9 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019447938097129497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019447938097129497 | validation: 0.04443403076375333]
	TIME [epoch: 11.9 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022834165935577796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022834165935577796 | validation: 0.061211191060092145]
	TIME [epoch: 11.9 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022510104460658615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022510104460658615 | validation: 0.047690655099654415]
	TIME [epoch: 11.9 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010436390827366328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010436390827366328 | validation: 0.04728082970998787]
	TIME [epoch: 12 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009986262452562749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009986262452562749 | validation: 0.033765541449981944]
	TIME [epoch: 12 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009140699274384233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009140699274384233 | validation: 0.02831369015339833]
	TIME [epoch: 11.9 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01024185058257488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01024185058257488 | validation: 0.04258873122690818]
	TIME [epoch: 11.9 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015224886230830535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015224886230830535 | validation: 0.04947841649236909]
	TIME [epoch: 12 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02591191566960201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02591191566960201 | validation: 0.04615819062628723]
	TIME [epoch: 11.9 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03027665424389544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03027665424389544 | validation: 0.04309602302363416]
	TIME [epoch: 11.9 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027707000615159006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027707000615159006 | validation: 0.03368005476331853]
	TIME [epoch: 11.8 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012208604275371548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012208604275371548 | validation: 0.023717793485008257]
	TIME [epoch: 11.8 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006134335747509568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006134335747509568 | validation: 0.02611463668718085]
	TIME [epoch: 11.8 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009861057177346142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009861057177346142 | validation: 0.04430378090737102]
	TIME [epoch: 11.8 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024910202847536117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024910202847536117 | validation: 0.036589749514041016]
	TIME [epoch: 11.8 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030652394606139426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030652394606139426 | validation: 0.02943795980205214]
	TIME [epoch: 11.8 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015669006613301433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015669006613301433 | validation: 0.074841021447568]
	TIME [epoch: 11.9 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02261951516459992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02261951516459992 | validation: 0.11707318563614165]
	TIME [epoch: 11.8 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02936016365435485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02936016365435485 | validation: 0.05750584018952562]
	TIME [epoch: 11.8 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029692511187555785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029692511187555785 | validation: 0.05693322736397697]
	TIME [epoch: 11.8 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03105488482203081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03105488482203081 | validation: 0.03623751593280065]
	TIME [epoch: 11.9 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02797422442173927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02797422442173927 | validation: 0.04816698099954794]
	TIME [epoch: 11.9 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030153557844340866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030153557844340866 | validation: 0.046694302796008405]
	TIME [epoch: 11.9 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01808952835835567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01808952835835567 | validation: 0.03155587041445843]
	TIME [epoch: 11.9 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011030615301783838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011030615301783838 | validation: 0.027438418149174082]
	TIME [epoch: 11.9 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00931335774092864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00931335774092864 | validation: 0.02512671280027844]
	TIME [epoch: 11.9 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03690951299132668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03690951299132668 | validation: 0.049981286408018015]
	TIME [epoch: 11.9 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015075677604678204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015075677604678204 | validation: 0.0340433539443187]
	TIME [epoch: 11.9 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020537795261184825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020537795261184825 | validation: 0.021825096641231112]
	TIME [epoch: 11.9 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01091938357982306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01091938357982306 | validation: 0.023145157932626816]
	TIME [epoch: 11.9 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009694731668568124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009694731668568124 | validation: 0.05289572972326789]
	TIME [epoch: 11.9 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01147868721383374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01147868721383374 | validation: 0.03449521525869694]
	TIME [epoch: 11.9 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012472005942629535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012472005942629535 | validation: 0.042209138524416205]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_203100/states/model_phi1_4b_v_mmd2_1344.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8780.711 seconds.
