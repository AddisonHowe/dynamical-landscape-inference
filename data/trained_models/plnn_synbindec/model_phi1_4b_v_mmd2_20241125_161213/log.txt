Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1309081427

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.389060900301717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.389060900301717 | validation: 5.078328462848285]
	TIME [epoch: 182 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.948244996094572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.948244996094572 | validation: 5.991144971616464]
	TIME [epoch: 1.34 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.066512389917175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.066512389917175 | validation: 5.374005696662647]
	TIME [epoch: 1.31 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.457707081035151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.457707081035151 | validation: 5.653560293866036]
	TIME [epoch: 1.31 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.859843360741775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.859843360741775 | validation: 4.5173140050890055]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.97096394166575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.97096394166575 | validation: 4.781863914524295]
	TIME [epoch: 1.31 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.824104764982808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.824104764982808 | validation: 4.350840088443519]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.435740307648709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435740307648709 | validation: 4.993900294942591]
	TIME [epoch: 1.31 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.850926461538344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.850926461538344 | validation: 4.2173322424510715]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.352143901074554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.352143901074554 | validation: 4.353099938381603]
	TIME [epoch: 1.3 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.373069946914478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.373069946914478 | validation: 4.11979908655991]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.233059254051552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.233059254051552 | validation: 4.2378019198361905]
	TIME [epoch: 1.31 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.168178238010301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.168178238010301 | validation: 4.225209595377229]
	TIME [epoch: 1.31 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.124515787816836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124515787816836 | validation: 4.090436144336635]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.077548871688308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.077548871688308 | validation: 4.145682756318053]
	TIME [epoch: 1.31 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.043376641534479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.043376641534479 | validation: 4.0781043588809665]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.023182207382818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.023182207382818 | validation: 4.204785061020754]
	TIME [epoch: 1.31 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0297058159529895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0297058159529895 | validation: 4.068691286594133]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.091348131886248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.091348131886248 | validation: 4.092374385415617]
	TIME [epoch: 1.31 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.966263401763291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.966263401763291 | validation: 4.021272863230765]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9151027664641016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9151027664641016 | validation: 4.032103956380198]
	TIME [epoch: 1.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8920763217953107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8920763217953107 | validation: 3.950357560117088]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8773312361461367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8773312361461367 | validation: 4.018466834854939]
	TIME [epoch: 1.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.866080532328964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.866080532328964 | validation: 3.9449612176848987]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.904699614186004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.904699614186004 | validation: 4.068162912945215]
	TIME [epoch: 1.31 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.920605358149145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.920605358149145 | validation: 3.912454616669478]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9160989945091593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9160989945091593 | validation: 3.8871231084208873]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.779688665812209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.779688665812209 | validation: 3.871730571139423]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7528035401252793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7528035401252793 | validation: 3.805051889930824]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.746050788043762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.746050788043762 | validation: 3.8612219657690368]
	TIME [epoch: 1.31 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.746360630445318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.746360630445318 | validation: 3.8259687086067182]
	TIME [epoch: 1.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7620392281715977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7620392281715977 | validation: 3.886146842432829]
	TIME [epoch: 1.31 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.766577156097968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.766577156097968 | validation: 3.784135541937724]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7456507369998917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7456507369998917 | validation: 3.764033024956505]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.668438032094266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.668438032094266 | validation: 3.7093245714198035]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.630600624606168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.630600624606168 | validation: 3.6922916420303835]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.606621065390133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.606621065390133 | validation: 3.662631025901471]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.590276129611307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.590276129611307 | validation: 3.6618755277242516]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.578266955265399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.578266955265399 | validation: 3.669336942349211]
	TIME [epoch: 1.31 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5872503001247567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5872503001247567 | validation: 3.7763993879605042]
	TIME [epoch: 1.31 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6858257831019205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6858257831019205 | validation: 3.8236722760381205]
	TIME [epoch: 1.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7429535104354494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7429535104354494 | validation: 3.608968055197125]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5614901908445673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5614901908445673 | validation: 3.5913988859116746]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5147647654865137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5147647654865137 | validation: 3.674711075543431]
	TIME [epoch: 1.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.561002800416073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.561002800416073 | validation: 3.59791306496601]
	TIME [epoch: 1.31 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.527225398892343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527225398892343 | validation: 3.534132231349007]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4786694067102566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4786694067102566 | validation: 3.529821045739128]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4362931455893757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4362931455893757 | validation: 3.5042676829049784]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4326979757673985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4326979757673985 | validation: 3.4948363932188147]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4201641295110403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4201641295110403 | validation: 3.4777849366252824]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4183968986657907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4183968986657907 | validation: 3.523490612977782]
	TIME [epoch: 1.31 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4344071876394175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4344071876394175 | validation: 3.48380411304162]
	TIME [epoch: 1.31 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4323987938604574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4323987938604574 | validation: 3.5023708489783902]
	TIME [epoch: 1.31 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.412270805819828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.412270805819828 | validation: 3.4018557611208196]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3520961934886375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3520961934886375 | validation: 3.396980149577378]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3092331296181916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3092331296181916 | validation: 3.356914478117338]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2864922388386946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2864922388386946 | validation: 3.3133526854659667]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2410705370643083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2410705370643083 | validation: 3.2425907114793135]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.149282687164892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.149282687164892 | validation: 3.065337128698434]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9942931533973143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9942931533973143 | validation: 3.0324103388798167]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8884949431072364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8884949431072364 | validation: 2.656866257345001]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.664103795812033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.664103795812033 | validation: 2.153427403476969]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0094601737582294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0094601737582294 | validation: 1.8678756322450825]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6655972391488518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6655972391488518 | validation: 1.2960677749393998]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375356735309214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1375356735309214 | validation: 1.0991563623211285]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0162740817322446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0162740817322446 | validation: 1.34708669485572]
	TIME [epoch: 1.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.406281137682426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.406281137682426 | validation: 2.000528598406332]
	TIME [epoch: 1.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.884472414161771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.884472414161771 | validation: 1.094817736734493]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0154237837093236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0154237837093236 | validation: 0.9982618269923877]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702023432703284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9702023432703284 | validation: 0.9681716125914686]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658594562593857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8658594562593857 | validation: 0.9207831992566441]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035955424827412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8035955424827412 | validation: 0.8626761739054996]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103550059783836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103550059783836 | validation: 0.8478909615803387]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782004309998781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782004309998781 | validation: 0.8887922599472735]
	TIME [epoch: 1.31 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930837572653207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7930837572653207 | validation: 0.8415173038174457]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916581376398145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7916581376398145 | validation: 0.8771403467025061]
	TIME [epoch: 1.31 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834299946807045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834299946807045 | validation: 0.8391097106169249]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739537871804976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739537871804976 | validation: 0.8470227629597947]
	TIME [epoch: 1.31 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680541055137314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7680541055137314 | validation: 0.8330498292711624]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608434519349152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7608434519349152 | validation: 0.8575915591360771]
	TIME [epoch: 1.31 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691589849770218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691589849770218 | validation: 0.815401817484394]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749331635519763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7749331635519763 | validation: 0.9001304852434505]
	TIME [epoch: 1.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775263894664744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775263894664744 | validation: 0.863641941622001]
	TIME [epoch: 1.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152662189283943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152662189283943 | validation: 1.1018437249271704]
	TIME [epoch: 1.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808001031214255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808001031214255 | validation: 0.9713367676897621]
	TIME [epoch: 1.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9690961956544157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9690961956544157 | validation: 1.093919923198962]
	TIME [epoch: 1.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848430081159961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8848430081159961 | validation: 0.8378518033449401]
	TIME [epoch: 1.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814807029456401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814807029456401 | validation: 0.8451536132385272]
	TIME [epoch: 1.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618694907598397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618694907598397 | validation: 0.8991961936233385]
	TIME [epoch: 1.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715500644507873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7715500644507873 | validation: 0.807135629872517]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643085605321247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643085605321247 | validation: 0.8949321487951627]
	TIME [epoch: 1.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557392315224373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557392315224373 | validation: 0.8275839285379769]
	TIME [epoch: 1.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428319339581717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7428319339581717 | validation: 0.8648975894673119]
	TIME [epoch: 1.31 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466666655953901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466666655953901 | validation: 0.8626421658954986]
	TIME [epoch: 1.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597826335694494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597826335694494 | validation: 0.8932388371156789]
	TIME [epoch: 1.31 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097825627604567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097825627604567 | validation: 0.955083281354737]
	TIME [epoch: 1.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620361389135742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8620361389135742 | validation: 0.8760928902637151]
	TIME [epoch: 1.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002956247349211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002956247349211 | validation: 0.8788023276460717]
	TIME [epoch: 1.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499941476289729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499941476289729 | validation: 0.8244262674762137]
	TIME [epoch: 1.31 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270537784270913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270537784270913 | validation: 0.8717865529788735]
	TIME [epoch: 1.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729112536170807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729112536170807 | validation: 0.8141948617398307]
	TIME [epoch: 1.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305753648592096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305753648592096 | validation: 0.9535181658883259]
	TIME [epoch: 1.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7512053866594028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7512053866594028 | validation: 1.0232897037243898]
	TIME [epoch: 1.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024569057491907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.024569057491907 | validation: 1.6970410933590003]
	TIME [epoch: 1.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.263846247369115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.263846247369115 | validation: 0.9201837723820552]
	TIME [epoch: 1.31 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192456418183534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192456418183534 | validation: 0.9018905773857873]
	TIME [epoch: 1.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897161099456408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.897161099456408 | validation: 1.0421131183907226]
	TIME [epoch: 1.31 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087928592232292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087928592232292 | validation: 0.9151577966522055]
	TIME [epoch: 1.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643959780181203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643959780181203 | validation: 0.7948763590754049]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756317999913727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756317999913727 | validation: 0.8640299587579847]
	TIME [epoch: 1.31 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389105150719871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7389105150719871 | validation: 0.8378101034255471]
	TIME [epoch: 1.31 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735635579375152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735635579375152 | validation: 0.7910667773154674]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417910568650179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417910568650179 | validation: 0.8240291422288302]
	TIME [epoch: 1.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270858363725751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270858363725751 | validation: 0.8309955466376842]
	TIME [epoch: 1.31 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203321037743879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203321037743879 | validation: 0.7960075967948131]
	TIME [epoch: 1.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158125155253253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7158125155253253 | validation: 0.8949195991050193]
	TIME [epoch: 1.31 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334915634572678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334915634572678 | validation: 0.8289087476383892]
	TIME [epoch: 1.31 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893668414318321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893668414318321 | validation: 1.274729640683106]
	TIME [epoch: 1.31 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8895452055827207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8895452055827207 | validation: 0.8806250586395479]
	TIME [epoch: 1.31 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761182317358371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761182317358371 | validation: 1.0683332829463037]
	TIME [epoch: 1.31 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973823031303002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973823031303002 | validation: 0.8316288297249979]
	TIME [epoch: 1.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587050134451556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7587050134451556 | validation: 0.8674863267413159]
	TIME [epoch: 1.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537650074450721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537650074450721 | validation: 0.8668795012759294]
	TIME [epoch: 1.31 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7300242564210736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7300242564210736 | validation: 0.8078458519845446]
	TIME [epoch: 1.31 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157398691837652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7157398691837652 | validation: 0.8984796793658859]
	TIME [epoch: 1.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202051727818133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7202051727818133 | validation: 0.7853090920564121]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203367697783144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203367697783144 | validation: 0.9475423012037387]
	TIME [epoch: 1.31 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729316841139591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729316841139591 | validation: 0.8221522921396889]
	TIME [epoch: 1.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182033311121303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182033311121303 | validation: 1.3579079828006235]
	TIME [epoch: 1.31 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9815632973227745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9815632973227745 | validation: 0.8732506426082005]
	TIME [epoch: 1.31 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804209961471949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804209961471949 | validation: 0.8712127501111468]
	TIME [epoch: 1.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852463459876142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852463459876142 | validation: 0.9987107344424274]
	TIME [epoch: 1.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820854344582793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7820854344582793 | validation: 0.8000809785952971]
	TIME [epoch: 1.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7029369675275531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7029369675275531 | validation: 0.800005559689579]
	TIME [epoch: 1.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71023259596908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71023259596908 | validation: 0.8714961417481955]
	TIME [epoch: 1.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228960923032074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228960923032074 | validation: 0.7818877366790502]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196520225395316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196520225395316 | validation: 0.8425237090550888]
	TIME [epoch: 1.31 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352292446528855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352292446528855 | validation: 0.9790738517730437]
	TIME [epoch: 1.31 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930006720683703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7930006720683703 | validation: 0.8947898562141359]
	TIME [epoch: 1.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82449176702237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82449176702237 | validation: 1.098832671142133]
	TIME [epoch: 1.31 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774813950466305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774813950466305 | validation: 0.7871460962511789]
	TIME [epoch: 1.31 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593228195281555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593228195281555 | validation: 1.0437053273885617]
	TIME [epoch: 1.31 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985807872227689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985807872227689 | validation: 0.7778542614266966]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7206549417860325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7206549417860325 | validation: 0.8107444674937329]
	TIME [epoch: 1.31 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952746141612312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6952746141612312 | validation: 0.7963171404283642]
	TIME [epoch: 1.31 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889488656969257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889488656969257 | validation: 0.7998993138161512]
	TIME [epoch: 1.31 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812467302493178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812467302493178 | validation: 0.8377021026390683]
	TIME [epoch: 1.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822068846925851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822068846925851 | validation: 0.7750277573935929]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102488401185585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7102488401185585 | validation: 1.314483252182286]
	TIME [epoch: 1.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862469626312159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862469626312159 | validation: 1.0471108250443821]
	TIME [epoch: 1.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121010716642905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.121010716642905 | validation: 1.085184909192251]
	TIME [epoch: 1.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283534211423569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283534211423569 | validation: 1.0623107934176932]
	TIME [epoch: 1.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651240614295238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651240614295238 | validation: 0.7790060595391473]
	TIME [epoch: 1.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628585384588239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7628585384588239 | validation: 0.9117976087606888]
	TIME [epoch: 1.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140206263577141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140206263577141 | validation: 0.8381138793446079]
	TIME [epoch: 1.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993446109392653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993446109392653 | validation: 0.7717166781118275]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193951740191836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193951740191836 | validation: 0.8872808576865211]
	TIME [epoch: 1.31 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699403996380215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699403996380215 | validation: 0.7388862110178084]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885955458804196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6885955458804196 | validation: 0.8200830829743613]
	TIME [epoch: 1.31 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838937489084623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838937489084623 | validation: 0.7646497256510587]
	TIME [epoch: 1.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675783054321113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675783054321113 | validation: 0.7706536507368733]
	TIME [epoch: 1.31 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569908717318107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569908717318107 | validation: 0.8217480232464395]
	TIME [epoch: 1.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786484463981103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6786484463981103 | validation: 0.7800374573339787]
	TIME [epoch: 1.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952866141513337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7952866141513337 | validation: 1.0626842092723863]
	TIME [epoch: 1.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191450595383671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9191450595383671 | validation: 1.029961057442609]
	TIME [epoch: 1.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549459664673167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549459664673167 | validation: 0.940389398525428]
	TIME [epoch: 1.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014967093023218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.014967093023218 | validation: 1.2284890561652713]
	TIME [epoch: 1.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099112367742157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099112367742157 | validation: 0.8163841150993048]
	TIME [epoch: 1.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854641181818059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854641181818059 | validation: 0.7626980853764336]
	TIME [epoch: 1.31 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955237162448301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955237162448301 | validation: 0.8196126963889269]
	TIME [epoch: 1.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668261208538939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668261208538939 | validation: 0.7532730595154596]
	TIME [epoch: 1.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580393379097249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580393379097249 | validation: 0.7295035093945286]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525731220575588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525731220575588 | validation: 0.8116165716075504]
	TIME [epoch: 1.31 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623659182727732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6623659182727732 | validation: 0.7444520601731984]
	TIME [epoch: 1.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7139046044742736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7139046044742736 | validation: 1.1613543293759823]
	TIME [epoch: 1.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086189691256527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8086189691256527 | validation: 0.8051760536748329]
	TIME [epoch: 1.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259682560794927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259682560794927 | validation: 0.831067183985558]
	TIME [epoch: 1.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549432221508684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549432221508684 | validation: 0.8430422903529533]
	TIME [epoch: 1.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385391366780989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6385391366780989 | validation: 0.7180189727660178]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570619827641835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6570619827641835 | validation: 1.0442118332845032]
	TIME [epoch: 1.31 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593302922280093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593302922280093 | validation: 0.7904559630544008]
	TIME [epoch: 1.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634994602884897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634994602884897 | validation: 1.0642858347796489]
	TIME [epoch: 1.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631919201643799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631919201643799 | validation: 0.7219567740891738]
	TIME [epoch: 1.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472352284958534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472352284958534 | validation: 0.7622993840049397]
	TIME [epoch: 1.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712824892241716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6712824892241716 | validation: 0.8523032672814529]
	TIME [epoch: 1.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291565931631806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291565931631806 | validation: 0.9505592707932866]
	TIME [epoch: 1.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214843076776051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214843076776051 | validation: 0.796671190679218]
	TIME [epoch: 1.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748455889181533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6748455889181533 | validation: 0.8344909956874252]
	TIME [epoch: 1.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370452312387002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370452312387002 | validation: 0.7542168520173518]
	TIME [epoch: 1.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148582402408775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6148582402408775 | validation: 0.7302139934332993]
	TIME [epoch: 1.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139400070392305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139400070392305 | validation: 0.7368651890618935]
	TIME [epoch: 1.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275992065480683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6275992065480683 | validation: 0.8234954145717662]
	TIME [epoch: 1.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023534283942688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023534283942688 | validation: 0.9790858690257617]
	TIME [epoch: 1.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966733054177526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966733054177526 | validation: 1.0750249529587055]
	TIME [epoch: 1.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734212006788505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734212006788505 | validation: 0.8070817496388824]
	TIME [epoch: 1.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318723942165086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6318723942165086 | validation: 0.9240176640919029]
	TIME [epoch: 1.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723895241869452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723895241869452 | validation: 0.7256936282836924]
	TIME [epoch: 1.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430774110078866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430774110078866 | validation: 0.9605567586823393]
	TIME [epoch: 1.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077223103841155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7077223103841155 | validation: 0.6830392323109096]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202446304386554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6202446304386554 | validation: 0.835550671425696]
	TIME [epoch: 1.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292341321209757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6292341321209757 | validation: 0.7447405538472442]
	TIME [epoch: 189 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917638709793085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917638709793085 | validation: 1.1806944939792288]
	TIME [epoch: 2.61 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669863190447188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669863190447188 | validation: 0.7125892679181659]
	TIME [epoch: 2.58 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789341406820444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789341406820444 | validation: 0.984340596957231]
	TIME [epoch: 2.59 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222027758516985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222027758516985 | validation: 0.7135738519736061]
	TIME [epoch: 2.59 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538988919544003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6538988919544003 | validation: 0.7676581251654204]
	TIME [epoch: 2.59 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191197288376545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6191197288376545 | validation: 0.6836557763594162]
	TIME [epoch: 2.58 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001332772739879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6001332772739879 | validation: 0.6989820983378692]
	TIME [epoch: 2.59 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857532378809419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5857532378809419 | validation: 0.7333769129750153]
	TIME [epoch: 2.59 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960040812782539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960040812782539 | validation: 0.7269232479702956]
	TIME [epoch: 2.59 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6171403345709539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6171403345709539 | validation: 0.9144276018690238]
	TIME [epoch: 2.58 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954194710150192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954194710150192 | validation: 0.8788829276763654]
	TIME [epoch: 2.59 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880721196496298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880721196496298 | validation: 0.740005416045491]
	TIME [epoch: 2.59 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028987983723199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6028987983723199 | validation: 0.7997663691297039]
	TIME [epoch: 2.61 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5884185314519403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5884185314519403 | validation: 0.6345330824165557]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625806771680397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.625806771680397 | validation: 1.3054478268904335]
	TIME [epoch: 2.59 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496782142344348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496782142344348 | validation: 0.7404964559190861]
	TIME [epoch: 2.59 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896658253878124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896658253878124 | validation: 0.8053634473005012]
	TIME [epoch: 2.59 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5861821378169718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5861821378169718 | validation: 0.846802623328236]
	TIME [epoch: 2.59 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5721714956657317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5721714956657317 | validation: 0.6882505014004325]
	TIME [epoch: 2.59 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755093621111117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5755093621111117 | validation: 0.7176124181610841]
	TIME [epoch: 2.59 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5572791303553462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572791303553462 | validation: 0.632370649270412]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5472024444256745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5472024444256745 | validation: 0.7354439899477305]
	TIME [epoch: 2.58 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5910095468335514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5910095468335514 | validation: 0.8397448469212903]
	TIME [epoch: 2.58 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6727447615371022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6727447615371022 | validation: 0.7991226737021113]
	TIME [epoch: 2.58 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888854816643802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5888854816643802 | validation: 0.6735649228774794]
	TIME [epoch: 2.58 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5171557120634198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5171557120634198 | validation: 0.737991214988848]
	TIME [epoch: 2.58 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165678672446364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5165678672446364 | validation: 0.580130986933669]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5604784341415419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5604784341415419 | validation: 1.1563467840957173]
	TIME [epoch: 2.57 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257938985883334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7257938985883334 | validation: 0.6250255024566881]
	TIME [epoch: 2.57 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157286003716824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6157286003716824 | validation: 0.7378847535887578]
	TIME [epoch: 2.57 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4974860828975098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4974860828975098 | validation: 0.6144535553076514]
	TIME [epoch: 2.57 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4672112840764329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4672112840764329 | validation: 0.5788647060790151]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5012921298426274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5012921298426274 | validation: 0.9000147264825359]
	TIME [epoch: 2.59 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566002813897758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5566002813897758 | validation: 0.7075085510713506]
	TIME [epoch: 2.59 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48490111221302445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48490111221302445 | validation: 0.5598030054284402]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45988544029159606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45988544029159606 | validation: 0.8402529688490749]
	TIME [epoch: 2.58 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722579662075366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722579662075366 | validation: 0.7484450924541605]
	TIME [epoch: 2.58 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602060997006843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.602060997006843 | validation: 0.707735331239014]
	TIME [epoch: 2.58 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521466706985939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.521466706985939 | validation: 0.6956843815891491]
	TIME [epoch: 2.59 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43064794567268855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43064794567268855 | validation: 0.5605462158307323]
	TIME [epoch: 2.58 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4254601574248273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4254601574248273 | validation: 0.6169530237176075]
	TIME [epoch: 2.59 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42231024903382386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42231024903382386 | validation: 0.5025228070739839]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44726469954695747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44726469954695747 | validation: 0.6577935049452188]
	TIME [epoch: 2.58 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4804936297281995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4804936297281995 | validation: 0.6862753329791602]
	TIME [epoch: 2.58 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49714734420106793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49714734420106793 | validation: 0.6230858431001486]
	TIME [epoch: 2.62 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.454077597243747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.454077597243747 | validation: 0.7958664152819397]
	TIME [epoch: 2.58 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673168061798894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673168061798894 | validation: 0.49092741677781704]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4433231747366226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4433231747366226 | validation: 0.6586046741349059]
	TIME [epoch: 2.59 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43264201866605745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43264201866605745 | validation: 0.4847430760214524]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39575550514983077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39575550514983077 | validation: 0.5266315443579999]
	TIME [epoch: 2.58 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3678906971600515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3678906971600515 | validation: 0.6248234435404344]
	TIME [epoch: 2.58 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101685409178984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4101685409178984 | validation: 0.5647226413917087]
	TIME [epoch: 2.59 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34979324696448516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34979324696448516 | validation: 0.4877590656077313]
	TIME [epoch: 2.58 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33709628816973036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33709628816973036 | validation: 0.6214609443295713]
	TIME [epoch: 2.59 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4235469440537317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4235469440537317 | validation: 0.49545786759737886]
	TIME [epoch: 2.59 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.417500052054453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.417500052054453 | validation: 0.5504551130584734]
	TIME [epoch: 2.59 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213646748939362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3213646748939362 | validation: 0.4761493786490621]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32317096621295205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32317096621295205 | validation: 0.46042934275458647]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28966710955321306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28966710955321306 | validation: 0.5872433214105519]
	TIME [epoch: 2.59 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31594684681931773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31594684681931773 | validation: 0.4609605661273379]
	TIME [epoch: 2.59 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3880190752586147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3880190752586147 | validation: 0.8467287975680403]
	TIME [epoch: 2.58 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46568891016774283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46568891016774283 | validation: 0.42926453343918336]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.306085944668909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.306085944668909 | validation: 0.6335576027806277]
	TIME [epoch: 2.58 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5239740850758625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5239740850758625 | validation: 0.6323302001711313]
	TIME [epoch: 2.58 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4840919494647848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4840919494647848 | validation: 0.596978507893814]
	TIME [epoch: 2.58 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33203834366786067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33203834366786067 | validation: 0.47348415966916674]
	TIME [epoch: 2.58 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27705917120547247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27705917120547247 | validation: 0.39852849419975683]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278353888411026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.278353888411026 | validation: 0.4622899780246379]
	TIME [epoch: 2.57 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26517340809563955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26517340809563955 | validation: 0.3926293128530458]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644898124703111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644898124703111 | validation: 0.46934130969772136]
	TIME [epoch: 2.59 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278566356047013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.278566356047013 | validation: 0.42825741246065896]
	TIME [epoch: 2.59 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629564518838425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2629564518838425 | validation: 0.441659181601281]
	TIME [epoch: 2.58 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25555140047017877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25555140047017877 | validation: 0.4176522811936025]
	TIME [epoch: 2.58 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2380597078289182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2380597078289182 | validation: 0.4017492215249984]
	TIME [epoch: 2.58 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21931648716076887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21931648716076887 | validation: 0.38056776637703044]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320952343211461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2320952343211461 | validation: 0.5597561706579864]
	TIME [epoch: 2.59 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38789450090277583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38789450090277583 | validation: 0.3727895571788062]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42738444564813116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42738444564813116 | validation: 0.4421227477149531]
	TIME [epoch: 2.57 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22612363059288093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22612363059288093 | validation: 0.3975414910755046]
	TIME [epoch: 2.56 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2489928154141676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2489928154141676 | validation: 0.40937336567734306]
	TIME [epoch: 2.57 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21142510193246714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21142510193246714 | validation: 0.37103356858214154]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18030410287971382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18030410287971382 | validation: 0.34793808613506605]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1833618115988731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1833618115988731 | validation: 0.3951658660264681]
	TIME [epoch: 2.6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936891776192578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936891776192578 | validation: 0.45635994230488086]
	TIME [epoch: 2.58 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28784044128003033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28784044128003033 | validation: 0.514127814614799]
	TIME [epoch: 2.57 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34148830494186216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34148830494186216 | validation: 0.41686379258661965]
	TIME [epoch: 2.57 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742836727114807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2742836727114807 | validation: 0.417953479921478]
	TIME [epoch: 2.57 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2096207455917645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2096207455917645 | validation: 0.31197722312357445]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22528991369059134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22528991369059134 | validation: 0.573619925488078]
	TIME [epoch: 2.58 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23721079652332144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23721079652332144 | validation: 0.29756305535361954]
	TIME [epoch: 3.17 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814626861384168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18814626861384168 | validation: 0.4887356322630514]
	TIME [epoch: 2.58 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20547961366747333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20547961366747333 | validation: 0.3673905134592598]
	TIME [epoch: 2.58 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2052573170472325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2052573170472325 | validation: 0.4150519903855752]
	TIME [epoch: 2.59 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21526304802942078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21526304802942078 | validation: 0.3656068032607772]
	TIME [epoch: 2.58 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2239996285419111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2239996285419111 | validation: 0.434791759062162]
	TIME [epoch: 2.58 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585720793057593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2585720793057593 | validation: 0.4312432586414341]
	TIME [epoch: 2.58 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514461338316639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2514461338316639 | validation: 0.43729131982739916]
	TIME [epoch: 2.58 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23602186447387977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23602186447387977 | validation: 0.30557021827331216]
	TIME [epoch: 2.58 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16297056962463102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16297056962463102 | validation: 0.3474414457752687]
	TIME [epoch: 2.58 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15648931750007816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15648931750007816 | validation: 0.33750103979136586]
	TIME [epoch: 2.59 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1707827269936797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1707827269936797 | validation: 0.44646957748861155]
	TIME [epoch: 2.58 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18619268084417587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18619268084417587 | validation: 0.3070117470862783]
	TIME [epoch: 2.58 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1895671626485988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1895671626485988 | validation: 0.49039096407967575]
	TIME [epoch: 2.58 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19041684784392054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19041684784392054 | validation: 0.2980257769357106]
	TIME [epoch: 2.59 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17239778606422218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17239778606422218 | validation: 0.539970055047429]
	TIME [epoch: 2.58 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1899921070787123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1899921070787123 | validation: 0.30332235965775106]
	TIME [epoch: 2.58 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18537374944564108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18537374944564108 | validation: 0.550061852828493]
	TIME [epoch: 2.58 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23133442222950035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23133442222950035 | validation: 0.411514286230874]
	TIME [epoch: 2.58 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17446072766320334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17446072766320334 | validation: 0.321681269259439]
	TIME [epoch: 2.58 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24256200018371996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24256200018371996 | validation: 0.40155003335065703]
	TIME [epoch: 2.58 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19928602865499898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19928602865499898 | validation: 0.3649062609710741]
	TIME [epoch: 2.58 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19972411857514022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19972411857514022 | validation: 0.3269350895300178]
	TIME [epoch: 2.58 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15453303790839581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15453303790839581 | validation: 0.33781374896970007]
	TIME [epoch: 2.58 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1169170341108415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1169170341108415 | validation: 0.303525258111329]
	TIME [epoch: 2.58 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11359086827996016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11359086827996016 | validation: 0.3083748437547283]
	TIME [epoch: 2.58 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289682395209752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289682395209752 | validation: 0.4397670307865813]
	TIME [epoch: 2.58 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181513953215292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181513953215292 | validation: 0.39482280979169265]
	TIME [epoch: 2.58 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23119812221305303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23119812221305303 | validation: 0.42057943267894804]
	TIME [epoch: 2.58 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1636719326829855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1636719326829855 | validation: 0.2866989407361098]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12844762010424282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12844762010424282 | validation: 0.42107185491261045]
	TIME [epoch: 2.59 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270657169372041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1270657169372041 | validation: 0.2697311401584374]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384545519645297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1384545519645297 | validation: 0.6085227786300078]
	TIME [epoch: 2.59 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21104735217031625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21104735217031625 | validation: 0.4889539017664499]
	TIME [epoch: 2.59 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48658058036093027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48658058036093027 | validation: 0.43234835701449725]
	TIME [epoch: 2.59 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40291877373153595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40291877373153595 | validation: 0.6344682598427238]
	TIME [epoch: 2.58 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2525225621820686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2525225621820686 | validation: 0.35554111701251634]
	TIME [epoch: 2.58 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1668937469754191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1668937469754191 | validation: 0.292456411078175]
	TIME [epoch: 2.58 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14010252886570498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14010252886570498 | validation: 0.3097499853797084]
	TIME [epoch: 2.58 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13351996279305742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13351996279305742 | validation: 0.3398304216120043]
	TIME [epoch: 2.58 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12503680037734627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12503680037734627 | validation: 0.3174838905902105]
	TIME [epoch: 2.58 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15062627446805477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15062627446805477 | validation: 0.3559224300567789]
	TIME [epoch: 2.58 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15574223001291795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15574223001291795 | validation: 0.33281899548263577]
	TIME [epoch: 2.58 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1495486044462075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1495486044462075 | validation: 0.3413409793581513]
	TIME [epoch: 2.58 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13946078773336854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13946078773336854 | validation: 0.3398630629379189]
	TIME [epoch: 2.58 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15471115908851385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15471115908851385 | validation: 0.4018473998628899]
	TIME [epoch: 2.58 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21868362128510246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21868362128510246 | validation: 0.3215500832325735]
	TIME [epoch: 2.58 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21594308730495634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21594308730495634 | validation: 0.34737212469499057]
	TIME [epoch: 2.58 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451781968169737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1451781968169737 | validation: 0.293601275340828]
	TIME [epoch: 2.58 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012366452475751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1012366452475751 | validation: 0.2642526796791478]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11447634845632938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11447634845632938 | validation: 0.28669713966516835]
	TIME [epoch: 2.59 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11214434876723868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11214434876723868 | validation: 0.3487508029907567]
	TIME [epoch: 2.58 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282619471505648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10282619471505648 | validation: 0.2950632583070559]
	TIME [epoch: 2.58 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15995273228646698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15995273228646698 | validation: 0.47148448833234097]
	TIME [epoch: 2.58 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423869144789422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2423869144789422 | validation: 0.3122872661794623]
	TIME [epoch: 2.58 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511049095245361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511049095245361 | validation: 0.3300275048191401]
	TIME [epoch: 2.58 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17768890238950064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17768890238950064 | validation: 0.625928373911363]
	TIME [epoch: 2.57 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28465537247169254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28465537247169254 | validation: 0.47734322615986713]
	TIME [epoch: 2.59 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14548826447038163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14548826447038163 | validation: 0.35085328759241974]
	TIME [epoch: 2.58 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.263808924806446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.263808924806446 | validation: 0.44661644664742756]
	TIME [epoch: 2.58 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670451345925709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670451345925709 | validation: 0.44914070217470603]
	TIME [epoch: 2.58 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1372377615841644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1372377615841644 | validation: 0.2843923670463598]
	TIME [epoch: 2.58 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833067609943943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10833067609943943 | validation: 0.31810741148987143]
	TIME [epoch: 2.58 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11848562649062278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11848562649062278 | validation: 0.3261407050729441]
	TIME [epoch: 2.58 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522459834639557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522459834639557 | validation: 0.4232244190587784]
	TIME [epoch: 2.58 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2213494437197465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2213494437197465 | validation: 0.3443095145686581]
	TIME [epoch: 2.58 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16975342516835304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16975342516835304 | validation: 0.256164698144374]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15540903547512602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15540903547512602 | validation: 0.46032047706915924]
	TIME [epoch: 2.57 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27433575309646324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27433575309646324 | validation: 0.2906548623080892]
	TIME [epoch: 2.57 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16462851259690955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16462851259690955 | validation: 0.28294158943625874]
	TIME [epoch: 2.57 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10353758173977841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10353758173977841 | validation: 0.2933632231905305]
	TIME [epoch: 2.56 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1314039778958685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1314039778958685 | validation: 0.30411113441042475]
	TIME [epoch: 2.56 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825120552090145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11825120552090145 | validation: 0.31623289624261863]
	TIME [epoch: 2.57 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13740658606657058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13740658606657058 | validation: 0.3247861621035899]
	TIME [epoch: 2.57 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12350594620517186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12350594620517186 | validation: 0.3020746037316402]
	TIME [epoch: 2.56 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12849535222175618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12849535222175618 | validation: 0.34374954228941884]
	TIME [epoch: 2.56 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439861370752865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439861370752865 | validation: 0.28407518489487743]
	TIME [epoch: 2.57 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12369987584618625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12369987584618625 | validation: 0.3043861338441834]
	TIME [epoch: 2.57 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131838000359305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1131838000359305 | validation: 0.2593890562988403]
	TIME [epoch: 2.57 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1079025850898696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1079025850898696 | validation: 0.29702614574485237]
	TIME [epoch: 2.57 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178184076106946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11178184076106946 | validation: 0.27820025140852744]
	TIME [epoch: 2.57 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802921156419262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13802921156419262 | validation: 0.5889346526623838]
	TIME [epoch: 2.57 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21130573508110317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21130573508110317 | validation: 0.23033595366980616]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09896530381733691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09896530381733691 | validation: 0.24749957893065]
	TIME [epoch: 2.6 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10206060757279069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10206060757279069 | validation: 0.44297939404393927]
	TIME [epoch: 2.59 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19065859905275745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19065859905275745 | validation: 0.3498251994360317]
	TIME [epoch: 2.59 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15251314682876108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15251314682876108 | validation: 0.24727639517767253]
	TIME [epoch: 2.56 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1803504554200158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1803504554200158 | validation: 0.44981598048944077]
	TIME [epoch: 2.57 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519679361351785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1519679361351785 | validation: 0.2486566386857573]
	TIME [epoch: 2.56 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779533034265832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09779533034265832 | validation: 0.23952499512343786]
	TIME [epoch: 2.56 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830691813012528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09830691813012528 | validation: 0.3155875835157962]
	TIME [epoch: 2.56 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09394127777722247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09394127777722247 | validation: 0.2632573479350719]
	TIME [epoch: 2.57 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545822000821738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09545822000821738 | validation: 0.3137078794917383]
	TIME [epoch: 2.56 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916561735553003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09916561735553003 | validation: 0.28366578732157993]
	TIME [epoch: 2.57 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14380186634336317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14380186634336317 | validation: 0.3278750999061583]
	TIME [epoch: 2.56 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20979284852248384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20979284852248384 | validation: 0.3717676749676573]
	TIME [epoch: 2.57 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825220617204482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825220617204482 | validation: 0.2705221931455348]
	TIME [epoch: 2.56 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08578377729847621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08578377729847621 | validation: 0.23241524256022592]
	TIME [epoch: 2.57 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11877970918900549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11877970918900549 | validation: 0.3881997494213876]
	TIME [epoch: 2.56 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678110630747666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1678110630747666 | validation: 0.3226893779762994]
	TIME [epoch: 2.57 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15526049536224976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15526049536224976 | validation: 0.3275612376225314]
	TIME [epoch: 2.57 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15998854918058256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15998854918058256 | validation: 0.46874266649704865]
	TIME [epoch: 2.57 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18823195533355475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18823195533355475 | validation: 0.3168855030508475]
	TIME [epoch: 2.56 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197262823573416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1197262823573416 | validation: 0.2503344540710103]
	TIME [epoch: 2.57 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11305066683404505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11305066683404505 | validation: 0.3747367836675639]
	TIME [epoch: 2.56 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125440709620514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.125440709620514 | validation: 0.2822320158123322]
	TIME [epoch: 2.57 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755303062979579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08755303062979579 | validation: 0.27912593015740333]
	TIME [epoch: 2.59 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090531743268974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12090531743268974 | validation: 0.31575108963668097]
	TIME [epoch: 2.58 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15268783799337185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15268783799337185 | validation: 0.360309514231157]
	TIME [epoch: 2.59 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711970618670271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711970618670271 | validation: 0.2544047881137354]
	TIME [epoch: 2.56 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286828179186778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12286828179186778 | validation: 0.2801945974610669]
	TIME [epoch: 2.59 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811146098997413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09811146098997413 | validation: 0.23613077688884052]
	TIME [epoch: 2.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09516919794689542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09516919794689542 | validation: 0.2883467616413436]
	TIME [epoch: 2.58 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08927765663129193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08927765663129193 | validation: 0.2181636038401238]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1063740112500431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1063740112500431 | validation: 0.4640389899560006]
	TIME [epoch: 2.57 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499926595239153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499926595239153 | validation: 0.2287665235323697]
	TIME [epoch: 2.56 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11665713375519139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11665713375519139 | validation: 0.25890167103965345]
	TIME [epoch: 2.56 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12483610740797066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12483610740797066 | validation: 0.39609547525098154]
	TIME [epoch: 2.57 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18697797277707792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18697797277707792 | validation: 0.2712162685871283]
	TIME [epoch: 2.57 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12216911298953584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12216911298953584 | validation: 0.2105092036406708]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06898290439338865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06898290439338865 | validation: 0.23774929190075014]
	TIME [epoch: 2.57 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10261833490443387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10261833490443387 | validation: 0.3254680655554977]
	TIME [epoch: 2.57 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15349374658015877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15349374658015877 | validation: 0.31520505805566024]
	TIME [epoch: 2.57 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15643061395580202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15643061395580202 | validation: 0.34375060677701974]
	TIME [epoch: 2.57 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522299522149004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11522299522149004 | validation: 0.2330290977928442]
	TIME [epoch: 2.57 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452884250853854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10452884250853854 | validation: 0.3593598974951844]
	TIME [epoch: 2.57 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568498858228094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568498858228094 | validation: 0.2255309893899936]
	TIME [epoch: 2.58 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06642506444241952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06642506444241952 | validation: 0.20459364706623817]
	TIME [epoch: 2.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06625843194917345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06625843194917345 | validation: 0.2871220140375549]
	TIME [epoch: 2.58 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08078754808151757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08078754808151757 | validation: 0.28435038069712915]
	TIME [epoch: 2.59 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16430576751411963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16430576751411963 | validation: 0.4783753716662813]
	TIME [epoch: 2.58 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23661446952359685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23661446952359685 | validation: 0.25259553318017963]
	TIME [epoch: 2.58 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1821099468434467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1821099468434467 | validation: 0.24329647584065994]
	TIME [epoch: 2.57 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0884326667082354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0884326667082354 | validation: 0.19771297966046397]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07886228856985825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07886228856985825 | validation: 0.22868845142906663]
	TIME [epoch: 2.58 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246915909039429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11246915909039429 | validation: 0.28573054956829486]
	TIME [epoch: 2.61 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19093738805073435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19093738805073435 | validation: 0.3021774226309297]
	TIME [epoch: 2.58 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09376167064653629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09376167064653629 | validation: 0.18984651821860302]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06299614879246968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06299614879246968 | validation: 0.24725988525798748]
	TIME [epoch: 2.58 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553376301832702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06553376301832702 | validation: 0.2071892834301406]
	TIME [epoch: 2.58 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08561222235992368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08561222235992368 | validation: 0.33778925970360907]
	TIME [epoch: 2.57 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15166430202513345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15166430202513345 | validation: 0.2509958154133415]
	TIME [epoch: 2.57 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15165110479889063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15165110479889063 | validation: 0.2435698754382882]
	TIME [epoch: 2.58 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1248946773770935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1248946773770935 | validation: 0.44405969091654934]
	TIME [epoch: 2.57 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14602206116322758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14602206116322758 | validation: 0.21692438067559838]
	TIME [epoch: 2.58 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955665056586949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11955665056586949 | validation: 0.31942455455160035]
	TIME [epoch: 2.58 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08839319559419455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08839319559419455 | validation: 0.22442149331785957]
	TIME [epoch: 2.58 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641087063499412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641087063499412 | validation: 0.1719325982807637]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719088195708582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07719088195708582 | validation: 0.3170182070410546]
	TIME [epoch: 2.59 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09639574131599059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09639574131599059 | validation: 0.1993295806915391]
	TIME [epoch: 2.58 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937577512143037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937577512143037 | validation: 0.2763772493876555]
	TIME [epoch: 2.58 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13016645961086354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13016645961086354 | validation: 0.24686755028179574]
	TIME [epoch: 2.58 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11970519233075017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11970519233075017 | validation: 0.20968502813770382]
	TIME [epoch: 2.58 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10705220721710866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10705220721710866 | validation: 0.29196297051038117]
	TIME [epoch: 2.58 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19717423096312536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19717423096312536 | validation: 0.39893857513096]
	TIME [epoch: 2.58 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1160069632277562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1160069632277562 | validation: 0.22347496871254077]
	TIME [epoch: 2.58 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828062192883659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04828062192883659 | validation: 0.20804142310252174]
	TIME [epoch: 2.58 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139564031748259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139564031748259 | validation: 0.34690921223128623]
	TIME [epoch: 2.58 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515205497957786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07515205497957786 | validation: 0.19773474392484724]
	TIME [epoch: 2.58 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839066888956815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07839066888956815 | validation: 0.3063211994087661]
	TIME [epoch: 2.58 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16841000860948183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16841000860948183 | validation: 0.2903460581864944]
	TIME [epoch: 2.58 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543922342106689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543922342106689 | validation: 0.2267563187078941]
	TIME [epoch: 2.58 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11027544102168975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11027544102168975 | validation: 0.18103298238345744]
	TIME [epoch: 2.58 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252887270465927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252887270465927 | validation: 0.2576774521302723]
	TIME [epoch: 2.58 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777877985143273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0777877985143273 | validation: 0.17447919575621743]
	TIME [epoch: 2.58 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08464811244799288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08464811244799288 | validation: 0.26134556361154954]
	TIME [epoch: 2.58 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09915503559514648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09915503559514648 | validation: 0.21859632202088541]
	TIME [epoch: 2.58 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09330541081305302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09330541081305302 | validation: 0.1926921915215375]
	TIME [epoch: 2.58 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10314432085214406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10314432085214406 | validation: 0.3655665043743845]
	TIME [epoch: 2.58 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259694298785376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11259694298785376 | validation: 0.18948711926016581]
	TIME [epoch: 2.58 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019862194664071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019862194664071 | validation: 0.2650505784616674]
	TIME [epoch: 2.58 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695122849626717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08695122849626717 | validation: 0.21554990669957502]
	TIME [epoch: 2.58 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219644929020416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11219644929020416 | validation: 0.3660674694763763]
	TIME [epoch: 2.58 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12698630241742848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12698630241742848 | validation: 0.19971975979833748]
	TIME [epoch: 2.58 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06977965498716293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06977965498716293 | validation: 0.18829104079260917]
	TIME [epoch: 2.58 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561626393691965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05561626393691965 | validation: 0.2548637163472147]
	TIME [epoch: 2.58 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660844529424704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08660844529424704 | validation: 0.2708542625900144]
	TIME [epoch: 2.58 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981249203797353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14981249203797353 | validation: 0.22181567669728045]
	TIME [epoch: 2.58 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954064341280878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10954064341280878 | validation: 0.23258042931952502]
	TIME [epoch: 2.57 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08307318707682328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08307318707682328 | validation: 0.20001942217692836]
	TIME [epoch: 2.58 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08765849434024425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08765849434024425 | validation: 0.2942531389916549]
	TIME [epoch: 2.58 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671257394370386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09671257394370386 | validation: 0.18580341947691392]
	TIME [epoch: 2.58 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09531697696814596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09531697696814596 | validation: 0.2771215362083136]
	TIME [epoch: 2.58 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1113165149056724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1113165149056724 | validation: 0.14746358280247535]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10912702373089976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10912702373089976 | validation: 0.21557919527951688]
	TIME [epoch: 2.58 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030529130344235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11030529130344235 | validation: 0.30868139973482367]
	TIME [epoch: 2.58 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08531922759193267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08531922759193267 | validation: 0.15433399512643226]
	TIME [epoch: 2.58 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08308515950072316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08308515950072316 | validation: 0.21953547154754385]
	TIME [epoch: 2.58 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921703418735185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921703418735185 | validation: 0.20623702418371803]
	TIME [epoch: 2.58 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09958012059442561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09958012059442561 | validation: 0.1721063245778106]
	TIME [epoch: 2.58 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902578187800948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06902578187800948 | validation: 0.2139049515668021]
	TIME [epoch: 2.58 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664930156431266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06664930156431266 | validation: 0.17809298255586287]
	TIME [epoch: 2.58 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263220357925558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09263220357925558 | validation: 0.32841039570960384]
	TIME [epoch: 2.58 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12947713971076594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12947713971076594 | validation: 0.18696801696680715]
	TIME [epoch: 2.58 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0905304660009697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0905304660009697 | validation: 0.17442738807989616]
	TIME [epoch: 2.58 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558359954626079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05558359954626079 | validation: 0.27068711914942434]
	TIME [epoch: 2.58 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604485169648894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08604485169648894 | validation: 0.23493614246601108]
	TIME [epoch: 2.58 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463532539095081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11463532539095081 | validation: 0.3084087180260011]
	TIME [epoch: 2.58 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16454002817379804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16454002817379804 | validation: 0.2197451865045065]
	TIME [epoch: 2.58 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10422404625216374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10422404625216374 | validation: 0.16760729178415393]
	TIME [epoch: 2.58 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07360142167958969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07360142167958969 | validation: 0.20514011494302872]
	TIME [epoch: 2.58 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0799669951713025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0799669951713025 | validation: 0.1553787988133548]
	TIME [epoch: 2.58 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.066713714459111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.066713714459111 | validation: 0.1911190776793894]
	TIME [epoch: 2.58 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058510074371346726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058510074371346726 | validation: 0.14787318959225457]
	TIME [epoch: 2.58 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484187418389825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07484187418389825 | validation: 0.41648504020309396]
	TIME [epoch: 2.58 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14146978345308153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14146978345308153 | validation: 0.21387471560889476]
	TIME [epoch: 2.58 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11932860405235107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11932860405235107 | validation: 0.21892170230661062]
	TIME [epoch: 2.58 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11134389887105428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11134389887105428 | validation: 0.28712472076261863]
	TIME [epoch: 2.58 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21507245513354026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21507245513354026 | validation: 0.36106567124558975]
	TIME [epoch: 2.58 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361271418962714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10361271418962714 | validation: 0.4826336148794031]
	TIME [epoch: 2.58 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16274622343692016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16274622343692016 | validation: 0.2411297834207285]
	TIME [epoch: 194 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220429211870812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220429211870812 | validation: 0.18995707428379138]
	TIME [epoch: 5.54 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0637969519959671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0637969519959671 | validation: 0.17279704524137152]
	TIME [epoch: 5.51 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05995158480307901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05995158480307901 | validation: 0.21103328253084924]
	TIME [epoch: 5.51 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680455203688009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06680455203688009 | validation: 0.2135784488628395]
	TIME [epoch: 5.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0907323449041417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0907323449041417 | validation: 0.23305491226356484]
	TIME [epoch: 5.52 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532809333046813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11532809333046813 | validation: 0.1962417951798781]
	TIME [epoch: 5.51 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09162091324866978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09162091324866978 | validation: 0.18643922586080444]
	TIME [epoch: 5.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06164684869231223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06164684869231223 | validation: 0.1843772156569052]
	TIME [epoch: 5.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011888032430706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06011888032430706 | validation: 0.18324856541871998]
	TIME [epoch: 5.51 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903006254699385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07903006254699385 | validation: 0.1800715852604371]
	TIME [epoch: 5.52 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337157329478928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09337157329478928 | validation: 0.20030456147051032]
	TIME [epoch: 5.52 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08485916793901392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08485916793901392 | validation: 0.15598289681990482]
	TIME [epoch: 5.52 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279905891171873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06279905891171873 | validation: 0.1603955731439151]
	TIME [epoch: 5.51 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06302936677499521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06302936677499521 | validation: 0.1630829582921588]
	TIME [epoch: 5.51 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777734513020151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07777734513020151 | validation: 0.20764601004668815]
	TIME [epoch: 5.52 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11193027044012677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11193027044012677 | validation: 0.29440873921147026]
	TIME [epoch: 5.52 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10277651158810233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10277651158810233 | validation: 0.18265985001752014]
	TIME [epoch: 5.51 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05718703844429499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05718703844429499 | validation: 0.16684942157226712]
	TIME [epoch: 5.51 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07885133349023742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07885133349023742 | validation: 0.5493092091554329]
	TIME [epoch: 5.52 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24039224053265648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24039224053265648 | validation: 0.1652959790990243]
	TIME [epoch: 5.52 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073808299144801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073808299144801 | validation: 0.1929959550782103]
	TIME [epoch: 5.52 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08041397449989342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08041397449989342 | validation: 0.17506505144773793]
	TIME [epoch: 5.52 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06166608852873033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06166608852873033 | validation: 0.1935469447670758]
	TIME [epoch: 5.51 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07862208137140131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07862208137140131 | validation: 0.4747435940159894]
	TIME [epoch: 5.51 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2242185869036299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2242185869036299 | validation: 0.3926121816618786]
	TIME [epoch: 5.51 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18099368014548692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18099368014548692 | validation: 0.3926686814997686]
	TIME [epoch: 5.51 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13425279872666568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13425279872666568 | validation: 0.22310547538366593]
	TIME [epoch: 5.52 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10072281043292262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10072281043292262 | validation: 0.18495381418904144]
	TIME [epoch: 5.51 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626249102708762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626249102708762 | validation: 0.17235939115214827]
	TIME [epoch: 5.52 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061325008804599034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061325008804599034 | validation: 0.20771896593121233]
	TIME [epoch: 5.51 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05661708161888513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05661708161888513 | validation: 0.198774213673261]
	TIME [epoch: 5.52 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0679964622913867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0679964622913867 | validation: 0.23395881209466785]
	TIME [epoch: 5.51 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556263108971955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09556263108971955 | validation: 0.20451319366189208]
	TIME [epoch: 5.51 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10040261830001762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10040261830001762 | validation: 0.2166273617903062]
	TIME [epoch: 5.52 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811002596026632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0811002596026632 | validation: 0.1544224601835553]
	TIME [epoch: 5.51 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06049020110898673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06049020110898673 | validation: 0.1425438407488039]
	TIME [epoch: 5.51 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05412787318993584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05412787318993584 | validation: 0.1700923598475067]
	TIME [epoch: 5.53 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143752731390592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07143752731390592 | validation: 0.18918142293140272]
	TIME [epoch: 5.52 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10091019813259616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10091019813259616 | validation: 0.21236001606984864]
	TIME [epoch: 5.52 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122446335367128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08122446335367128 | validation: 0.15102812201446972]
	TIME [epoch: 5.52 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06555690673697677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06555690673697677 | validation: 0.17516083549127012]
	TIME [epoch: 5.52 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051902889127081464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051902889127081464 | validation: 0.1582373693446076]
	TIME [epoch: 5.51 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053242657644197444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053242657644197444 | validation: 0.15028798742321078]
	TIME [epoch: 5.52 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059530390133661584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059530390133661584 | validation: 0.16928906877954975]
	TIME [epoch: 5.52 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742583879556247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742583879556247 | validation: 0.14557447594392986]
	TIME [epoch: 5.52 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07941797389433247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07941797389433247 | validation: 0.18224508497946917]
	TIME [epoch: 5.52 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213061373148425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08213061373148425 | validation: 0.1450857817502696]
	TIME [epoch: 5.52 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220121757725976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220121757725976 | validation: 0.1695709146451453]
	TIME [epoch: 5.52 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567390884803122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567390884803122 | validation: 0.2336441654305335]
	TIME [epoch: 5.51 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10008877096407509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10008877096407509 | validation: 0.14603608857520764]
	TIME [epoch: 5.51 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07620608347567835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07620608347567835 | validation: 0.16733689500576926]
	TIME [epoch: 5.51 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041345518488852255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041345518488852255 | validation: 0.12741908995843418]
	TIME [epoch: 5.52 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0523973881034373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0523973881034373 | validation: 0.1597672537119937]
	TIME [epoch: 5.51 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914740614043755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05914740614043755 | validation: 0.1682541905973438]
	TIME [epoch: 5.52 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08911646660084148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08911646660084148 | validation: 0.2555371186954744]
	TIME [epoch: 5.51 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453685755947155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1453685755947155 | validation: 0.1856563296283403]
	TIME [epoch: 5.51 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038078106903355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10038078106903355 | validation: 0.3624838599461198]
	TIME [epoch: 5.51 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09573835072185567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09573835072185567 | validation: 0.14963957145697868]
	TIME [epoch: 5.52 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732376367072627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732376367072627 | validation: 0.13956344113741678]
	TIME [epoch: 5.51 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426199440792787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426199440792787 | validation: 0.24613622452758366]
	TIME [epoch: 5.51 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044347080412328065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044347080412328065 | validation: 0.14626486022673993]
	TIME [epoch: 5.52 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036230117000741954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036230117000741954 | validation: 0.20482353943209583]
	TIME [epoch: 5.53 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09884543577721847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09884543577721847 | validation: 0.24330027628189208]
	TIME [epoch: 5.52 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20577123689394713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20577123689394713 | validation: 0.2177989368456058]
	TIME [epoch: 5.52 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14001671919140724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14001671919140724 | validation: 0.18341520709522618]
	TIME [epoch: 5.51 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10708505551313831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10708505551313831 | validation: 0.21428193752860444]
	TIME [epoch: 5.51 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08181420864693394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08181420864693394 | validation: 0.14725746610161702]
	TIME [epoch: 5.52 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06020468980204075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06020468980204075 | validation: 0.1706808936774855]
	TIME [epoch: 5.51 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684534531027968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684534531027968 | validation: 0.14221977790488533]
	TIME [epoch: 5.51 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05665001374061461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05665001374061461 | validation: 0.15580571511893956]
	TIME [epoch: 5.51 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048308130319513705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048308130319513705 | validation: 0.1571166921088067]
	TIME [epoch: 5.52 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05195459516048997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05195459516048997 | validation: 0.14701127694415575]
	TIME [epoch: 5.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825506139200481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825506139200481 | validation: 0.1874404585088687]
	TIME [epoch: 5.51 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07377916618178425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07377916618178425 | validation: 0.13622934069714082]
	TIME [epoch: 5.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925217373299859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06925217373299859 | validation: 0.2041989564698653]
	TIME [epoch: 5.52 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443451461824047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06443451461824047 | validation: 0.1695245663665398]
	TIME [epoch: 5.51 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10142345525282426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10142345525282426 | validation: 0.19932670650637105]
	TIME [epoch: 5.51 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053936948097640956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053936948097640956 | validation: 0.17638430438557826]
	TIME [epoch: 5.51 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729527035766465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729527035766465 | validation: 0.17081052244074008]
	TIME [epoch: 5.51 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11440256411691621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11440256411691621 | validation: 0.2471612713638276]
	TIME [epoch: 5.52 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08926026092345186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08926026092345186 | validation: 0.1240419867875656]
	TIME [epoch: 5.52 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688685048798271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0688685048798271 | validation: 0.13453486747487356]
	TIME [epoch: 5.51 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051724336115500855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051724336115500855 | validation: 0.12005780369265993]
	TIME [epoch: 5.52 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551256974753577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03551256974753577 | validation: 0.11529334343479154]
	TIME [epoch: 5.51 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03488329732004723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03488329732004723 | validation: 0.12276238400179054]
	TIME [epoch: 5.57 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051053467450474975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051053467450474975 | validation: 0.17832962436013078]
	TIME [epoch: 5.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165234864216309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08165234864216309 | validation: 0.1696919589063941]
	TIME [epoch: 5.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08305162194648721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08305162194648721 | validation: 0.14122738084102987]
	TIME [epoch: 5.53 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874074554329669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06874074554329669 | validation: 0.18678386890075901]
	TIME [epoch: 5.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05230069057838756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05230069057838756 | validation: 0.13881183791465695]
	TIME [epoch: 5.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049744422118165814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049744422118165814 | validation: 0.18518529448845952]
	TIME [epoch: 5.52 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04198793238889668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04198793238889668 | validation: 0.1266385172657582]
	TIME [epoch: 5.52 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04754142015217154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04754142015217154 | validation: 0.21715498583715825]
	TIME [epoch: 5.52 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892744439804468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05892744439804468 | validation: 0.15209052588862715]
	TIME [epoch: 5.52 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07641448746575946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07641448746575946 | validation: 0.2473457809205476]
	TIME [epoch: 5.52 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678529159034712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678529159034712 | validation: 0.1566241290642313]
	TIME [epoch: 5.51 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05982012870495416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05982012870495416 | validation: 0.15663804712255255]
	TIME [epoch: 5.53 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019434809355213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04019434809355213 | validation: 0.1524220185060926]
	TIME [epoch: 5.52 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046224588724328726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046224588724328726 | validation: 0.1310007384143117]
	TIME [epoch: 5.52 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07403015217285885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07403015217285885 | validation: 0.2345666720048608]
	TIME [epoch: 5.58 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10769803224755002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10769803224755002 | validation: 0.17926040997898027]
	TIME [epoch: 5.57 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11971801596760123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11971801596760123 | validation: 0.15879941804158904]
	TIME [epoch: 5.57 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671072917054974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09671072917054974 | validation: 0.1260340497576536]
	TIME [epoch: 5.58 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395442696416009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05395442696416009 | validation: 0.12727438757027615]
	TIME [epoch: 5.59 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0427027094486918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0427027094486918 | validation: 0.12056977897651328]
	TIME [epoch: 5.59 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03822305356997962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03822305356997962 | validation: 0.12347477567393779]
	TIME [epoch: 5.58 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050375010452490585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050375010452490585 | validation: 0.16600415883248085]
	TIME [epoch: 5.55 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07012578382770382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07012578382770382 | validation: 0.1620961769520681]
	TIME [epoch: 5.52 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10142606569820325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10142606569820325 | validation: 0.26810637323830405]
	TIME [epoch: 5.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09191594441947835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09191594441947835 | validation: 0.15174744546473204]
	TIME [epoch: 5.52 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197897853024208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05197897853024208 | validation: 0.15137936007673655]
	TIME [epoch: 5.51 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06226336197822141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06226336197822141 | validation: 0.38215840024097086]
	TIME [epoch: 5.53 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09443280708735344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09443280708735344 | validation: 0.0828808412847663]
	TIME [epoch: 5.51 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039002752017767595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039002752017767595 | validation: 0.18187267304972887]
	TIME [epoch: 5.57 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06821919083389633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06821919083389633 | validation: 0.18532874131333454]
	TIME [epoch: 5.58 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08569046580382743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08569046580382743 | validation: 0.18552080511206515]
	TIME [epoch: 5.55 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08766629611230344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08766629611230344 | validation: 0.133492123944713]
	TIME [epoch: 5.51 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07457695566458816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07457695566458816 | validation: 0.0946974500452621]
	TIME [epoch: 5.51 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072867539847695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1072867539847695 | validation: 0.1131059352022291]
	TIME [epoch: 5.51 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053592747057261365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053592747057261365 | validation: 0.11113666273530357]
	TIME [epoch: 5.52 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049748873681707335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049748873681707335 | validation: 0.14295236364530833]
	TIME [epoch: 5.52 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05643184334242097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05643184334242097 | validation: 0.13149481046746544]
	TIME [epoch: 5.52 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06898149398969436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06898149398969436 | validation: 0.13665358789248408]
	TIME [epoch: 5.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113373586893251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113373586893251 | validation: 0.1491257286876072]
	TIME [epoch: 5.52 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06794090994937778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06794090994937778 | validation: 0.13034902388580666]
	TIME [epoch: 5.51 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056929902175625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056929902175625 | validation: 0.1340228591737272]
	TIME [epoch: 5.56 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049628890176237006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049628890176237006 | validation: 0.11300639525739441]
	TIME [epoch: 5.55 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05006451646319875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05006451646319875 | validation: 0.17012508792275638]
	TIME [epoch: 5.55 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07662062007608875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07662062007608875 | validation: 0.11641955042383056]
	TIME [epoch: 5.56 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931737071753191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04931737071753191 | validation: 0.1279531026461081]
	TIME [epoch: 5.55 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04979252541721312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04979252541721312 | validation: 0.12994674344714993]
	TIME [epoch: 5.55 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05909745463406214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05909745463406214 | validation: 0.13783038296349148]
	TIME [epoch: 5.55 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865393655164288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06865393655164288 | validation: 1.113110309325419]
	TIME [epoch: 5.56 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5465508144344922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5465508144344922 | validation: 0.6074759205408086]
	TIME [epoch: 5.56 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26782389241426885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26782389241426885 | validation: 0.1660164115514225]
	TIME [epoch: 5.56 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937570920811521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937570920811521 | validation: 0.15872048653906928]
	TIME [epoch: 5.56 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07123918644275093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07123918644275093 | validation: 0.11379053079613506]
	TIME [epoch: 5.56 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397425853476292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05397425853476292 | validation: 0.15649010740071428]
	TIME [epoch: 5.56 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23637167325786265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23637167325786265 | validation: 0.27294204702971747]
	TIME [epoch: 5.56 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17507274458382263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17507274458382263 | validation: 0.14552036596157256]
	TIME [epoch: 5.56 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058692756443218363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058692756443218363 | validation: 0.11836287396253108]
	TIME [epoch: 5.55 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05147605687222657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05147605687222657 | validation: 0.16267252082713055]
	TIME [epoch: 5.55 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05999933335051713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05999933335051713 | validation: 0.1388890158986907]
	TIME [epoch: 5.57 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434867831887607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06434867831887607 | validation: 0.18494187313022895]
	TIME [epoch: 5.55 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887561388357175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887561388357175 | validation: 0.1380760441061074]
	TIME [epoch: 5.56 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448584422153825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448584422153825 | validation: 0.1542954206686459]
	TIME [epoch: 5.56 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07463139217549528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07463139217549528 | validation: 0.15363589210024933]
	TIME [epoch: 5.57 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08370092078424955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08370092078424955 | validation: 0.13792327778106572]
	TIME [epoch: 5.57 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047177345469518475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047177345469518475 | validation: 0.11857229454776666]
	TIME [epoch: 5.58 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03486473591816888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03486473591816888 | validation: 0.09227767519416404]
	TIME [epoch: 5.58 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024903038632230902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024903038632230902 | validation: 0.10932758399543135]
	TIME [epoch: 5.58 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901444094531205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02901444094531205 | validation: 0.11527656085682975]
	TIME [epoch: 5.57 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631682659303557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04631682659303557 | validation: 0.13298872999270303]
	TIME [epoch: 5.58 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113467658260778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113467658260778 | validation: 0.14015159885203388]
	TIME [epoch: 5.58 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935874981541143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07935874981541143 | validation: 0.12981179911379895]
	TIME [epoch: 5.57 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04939852337123823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04939852337123823 | validation: 0.13179976524138556]
	TIME [epoch: 5.56 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04683682971562139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04683682971562139 | validation: 0.13503644680397797]
	TIME [epoch: 5.57 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07429528511186316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07429528511186316 | validation: 0.15783461671840252]
	TIME [epoch: 5.56 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285442676828339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08285442676828339 | validation: 0.09863069931822582]
	TIME [epoch: 5.57 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599107262955744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04599107262955744 | validation: 0.10321692001391825]
	TIME [epoch: 5.57 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034143052290096024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034143052290096024 | validation: 0.11099925303888192]
	TIME [epoch: 5.56 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967792163063872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03967792163063872 | validation: 0.12965728458358536]
	TIME [epoch: 5.57 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07711219066464821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07711219066464821 | validation: 0.21955817445697107]
	TIME [epoch: 5.56 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07501529372113262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07501529372113262 | validation: 0.17227187420682774]
	TIME [epoch: 5.57 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06283054157109509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06283054157109509 | validation: 0.11104759981649215]
	TIME [epoch: 5.57 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03199949153031229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03199949153031229 | validation: 0.15397049193134194]
	TIME [epoch: 5.57 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03775718252466858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03775718252466858 | validation: 0.10424013649484286]
	TIME [epoch: 5.56 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04198960837498616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04198960837498616 | validation: 0.1407587734211114]
	TIME [epoch: 5.56 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06704916185316577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06704916185316577 | validation: 0.15476194202444704]
	TIME [epoch: 5.57 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081248974422678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08081248974422678 | validation: 0.14753754353288703]
	TIME [epoch: 5.57 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07017751473453081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07017751473453081 | validation: 0.12953415018002853]
	TIME [epoch: 5.57 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04856381781402697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04856381781402697 | validation: 0.10516171442131593]
	TIME [epoch: 5.58 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028972807929445406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028972807929445406 | validation: 0.07580736988301388]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026934063106191664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026934063106191664 | validation: 0.12929685711646863]
	TIME [epoch: 5.54 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03758420751549708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03758420751549708 | validation: 0.12939391027503727]
	TIME [epoch: 5.53 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058671919137040174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058671919137040174 | validation: 0.12144818782902692]
	TIME [epoch: 5.54 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124539164537513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07124539164537513 | validation: 0.1485126901827249]
	TIME [epoch: 5.53 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08058430098622583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08058430098622583 | validation: 0.1050109618485823]
	TIME [epoch: 5.53 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053028517510839625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053028517510839625 | validation: 0.12329338767380267]
	TIME [epoch: 5.52 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05861828964025307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05861828964025307 | validation: 0.11886974260603536]
	TIME [epoch: 5.53 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902600717782563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06902600717782563 | validation: 0.15269411385948423]
	TIME [epoch: 5.51 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181664335727712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05181664335727712 | validation: 0.11901756217364695]
	TIME [epoch: 5.53 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064570988041494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05064570988041494 | validation: 0.314507782171887]
	TIME [epoch: 5.52 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3398314767879064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3398314767879064 | validation: 0.48276330960214814]
	TIME [epoch: 5.53 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557965951663118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2557965951663118 | validation: 0.3087183801129177]
	TIME [epoch: 5.51 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748251555549376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11748251555549376 | validation: 0.19064290167502512]
	TIME [epoch: 5.53 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05468941024336237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05468941024336237 | validation: 0.15375200124810048]
	TIME [epoch: 5.52 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054042056087879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05054042056087879 | validation: 0.1842465652642767]
	TIME [epoch: 5.53 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030484286330134946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030484286330134946 | validation: 0.11032814349591297]
	TIME [epoch: 5.53 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025064893210121328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025064893210121328 | validation: 0.12196464253592194]
	TIME [epoch: 5.53 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02556603781999183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02556603781999183 | validation: 0.1147638886717307]
	TIME [epoch: 5.53 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053930862252523894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053930862252523894 | validation: 0.2471494815787256]
	TIME [epoch: 5.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13009020649471356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13009020649471356 | validation: 0.15562404664628116]
	TIME [epoch: 5.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172697637765355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09172697637765355 | validation: 0.14138593178553716]
	TIME [epoch: 5.53 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07992330986228115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07992330986228115 | validation: 0.13606110301494476]
	TIME [epoch: 5.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732785411317884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04732785411317884 | validation: 0.09888717559961105]
	TIME [epoch: 5.56 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03361811533891992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03361811533891992 | validation: 0.1208677472055437]
	TIME [epoch: 5.56 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032760577716284815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032760577716284815 | validation: 0.11985805222800923]
	TIME [epoch: 5.57 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04507183091819239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04507183091819239 | validation: 0.1362022642419097]
	TIME [epoch: 5.57 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04649013193527275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04649013193527275 | validation: 0.17417497957904146]
	TIME [epoch: 5.54 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649584776160893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0649584776160893 | validation: 0.10604165810585608]
	TIME [epoch: 5.56 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04521595597167233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04521595597167233 | validation: 0.13085504584058985]
	TIME [epoch: 5.57 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051606357482929466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051606357482929466 | validation: 0.1342204399006369]
	TIME [epoch: 5.56 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05844859417819372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05844859417819372 | validation: 0.13869669997299994]
	TIME [epoch: 5.56 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06482787653816958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06482787653816958 | validation: 0.12614892737009462]
	TIME [epoch: 5.55 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056205506371161874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056205506371161874 | validation: 0.12224834051176375]
	TIME [epoch: 5.55 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039091237023162466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039091237023162466 | validation: 0.09334595059274474]
	TIME [epoch: 5.56 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03050119280398687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03050119280398687 | validation: 0.10977377137340849]
	TIME [epoch: 5.56 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035847851807553406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035847851807553406 | validation: 0.08149239933817744]
	TIME [epoch: 5.56 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04154245517110674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04154245517110674 | validation: 0.11664728625819956]
	TIME [epoch: 5.56 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05361704793503789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05361704793503789 | validation: 0.12415765814319726]
	TIME [epoch: 5.55 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05174470901717666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05174470901717666 | validation: 0.09571378913597811]
	TIME [epoch: 5.55 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04651967933636776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04651967933636776 | validation: 0.11594960595262181]
	TIME [epoch: 5.55 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190748650754515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04190748650754515 | validation: 0.11342470248763935]
	TIME [epoch: 5.56 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06017606504875287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06017606504875287 | validation: 0.13011773984758612]
	TIME [epoch: 5.57 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685189605003324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06685189605003324 | validation: 0.09031556970312053]
	TIME [epoch: 5.57 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842341401780682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03842341401780682 | validation: 0.09818370064292127]
	TIME [epoch: 5.56 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022147079641443836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022147079641443836 | validation: 0.09418776146396068]
	TIME [epoch: 5.52 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028730456673780564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028730456673780564 | validation: 0.17511659862916973]
	TIME [epoch: 5.53 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048373293478672814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048373293478672814 | validation: 0.12325230829469391]
	TIME [epoch: 5.52 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224060467733173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05224060467733173 | validation: 0.6221006995252853]
	TIME [epoch: 5.52 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21056241268122122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21056241268122122 | validation: 0.32823419165624623]
	TIME [epoch: 5.53 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10078040608943742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10078040608943742 | validation: 0.20938564287016384]
	TIME [epoch: 5.52 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1760700076637893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1760700076637893 | validation: 0.061236256931605976]
	TIME [epoch: 5.52 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700939870545393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06700939870545393 | validation: 0.10097961904994013]
	TIME [epoch: 5.57 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024312373297100627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024312373297100627 | validation: 0.11206096056763133]
	TIME [epoch: 5.52 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0271556125523482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0271556125523482 | validation: 0.0861475849644547]
	TIME [epoch: 5.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02290103076190423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02290103076190423 | validation: 0.06847697297464984]
	TIME [epoch: 5.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039881562321092905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039881562321092905 | validation: 0.14004642299085854]
	TIME [epoch: 5.52 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08543743229473764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08543743229473764 | validation: 0.1680851672747159]
	TIME [epoch: 5.51 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246314781686392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14246314781686392 | validation: 0.1271837306630752]
	TIME [epoch: 5.52 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07450177495260661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07450177495260661 | validation: 0.8053822950972505]
	TIME [epoch: 5.52 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37764805031606336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37764805031606336 | validation: 0.6415216338581221]
	TIME [epoch: 5.53 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906252744945933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906252744945933 | validation: 0.193717975476967]
	TIME [epoch: 5.57 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10257270756110046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10257270756110046 | validation: 0.2449865014902094]
	TIME [epoch: 5.56 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08438612040088417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08438612040088417 | validation: 0.16750564639288934]
	TIME [epoch: 5.56 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445486353314994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06445486353314994 | validation: 0.13977158224007696]
	TIME [epoch: 5.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360027720870528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05360027720870528 | validation: 0.14813238237226473]
	TIME [epoch: 5.57 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0459331214702328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0459331214702328 | validation: 0.1345939847180308]
	TIME [epoch: 5.57 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039582251091592247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039582251091592247 | validation: 0.1219341359815859]
	TIME [epoch: 5.57 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04173101843369619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04173101843369619 | validation: 0.12944497253990597]
	TIME [epoch: 5.57 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396770684752666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06396770684752666 | validation: 0.22315054567922418]
	TIME [epoch: 5.57 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506974936364799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1506974936364799 | validation: 0.15277317849950892]
	TIME [epoch: 5.58 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08588382145063825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08588382145063825 | validation: 0.15611350453063758]
	TIME [epoch: 5.56 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062193220291155774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062193220291155774 | validation: 0.12781805072248473]
	TIME [epoch: 5.58 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06201650657300533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06201650657300533 | validation: 0.10641926072106869]
	TIME [epoch: 5.53 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764708235559597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03764708235559597 | validation: 0.08296690288866668]
	TIME [epoch: 5.52 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023525488522987744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023525488522987744 | validation: 0.09536292824128505]
	TIME [epoch: 5.56 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02320404764073683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02320404764073683 | validation: 0.09573098294490054]
	TIME [epoch: 5.56 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030421760232416554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030421760232416554 | validation: 0.11237898865169921]
	TIME [epoch: 5.55 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057272529487273856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057272529487273856 | validation: 0.1432629788240811]
	TIME [epoch: 5.55 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641918094293843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641918094293843 | validation: 0.13565095115697515]
	TIME [epoch: 5.55 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09697430895195847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09697430895195847 | validation: 0.13093143468691243]
	TIME [epoch: 5.55 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03796984525305951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03796984525305951 | validation: 0.12046661850877023]
	TIME [epoch: 5.55 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259330484044616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04259330484044616 | validation: 0.118302820046657]
	TIME [epoch: 5.55 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033529825156433574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033529825156433574 | validation: 0.11173591859118269]
	TIME [epoch: 5.54 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692889243967682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03692889243967682 | validation: 0.11924901723892174]
	TIME [epoch: 5.55 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04883162748452422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04883162748452422 | validation: 0.12421225680112047]
	TIME [epoch: 5.55 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055746319236238656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055746319236238656 | validation: 0.11088650948747802]
	TIME [epoch: 5.55 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04977663174095806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04977663174095806 | validation: 0.11045342321197633]
	TIME [epoch: 5.54 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03947429135338448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03947429135338448 | validation: 0.08383291454935567]
	TIME [epoch: 5.55 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02886575197422799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02886575197422799 | validation: 0.08605663363892635]
	TIME [epoch: 5.55 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025599915550737082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025599915550737082 | validation: 0.0833191143588498]
	TIME [epoch: 5.55 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02658449238180081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02658449238180081 | validation: 0.08228711522148846]
	TIME [epoch: 5.55 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03344297143198096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03344297143198096 | validation: 0.10070202615104518]
	TIME [epoch: 5.55 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046495122661333196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046495122661333196 | validation: 0.12199017572626181]
	TIME [epoch: 5.56 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471858244079335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07471858244079335 | validation: 0.14237590298231195]
	TIME [epoch: 5.55 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936276340069345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936276340069345 | validation: 0.1291858175435023]
	TIME [epoch: 5.55 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07108078841396333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07108078841396333 | validation: 0.17265332215262116]
	TIME [epoch: 5.55 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141340522168347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05141340522168347 | validation: 0.08561569691077814]
	TIME [epoch: 5.55 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035874826628174414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035874826628174414 | validation: 0.06664553328564286]
	TIME [epoch: 5.56 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02343764514421272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02343764514421272 | validation: 0.1344271416306002]
	TIME [epoch: 5.55 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03834877698191747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03834877698191747 | validation: 0.08123992716754497]
	TIME [epoch: 5.55 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048717263462102124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048717263462102124 | validation: 0.12838323079507272]
	TIME [epoch: 5.54 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953039556380923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09953039556380923 | validation: 0.1414101608984518]
	TIME [epoch: 5.56 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052477468468366456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052477468468366456 | validation: 0.17701667393977571]
	TIME [epoch: 5.55 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986822499188122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05986822499188122 | validation: 0.22646700675959616]
	TIME [epoch: 5.56 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05485052027020708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05485052027020708 | validation: 0.15325406363218216]
	TIME [epoch: 5.55 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765466862703297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04765466862703297 | validation: 0.14789001069569785]
	TIME [epoch: 5.55 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189165896087643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05189165896087643 | validation: 0.1374722953822961]
	TIME [epoch: 5.56 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046901511135429445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046901511135429445 | validation: 0.13165163383767903]
	TIME [epoch: 5.55 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044416227804048795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044416227804048795 | validation: 0.10571573678415364]
	TIME [epoch: 5.54 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04069938578890159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04069938578890159 | validation: 0.5945107158164648]
	TIME [epoch: 5.56 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2496115932104393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2496115932104393 | validation: 0.29134982463372394]
	TIME [epoch: 5.55 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18496665484664473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18496665484664473 | validation: 0.26013093080469973]
	TIME [epoch: 5.56 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12478084541358982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12478084541358982 | validation: 0.14926747418140965]
	TIME [epoch: 5.55 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818472375618116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0818472375618116 | validation: 0.17651140319917766]
	TIME [epoch: 5.56 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435977491330963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06435977491330963 | validation: 0.13962993225464793]
	TIME [epoch: 5.55 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059461069603618626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059461069603618626 | validation: 0.14856545553733583]
	TIME [epoch: 5.55 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04627431339187634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04627431339187634 | validation: 0.13504655319480216]
	TIME [epoch: 5.55 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04237195001965761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04237195001965761 | validation: 0.10900928574560523]
	TIME [epoch: 5.56 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04210895945891572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04210895945891572 | validation: 0.17584351327267164]
	TIME [epoch: 5.55 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940260842172498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06940260842172498 | validation: 0.14701346858945735]
	TIME [epoch: 5.56 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07992184694159649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07992184694159649 | validation: 0.15459342412018917]
	TIME [epoch: 5.55 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07105057072576604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07105057072576604 | validation: 0.15592501609363732]
	TIME [epoch: 5.55 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06663142514112573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06663142514112573 | validation: 0.09701050020615716]
	TIME [epoch: 5.55 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054352965962241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054352965962241 | validation: 0.19078789521857661]
	TIME [epoch: 5.56 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07572621326031499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07572621326031499 | validation: 0.1393339407451067]
	TIME [epoch: 5.56 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383021326592198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06383021326592198 | validation: 0.10585939421123372]
	TIME [epoch: 5.55 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04698021389860187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04698021389860187 | validation: 0.09398770723387276]
	TIME [epoch: 5.54 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032441028214951866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032441028214951866 | validation: 0.06084427674607384]
	TIME [epoch: 5.54 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021526363316243132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021526363316243132 | validation: 0.07596962066053684]
	TIME [epoch: 5.58 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016366884543967845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016366884543967845 | validation: 0.05974448978672368]
	TIME [epoch: 5.57 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01595609073368708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01595609073368708 | validation: 0.0752263375865483]
	TIME [epoch: 5.55 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035858855467002015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035858855467002015 | validation: 0.15662838721209357]
	TIME [epoch: 5.55 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848330263160111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848330263160111 | validation: 0.21437437856372843]
	TIME [epoch: 5.54 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1082275368473522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1082275368473522 | validation: 0.11537971020881146]
	TIME [epoch: 5.56 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992369556577825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06992369556577825 | validation: 0.07279582730213527]
	TIME [epoch: 5.55 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03550667642631261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03550667642631261 | validation: 0.08610261258323311]
	TIME [epoch: 5.56 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025434606508503103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025434606508503103 | validation: 0.08128698992246758]
	TIME [epoch: 5.57 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026216353658669346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026216353658669346 | validation: 0.07536267567753388]
	TIME [epoch: 5.55 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024693701847241768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024693701847241768 | validation: 0.07644368164351642]
	TIME [epoch: 5.55 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027176783894633197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027176783894633197 | validation: 0.0853001612311563]
	TIME [epoch: 5.55 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03482284130010437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03482284130010437 | validation: 0.08856777595792037]
	TIME [epoch: 5.56 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638018421151651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04638018421151651 | validation: 0.1059745708114883]
	TIME [epoch: 5.55 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06886039500893298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06886039500893298 | validation: 0.10772592777604495]
	TIME [epoch: 5.55 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07279369286283573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07279369286283573 | validation: 0.0855068564379443]
	TIME [epoch: 5.56 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04332544475499371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04332544475499371 | validation: 0.06225149407981169]
	TIME [epoch: 5.54 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023742098343906656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023742098343906656 | validation: 0.08878882464797166]
	TIME [epoch: 5.54 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02230731938092748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02230731938092748 | validation: 0.12319954235953096]
	TIME [epoch: 5.54 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495626577136472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0495626577136472 | validation: 0.26682881531848446]
	TIME [epoch: 5.54 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05172047228259457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05172047228259457 | validation: 0.07367547274342978]
	TIME [epoch: 5.54 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03793381904851098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03793381904851098 | validation: 0.08009221590299034]
	TIME [epoch: 5.55 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03248936006539182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03248936006539182 | validation: 0.17467120708228273]
	TIME [epoch: 5.55 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05302169440660061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05302169440660061 | validation: 0.20804854048887816]
	TIME [epoch: 5.54 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07029357405317724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07029357405317724 | validation: 0.11191580668446788]
	TIME [epoch: 5.54 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330305471487449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06330305471487449 | validation: 0.11086253537975242]
	TIME [epoch: 5.54 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048876305547823344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048876305547823344 | validation: 0.0912001682516998]
	TIME [epoch: 5.55 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03297539039828793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03297539039828793 | validation: 0.10758933674009069]
	TIME [epoch: 5.54 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434145852947135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04434145852947135 | validation: 0.09054881926991541]
	TIME [epoch: 5.55 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693292508966446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03693292508966446 | validation: 0.09853797001579388]
	TIME [epoch: 5.55 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02792818291959544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02792818291959544 | validation: 0.09854066697085595]
	TIME [epoch: 5.54 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028491415812718583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028491415812718583 | validation: 0.07860943412857258]
	TIME [epoch: 5.55 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014904009311245602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014904009311245602 | validation: 0.07152443948455083]
	TIME [epoch: 5.54 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012251543995128731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012251543995128731 | validation: 0.16264655669281702]
	TIME [epoch: 5.55 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053537037489413616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053537037489413616 | validation: 0.14064175243246244]
	TIME [epoch: 5.54 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031797574244247444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031797574244247444 | validation: 0.16080940452724812]
	TIME [epoch: 5.56 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235591379010585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07235591379010585 | validation: 0.17644219426004454]
	TIME [epoch: 5.53 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11961337720902075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11961337720902075 | validation: 0.15363411965069101]
	TIME [epoch: 5.55 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08331689532053307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08331689532053307 | validation: 0.11035697017002698]
	TIME [epoch: 5.54 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02649279741290091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02649279741290091 | validation: 0.08544826809078904]
	TIME [epoch: 5.55 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013842825454888599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013842825454888599 | validation: 0.2840942642134452]
	TIME [epoch: 5.54 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053053081588777744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053053081588777744 | validation: 0.08007645500270802]
	TIME [epoch: 5.54 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030633869662596992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030633869662596992 | validation: 0.15885794410869672]
	TIME [epoch: 5.54 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282674816829898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05282674816829898 | validation: 0.08018030922602071]
	TIME [epoch: 5.54 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581732952360242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03581732952360242 | validation: 0.12460865629408358]
	TIME [epoch: 5.55 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657805547664639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04657805547664639 | validation: 0.08518278401091584]
	TIME [epoch: 5.54 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556840819665137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03556840819665137 | validation: 0.11380533486345988]
	TIME [epoch: 5.55 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788738660634876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06788738660634876 | validation: 0.17660175392489055]
	TIME [epoch: 5.54 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10437366199906084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10437366199906084 | validation: 0.1202198660551755]
	TIME [epoch: 5.54 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23690040328566533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23690040328566533 | validation: 0.1393866299855471]
	TIME [epoch: 5.55 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08664517317005423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08664517317005423 | validation: 0.17777092710280618]
	TIME [epoch: 5.55 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13485527730695485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13485527730695485 | validation: 0.10219362167130264]
	TIME [epoch: 5.55 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588075332700044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0588075332700044 | validation: 0.10803610040914782]
	TIME [epoch: 5.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08924572301715808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08924572301715808 | validation: 0.11862905727908453]
	TIME [epoch: 5.54 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352974943689794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0352974943689794 | validation: 0.10721297988017875]
	TIME [epoch: 5.58 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04208286537480479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04208286537480479 | validation: 0.09926249165813629]
	TIME [epoch: 5.58 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02772751818245462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02772751818245462 | validation: 0.08648973600172777]
	TIME [epoch: 5.58 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027695147579320026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027695147579320026 | validation: 0.07817129187038499]
	TIME [epoch: 5.58 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02201910912459951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02201910912459951 | validation: 0.08480048748212732]
	TIME [epoch: 5.57 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026615845648340297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026615845648340297 | validation: 0.08760621928272458]
	TIME [epoch: 5.57 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03571550143603079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03571550143603079 | validation: 0.11668499982142203]
	TIME [epoch: 5.57 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05468530681232669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05468530681232669 | validation: 0.11670105701297101]
	TIME [epoch: 5.57 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215478787867817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06215478787867817 | validation: 0.11023601049876537]
	TIME [epoch: 5.57 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241568894242749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05241568894242749 | validation: 0.09674699149740144]
	TIME [epoch: 5.57 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040420330943948576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040420330943948576 | validation: 0.1166310223272395]
	TIME [epoch: 5.58 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02155213632614629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02155213632614629 | validation: 0.07870970078301022]
	TIME [epoch: 5.57 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01594159264574397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01594159264574397 | validation: 0.07516054957402227]
	TIME [epoch: 5.58 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014099423100053209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014099423100053209 | validation: 0.0689890592067344]
	TIME [epoch: 5.58 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019896269836268556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019896269836268556 | validation: 0.12655513363983287]
	TIME [epoch: 5.58 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705289391654818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03705289391654818 | validation: 0.13173732881674835]
	TIME [epoch: 5.57 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316969358945755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07316969358945755 | validation: 0.15030950507061777]
	TIME [epoch: 5.58 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510397708503037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06510397708503037 | validation: 0.08072347492952196]
	TIME [epoch: 5.57 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867459037981782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03867459037981782 | validation: 0.07983863865522378]
	TIME [epoch: 5.58 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03060194828582942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03060194828582942 | validation: 0.09421763276627573]
	TIME [epoch: 5.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429712894912228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03429712894912228 | validation: 0.0870180255217628]
	TIME [epoch: 5.57 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361437922019589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0361437922019589 | validation: 0.08757177679686841]
	TIME [epoch: 5.55 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0371312867112262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0371312867112262 | validation: 0.09973203126766114]
	TIME [epoch: 5.56 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05448017026183642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05448017026183642 | validation: 0.10113465526933269]
	TIME [epoch: 5.55 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051544240918274795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051544240918274795 | validation: 0.07998233573368059]
	TIME [epoch: 5.55 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032582197482128324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032582197482128324 | validation: 0.07175973411654457]
	TIME [epoch: 5.54 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02663767526130211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02663767526130211 | validation: 0.11155572716356861]
	TIME [epoch: 5.55 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034161472787027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034161472787027 | validation: 0.07707255210884162]
	TIME [epoch: 5.55 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024986120546583994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024986120546583994 | validation: 0.10047886122146865]
	TIME [epoch: 5.54 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031457558970904236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031457558970904236 | validation: 0.08890897045501446]
	TIME [epoch: 5.55 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052105096704261056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052105096704261056 | validation: 0.09603616082633487]
	TIME [epoch: 5.55 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612564108148204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04612564108148204 | validation: 0.09739005434641626]
	TIME [epoch: 5.54 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041287166968847606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041287166968847606 | validation: 0.09819147428474888]
	TIME [epoch: 5.55 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03205876273690599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03205876273690599 | validation: 0.13538502194195975]
	TIME [epoch: 5.54 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04021426247205934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04021426247205934 | validation: 0.10119208357244691]
	TIME [epoch: 5.52 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332510621368752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06332510621368752 | validation: 0.10306526198983348]
	TIME [epoch: 5.56 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05408429203181923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05408429203181923 | validation: 0.08348341333625675]
	TIME [epoch: 5.56 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039415540385199156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039415540385199156 | validation: 0.14578582124116224]
	TIME [epoch: 5.54 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540093219594094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03540093219594094 | validation: 0.07746737030373635]
	TIME [epoch: 5.55 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024610328041357276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024610328041357276 | validation: 0.160505414262333]
	TIME [epoch: 5.54 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0499459234543963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0499459234543963 | validation: 0.27033470571242796]
	TIME [epoch: 5.56 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06159787773194242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06159787773194242 | validation: 0.1278683452878791]
	TIME [epoch: 5.56 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282234009461908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04282234009461908 | validation: 0.09548746313871666]
	TIME [epoch: 5.56 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039596410611211476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039596410611211476 | validation: 0.09379370500357623]
	TIME [epoch: 5.56 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02971088478608597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02971088478608597 | validation: 0.08623656843117522]
	TIME [epoch: 5.54 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02899802428520503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02899802428520503 | validation: 0.08909606613490986]
	TIME [epoch: 5.55 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166537775200725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03166537775200725 | validation: 0.08018879229453561]
	TIME [epoch: 5.55 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578941342378801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03578941342378801 | validation: 0.10471943497715075]
	TIME [epoch: 5.55 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036187205475778104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036187205475778104 | validation: 0.08546622085718042]
	TIME [epoch: 5.55 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_161213/states/model_phi1_4b_v_mmd2_905.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3932.932 seconds.
