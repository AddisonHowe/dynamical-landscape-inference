Args:
Namespace(name='model_algphiq_1a_v_mmd6', outdir='out/model_training/model_algphiq_1a_v_mmd6', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=0, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='constant', dt_schedule_bounds=[0], dt_schedule_scales=[1.0], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[10.0], optimizer='rms', momentum=0.0, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3866038217

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7824778333493334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824778333493334 | validation: 0.7851268465908325]
	TIME [epoch: 88.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7679706528466181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7679706528466181 | validation: 0.7750824807955019]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 0.758861615321447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758861615321447 | validation: 0.7674021782256495]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7514914728376667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514914728376667 | validation: 0.7608390008193746]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7451040255508684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451040255508684 | validation: 0.754985313889379]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7393577613908325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393577613908325 | validation: 0.7496692617322476]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7340681544392642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7340681544392642 | validation: 0.7447874649408046]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7291247197677451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291247197677451 | validation: 0.7401839387195326]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7245236551777923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7245236551777923 | validation: 0.7358607295301567]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7201352380010009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201352380010009 | validation: 0.731766638671816]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.715965334674188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715965334674188 | validation: 0.727931571086113]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.711994625714794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711994625714794 | validation: 0.7242396621279406]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7082152538941993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7082152538941993 | validation: 0.7207060534351934]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7045649287842022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045649287842022 | validation: 0.7174186615848059]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7010745020362341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7010745020362341 | validation: 0.7142078406486967]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6977140578533969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6977140578533969 | validation: 0.7111256031227178]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.694483463198833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.694483463198833 | validation: 0.7082235012410267]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6913778973395621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913778973395621 | validation: 0.705416607521147]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6883546103015291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883546103015291 | validation: 0.7026636521680707]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6854405391206313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854405391206313 | validation: 0.700021748324585]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6826418880232971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826418880232971 | validation: 0.6974800388965533]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6799281379044797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6799281379044797 | validation: 0.6950650786531649]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6772534045666347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772534045666347 | validation: 0.6926805117919306]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.674680880002036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674680880002036 | validation: 0.690375447436083]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6721471565105778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721471565105778 | validation: 0.6880909150244485]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6696487983329391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6696487983329391 | validation: 0.6858725303709079]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_26.pth
	Model improved!!!
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6671573793333243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671573793333243 | validation: 0.6836513517529881]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6646642559431726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6646642559431726 | validation: 0.6814073683364339]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6620837842318475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620837842318475 | validation: 0.6790279449870706]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6593570857241693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6593570857241693 | validation: 0.6765323731493756]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_30.pth
	Model improved!!!
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6563855655184414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563855655184414 | validation: 0.6736635089582779]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6531670176854171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531670176854171 | validation: 0.6705706323648386]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_32.pth
	Model improved!!!
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6496454178937336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496454178937336 | validation: 0.6672314475909752]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6458070511131291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6458070511131291 | validation: 0.6636482433623068]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6416989268439611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6416989268439611 | validation: 0.6597274161474365]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_35.pth
	Model improved!!!
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6372473705384122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6372473705384122 | validation: 0.6554992257111145]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_36.pth
	Model improved!!!
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6324341598032902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6324341598032902 | validation: 0.6508683574016867]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_37.pth
	Model improved!!!
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6272084929963774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272084929963774 | validation: 0.645868391253549]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_38.pth
	Model improved!!!
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6215317864542833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6215317864542833 | validation: 0.6404896686825954]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_39.pth
	Model improved!!!
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6153762916282272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6153762916282272 | validation: 0.6345212504366409]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_40.pth
	Model improved!!!
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6087150292354009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6087150292354009 | validation: 0.6280257435576135]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_41.pth
	Model improved!!!
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6013485044905853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013485044905853 | validation: 0.6209327401590838]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_42.pth
	Model improved!!!
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5932723938449638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932723938449638 | validation: 0.613051117057485]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_43.pth
	Model improved!!!
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5843061252741149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5843061252741149 | validation: 0.6042251532706295]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_44.pth
	Model improved!!!
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5743826073931706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5743826073931706 | validation: 0.5944285741258156]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_45.pth
	Model improved!!!
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5632784015110136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5632784015110136 | validation: 0.5834788356989591]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_46.pth
	Model improved!!!
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5508386349904089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508386349904089 | validation: 0.5709835467754574]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_47.pth
	Model improved!!!
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5368030111373135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5368030111373135 | validation: 0.5569672028017819]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_48.pth
	Model improved!!!
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5207885839805237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5207885839805237 | validation: 0.540741009259949]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_49.pth
	Model improved!!!
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5023735549376456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5023735549376456 | validation: 0.522015629353946]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_50.pth
	Model improved!!!
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.48134402936893556		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.48134402936893556 | validation: 0.5004483682594959]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_51.pth
	Model improved!!!
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4572269300714663		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.4572269300714663 | validation: 0.4756203339906745]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_52.pth
	Model improved!!!
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4298187809667987		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.4298187809667987 | validation: 0.4465805745564687]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_53.pth
	Model improved!!!
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.398885293363484		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.398885293363484 | validation: 0.41323477899453614]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_54.pth
	Model improved!!!
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.36376169585733875		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.36376169585733875 | validation: 0.3747982159701721]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_55.pth
	Model improved!!!
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32334240946337484		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.32334240946337484 | validation: 0.3294593776075185]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_56.pth
	Model improved!!!
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2740416875893315		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.2740416875893315 | validation: 0.2708536045142582]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_57.pth
	Model improved!!!
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21109423577996803		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.21109423577996803 | validation: 0.19669165103936045]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_58.pth
	Model improved!!!
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13888153682553692		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.13888153682553692 | validation: 0.11580739116081937]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_59.pth
	Model improved!!!
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06916127979310653		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.06916127979310653 | validation: 0.04324044758022948]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.027341298772211944		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.027341298772211944 | validation: 0.020499583542412245]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_61.pth
	Model improved!!!
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01721364103237145		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.01721364103237145 | validation: 0.014251623056737165]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_62.pth
	Model improved!!!
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012102925157715404		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.012102925157715404 | validation: 0.009211825048809847]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_63.pth
	Model improved!!!
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008881696035468019		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.008881696035468019 | validation: 0.007126256113989567]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_64.pth
	Model improved!!!
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006486070076468403		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.006486070076468403 | validation: 0.005604164996400283]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_65.pth
	Model improved!!!
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005094388296256207		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.005094388296256207 | validation: 0.0041262620898759265]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_66.pth
	Model improved!!!
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004122916341432361		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.004122916341432361 | validation: 0.004698339078030522]
	TIME [epoch: 4.06 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004204973785113541		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.004204973785113541 | validation: 0.003564509113573534]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_68.pth
	Model improved!!!
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0035003841109766728		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.0035003841109766728 | validation: 0.0026202088141582197]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033298728185444156		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.0033298728185444156 | validation: 0.003119342014044209]
	TIME [epoch: 4.1 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003190030333092808		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.003190030333092808 | validation: 0.002619148802291261]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002854435215828452		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.002854435215828452 | validation: 0.0024222598065623763]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027011064415615412		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.0027011064415615412 | validation: 0.0023756207996227857]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_73.pth
	Model improved!!!
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033024131226313093		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.0033024131226313093 | validation: 0.003071018258792957]
	TIME [epoch: 4.07 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003246983608299845		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.003246983608299845 | validation: 0.002154531248093597]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_75.pth
	Model improved!!!
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002841090439033608		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.002841090439033608 | validation: 0.0023105111160816438]
	TIME [epoch: 4.06 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025376035408438908		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0025376035408438908 | validation: 0.001982693460323588]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_77.pth
	Model improved!!!
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00265243294394914		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.00265243294394914 | validation: 0.00295687446291414]
	TIME [epoch: 4.06 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028990654650391696		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.0028990654650391696 | validation: 0.00224904501559927]
	TIME [epoch: 4.09 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025486631003769555		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.0025486631003769555 | validation: 0.00200358397030703]
	TIME [epoch: 4.06 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025850968723533157		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.0025850968723533157 | validation: 0.0024878941033268463]
	TIME [epoch: 4.06 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026411301539219063		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.0026411301539219063 | validation: 0.001789183272482889]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_82.pth
	Model improved!!!
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020548546411456067		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.0020548546411456067 | validation: 0.0016545826688268523]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_83.pth
	Model improved!!!
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002107079272419104		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.002107079272419104 | validation: 0.0018957545247525322]
	TIME [epoch: 4.06 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029403712473371167		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.0029403712473371167 | validation: 0.00248177212767166]
	TIME [epoch: 4.06 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026093343257368013		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.0026093343257368013 | validation: 0.0015597605025811907]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_86.pth
	Model improved!!!
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018803594888285662		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.0018803594888285662 | validation: 0.0016785574522942001]
	TIME [epoch: 4.09 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021211928635616524		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.0021211928635616524 | validation: 0.001703704884784925]
	TIME [epoch: 4.08 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020318984381082986		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.0020318984381082986 | validation: 0.001756555627231878]
	TIME [epoch: 4.07 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002375247298757844		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.002375247298757844 | validation: 0.0017591727333967832]
	TIME [epoch: 4.06 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019565557902538402		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.0019565557902538402 | validation: 0.0016408914021176627]
	TIME [epoch: 4.06 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022509699245376797		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0022509699245376797 | validation: 0.0015191986656820184]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_92.pth
	Model improved!!!
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018129147237660368		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.0018129147237660368 | validation: 0.0015947243355445857]
	TIME [epoch: 4.06 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001822947572556596		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.001822947572556596 | validation: 0.0013414688651404725]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_94.pth
	Model improved!!!
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017405620258377488		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.0017405620258377488 | validation: 0.0014318000717934726]
	TIME [epoch: 4.06 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019872870195247094		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0019872870195247094 | validation: 0.0017549433737723814]
	TIME [epoch: 4.08 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001741102947416626		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.001741102947416626 | validation: 0.0011094421022988246]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_97.pth
	Model improved!!!
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014322815382518137		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.0014322815382518137 | validation: 0.0011911530469626553]
	TIME [epoch: 4.07 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014914848655803849		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.0014914848655803849 | validation: 0.0012705136544095804]
	TIME [epoch: 4.05 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018156995806599798		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0018156995806599798 | validation: 0.002200983454643226]
	TIME [epoch: 4.05 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019582942185784828		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.0019582942185784828 | validation: 0.0010991492306876834]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_101.pth
	Model improved!!!
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013384327222734941		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.0013384327222734941 | validation: 0.0009704910670381435]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_102.pth
	Model improved!!!
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012567480751609968		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.0012567480751609968 | validation: 0.001038666439680555]
	TIME [epoch: 4.06 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001258794080322071		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.001258794080322071 | validation: 0.0011561012152130385]
	TIME [epoch: 4.07 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002105263288886676		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.002105263288886676 | validation: 0.0013872931693084023]
	TIME [epoch: 4.1 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014314297296873306		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.0014314297296873306 | validation: 0.0009069364299865571]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_106.pth
	Model improved!!!
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011593216815334586		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.0011593216815334586 | validation: 0.00086704475532277]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_107.pth
	Model improved!!!
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001268325759041599		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.001268325759041599 | validation: 0.0012975325087850233]
	TIME [epoch: 4.05 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017986301196496698		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.0017986301196496698 | validation: 0.0011906194459936283]
	TIME [epoch: 4.05 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011462087029717523		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.0011462087029717523 | validation: 0.0008469136009425862]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_110.pth
	Model improved!!!
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010434765513045714		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.0010434765513045714 | validation: 0.0007922772829741943]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_111.pth
	Model improved!!!
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010438180683309192		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.0010438180683309192 | validation: 0.000950941084744462]
	TIME [epoch: 4.06 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001462451950900191		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.001462451950900191 | validation: 0.0014966108414133763]
	TIME [epoch: 4.08 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012288058595205699		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.0012288058595205699 | validation: 0.0008003535681191272]
	TIME [epoch: 4.07 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009559347492769072		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.0009559347492769072 | validation: 0.0007906172693611882]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_115.pth
	Model improved!!!
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011190327957472498		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.0011190327957472498 | validation: 0.0011778299509903902]
	TIME [epoch: 4.07 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013913076979795676		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.0013913076979795676 | validation: 0.0010079308792832618]
	TIME [epoch: 4.06 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010167128862467374		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.0010167128862467374 | validation: 0.000720508718608762]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_118.pth
	Model improved!!!
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009014717574907245		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.0009014717574907245 | validation: 0.0007067876619329643]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_119.pth
	Model improved!!!
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000867706005721456		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.000867706005721456 | validation: 0.0006700523176011649]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_120.pth
	Model improved!!!
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000873763594276675		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.000873763594276675 | validation: 0.000821100647795233]
	TIME [epoch: 4.06 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013445230283531883		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.0013445230283531883 | validation: 0.0008323734867857118]
	TIME [epoch: 4.09 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008204199348131329		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.0008204199348131329 | validation: 0.0006067702207446588]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_123.pth
	Model improved!!!
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008781152556610921		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.0008781152556610921 | validation: 0.000717403745648757]
	TIME [epoch: 4.06 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009190906502059093		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.0009190906502059093 | validation: 0.0010193540335053522]
	TIME [epoch: 4.06 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010211784107682172		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.0010211784107682172 | validation: 0.0005692804149191339]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_126.pth
	Model improved!!!
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007687743366589114		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.0007687743366589114 | validation: 0.0005827918096685035]
	TIME [epoch: 4.05 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007605419740077223		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.0007605419740077223 | validation: 0.000546999392131479]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_128.pth
	Model improved!!!
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007703732022355791		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.0007703732022355791 | validation: 0.0006901597658202789]
	TIME [epoch: 4.06 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007731514033914418		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.0007731514033914418 | validation: 0.0007627263719127341]
	TIME [epoch: 4.08 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009071194047590609		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.0009071194047590609 | validation: 0.0006165122420861121]
	TIME [epoch: 4.09 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007163356419041852		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.0007163356419041852 | validation: 0.0005293076505215801]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_132.pth
	Model improved!!!
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006797867626912023		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.0006797867626912023 | validation: 0.0005715069785723415]
	TIME [epoch: 4.06 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000819949686176013		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.000819949686176013 | validation: 0.0005070483458992159]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_134.pth
	Model improved!!!
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007425024533970547		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.0007425024533970547 | validation: 0.0005795174243482812]
	TIME [epoch: 4.06 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000679903200390742		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.000679903200390742 | validation: 0.00046236271012133523]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_136.pth
	Model improved!!!
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005790064138555633		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.0005790064138555633 | validation: 0.0004156109375347308]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_137.pth
	Model improved!!!
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005304597472815906		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.0005304597472815906 | validation: 0.0004239106079426409]
	TIME [epoch: 4.05 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006042714569305274		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.0006042714569305274 | validation: 0.0009663567416875493]
	TIME [epoch: 4.07 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008516651151651624		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.0008516651151651624 | validation: 0.00042465888746829217]
	TIME [epoch: 4.07 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004976346027221982		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.0004976346027221982 | validation: 0.0004023138247973042]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_141.pth
	Model improved!!!
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005030669893708252		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.0005030669893708252 | validation: 0.00039287188756000166]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_142.pth
	Model improved!!!
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005556330780795016		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.0005556330780795016 | validation: 0.0007039987133858213]
	TIME [epoch: 4.05 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006406437824548883		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.0006406437824548883 | validation: 0.00035216507867657934]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_144.pth
	Model improved!!!
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004528482398601653		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.0004528482398601653 | validation: 0.00036868725557295633]
	TIME [epoch: 4.07 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004414548506814654		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.0004414548506814654 | validation: 0.00031489670654726726]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_146.pth
	Model improved!!!
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005618251010960484		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.0005618251010960484 | validation: 0.0006617448567358735]
	TIME [epoch: 4.07 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006144458775302518		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.0006144458775302518 | validation: 0.00030782405452386334]
	TIME [epoch: 4.09 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_148.pth
	Model improved!!!
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040795520313620804		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.00040795520313620804 | validation: 0.0003111249028415632]
	TIME [epoch: 4.06 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040142371310915737		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.00040142371310915737 | validation: 0.00028426411471845457]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_150.pth
	Model improved!!!
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00044340424947140415		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.00044340424947140415 | validation: 0.0003766564993798223]
	TIME [epoch: 4.06 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005533949695553796		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.0005533949695553796 | validation: 0.0003219448080648053]
	TIME [epoch: 4.06 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00041485206784171635		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.00041485206784171635 | validation: 0.0003636266166050857]
	TIME [epoch: 4.06 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00037721581261850333		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.00037721581261850333 | validation: 0.00027456792730419163]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_154.pth
	Model improved!!!
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003691680179542605		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.0003691680179542605 | validation: 0.00031946794567329454]
	TIME [epoch: 4.05 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004794029321768564		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.0004794029321768564 | validation: 0.0002983186119281887]
	TIME [epoch: 4.06 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003958430071120004		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.0003958430071120004 | validation: 0.0002966042622777003]
	TIME [epoch: 4.08 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003609770776958107		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.0003609770776958107 | validation: 0.0002440967055477028]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_158.pth
	Model improved!!!
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003354344961258327		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.0003354344961258327 | validation: 0.0002510088605755336]
	TIME [epoch: 4.05 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032939755443978287		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.00032939755443978287 | validation: 0.00031270060907695175]
	TIME [epoch: 4.05 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004082661274483087		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.0004082661274483087 | validation: 0.00032062632774217304]
	TIME [epoch: 4.05 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000325509365997929		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.000325509365997929 | validation: 0.00022524327645060739]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_162.pth
	Model improved!!!
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002684941816658818		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.0002684941816658818 | validation: 0.00021072533607131638]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_163.pth
	Model improved!!!
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003242586843133156		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.0003242586843133156 | validation: 0.0003342372000612581]
	TIME [epoch: 4.06 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00036447655886915877		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.00036447655886915877 | validation: 0.0002346114545151179]
	TIME [epoch: 4.08 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002847949805507478		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.0002847949805507478 | validation: 0.00021482008034214828]
	TIME [epoch: 4.07 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002635725371589138		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.0002635725371589138 | validation: 0.00020926334040789632]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_167.pth
	Model improved!!!
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032253876919605053		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.00032253876919605053 | validation: 0.0002076386182528256]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_168.pth
	Model improved!!!
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023784621680063615		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.00023784621680063615 | validation: 0.00017958869967538084]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_169.pth
	Model improved!!!
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022919180048606324		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.00022919180048606324 | validation: 0.00021907086394309605]
	TIME [epoch: 4.05 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032076892578679307		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.00032076892578679307 | validation: 0.0003068996504597448]
	TIME [epoch: 4.05 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00025481694212124896		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.00025481694212124896 | validation: 0.00013469978288936256]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_172.pth
	Model improved!!!
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021637357412251245		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.00021637357412251245 | validation: 0.0001954814875100008]
	TIME [epoch: 4.06 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002735857189016247		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.0002735857189016247 | validation: 0.00018728517319349837]
	TIME [epoch: 4.08 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002366241530881751		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.0002366241530881751 | validation: 0.00018132573099962637]
	TIME [epoch: 4.04 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022263488370386032		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.00022263488370386032 | validation: 0.00011968569972314348]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_176.pth
	Model improved!!!
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002230547281807893		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.0002230547281807893 | validation: 0.00018501244902069971]
	TIME [epoch: 4.06 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019939442588361387		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.00019939442588361387 | validation: 0.00013564341059626962]
	TIME [epoch: 4.06 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019352961857606153		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.00019352961857606153 | validation: 0.0001450253301833191]
	TIME [epoch: 4.06 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024037660556891282		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.00024037660556891282 | validation: 0.0001960867884129238]
	TIME [epoch: 4.06 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001922570529593184		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.0001922570529593184 | validation: 0.00012197911087301127]
	TIME [epoch: 4.06 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016696341047999408		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 0.00016696341047999408 | validation: 0.0001499972918055299]
	TIME [epoch: 4.06 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002029320362243233		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.0002029320362243233 | validation: 0.00015884886278398592]
	TIME [epoch: 4.1 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019605631743050466		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.00019605631743050466 | validation: 0.00011692463598208126]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_184.pth
	Model improved!!!
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001517785872660109		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.0001517785872660109 | validation: 9.679536536861355e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_185.pth
	Model improved!!!
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015370565039141327		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.00015370565039141327 | validation: 0.00011627709467231751]
	TIME [epoch: 4.06 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021864235309620062		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.00021864235309620062 | validation: 0.00010917436461922936]
	TIME [epoch: 4.06 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015472713945577964		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.00015472713945577964 | validation: 7.570718033642621e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_188.pth
	Model improved!!!
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014032123518833306		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.00014032123518833306 | validation: 9.164114284731695e-05]
	TIME [epoch: 4.05 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017956587901469956		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.00017956587901469956 | validation: 0.0001294747118602777]
	TIME [epoch: 4.06 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015854084990770645		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.00015854084990770645 | validation: 0.0001078749149461653]
	TIME [epoch: 4.07 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012967692827204957		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 0.00012967692827204957 | validation: 9.304386396882824e-05]
	TIME [epoch: 4.06 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012746398286770443		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 0.00012746398286770443 | validation: 9.183017589395193e-05]
	TIME [epoch: 4.05 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017230831665967082		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.00017230831665967082 | validation: 0.00011126645819551473]
	TIME [epoch: 4.05 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001222429017465263		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.0001222429017465263 | validation: 9.763048159650123e-05]
	TIME [epoch: 4.05 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000138716865213013		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.000138716865213013 | validation: 0.00010804147198074078]
	TIME [epoch: 4.05 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001183919880495934		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.0001183919880495934 | validation: 7.012234170371712e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_197.pth
	Model improved!!!
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001048325740907894		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 0.0001048325740907894 | validation: 8.464629700665461e-05]
	TIME [epoch: 4.06 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011853457419315106		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 0.00011853457419315106 | validation: 7.831659038662765e-05]
	TIME [epoch: 4.05 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011914864770770216		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 0.00011914864770770216 | validation: 8.447978111953192e-05]
	TIME [epoch: 4.06 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012041551139289798		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.00012041551139289798 | validation: 5.8681000580773415e-05]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_201.pth
	Model improved!!!
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 9.669605317153308e-05		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 9.669605317153308e-05 | validation: 6.973503105307889e-05]
	TIME [epoch: 4.09 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010407011546161393		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.00010407011546161393 | validation: 8.605552052301713e-05]
	TIME [epoch: 4.06 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011214780418680281		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.00011214780418680281 | validation: 5.422729704115881e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_204.pth
	Model improved!!!
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 9.625838485218929e-05		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 9.625838485218929e-05 | validation: 5.513869328205367e-05]
	TIME [epoch: 4.05 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 9.448204074740361e-05		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 9.448204074740361e-05 | validation: 6.00721113473659e-05]
	TIME [epoch: 4.05 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011357696896107583		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 0.00011357696896107583 | validation: 0.00013553468739311913]
	TIME [epoch: 4.05 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 9.979361399582488e-05		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 9.979361399582488e-05 | validation: 6.176575453697031e-05]
	TIME [epoch: 4.06 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 8.085112474568866e-05		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 8.085112474568866e-05 | validation: 8.503775046796603e-05]
	TIME [epoch: 4.08 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 8.229216356471513e-05		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 8.229216356471513e-05 | validation: 7.853096377568969e-05]
	TIME [epoch: 4.07 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 7.018575606504673e-05		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 7.018575606504673e-05 | validation: 6.913837630567054e-05]
	TIME [epoch: 4.06 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 8.664073985152333e-05		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 8.664073985152333e-05 | validation: 7.5610216893087e-05]
	TIME [epoch: 4.06 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 9.592207353034155e-05		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 9.592207353034155e-05 | validation: 6.847594204908547e-05]
	TIME [epoch: 4.06 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 8.07324723693048e-05		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 8.07324723693048e-05 | validation: 7.371217976581579e-05]
	TIME [epoch: 4.05 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 5.8414061885192385e-05		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 5.8414061885192385e-05 | validation: 5.822372472683135e-05]
	TIME [epoch: 4.05 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 5.876171952410603e-05		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 5.876171952410603e-05 | validation: 5.419690544928457e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_216.pth
	Model improved!!!
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 7.724723533954858e-05		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 7.724723533954858e-05 | validation: 7.402771492299044e-05]
	TIME [epoch: 4.05 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 7.209075822108203e-05		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 7.209075822108203e-05 | validation: 3.1978029840018515e-05]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_218.pth
	Model improved!!!
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 8.02103288053887e-05		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 8.02103288053887e-05 | validation: 4.396811096857123e-05]
	TIME [epoch: 4.06 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 6.411755467379098e-05		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 6.411755467379098e-05 | validation: 3.060166131067899e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_220.pth
	Model improved!!!
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 5.590350349463624e-05		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 5.590350349463624e-05 | validation: 4.255661777122044e-05]
	TIME [epoch: 4.05 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 5.954684884575878e-05		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 5.954684884575878e-05 | validation: 2.206545999889742e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_222.pth
	Model improved!!!
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 6.11258951184831e-05		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 6.11258951184831e-05 | validation: 2.5139715639904027e-05]
	TIME [epoch: 4.05 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 5.389826380762442e-05		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 5.389826380762442e-05 | validation: 3.8583228852887785e-05]
	TIME [epoch: 4.05 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 6.6228215671153e-05		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 6.6228215671153e-05 | validation: 3.498014737665111e-05]
	TIME [epoch: 4.05 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 4.7117013192323424e-05		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 4.7117013192323424e-05 | validation: 3.882884111234675e-05]
	TIME [epoch: 4.05 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5802460050344933e-05		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 3.5802460050344933e-05 | validation: 2.082779329212481e-05]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_227.pth
	Model improved!!!
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 5.1168258980534347e-05		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 5.1168258980534347e-05 | validation: 4.935628960942684e-05]
	TIME [epoch: 4.07 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 6.542830320787585e-05		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 6.542830320787585e-05 | validation: 2.3429417281892297e-05]
	TIME [epoch: 4.06 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 3.912648978613631e-05		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 3.912648978613631e-05 | validation: 3.2740579550293974e-05]
	TIME [epoch: 4.06 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 5.670332407154511e-05		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 5.670332407154511e-05 | validation: 1.5060142722750136e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_231.pth
	Model improved!!!
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 4.325808048324686e-05		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 4.325808048324686e-05 | validation: 4.6218616166075584e-05]
	TIME [epoch: 4.06 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 3.921755548040684e-05		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 3.921755548040684e-05 | validation: 3.3355344841234704e-05]
	TIME [epoch: 4.05 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 4.6944877353319644e-05		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 4.6944877353319644e-05 | validation: 2.166931446496623e-05]
	TIME [epoch: 4.05 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 3.257096213284339e-05		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 3.257096213284339e-05 | validation: 5.416815919808226e-05]
	TIME [epoch: 4.06 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 5.527218783228671e-05		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 5.527218783228671e-05 | validation: 1.9439053429319398e-05]
	TIME [epoch: 4.08 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 4.163527326018257e-05		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 4.163527326018257e-05 | validation: 1.7673844485469602e-05]
	TIME [epoch: 4.06 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 3.482555142361377e-05		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 3.482555142361377e-05 | validation: 3.371249393024645e-05]
	TIME [epoch: 4.05 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9017886657637945e-05		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 3.9017886657637945e-05 | validation: 3.250635384450762e-05]
	TIME [epoch: 4.05 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9570708125371e-05		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 3.9570708125371e-05 | validation: 1.2805334300713068e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_240.pth
	Model improved!!!
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3355801329821185e-05		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 3.3355801329821185e-05 | validation: 4.2974093342977416e-05]
	TIME [epoch: 4.05 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 3.999440259741804e-05		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 3.999440259741804e-05 | validation: 2.317742700783332e-05]
	TIME [epoch: 4.06 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 3.149547373223949e-05		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 3.149547373223949e-05 | validation: 1.245480172984137e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_243.pth
	Model improved!!!
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 3.368617437983923e-05		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 3.368617437983923e-05 | validation: 2.9462284840857224e-05]
	TIME [epoch: 4.08 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2719527167234476e-05		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 3.2719527167234476e-05 | validation: 1.2909603701360517e-05]
	TIME [epoch: 4.1 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8906145770822067e-05		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 2.8906145770822067e-05 | validation: 2.9407329014858653e-05]
	TIME [epoch: 4.04 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 3.489146938154519e-05		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 3.489146938154519e-05 | validation: 9.953951164956677e-06]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_247.pth
	Model improved!!!
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8262720980421488e-05		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 2.8262720980421488e-05 | validation: 2.857750480057808e-05]
	TIME [epoch: 4.05 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 2.697899341399779e-05		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 2.697899341399779e-05 | validation: 8.127180084538609e-06]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_249.pth
	Model improved!!!
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 3.059741236420621e-05		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 3.059741236420621e-05 | validation: 2.6819308667287878e-05]
	TIME [epoch: 4.06 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 3.112051134828297e-05		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 3.112051134828297e-05 | validation: 3.610655858303846e-05]
	TIME [epoch: 4.05 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9866203330973335e-05		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 1.9866203330973335e-05 | validation: 2.555456791860178e-05]
	TIME [epoch: 4.05 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 2.637266685984652e-05		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 2.637266685984652e-05 | validation: 3.142054989083864e-05]
	TIME [epoch: 4.07 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5018997235992429e-05		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 1.5018997235992429e-05 | validation: 2.0556432919493383e-05]
	TIME [epoch: 4.07 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0138148944389156e-05		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 3.0138148944389156e-05 | validation: 3.809909506405407e-05]
	TIME [epoch: 4.06 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 2.9193192596249197e-05		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 2.9193192596249197e-05 | validation: 2.000507934880824e-05]
	TIME [epoch: 4.05 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 2.1530886879079892e-05		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 2.1530886879079892e-05 | validation: 1.4677909912587771e-05]
	TIME [epoch: 4.05 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1134156822276604e-05		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 3.1134156822276604e-05 | validation: -4.1548544535674465e-06]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_258.pth
	Model improved!!!
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7547842578972352e-05		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 1.7547842578972352e-05 | validation: 1.5407118148436103e-06]
	TIME [epoch: 4.06 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 3.063227610640929e-05		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 3.063227610640929e-05 | validation: 3.001356175332992e-05]
	TIME [epoch: 4.05 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8627655360452744e-05		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 1.8627655360452744e-05 | validation: 4.0564111291038654e-05]
	TIME [epoch: 4.05 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3275394748970446e-05		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 2.3275394748970446e-05 | validation: 9.343123539727439e-06]
	TIME [epoch: 4.07 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8704999461773728e-05		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 1.8704999461773728e-05 | validation: 2.9829565904704314e-06]
	TIME [epoch: 4.08 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9811608603410023e-05		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 1.9811608603410023e-05 | validation: 2.015625884511607e-05]
	TIME [epoch: 4.05 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3826131525365393e-05		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 2.3826131525365393e-05 | validation: 9.20145702025499e-06]
	TIME [epoch: 4.06 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 2.9673910625352297e-05		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 2.9673910625352297e-05 | validation: 8.20455209389892e-06]
	TIME [epoch: 4.05 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5863539419026408e-05		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 1.5863539419026408e-05 | validation: 1.2776721493695798e-05]
	TIME [epoch: 4.06 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6543652624036566e-05		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 1.6543652624036566e-05 | validation: 2.016808606192444e-05]
	TIME [epoch: 4.05 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8794311781574757e-05		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 1.8794311781574757e-05 | validation: 2.960022889925695e-05]
	TIME [epoch: 4.06 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9416663328066132e-05		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 1.9416663328066132e-05 | validation: 2.3078315767036672e-05]
	TIME [epoch: 4.06 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5242405328314223e-05		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 1.5242405328314223e-05 | validation: 6.593154249951771e-06]
	TIME [epoch: 4.06 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 8.833320879601757e-06		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 8.833320879601757e-06 | validation: 6.8385819500524915e-06]
	TIME [epoch: 4.09 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 2.330524838231185e-05		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 2.330524838231185e-05 | validation: 3.345895759391548e-05]
	TIME [epoch: 4.06 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8980569980907257e-05		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 2.8980569980907257e-05 | validation: 1.042080313112681e-05]
	TIME [epoch: 4.06 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1870350110882556e-05		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 1.1870350110882556e-05 | validation: 6.2256862956859835e-06]
	TIME [epoch: 4.05 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4866441807798726e-05		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 1.4866441807798726e-05 | validation: 1.9000569787253286e-05]
	TIME [epoch: 4.06 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 9.71206245219658e-06		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 9.71206245219658e-06 | validation: 9.023676994218689e-06]
	TIME [epoch: 4.06 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7014526547179163e-05		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 1.7014526547179163e-05 | validation: 5.86719674734848e-06]
	TIME [epoch: 4.06 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1736882311276121e-05		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 1.1736882311276121e-05 | validation: 2.2964123397212213e-05]
	TIME [epoch: 4.06 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3547574506493577e-05		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 1.3547574506493577e-05 | validation: 7.382810320720968e-06]
	TIME [epoch: 4.06 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1072500849686918e-05		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 1.1072500849686918e-05 | validation: 4.41686046470613e-06]
	TIME [epoch: 4.1 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0947878178598881e-05		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 1.0947878178598881e-05 | validation: 2.6594332996052737e-06]
	TIME [epoch: 4.07 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 1.647976172576182e-05		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 1.647976172576182e-05 | validation: 3.3750812606867526e-06]
	TIME [epoch: 4.06 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 5.418025473346577e-06		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 5.418025473346577e-06 | validation: 1.804470488117338e-05]
	TIME [epoch: 4.06 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0549843636945223e-05		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 1.0549843636945223e-05 | validation: 8.338737531256913e-06]
	TIME [epoch: 4.08 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4665281491977877e-05		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 1.4665281491977877e-05 | validation: 1.0778950217928519e-05]
	TIME [epoch: 4.06 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 1.156923216483674e-05		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 1.156923216483674e-05 | validation: 2.128516067146924e-06]
	TIME [epoch: 4.06 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 9.92182974664624e-06		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 9.92182974664624e-06 | validation: 1.021339543658817e-05]
	TIME [epoch: 4.06 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5526939056134116e-05		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 1.5526939056134116e-05 | validation: -9.642863396757796e-06]
	TIME [epoch: 4.07 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_289.pth
	Model improved!!!
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8004649777248472e-05		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 1.8004649777248472e-05 | validation: 4.906988430604775e-06]
	TIME [epoch: 4.09 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4242513229772995e-06		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 1.4242513229772995e-06 | validation: 1.3678410339330816e-05]
	TIME [epoch: 4.06 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 6.913320842613202e-06		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 6.913320842613202e-06 | validation: 3.2313247823230196e-06]
	TIME [epoch: 4.06 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9651508793432694e-05		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 1.9651508793432694e-05 | validation: 2.0518107241327854e-05]
	TIME [epoch: 4.05 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0891743573577693e-05		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 1.0891743573577693e-05 | validation: 1.4834551371554873e-05]
	TIME [epoch: 4.05 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 9.862860362108707e-06		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 9.862860362108707e-06 | validation: 1.1729000421912385e-05]
	TIME [epoch: 4.05 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 9.949405022726455e-06		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 9.949405022726455e-06 | validation: -3.6381358342767457e-06]
	TIME [epoch: 4.05 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 9.797287101025787e-06		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 9.797287101025787e-06 | validation: 9.2957603180146e-06]
	TIME [epoch: 4.05 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0928621752283152e-05		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 1.0928621752283152e-05 | validation: 2.074055034261502e-06]
	TIME [epoch: 4.05 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 9.310424098111891e-06		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 9.310424098111891e-06 | validation: -3.0360048979380946e-06]
	TIME [epoch: 4.08 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3513228890670324e-06		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 3.3513228890670324e-06 | validation: -3.890590281950379e-06]
	TIME [epoch: 4.07 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1350483140779467e-05		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 1.1350483140779467e-05 | validation: -6.585221390718265e-06]
	TIME [epoch: 4.05 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 4.449320559632431e-06		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 4.449320559632431e-06 | validation: 4.983538659646405e-07]
	TIME [epoch: 4.05 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2985761338924486e-05		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 1.2985761338924486e-05 | validation: 9.804617758102596e-06]
	TIME [epoch: 4.05 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 6.4027846705584365e-06		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 6.4027846705584365e-06 | validation: -7.786138788168583e-06]
	TIME [epoch: 4.05 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 2.164861231482851e-06		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 2.164861231482851e-06 | validation: 5.878619130273544e-06]
	TIME [epoch: 4.05 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 8.875543173153532e-06		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 8.875543173153532e-06 | validation: 1.3901179589069335e-05]
	TIME [epoch: 4.05 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0661201916520647e-05		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 1.0661201916520647e-05 | validation: 6.322816506322049e-06]
	TIME [epoch: 4.05 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0312382033463851e-05		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 1.0312382033463851e-05 | validation: -7.227790128145006e-06]
	TIME [epoch: 4.07 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 6.148398047912562e-06		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 6.148398047912562e-06 | validation: 1.1106820254079386e-05]
	TIME [epoch: 4.06 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 9.279909962253696e-06		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 9.279909962253696e-06 | validation: 3.933356177249215e-06]
	TIME [epoch: 4.05 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4521582978067827e-05		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 1.4521582978067827e-05 | validation: -1.1569067255042587e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_311.pth
	Model improved!!!
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 9.543170917069689e-06		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 9.543170917069689e-06 | validation: 7.2044104963597814e-06]
	TIME [epoch: 4.06 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6780667976000762e-06		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 2.6780667976000762e-06 | validation: 5.426233527240587e-06]
	TIME [epoch: 4.05 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 6.687808079648416e-06		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 6.687808079648416e-06 | validation: -6.238155876974182e-06]
	TIME [epoch: 4.05 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 8.892162099102152e-06		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 8.892162099102152e-06 | validation: 1.5685521176089346e-05]
	TIME [epoch: 4.05 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 6.513423682355968e-06		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 6.513423682355968e-06 | validation: 4.25000329742331e-06]
	TIME [epoch: 4.06 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 9.924335171637933e-06		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 9.924335171637933e-06 | validation: -3.8042796188724182e-06]
	TIME [epoch: 4.08 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 7.383066640940683e-06		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 7.383066640940683e-06 | validation: 5.546986161079959e-06]
	TIME [epoch: 4.07 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8008192734042525e-07		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 1.8008192734042525e-07 | validation: 1.0716702438575033e-05]
	TIME [epoch: 4.05 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 6.612086766544345e-06		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 6.612086766544345e-06 | validation: 8.898640834903216e-06]
	TIME [epoch: 4.05 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5802761883148332e-05		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 1.5802761883148332e-05 | validation: 1.4425203752788996e-05]
	TIME [epoch: 4.06 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 7.501681865106136e-06		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 7.501681865106136e-06 | validation: -2.9971840818823204e-06]
	TIME [epoch: 4.05 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 1.057343348883133e-05		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 1.057343348883133e-05 | validation: 4.562469566415039e-06]
	TIME [epoch: 4.05 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 9.555085520161378e-06		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 9.555085520161378e-06 | validation: -3.617822438599649e-06]
	TIME [epoch: 4.05 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 6.192354063339844e-06		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 6.192354063339844e-06 | validation: 8.108126123825831e-06]
	TIME [epoch: 4.05 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0685113624262387e-05		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 1.0685113624262387e-05 | validation: 2.0090038437116763e-05]
	TIME [epoch: 4.06 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 2.200393232435038e-06		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 2.200393232435038e-06 | validation: -1.5533461248427248e-05]
	TIME [epoch: 4.08 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_327.pth
	Model improved!!!
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2692552220853636e-05		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 1.2692552220853636e-05 | validation: 7.269296886619703e-06]
	TIME [epoch: 4.05 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 7.502852248596482e-06		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 7.502852248596482e-06 | validation: 9.880261322227524e-06]
	TIME [epoch: 4.05 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 2.163768666114785e-06		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 2.163768666114785e-06 | validation: -5.569588891694677e-06]
	TIME [epoch: 4.05 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6360130105379511e-06		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 1.6360130105379511e-06 | validation: 7.184266889617153e-07]
	TIME [epoch: 4.05 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 1.772032685500746e-06		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 1.772032685500746e-06 | validation: 2.217196390180698e-06]
	TIME [epoch: 4.05 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 8.258910161653143e-06		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 8.258910161653143e-06 | validation: -4.2478631208937575e-06]
	TIME [epoch: 4.05 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 9.413094528522125e-06		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 9.413094528522125e-06 | validation: 1.0136255689528584e-05]
	TIME [epoch: 4.05 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 6.989661090009646e-06		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 6.989661090009646e-06 | validation: 9.323357502827091e-06]
	TIME [epoch: 4.06 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5788087289010396e-06		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 3.5788087289010396e-06 | validation: -6.548809133767319e-06]
	TIME [epoch: 4.07 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4158550748085714e-06		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 3.4158550748085714e-06 | validation: 1.831991678124023e-05]
	TIME [epoch: 4.06 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0361006895392997e-06		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 2.0361006895392997e-06 | validation: 3.848923100658208e-06]
	TIME [epoch: 4.05 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 6.858823316551521e-06		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 6.858823316551521e-06 | validation: -8.467004502941135e-06]
	TIME [epoch: 4.06 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 8.055640999783399e-07		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 8.055640999783399e-07 | validation: 7.2782612364632455e-06]
	TIME [epoch: 4.05 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 7.735460836397201e-06		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 7.735460836397201e-06 | validation: 1.7987837225308388e-05]
	TIME [epoch: 4.05 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 7.276720972944072e-07		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 7.276720972944072e-07 | validation: -2.247883454131516e-06]
	TIME [epoch: 4.05 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 1.145977798887543e-05		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 1.145977798887543e-05 | validation: 6.108079298806413e-06]
	TIME [epoch: 4.05 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3930857516577534e-06		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 2.3930857516577534e-06 | validation: 1.3111106017221542e-05]
	TIME [epoch: 4.06 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 6.305824426312889e-06		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 6.305824426312889e-06 | validation: 1.5833469839456882e-06]
	TIME [epoch: 4.09 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 8.297864175681457e-06		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 8.297864175681457e-06 | validation: 8.108074078472242e-08]
	TIME [epoch: 4.06 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 7.23832890145637e-06		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 7.23832890145637e-06 | validation: -1.1536737419886255e-05]
	TIME [epoch: 4.05 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4000267813371065e-06		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 3.4000267813371065e-06 | validation: -4.264399143995012e-06]
	TIME [epoch: 4.05 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 6.570429269851742e-06		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 6.570429269851742e-06 | validation: -3.2324393524290063e-06]
	TIME [epoch: 4.05 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6222482199003146e-06		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 3.6222482199003146e-06 | validation: -6.838907129481209e-06]
	TIME [epoch: 4.05 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 7.467648754490575e-06		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 7.467648754490575e-06 | validation: 2.5658584712669352e-05]
	TIME [epoch: 4.06 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 4.28511382796104e-06		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 4.28511382796104e-06 | validation: 8.284020927984193e-06]
	TIME [epoch: 4.05 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3915629887357728e-06		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 1.3915629887357728e-06 | validation: -6.9322446219493014e-06]
	TIME [epoch: 4.05 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: -1.3028656467860556e-06		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: -1.3028656467860556e-06 | validation: -1.2446042806946163e-06]
	TIME [epoch: 4.09 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 5.332886716923558e-06		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 5.332886716923558e-06 | validation: 1.0639670207546859e-05]
	TIME [epoch: 4.06 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 8.226640004298846e-06		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 8.226640004298846e-06 | validation: -3.697997143654153e-06]
	TIME [epoch: 4.05 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 6.550414134172678e-06		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 6.550414134172678e-06 | validation: -1.7724385046959946e-06]
	TIME [epoch: 4.05 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 4.616666567105354e-06		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 4.616666567105354e-06 | validation: 7.861887815414193e-06]
	TIME [epoch: 4.05 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 1.054567277452013e-07		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 1.054567277452013e-07 | validation: -5.4661750159203944e-06]
	TIME [epoch: 4.05 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: -1.598867757492739e-06		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: -1.598867757492739e-06 | validation: -7.773310956646906e-06]
	TIME [epoch: 4.05 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: -1.8229171710202332e-06		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: -1.8229171710202332e-06 | validation: 4.275524581799583e-06]
	TIME [epoch: 4.05 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1782163842894987e-05		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 1.1782163842894987e-05 | validation: 8.621963071704377e-07]
	TIME [epoch: 4.05 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 8.067918922042062e-06		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 8.067918922042062e-06 | validation: 3.163228102019078e-06]
	TIME [epoch: 4.07 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1017424406248199e-05		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 1.1017424406248199e-05 | validation: 9.097159083740758e-06]
	TIME [epoch: 4.07 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9976941015631414e-06		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 1.9976941015631414e-06 | validation: 5.907592491529279e-06]
	TIME [epoch: 4.05 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2054663036128477e-06		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 3.2054663036128477e-06 | validation: 7.1541249700231015e-06]
	TIME [epoch: 4.05 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7440780692167266e-06		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 2.7440780692167266e-06 | validation: -1.3893704627702564e-05]
	TIME [epoch: 4.05 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 9.374325245764536e-06		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 9.374325245764536e-06 | validation: -1.1124873411014767e-06]
	TIME [epoch: 4.05 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 5.150127214467948e-06		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 5.150127214467948e-06 | validation: 4.965041100742029e-06]
	TIME [epoch: 4.05 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9214908020565026e-06		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 1.9214908020565026e-06 | validation: 2.407700128249424e-06]
	TIME [epoch: 4.05 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 2.314115186652699e-06		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 2.314115186652699e-06 | validation: 1.2297540586963197e-05]
	TIME [epoch: 4.05 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2566110479620427e-06		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 2.2566110479620427e-06 | validation: 7.320731808157843e-06]
	TIME [epoch: 4.07 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 9.312165721190314e-06		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 9.312165721190314e-06 | validation: 1.4169653068654147e-06]
	TIME [epoch: 4.06 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4512223526071965e-06		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 2.4512223526071965e-06 | validation: -8.223122083170554e-06]
	TIME [epoch: 4.05 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: -1.1991822024673971e-06		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: -1.1991822024673971e-06 | validation: 1.5164176789741469e-05]
	TIME [epoch: 4.05 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 5.443365912958243e-07		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 5.443365912958243e-07 | validation: 3.741461162724669e-06]
	TIME [epoch: 4.06 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 5.085701013132882e-06		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 5.085701013132882e-06 | validation: 2.70470247607264e-05]
	TIME [epoch: 4.05 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: -2.574346263198745e-06		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: -2.574346263198745e-06 | validation: 3.509326947656044e-06]
	TIME [epoch: 4.06 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: -3.1170441003345326e-06		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: -3.1170441003345326e-06 | validation: -4.535616369421458e-06]
	TIME [epoch: 4.06 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1280223609508866e-06		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 1.1280223609508866e-06 | validation: -1.8234805116384977e-05]
	TIME [epoch: 4.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_380.pth
	Model improved!!!
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 9.455547263340192e-06		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 9.455547263340192e-06 | validation: -1.5469542436710173e-06]
	TIME [epoch: 4.08 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 4.316906450312685e-06		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 4.316906450312685e-06 | validation: -1.2251820065388495e-05]
	TIME [epoch: 4.09 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1930359874908776e-06		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 3.1930359874908776e-06 | validation: -3.3907256254823714e-06]
	TIME [epoch: 4.06 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1371266529612864e-06		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 3.1371266529612864e-06 | validation: 1.408332072929741e-05]
	TIME [epoch: 4.05 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 2.909292138968067e-06		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 2.909292138968067e-06 | validation: -6.114308293452274e-07]
	TIME [epoch: 4.05 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: -1.7375663237615415e-06		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: -1.7375663237615415e-06 | validation: 6.83723663345015e-06]
	TIME [epoch: 4.06 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 2.027789969332505e-06		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 2.027789969332505e-06 | validation: 1.3313127130880177e-05]
	TIME [epoch: 4.05 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 3.817313983444093e-06		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 3.817313983444093e-06 | validation: -4.640365105309075e-06]
	TIME [epoch: 4.05 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 5.072940972046247e-06		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 5.072940972046247e-06 | validation: -1.2898765840802629e-05]
	TIME [epoch: 4.05 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 3.720809149349469e-06		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 3.720809149349469e-06 | validation: -1.332536433704678e-07]
	TIME [epoch: 4.07 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7489360683682363e-06		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 3.7489360683682363e-06 | validation: -2.0731655540218154e-07]
	TIME [epoch: 4.09 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 7.223126400371925e-07		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 7.223126400371925e-07 | validation: -2.467508591532663e-07]
	TIME [epoch: 4.06 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 6.296327542990477e-06		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 6.296327542990477e-06 | validation: 1.4931973803605336e-05]
	TIME [epoch: 4.06 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 5.830609635633977e-06		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 5.830609635633977e-06 | validation: 5.040174189554714e-06]
	TIME [epoch: 4.06 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 4.137421812955622e-06		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 4.137421812955622e-06 | validation: -1.2320898764289323e-05]
	TIME [epoch: 4.06 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0737769662059195e-06		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 1.0737769662059195e-06 | validation: 3.844983645851023e-08]
	TIME [epoch: 4.06 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 2.219794084428628e-06		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 2.219794084428628e-06 | validation: 7.926885523403815e-07]
	TIME [epoch: 4.07 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3573156413148049e-06		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 1.3573156413148049e-06 | validation: 8.973517117347728e-07]
	TIME [epoch: 4.07 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 8.018633433705347e-06		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 8.018633433705347e-06 | validation: -9.251444812877763e-06]
	TIME [epoch: 4.06 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: -1.459268094138988e-06		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: -1.459268094138988e-06 | validation: 2.032356337780783e-05]
	TIME [epoch: 4.09 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 6.570855672482612e-06		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 6.570855672482612e-06 | validation: -6.189932899636652e-06]
	TIME [epoch: 4.06 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 4.493217934572802e-06		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 4.493217934572802e-06 | validation: -6.660823053082243e-06]
	TIME [epoch: 4.05 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2624403911226667e-06		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 2.2624403911226667e-06 | validation: -2.3612897459437397e-06]
	TIME [epoch: 4.05 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 6.352372522674397e-07		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 6.352372522674397e-07 | validation: -7.933067483494318e-06]
	TIME [epoch: 4.05 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: -1.7367789861211228e-07		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: -1.7367789861211228e-07 | validation: 7.893883460217133e-06]
	TIME [epoch: 4.05 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 4.169546876411355e-07		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 4.169546876411355e-07 | validation: -2.1146594727641243e-06]
	TIME [epoch: 4.05 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: -8.620416083352511e-07		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: -8.620416083352511e-07 | validation: -4.210155849166064e-06]
	TIME [epoch: 4.05 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 3.021761190753036e-06		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 3.021761190753036e-06 | validation: -1.5039519013816836e-05]
	TIME [epoch: 4.05 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 8.705979364865058e-06		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 8.705979364865058e-06 | validation: -3.2125118600354606e-06]
	TIME [epoch: 4.08 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0176541556789597e-05		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 1.0176541556789597e-05 | validation: -8.846179442880464e-06]
	TIME [epoch: 4.05 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 3.148743464480574e-07		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 3.148743464480574e-07 | validation: -1.2118374330690961e-05]
	TIME [epoch: 4.05 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 5.2299342281348914e-06		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 5.2299342281348914e-06 | validation: -3.4664633400307564e-06]
	TIME [epoch: 4.05 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5643235827433868e-07		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 1.5643235827433868e-07 | validation: 2.601071650494946e-06]
	TIME [epoch: 4.05 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2280991333339936e-06		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 2.2280991333339936e-06 | validation: -5.2616172731476675e-06]
	TIME [epoch: 4.05 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 5.13635747022334e-06		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 5.13635747022334e-06 | validation: -2.9058620585215024e-05]
	TIME [epoch: 4.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20241015_180954/states/model_algphiq_1a_v_mmd6_415.pth
	Model improved!!!
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: -2.4499561670418004e-06		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: -2.4499561670418004e-06 | validation: -1.0319814029071229e-05]
	TIME [epoch: 4.05 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: -6.853464529061882e-07		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: -6.853464529061882e-07 | validation: -4.023273305209685e-06]
	TIME [epoch: 4.05 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 2.45620199139984e-07		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 2.45620199139984e-07 | validation: -2.1537390238302037e-06]
	TIME [epoch: 4.09 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 7.93214581442503e-06		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 7.93214581442503e-06 | validation: 1.0035237007876896e-05]
	TIME [epoch: 4.05 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2524922318579713e-07		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 1.2524922318579713e-07 | validation: -4.168186303925347e-06]
	TIME [epoch: 4.05 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 6.342514209526362e-06		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 6.342514209526362e-06 | validation: 6.928115964924819e-06]
	TIME [epoch: 4.05 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: -3.508530048037195e-06		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: -3.508530048037195e-06 | validation: 1.221235539808907e-06]
	TIME [epoch: 4.05 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: -1.4260768161306057e-06		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: -1.4260768161306057e-06 | validation: 2.8168989398236864e-05]
	TIME [epoch: 4.06 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2300546311750925e-06		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 1.2300546311750925e-06 | validation: -1.91740015791031e-06]
	TIME [epoch: 4.05 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: 8.37224791022817e-06		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: 8.37224791022817e-06 | validation: 7.697510703736476e-06]
	TIME [epoch: 4.05 sec]
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0921147521952853e-06		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 3.0921147521952853e-06 | validation: 9.435296200918763e-06]
	TIME [epoch: 4.05 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 6.970296794444164e-06		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 6.970296794444164e-06 | validation: -1.2915160347231059e-06]
	TIME [epoch: 4.09 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 1.875526950666062e-06		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 1.875526950666062e-06 | validation: -5.786751399630363e-06]
	TIME [epoch: 4.06 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 1.968833734672715e-06		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 1.968833734672715e-06 | validation: -8.60271125474066e-06]
	TIME [epoch: 4.05 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 5.599915931169931e-06		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 5.599915931169931e-06 | validation: 1.2809190352490286e-05]
	TIME [epoch: 4.05 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 4.562268135829162e-06		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 4.562268135829162e-06 | validation: -2.868993813697429e-06]
	TIME [epoch: 4.05 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 2.046281943537065e-06		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 2.046281943537065e-06 | validation: 3.6308878196356087e-06]
	TIME [epoch: 4.05 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 5.462798395867163e-06		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 5.462798395867163e-06 | validation: 1.1274913359002438e-05]
	TIME [epoch: 4.06 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7810304789180896e-06		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 2.7810304789180896e-06 | validation: 9.04205011041448e-06]
	TIME [epoch: 4.1 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 6.323875010987412e-06		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 6.323875010987412e-06 | validation: -6.291264789186179e-06]
	TIME [epoch: 4.06 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 2.979444407292142e-06		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 2.979444407292142e-06 | validation: -4.302127920488897e-06]
	TIME [epoch: 4.09 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4845266563532353e-06		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 1.4845266563532353e-06 | validation: -1.550010244851441e-05]
	TIME [epoch: 4.07 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 9.768108823263511e-06		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 9.768108823263511e-06 | validation: -7.057039040858638e-07]
	TIME [epoch: 4.06 sec]
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: -2.838168746721602e-07		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: -2.838168746721602e-07 | validation: 7.4403588066627175e-06]
	TIME [epoch: 4.06 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3544018266944416e-06		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 2.3544018266944416e-06 | validation: 1.2182249238253506e-05]
	TIME [epoch: 4.06 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7486685397529745e-06		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 2.7486685397529745e-06 | validation: -8.623322061540598e-07]
	TIME [epoch: 4.06 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 4.403112928563658e-06		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 4.403112928563658e-06 | validation: -2.400093628728284e-06]
	TIME [epoch: 4.06 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: -6.656163692845274e-07		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: -6.656163692845274e-07 | validation: -6.817664943617574e-06]
	TIME [epoch: 4.06 sec]
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 4.541846081719592e-06		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 4.541846081719592e-06 | validation: 1.0723728940916999e-05]
	TIME [epoch: 4.06 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4033154894298473e-06		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 2.4033154894298473e-06 | validation: 1.117394494744306e-05]
	TIME [epoch: 4.09 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 1.539993023168429e-06		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 1.539993023168429e-06 | validation: 1.4218257954305978e-06]
	TIME [epoch: 4.08 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: -6.206404669770735e-07		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: -6.206404669770735e-07 | validation: -1.5783509947782106e-05]
	TIME [epoch: 4.06 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: -1.8137809825958808e-06		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: -1.8137809825958808e-06 | validation: -5.525388609480819e-06]
	TIME [epoch: 4.06 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: -8.47000043442736e-07		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: -8.47000043442736e-07 | validation: 2.0933075970091865e-05]
	TIME [epoch: 4.06 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: -3.884648030499482e-06		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: -3.884648030499482e-06 | validation: 8.145769238379508e-06]
	TIME [epoch: 4.06 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: -2.8856844759689306e-06		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: -2.8856844759689306e-06 | validation: 5.802862733406134e-07]
	TIME [epoch: 4.06 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 5.987768327252007e-06		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 5.987768327252007e-06 | validation: -1.2577181132254811e-05]
	TIME [epoch: 4.07 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7755591220447465e-06		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 2.7755591220447465e-06 | validation: 8.95861332196679e-06]
	TIME [epoch: 4.06 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 7.3917771961955485e-06		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 7.3917771961955485e-06 | validation: 1.749577261058177e-05]
	TIME [epoch: 4.07 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 1.470395772652289e-06		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 1.470395772652289e-06 | validation: -1.1430447121472343e-05]
	TIME [epoch: 4.08 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 5.735991934585072e-06		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 5.735991934585072e-06 | validation: 9.78973314999343e-06]
	TIME [epoch: 4.06 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: -1.2979751292875317e-06		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: -1.2979751292875317e-06 | validation: 1.513521068548407e-06]
	TIME [epoch: 4.06 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: -8.839509421880716e-07		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: -8.839509421880716e-07 | validation: 1.5423090649291374e-06]
	TIME [epoch: 4.06 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 5.5577773242323495e-06		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 5.5577773242323495e-06 | validation: 3.098245717571002e-06]
	TIME [epoch: 4.06 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 4.741220089905296e-06		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 4.741220089905296e-06 | validation: -1.3533526741137613e-05]
	TIME [epoch: 4.06 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8413546796989265e-06		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 3.8413546796989265e-06 | validation: 7.1008923576452165e-06]
	TIME [epoch: 4.06 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 3.140343382629361e-07		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 3.140343382629361e-07 | validation: 1.5488698356220798e-05]
	TIME [epoch: 4.05 sec]
EPOCH 463/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6265496867273756e-07		[learning rate: 1.7715e-05]
	Learning Rate: 1.77147e-05
	LOSS [training: 1.6265496867273756e-07 | validation: -4.055252530549502e-06]
	TIME [epoch: 4.06 sec]
EPOCH 464/500:
	Training over batches...
		[batch 4/4] avg loss: 8.428297089039694e-06		[learning rate: 1.7445e-05]
	Learning Rate: 1.74448e-05
	LOSS [training: 8.428297089039694e-06 | validation: 3.237784038830016e-06]
	TIME [epoch: 4.1 sec]
EPOCH 465/500:
	Training over batches...
		[batch 4/4] avg loss: 5.182137933504305e-06		[learning rate: 1.7179e-05]
	Learning Rate: 1.71791e-05
	LOSS [training: 5.182137933504305e-06 | validation: 7.369353623276177e-06]
	TIME [epoch: 4.06 sec]
EPOCH 466/500:
	Training over batches...
		[batch 4/4] avg loss: 5.005939691090977e-06		[learning rate: 1.6917e-05]
	Learning Rate: 1.69174e-05
	LOSS [training: 5.005939691090977e-06 | validation: -2.886168709375081e-06]
	TIME [epoch: 4.06 sec]
EPOCH 467/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5186418656943944e-06		[learning rate: 1.666e-05]
	Learning Rate: 1.66597e-05
	LOSS [training: 3.5186418656943944e-06 | validation: 3.966257611798696e-06]
	TIME [epoch: 4.06 sec]
EPOCH 468/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4127821353526764e-06		[learning rate: 1.6406e-05]
	Learning Rate: 1.64059e-05
	LOSS [training: 3.4127821353526764e-06 | validation: -1.2372912823821914e-05]
	TIME [epoch: 4.06 sec]
EPOCH 469/500:
	Training over batches...
		[batch 4/4] avg loss: -3.4180788290459893e-06		[learning rate: 1.6156e-05]
	Learning Rate: 1.6156e-05
	LOSS [training: -3.4180788290459893e-06 | validation: -2.465144285989762e-06]
	TIME [epoch: 4.05 sec]
EPOCH 470/500:
	Training over batches...
		[batch 4/4] avg loss: -6.985874359910492e-07		[learning rate: 1.591e-05]
	Learning Rate: 1.59099e-05
	LOSS [training: -6.985874359910492e-07 | validation: 7.0626339283810236e-06]
	TIME [epoch: 4.06 sec]
EPOCH 471/500:
	Training over batches...
		[batch 4/4] avg loss: -1.0547378429681672e-07		[learning rate: 1.5668e-05]
	Learning Rate: 1.56675e-05
	LOSS [training: -1.0547378429681672e-07 | validation: 8.841907586454979e-06]
	TIME [epoch: 4.06 sec]
EPOCH 472/500:
	Training over batches...
		[batch 4/4] avg loss: 1.313089818427038e-06		[learning rate: 1.5429e-05]
	Learning Rate: 1.54288e-05
	LOSS [training: 1.313089818427038e-06 | validation: -1.1899795756608044e-05]
	TIME [epoch: 4.06 sec]
EPOCH 473/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0742687671451924e-05		[learning rate: 1.5194e-05]
	Learning Rate: 1.51938e-05
	LOSS [training: 1.0742687671451924e-05 | validation: -9.198168054993296e-06]
	TIME [epoch: 4.1 sec]
EPOCH 474/500:
	Training over batches...
		[batch 4/4] avg loss: -5.442532759583951e-06		[learning rate: 1.4962e-05]
	Learning Rate: 1.49624e-05
	LOSS [training: -5.442532759583951e-06 | validation: -1.494894961465354e-05]
	TIME [epoch: 4.06 sec]
EPOCH 475/500:
	Training over batches...
		[batch 4/4] avg loss: 6.251762160465196e-07		[learning rate: 1.4734e-05]
	Learning Rate: 1.47344e-05
	LOSS [training: 6.251762160465196e-07 | validation: 1.6783220315559345e-06]
	TIME [epoch: 4.06 sec]
EPOCH 476/500:
	Training over batches...
		[batch 4/4] avg loss: 9.50396557152234e-08		[learning rate: 1.451e-05]
	Learning Rate: 1.451e-05
	LOSS [training: 9.50396557152234e-08 | validation: 2.1802002034216893e-06]
	TIME [epoch: 4.06 sec]
EPOCH 477/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3732874278495898e-06		[learning rate: 1.4289e-05]
	Learning Rate: 1.42889e-05
	LOSS [training: 1.3732874278495898e-06 | validation: 3.710477493511055e-06]
	TIME [epoch: 4.06 sec]
EPOCH 478/500:
	Training over batches...
		[batch 4/4] avg loss: 6.564355250931465e-06		[learning rate: 1.4071e-05]
	Learning Rate: 1.40713e-05
	LOSS [training: 6.564355250931465e-06 | validation: -4.890945545493742e-06]
	TIME [epoch: 4.09 sec]
EPOCH 479/500:
	Training over batches...
		[batch 4/4] avg loss: 4.816632987276526e-06		[learning rate: 1.3857e-05]
	Learning Rate: 1.38569e-05
	LOSS [training: 4.816632987276526e-06 | validation: 8.028904091063272e-06]
	TIME [epoch: 4.06 sec]
EPOCH 480/500:
	Training over batches...
		[batch 4/4] avg loss: -3.554686217103864e-06		[learning rate: 1.3646e-05]
	Learning Rate: 1.36458e-05
	LOSS [training: -3.554686217103864e-06 | validation: -8.733204827295005e-06]
	TIME [epoch: 4.06 sec]
EPOCH 481/500:
	Training over batches...
		[batch 4/4] avg loss: -8.515124424562925e-07		[learning rate: 1.3438e-05]
	Learning Rate: 1.3438e-05
	LOSS [training: -8.515124424562925e-07 | validation: 1.040735066719646e-07]
	TIME [epoch: 4.06 sec]
EPOCH 482/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0616978158248812e-06		[learning rate: 1.3233e-05]
	Learning Rate: 1.32333e-05
	LOSS [training: 2.0616978158248812e-06 | validation: 4.487867556407732e-06]
	TIME [epoch: 4.08 sec]
EPOCH 483/500:
	Training over batches...
		[batch 4/4] avg loss: 7.28344070066278e-06		[learning rate: 1.3032e-05]
	Learning Rate: 1.30317e-05
	LOSS [training: 7.28344070066278e-06 | validation: 1.2653025456090151e-05]
	TIME [epoch: 4.08 sec]
EPOCH 484/500:
	Training over batches...
		[batch 4/4] avg loss: 2.5127887296970687e-06		[learning rate: 1.2833e-05]
	Learning Rate: 1.28332e-05
	LOSS [training: 2.5127887296970687e-06 | validation: -3.3491997997836536e-06]
	TIME [epoch: 4.06 sec]
EPOCH 485/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8704210577469295e-06		[learning rate: 1.2638e-05]
	Learning Rate: 1.26377e-05
	LOSS [training: 1.8704210577469295e-06 | validation: -1.2494690045209867e-05]
	TIME [epoch: 4.06 sec]
EPOCH 486/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9724792760225094e-06		[learning rate: 1.2445e-05]
	Learning Rate: 1.24451e-05
	LOSS [training: 1.9724792760225094e-06 | validation: -1.8704740818147326e-06]
	TIME [epoch: 4.06 sec]
EPOCH 487/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0424163797058143e-06		[learning rate: 1.2256e-05]
	Learning Rate: 1.22556e-05
	LOSS [training: 2.0424163797058143e-06 | validation: 6.189954679960419e-06]
	TIME [epoch: 4.09 sec]
EPOCH 488/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5055977715688915e-06		[learning rate: 1.2069e-05]
	Learning Rate: 1.20689e-05
	LOSS [training: 3.5055977715688915e-06 | validation: 1.1445410486721211e-07]
	TIME [epoch: 4.06 sec]
EPOCH 489/500:
	Training over batches...
		[batch 4/4] avg loss: -2.0536439512628402e-06		[learning rate: 1.1885e-05]
	Learning Rate: 1.1885e-05
	LOSS [training: -2.0536439512628402e-06 | validation: 4.5006819961379565e-06]
	TIME [epoch: 4.07 sec]
EPOCH 490/500:
	Training over batches...
		[batch 4/4] avg loss: 6.058960836861084e-07		[learning rate: 1.1704e-05]
	Learning Rate: 1.1704e-05
	LOSS [training: 6.058960836861084e-07 | validation: 4.596272017576952e-06]
	TIME [epoch: 4.07 sec]
EPOCH 491/500:
	Training over batches...
		[batch 4/4] avg loss: 2.1192524157487296e-07		[learning rate: 1.1526e-05]
	Learning Rate: 1.15257e-05
	LOSS [training: 2.1192524157487296e-07 | validation: 8.787779111415261e-06]
	TIME [epoch: 4.09 sec]
EPOCH 492/500:
	Training over batches...
		[batch 4/4] avg loss: -2.535080872585138e-06		[learning rate: 1.135e-05]
	Learning Rate: 1.13501e-05
	LOSS [training: -2.535080872585138e-06 | validation: 3.985824298671003e-06]
	TIME [epoch: 4.08 sec]
EPOCH 493/500:
	Training over batches...
		[batch 4/4] avg loss: -2.2875057941471603e-06		[learning rate: 1.1177e-05]
	Learning Rate: 1.11772e-05
	LOSS [training: -2.2875057941471603e-06 | validation: -4.736968309509981e-06]
	TIME [epoch: 4.07 sec]
EPOCH 494/500:
	Training over batches...
		[batch 4/4] avg loss: -1.009814158168476e-06		[learning rate: 1.1007e-05]
	Learning Rate: 1.10069e-05
	LOSS [training: -1.009814158168476e-06 | validation: 4.894962562759231e-06]
	TIME [epoch: 4.06 sec]
EPOCH 495/500:
	Training over batches...
		[batch 4/4] avg loss: 8.777772402271733e-06		[learning rate: 1.0839e-05]
	Learning Rate: 1.08393e-05
	LOSS [training: 8.777772402271733e-06 | validation: -1.313200931387648e-07]
	TIME [epoch: 4.06 sec]
EPOCH 496/500:
	Training over batches...
		[batch 4/4] avg loss: 5.440835738157568e-06		[learning rate: 1.0674e-05]
	Learning Rate: 1.06741e-05
	LOSS [training: 5.440835738157568e-06 | validation: 3.3835033943367957e-06]
	TIME [epoch: 4.06 sec]
EPOCH 497/500:
	Training over batches...
		[batch 4/4] avg loss: 3.910029648589331e-06		[learning rate: 1.0512e-05]
	Learning Rate: 1.05115e-05
	LOSS [training: 3.910029648589331e-06 | validation: -1.0035064782661252e-05]
	TIME [epoch: 4.07 sec]
EPOCH 498/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3177430568681613e-06		[learning rate: 1.0351e-05]
	Learning Rate: 1.03514e-05
	LOSS [training: 3.3177430568681613e-06 | validation: -6.312452263703916e-06]
	TIME [epoch: 4.06 sec]
EPOCH 499/500:
	Training over batches...
		[batch 4/4] avg loss: -1.6620348599597487e-06		[learning rate: 1.0194e-05]
	Learning Rate: 1.01937e-05
	LOSS [training: -1.6620348599597487e-06 | validation: 1.1984944080973837e-05]
	TIME [epoch: 4.06 sec]
EPOCH 500/500:
	Training over batches...
		[batch 4/4] avg loss: 8.345996315591742e-06		[learning rate: 1.0038e-05]
	Learning Rate: 1.00384e-05
	LOSS [training: 8.345996315591742e-06 | validation: -2.9373898071809457e-06]
	TIME [epoch: 4.07 sec]
Finished training in 2215.077 seconds.
