Args:
Namespace(name='model_algphiq_1a_v_mmd5', outdir='out/model_training/model_algphiq_1a_v_mmd5', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=0, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[10.0], optimizer='rms', momentum=0.0, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2824552978

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5590524629852726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590524629852726 | validation: 0.5663523472834602]
	TIME [epoch: 97.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 0.538993887939307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.538993887939307 | validation: 0.5493043474843238]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5209071583172493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5209071583172493 | validation: 0.531817562103185]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 0.501795957870934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501795957870934 | validation: 0.512651903037788]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4804940406706722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4804940406706722 | validation: 0.4906530845354745]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4562306375391565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4562306375391565 | validation: 0.46517102741707195]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42760251508490205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42760251508490205 | validation: 0.4345860853918586]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 0.39359532362790206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39359532362790206 | validation: 0.39763685715777486]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 0.35256842289103285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35256842289103285 | validation: 0.3518397675799635]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30204043021295657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30204043021295657 | validation: 0.29363224896499607]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2390161556860711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2390161556860711 | validation: 0.21865584924393106]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16306460266370376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16306460266370376 | validation: 0.13336435026423993]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08428705605042897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08428705605042897 | validation: 0.051276589384571476]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0268116925483511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0268116925483511 | validation: 0.014214709209297235]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011811280736146002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011811280736146002 | validation: 0.008737703109427388]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008156948620569981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008156948620569981 | validation: 0.005995687259047831]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006185674359390802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006185674359390802 | validation: 0.004683746810247572]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005287831347494831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005287831347494831 | validation: 0.004072328153544663]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004636285250517416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004636285250517416 | validation: 0.003815933990819069]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004417468250245934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004417468250245934 | validation: 0.003633325266774831]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0043004439307952405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0043004439307952405 | validation: 0.0035969688305661756]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006476451207605505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006476451207605505 | validation: 0.0070563851435453025]
	TIME [epoch: 4.12 sec]
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005653350924633678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005653350924633678 | validation: 0.0035862315440997584]
	TIME [epoch: 4.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00443957549851908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00443957549851908 | validation: 0.004332648292821807]
	TIME [epoch: 4.12 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00511101255326928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00511101255326928 | validation: 0.0040199254173218886]
	TIME [epoch: 4.11 sec]
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005348040935083223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005348040935083223 | validation: 0.00426936157366087]
	TIME [epoch: 4.12 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005401148075337496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005401148075337496 | validation: 0.0039000526757888504]
	TIME [epoch: 4.12 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004747610321401964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004747610321401964 | validation: 0.003975122405686457]
	TIME [epoch: 4.12 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0048098122047601785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0048098122047601785 | validation: 0.004184602130660386]
	TIME [epoch: 4.14 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004630534304913005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004630534304913005 | validation: 0.003717823058815546]
	TIME [epoch: 4.14 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00552140895421357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00552140895421357 | validation: 0.004709786912202046]
	TIME [epoch: 4.13 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005045026522209957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005045026522209957 | validation: 0.003982699016756486]
	TIME [epoch: 4.1 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004862159452143529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004862159452143529 | validation: 0.003670023958759954]
	TIME [epoch: 4.1 sec]
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004667136272919312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004667136272919312 | validation: 0.004070421108772038]
	TIME [epoch: 4.1 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005316389528631523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005316389528631523 | validation: 0.0039861823276820425]
	TIME [epoch: 4.11 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004263147925306271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004263147925306271 | validation: 0.00395959272883214]
	TIME [epoch: 4.11 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005178835950258509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005178835950258509 | validation: 0.0033617959030646087]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_37.pth
	Model improved!!!
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004281620292507374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004281620292507374 | validation: 0.003571444121594767]
	TIME [epoch: 4.12 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004703366890399418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004703366890399418 | validation: 0.004112339060263318]
	TIME [epoch: 4.11 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004681149805735225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004681149805735225 | validation: 0.004013357153408482]
	TIME [epoch: 4.1 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0044794390552487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0044794390552487 | validation: 0.0035918362716822125]
	TIME [epoch: 4.1 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004723687084354396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004723687084354396 | validation: 0.004360075449611343]
	TIME [epoch: 4.1 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004486270872334832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004486270872334832 | validation: 0.0035069885156584586]
	TIME [epoch: 4.1 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004189633669278565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004189633669278565 | validation: 0.004464429134392207]
	TIME [epoch: 4.1 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00471166055982304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00471166055982304 | validation: 0.004379302739478279]
	TIME [epoch: 4.11 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0044655017968259336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0044655017968259336 | validation: 0.003218467377162049]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_46.pth
	Model improved!!!
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0040989489515796396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0040989489515796396 | validation: 0.004375116165837568]
	TIME [epoch: 4.11 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005106618162742001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005106618162742001 | validation: 0.0033285488676538704]
	TIME [epoch: 4.11 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00399278276679594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00399278276679594 | validation: 0.003367975255061169]
	TIME [epoch: 4.11 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0038571829993686866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0038571829993686866 | validation: 0.003952231584887366]
	TIME [epoch: 4.11 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00484252716418509		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.00484252716418509 | validation: 0.003969635431512537]
	TIME [epoch: 99.2 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004319786862323717		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.004319786862323717 | validation: 0.0028575357343053097]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_52.pth
	Model improved!!!
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033122962246174177		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.0033122962246174177 | validation: 0.0025610456906422635]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_53.pth
	Model improved!!!
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034986039739499947		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.0034986039739499947 | validation: 0.004145837192495649]
	TIME [epoch: 8.08 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005336920988171674		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.005336920988171674 | validation: 0.003369538018796013]
	TIME [epoch: 8.04 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003681762471660926		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.003681762471660926 | validation: 0.002774815403884033]
	TIME [epoch: 8.05 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003262244084211127		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.003262244084211127 | validation: 0.0028361001043879855]
	TIME [epoch: 8.04 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003366637375214434		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.003366637375214434 | validation: 0.003326612234193142]
	TIME [epoch: 8.08 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004043791009628787		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.004043791009628787 | validation: 0.003269529279816978]
	TIME [epoch: 8.1 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003651310360607077		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.003651310360607077 | validation: 0.003112662029146577]
	TIME [epoch: 8.08 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003428833208382915		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.003428833208382915 | validation: 0.003067988214440022]
	TIME [epoch: 8.04 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003542495709473761		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.003542495709473761 | validation: 0.002929608414353735]
	TIME [epoch: 8.04 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003427516555409359		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.003427516555409359 | validation: 0.0028185615256188915]
	TIME [epoch: 8.08 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003701631717325106		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.003701631717325106 | validation: 0.0032135136473340382]
	TIME [epoch: 8.08 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0032476753318892546		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.0032476753318892546 | validation: 0.002293213973272432]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_65.pth
	Model improved!!!
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026149311664672385		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.0026149311664672385 | validation: 0.002168333891442689]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_66.pth
	Model improved!!!
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030696358358284223		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.0030696358358284223 | validation: 0.003244661086906419]
	TIME [epoch: 8.03 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004087108206291723		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.004087108206291723 | validation: 0.002486657599141187]
	TIME [epoch: 8.07 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029765062066129553		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.0029765062066129553 | validation: 0.0021495569777562006]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002779287329018739		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.002779287329018739 | validation: 0.0023139411636582093]
	TIME [epoch: 8.05 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028687867287866377		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.0028687867287866377 | validation: 0.002363026061423231]
	TIME [epoch: 8.05 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030007794812962076		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.0030007794812962076 | validation: 0.0025969746256408787]
	TIME [epoch: 8.04 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0031656504325108022		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.0031656504325108022 | validation: 0.002302581663157505]
	TIME [epoch: 8.07 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028097156223015098		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.0028097156223015098 | validation: 0.002198967197186277]
	TIME [epoch: 8.03 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00268702594484326		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.00268702594484326 | validation: 0.002146525860789132]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_75.pth
	Model improved!!!
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00274318435416307		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.00274318435416307 | validation: 0.002641445748745796]
	TIME [epoch: 8.01 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030099307281989435		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0030099307281989435 | validation: 0.0022211886602222317]
	TIME [epoch: 8.02 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026250050007097046		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.0026250050007097046 | validation: 0.0019248217566637529]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_78.pth
	Model improved!!!
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00256388863500077		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.00256388863500077 | validation: 0.0022514683611773584]
	TIME [epoch: 8.01 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002701381046977093		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.002701381046977093 | validation: 0.002477474668619701]
	TIME [epoch: 7.96 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002648604951061867		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.002648604951061867 | validation: 0.001710207751144012]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_81.pth
	Model improved!!!
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020350978365421247		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.0020350978365421247 | validation: 0.0015984796364648451]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_82.pth
	Model improved!!!
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019852823674950774		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.0019852823674950774 | validation: 0.001855867604684201]
	TIME [epoch: 8.02 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003074193762053202		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.003074193762053202 | validation: 0.0032011915447462493]
	TIME [epoch: 8 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002285179521500924		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.002285179521500924 | validation: 0.0016019578588263677]
	TIME [epoch: 8 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001970727239633661		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.001970727239633661 | validation: 0.0017531072248189459]
	TIME [epoch: 8.01 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022239378416888753		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.0022239378416888753 | validation: 0.0026279046673162564]
	TIME [epoch: 8.04 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025015474101679866		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.0025015474101679866 | validation: 0.001784361201009682]
	TIME [epoch: 8 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002250290928678973		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.002250290928678973 | validation: 0.001593967379258634]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_89.pth
	Model improved!!!
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019414783165248631		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.0019414783165248631 | validation: 0.0014907521344460681]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_90.pth
	Model improved!!!
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019281683704950102		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.0019281683704950102 | validation: 0.0020023400597895787]
	TIME [epoch: 8.02 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021596367483427384		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0021596367483427384 | validation: 0.0019900382105597306]
	TIME [epoch: 8.02 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021986446938727167		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.0021986446938727167 | validation: 0.0016756914561105536]
	TIME [epoch: 7.99 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019056875185206486		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.0019056875185206486 | validation: 0.001414503802151359]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_94.pth
	Model improved!!!
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016041661106108193		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.0016041661106108193 | validation: 0.0013449205153980661]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_95.pth
	Model improved!!!
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019746981562844676		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0019746981562844676 | validation: 0.0024690765003589892]
	TIME [epoch: 8.07 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023039003048828046		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.0023039003048828046 | validation: 0.001471456869429118]
	TIME [epoch: 8.04 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016968817441920894		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.0016968817441920894 | validation: 0.0013298785289116003]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_98.pth
	Model improved!!!
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015452265753040496		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.0015452265753040496 | validation: 0.0012888034199545109]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_99.pth
	Model improved!!!
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019018626443050627		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0019018626443050627 | validation: 0.0017001449949154126]
	TIME [epoch: 8.05 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018981580060725637		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.0018981580060725637 | validation: 0.0012475251981664964]
	TIME [epoch: 110 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_101.pth
	Model improved!!!
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001444872694431455		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.001444872694431455 | validation: 0.0011862899270297287]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_102.pth
	Model improved!!!
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001684104721033359		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.001684104721033359 | validation: 0.0016251520828952453]
	TIME [epoch: 18.4 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017896267074686023		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.0017896267074686023 | validation: 0.001486609526807858]
	TIME [epoch: 18.5 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00145987243343489		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.00145987243343489 | validation: 0.00110893315154893]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_105.pth
	Model improved!!!
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013657308820762889		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.0013657308820762889 | validation: 0.001138269649720997]
	TIME [epoch: 18.4 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001517448759169476		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.001517448759169476 | validation: 0.0015707855107576026]
	TIME [epoch: 18.3 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017467755399836534		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.0017467755399836534 | validation: 0.0014500139140652957]
	TIME [epoch: 18.4 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014188457337234346		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.0014188457337234346 | validation: 0.000989181895118648]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_109.pth
	Model improved!!!
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012241202139593099		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.0012241202139593099 | validation: 0.0009594140292362246]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_110.pth
	Model improved!!!
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012780530590549228		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.0012780530590549228 | validation: 0.0014007683137474995]
	TIME [epoch: 18.3 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017811376352417582		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.0017811376352417582 | validation: 0.0011012094059020335]
	TIME [epoch: 18.3 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011647155675370228		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.0011647155675370228 | validation: 0.0009199246630222238]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_113.pth
	Model improved!!!
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011037857154338805		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.0011037857154338805 | validation: 0.00091610865782682]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_114.pth
	Model improved!!!
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011519642072079742		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.0011519642072079742 | validation: 0.001205481645747289]
	TIME [epoch: 18.3 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017349552479839672		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.0017349552479839672 | validation: 0.0009448810322088717]
	TIME [epoch: 18.3 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010920769744319342		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.0010920769744319342 | validation: 0.0008828652462624289]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_117.pth
	Model improved!!!
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011002883289334712		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.0011002883289334712 | validation: 0.0009339922363991616]
	TIME [epoch: 18.3 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00131437219363631		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.00131437219363631 | validation: 0.0011501529234942058]
	TIME [epoch: 18.3 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011905519241629048		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.0011905519241629048 | validation: 0.0008346963282634802]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_120.pth
	Model improved!!!
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009787199317630008		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.0009787199317630008 | validation: 0.000792456146463591]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_121.pth
	Model improved!!!
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010294198138754984		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.0010294198138754984 | validation: 0.000971411354936242]
	TIME [epoch: 18.3 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001242973474141769		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.001242973474141769 | validation: 0.0008796217788479577]
	TIME [epoch: 18.3 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010965967694962233		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.0010965967694962233 | validation: 0.0008634004235225626]
	TIME [epoch: 18.2 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009245488631477471		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.0009245488631477471 | validation: 0.0006912003958412272]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_125.pth
	Model improved!!!
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008533138788366651		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.0008533138788366651 | validation: 0.0006941204794211655]
	TIME [epoch: 18.3 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000998635375184004		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.000998635375184004 | validation: 0.0016070863841881526]
	TIME [epoch: 18.3 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012526002463593322		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.0012526002463593322 | validation: 0.0006848929199779212]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_128.pth
	Model improved!!!
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008352290407431672		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.0008352290407431672 | validation: 0.0006894434784648667]
	TIME [epoch: 18.3 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008073314074033823		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.0008073314074033823 | validation: 0.0006749319865919879]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_130.pth
	Model improved!!!
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009411320330625439		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.0009411320330625439 | validation: 0.0010139236907544583]
	TIME [epoch: 18.3 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010197634495289268		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.0010197634495289268 | validation: 0.0006275471091611629]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_132.pth
	Model improved!!!
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000785652686145533		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.000785652686145533 | validation: 0.0006747609592403785]
	TIME [epoch: 18.3 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008250472023543672		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.0008250472023543672 | validation: 0.0007583828309892684]
	TIME [epoch: 18.4 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008874531839674643		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.0008874531839674643 | validation: 0.0007147105452557132]
	TIME [epoch: 18.3 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008131094975064852		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.0008131094975064852 | validation: 0.0006674389029123537]
	TIME [epoch: 18.3 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007732534598776625		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.0007732534598776625 | validation: 0.0007039163027145392]
	TIME [epoch: 18.3 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007903338984507082		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.0007903338984507082 | validation: 0.0006554958454260836]
	TIME [epoch: 18.3 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007235589867562663		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.0007235589867562663 | validation: 0.0006871207755263385]
	TIME [epoch: 18.3 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007706768675093021		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.0007706768675093021 | validation: 0.0005658132615604756]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_140.pth
	Model improved!!!
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007160773379681506		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.0007160773379681506 | validation: 0.0006835736886271326]
	TIME [epoch: 18.4 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007054029898763201		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.0007054029898763201 | validation: 0.0005370945373576788]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_142.pth
	Model improved!!!
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006496864532027597		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.0006496864532027597 | validation: 0.0006245565256105261]
	TIME [epoch: 18.4 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007651515262424678		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.0007651515262424678 | validation: 0.0005754471268355865]
	TIME [epoch: 18.3 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00063334144402794		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.00063334144402794 | validation: 0.00047807263572364025]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_145.pth
	Model improved!!!
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006036478532653724		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.0006036478532653724 | validation: 0.0005205320136550137]
	TIME [epoch: 18.3 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005939690838645479		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.0005939690838645479 | validation: 0.0006151754187725198]
	TIME [epoch: 18.4 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007488941264750578		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.0007488941264750578 | validation: 0.0005595563938187582]
	TIME [epoch: 18.3 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005898489315904773		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.0005898489315904773 | validation: 0.00045323480781161844]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_149.pth
	Model improved!!!
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005398845482298255		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.0005398845482298255 | validation: 0.00046534333942142565]
	TIME [epoch: 18.3 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005669498115958452		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.0005669498115958452 | validation: 0.0005301742284624814]
	TIME [epoch: 18.3 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006227684655406617		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.0006227684655406617 | validation: 0.0005225333851156486]
	TIME [epoch: 18.3 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005579305358045167		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.0005579305358045167 | validation: 0.0004250456238696747]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_153.pth
	Model improved!!!
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005067451072501283		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.0005067451072501283 | validation: 0.0004499077368405926]
	TIME [epoch: 18.3 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005355193627713089		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.0005355193627713089 | validation: 0.00044070855360050425]
	TIME [epoch: 18.3 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005773399953171812		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.0005773399953171812 | validation: 0.00041971557247518]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_156.pth
	Model improved!!!
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005094527080554864		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.0005094527080554864 | validation: 0.0004010238469552412]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_157.pth
	Model improved!!!
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005154256240277755		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.0005154256240277755 | validation: 0.00041379950592737406]
	TIME [epoch: 18.3 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00045940043063005104		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.00045940043063005104 | validation: 0.000360131454066831]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_159.pth
	Model improved!!!
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004324904155360041		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.0004324904155360041 | validation: 0.0004240473719458788]
	TIME [epoch: 18.3 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005529677988214447		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.0005529677988214447 | validation: 0.00043265121963874224]
	TIME [epoch: 18.3 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00043694970696335164		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.00043694970696335164 | validation: 0.0003513649984452045]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_162.pth
	Model improved!!!
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003956209955190825		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.0003956209955190825 | validation: 0.0003305353449961759]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_163.pth
	Model improved!!!
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004089417644067984		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.0004089417644067984 | validation: 0.00046465516032629425]
	TIME [epoch: 18.4 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005480882586007843		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.0005480882586007843 | validation: 0.0003445394019963331]
	TIME [epoch: 18.2 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00041540319417354774		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.00041540319417354774 | validation: 0.00032609273376416547]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_166.pth
	Model improved!!!
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003771127888564802		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.0003771127888564802 | validation: 0.0003081389457200587]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_167.pth
	Model improved!!!
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000406746622192795		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.000406746622192795 | validation: 0.00038648809840664323]
	TIME [epoch: 18.4 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00043615977277678433		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.00043615977277678433 | validation: 0.0003091009813381973]
	TIME [epoch: 18.4 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00035836037719566723		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.00035836037719566723 | validation: 0.0002846837019208939]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_170.pth
	Model improved!!!
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00036487654763491363		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.00036487654763491363 | validation: 0.00030828598926566466]
	TIME [epoch: 18.4 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003794205590769014		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.0003794205590769014 | validation: 0.0003234063144970625]
	TIME [epoch: 18.3 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040555996421387183		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.00040555996421387183 | validation: 0.00029791116990397336]
	TIME [epoch: 18.4 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00035354358714421676		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.00035354358714421676 | validation: 0.0002792184271771741]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_174.pth
	Model improved!!!
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032326150357515383		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.00032326150357515383 | validation: 0.0002589961826034422]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_175.pth
	Model improved!!!
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00030773794616502115		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.00030773794616502115 | validation: 0.0002725874612731003]
	TIME [epoch: 18.3 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003594967088793124		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.0003594967088793124 | validation: 0.00031290156321057006]
	TIME [epoch: 18.4 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003460638991443583		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.0003460638991443583 | validation: 0.0002642587192900832]
	TIME [epoch: 18.3 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002982213221944743		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.0002982213221944743 | validation: 0.0002530402914423557]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_179.pth
	Model improved!!!
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003095049985685846		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.0003095049985685846 | validation: 0.0002609575400986903]
	TIME [epoch: 18.3 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031382124230243966		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.00031382124230243966 | validation: 0.00028800044142437357]
	TIME [epoch: 18.3 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000295756679003312		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 0.000295756679003312 | validation: 0.00022371417424824647]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_182.pth
	Model improved!!!
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002701955915027994		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.0002701955915027994 | validation: 0.00025198906474166406]
	TIME [epoch: 18.3 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003169949883703789		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.0003169949883703789 | validation: 0.0002617640842045781]
	TIME [epoch: 18.3 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00029033612007582345		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.00029033612007582345 | validation: 0.00023485552037943094]
	TIME [epoch: 18.3 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002626704101268268		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.0002626704101268268 | validation: 0.00022093829027975342]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_186.pth
	Model improved!!!
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024106235718230438		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.00024106235718230438 | validation: 0.00020372627085878304]
	TIME [epoch: 18.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_187.pth
	Model improved!!!
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023371993283693852		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.00023371993283693852 | validation: 0.000197515002620964]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_188.pth
	Model improved!!!
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024553562847809515		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.00024553562847809515 | validation: 0.0003497183370532444]
	TIME [epoch: 18.2 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003268280966347098		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.0003268280966347098 | validation: 0.0002185815958957664]
	TIME [epoch: 18.3 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023933155765287528		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.00023933155765287528 | validation: 0.00019158428630710933]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_191.pth
	Model improved!!!
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021788775662786664		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 0.00021788775662786664 | validation: 0.0001811110658724604]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_192.pth
	Model improved!!!
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022827127743925933		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 0.00022827127743925933 | validation: 0.00023607014187932628]
	TIME [epoch: 18.3 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024724413404165726		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.00024724413404165726 | validation: 0.00017775101773273994]
	TIME [epoch: 18.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_194.pth
	Model improved!!!
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020904758913936784		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.00020904758913936784 | validation: 0.0001855303391085137]
	TIME [epoch: 18.3 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020753965452845159		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.00020753965452845159 | validation: 0.00016944110715864079]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_196.pth
	Model improved!!!
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002285524127838785		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.0002285524127838785 | validation: 0.00021168927365374234]
	TIME [epoch: 18.4 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021492257281409755		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 0.00021492257281409755 | validation: 0.0001838501818335441]
	TIME [epoch: 18.3 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002084186860567331		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 0.0002084186860567331 | validation: 0.0001763016042013421]
	TIME [epoch: 18.4 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019284416942151397		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 0.00019284416942151397 | validation: 0.00017073882218408465]
	TIME [epoch: 18.4 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020548095780825027		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.00020548095780825027 | validation: 0.00019100432254460852]
	TIME [epoch: 18.4 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020903396604770555		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 0.00020903396604770555 | validation: 0.00016092779949950043]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_202.pth
	Model improved!!!
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019384429981569108		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.00019384429981569108 | validation: 0.00015237127774079553]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_203.pth
	Model improved!!!
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017474520901771651		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.00017474520901771651 | validation: 0.00015485555718160282]
	TIME [epoch: 18.3 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018160775302789077		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 0.00018160775302789077 | validation: 0.00016173387616652681]
	TIME [epoch: 18.5 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019671972635829506		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 0.00019671972635829506 | validation: 0.00014833804217635716]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_206.pth
	Model improved!!!
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018074116887931756		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 0.00018074116887931756 | validation: 0.00014833685025847788]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_207.pth
	Model improved!!!
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016637481167545165		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 0.00016637481167545165 | validation: 0.0001319752449909335]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_208.pth
	Model improved!!!
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015749200021252207		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 0.00015749200021252207 | validation: 0.00013995013208860697]
	TIME [epoch: 18.3 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015474451274430733		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.00015474451274430733 | validation: 0.0001492346886177687]
	TIME [epoch: 18.3 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019089334736873456		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 0.00019089334736873456 | validation: 0.00014594019043529083]
	TIME [epoch: 18.3 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016295411688081952		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 0.00016295411688081952 | validation: 0.00011756939065295336]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_212.pth
	Model improved!!!
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001511617012724128		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 0.0001511617012724128 | validation: 0.00012029251392623209]
	TIME [epoch: 18.2 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001534554809367411		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 0.0001534554809367411 | validation: 0.00013887820522000637]
	TIME [epoch: 18.3 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015635448810033062		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 0.00015635448810033062 | validation: 0.00012938018290699005]
	TIME [epoch: 18.2 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014189390673226556		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 0.00014189390673226556 | validation: 0.00012164210222653594]
	TIME [epoch: 18.3 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001527116096367515		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 0.0001527116096367515 | validation: 0.00012953764090550888]
	TIME [epoch: 18.2 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013716011856443366		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 0.00013716011856443366 | validation: 0.00012360955217128878]
	TIME [epoch: 18.2 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013635698677339292		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 0.00013635698677339292 | validation: 0.0001259744632356068]
	TIME [epoch: 18.3 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001359756371176314		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 0.0001359756371176314 | validation: 0.00012527136708469634]
	TIME [epoch: 18.3 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014457439280873008		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 0.00014457439280873008 | validation: 0.00014902989807761234]
	TIME [epoch: 18.3 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013999692911777017		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 0.00013999692911777017 | validation: 0.00010872637337174497]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_222.pth
	Model improved!!!
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012068634075016794		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 0.00012068634075016794 | validation: 0.0001086058637882812]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_223.pth
	Model improved!!!
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001273937944397774		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 0.0001273937944397774 | validation: 0.00011912976154315303]
	TIME [epoch: 18.3 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014301951477453144		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.00014301951477453144 | validation: 0.00010916300999029138]
	TIME [epoch: 18.3 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001175942215960568		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 0.0001175942215960568 | validation: 0.0001171161023261671]
	TIME [epoch: 18.2 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011290633473835354		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 0.00011290633473835354 | validation: 0.00010068122651387678]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_227.pth
	Model improved!!!
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001141132300673513		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 0.0001141132300673513 | validation: 9.808269271141002e-05]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_228.pth
	Model improved!!!
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013534636871692776		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 0.00013534636871692776 | validation: 0.00011892987071273465]
	TIME [epoch: 18.3 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011609016418282514		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 0.00011609016418282514 | validation: 9.974871773904481e-05]
	TIME [epoch: 18.2 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010981955537742116		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 0.00010981955537742116 | validation: 8.96612488433246e-05]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_231.pth
	Model improved!!!
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011412324798867847		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 0.00011412324798867847 | validation: 9.416177029773044e-05]
	TIME [epoch: 18.4 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010364320931366321		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 0.00010364320931366321 | validation: 0.00010041349213590545]
	TIME [epoch: 18.4 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001111806695710168		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 0.0001111806695710168 | validation: 0.00010161361723883312]
	TIME [epoch: 18.4 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011032945112449188		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 0.00011032945112449188 | validation: 8.823929384879525e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_235.pth
	Model improved!!!
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010567523507224629		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 0.00010567523507224629 | validation: 8.456009920802621e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_236.pth
	Model improved!!!
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010354772603956419		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 0.00010354772603956419 | validation: 8.084469399850458e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_237.pth
	Model improved!!!
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010103188679346731		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 0.00010103188679346731 | validation: 8.347374736245383e-05]
	TIME [epoch: 18.4 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 9.552006041560723e-05		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 9.552006041560723e-05 | validation: 9.276112034592909e-05]
	TIME [epoch: 18.3 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010442739391754364		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 0.00010442739391754364 | validation: 8.474880412754105e-05]
	TIME [epoch: 18.4 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010719800015602676		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 0.00010719800015602676 | validation: 7.575884982322023e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_241.pth
	Model improved!!!
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 9.487003997543409e-05		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 9.487003997543409e-05 | validation: 9.558659262796598e-05]
	TIME [epoch: 18.4 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 9.567574720275218e-05		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 9.567574720275218e-05 | validation: 8.744325199410818e-05]
	TIME [epoch: 18.3 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 8.487910413340017e-05		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 8.487910413340017e-05 | validation: 7.937281763895432e-05]
	TIME [epoch: 18.4 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 9.724413843225289e-05		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 9.724413843225289e-05 | validation: 0.00011134389751337382]
	TIME [epoch: 18.3 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 9.535858387603447e-05		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 9.535858387603447e-05 | validation: 7.622925262398939e-05]
	TIME [epoch: 18.3 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 8.21865681176065e-05		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 8.21865681176065e-05 | validation: 7.092337438162067e-05]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_247.pth
	Model improved!!!
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 9.009463162493959e-05		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 9.009463162493959e-05 | validation: 6.843338936369015e-05]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_248.pth
	Model improved!!!
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 8.511118491196723e-05		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 8.511118491196723e-05 | validation: 9.10516710929723e-05]
	TIME [epoch: 18.3 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 9.261235077258367e-05		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 9.261235077258367e-05 | validation: 7.211373486105054e-05]
	TIME [epoch: 18.3 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 8.692315420676034e-05		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 8.692315420676034e-05 | validation: 7.711573115057813e-05]
	TIME [epoch: 132 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 8.382260836615143e-05		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 8.382260836615143e-05 | validation: 6.660848786807082e-05]
	TIME [epoch: 40.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_252.pth
	Model improved!!!
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 8.394526549185067e-05		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 8.394526549185067e-05 | validation: 8.294948083820719e-05]
	TIME [epoch: 40.8 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 8.242604327080061e-05		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 8.242604327080061e-05 | validation: 7.186376869896939e-05]
	TIME [epoch: 40.7 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 8.142749420326168e-05		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 8.142749420326168e-05 | validation: 6.635116835085463e-05]
	TIME [epoch: 40.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_255.pth
	Model improved!!!
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 7.894043239203308e-05		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 7.894043239203308e-05 | validation: 7.377484398258894e-05]
	TIME [epoch: 40.8 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 7.513269714335292e-05		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 7.513269714335292e-05 | validation: 8.634747829834068e-05]
	TIME [epoch: 40.7 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 7.920931551750609e-05		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 7.920931551750609e-05 | validation: 6.96479036163229e-05]
	TIME [epoch: 40.7 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 7.56771422472663e-05		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 7.56771422472663e-05 | validation: 6.671691442830841e-05]
	TIME [epoch: 40.7 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 8.34388028685179e-05		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 8.34388028685179e-05 | validation: 7.89203579523734e-05]
	TIME [epoch: 40.7 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 7.173194362453673e-05		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 7.173194362453673e-05 | validation: 6.004696819482103e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_261.pth
	Model improved!!!
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 7.719664985146368e-05		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 7.719664985146368e-05 | validation: 7.387192158210465e-05]
	TIME [epoch: 40.7 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 7.307919814745856e-05		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 7.307919814745856e-05 | validation: 5.708173289344143e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_263.pth
	Model improved!!!
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 7.411809216646092e-05		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 7.411809216646092e-05 | validation: 7.460046512164608e-05]
	TIME [epoch: 40.5 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 6.730875619995912e-05		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 6.730875619995912e-05 | validation: 7.175613462461294e-05]
	TIME [epoch: 40.6 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 6.882430605113576e-05		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 6.882430605113576e-05 | validation: 5.965346298266572e-05]
	TIME [epoch: 40.6 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 7.785320666646712e-05		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 7.785320666646712e-05 | validation: 6.926899845660928e-05]
	TIME [epoch: 40.6 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 7.612861253144477e-05		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 7.612861253144477e-05 | validation: 6.210192397338888e-05]
	TIME [epoch: 40.6 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 6.70395628440693e-05		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 6.70395628440693e-05 | validation: 6.020546944627525e-05]
	TIME [epoch: 40.6 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 6.458384210710466e-05		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 6.458384210710466e-05 | validation: 7.80064722466729e-05]
	TIME [epoch: 40.5 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 6.592502628421793e-05		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 6.592502628421793e-05 | validation: 5.9411622546949875e-05]
	TIME [epoch: 40.6 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 6.742668595080515e-05		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 6.742668595080515e-05 | validation: 5.425835892314379e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_272.pth
	Model improved!!!
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 6.846999530923737e-05		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 6.846999530923737e-05 | validation: 6.406907379707771e-05]
	TIME [epoch: 40.6 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 6.944136742370477e-05		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 6.944136742370477e-05 | validation: 6.193617167487786e-05]
	TIME [epoch: 40.6 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 6.911436166852103e-05		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 6.911436166852103e-05 | validation: 6.19840448609883e-05]
	TIME [epoch: 40.7 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 6.733586483462295e-05		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 6.733586483462295e-05 | validation: 6.13204400767946e-05]
	TIME [epoch: 40.6 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 6.448096398728197e-05		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 6.448096398728197e-05 | validation: 5.613122846128671e-05]
	TIME [epoch: 40.6 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 6.213975409537265e-05		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 6.213975409537265e-05 | validation: 5.1527435023252416e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_278.pth
	Model improved!!!
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 6.261559720947185e-05		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 6.261559720947185e-05 | validation: 5.3283246627455096e-05]
	TIME [epoch: 40.6 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 6.315725025890906e-05		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 6.315725025890906e-05 | validation: 5.78967489467912e-05]
	TIME [epoch: 40.6 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 5.9958855717835774e-05		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 5.9958855717835774e-05 | validation: 5.000276391918979e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_281.pth
	Model improved!!!
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 5.679401948345963e-05		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 5.679401948345963e-05 | validation: 4.743499759894654e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_282.pth
	Model improved!!!
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 6.767317103637171e-05		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 6.767317103637171e-05 | validation: 5.1105084469752705e-05]
	TIME [epoch: 40.7 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 5.924379240420752e-05		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 5.924379240420752e-05 | validation: 6.228366021485243e-05]
	TIME [epoch: 40.7 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 6.316092626443836e-05		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 6.316092626443836e-05 | validation: 5.185586543227383e-05]
	TIME [epoch: 40.7 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 5.547177621726429e-05		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 5.547177621726429e-05 | validation: 5.444008106625908e-05]
	TIME [epoch: 40.7 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 5.728051087219343e-05		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 5.728051087219343e-05 | validation: 7.007868623333513e-05]
	TIME [epoch: 40.8 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 5.573872260748925e-05		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 5.573872260748925e-05 | validation: 6.458399792385072e-05]
	TIME [epoch: 40.7 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 5.245403787588821e-05		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 5.245403787588821e-05 | validation: 5.2886502844013086e-05]
	TIME [epoch: 40.8 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 5.8003650547477474e-05		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 5.8003650547477474e-05 | validation: 4.9293488316541016e-05]
	TIME [epoch: 40.8 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 5.490443813928747e-05		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 5.490443813928747e-05 | validation: 4.496613659294147e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_291.pth
	Model improved!!!
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 5.660640288648833e-05		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 5.660640288648833e-05 | validation: 4.332362133845069e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_292.pth
	Model improved!!!
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 5.304498229857835e-05		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 5.304498229857835e-05 | validation: 5.398712637560621e-05]
	TIME [epoch: 40.7 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 5.704005375005328e-05		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 5.704005375005328e-05 | validation: 5.654026539838797e-05]
	TIME [epoch: 40.8 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 5.408662176672929e-05		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 5.408662176672929e-05 | validation: 4.1570218271296474e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_295.pth
	Model improved!!!
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 5.2546460751370035e-05		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 5.2546460751370035e-05 | validation: 4.268973233096762e-05]
	TIME [epoch: 40.6 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 5.444235385141005e-05		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 5.444235385141005e-05 | validation: 5.111762716136492e-05]
	TIME [epoch: 40.6 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 5.284197474219265e-05		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 5.284197474219265e-05 | validation: 3.666281661727244e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_298.pth
	Model improved!!!
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 5.029510219068101e-05		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 5.029510219068101e-05 | validation: 3.886712877772025e-05]
	TIME [epoch: 40.6 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 5.1806008694113116e-05		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 5.1806008694113116e-05 | validation: 5.245762125115583e-05]
	TIME [epoch: 40.7 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 5.427340243727885e-05		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 5.427340243727885e-05 | validation: 4.79448224497756e-05]
	TIME [epoch: 40.6 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 5.416462720299675e-05		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 5.416462720299675e-05 | validation: 4.999108722022116e-05]
	TIME [epoch: 40.6 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 5.364911178156163e-05		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 5.364911178156163e-05 | validation: 3.681284087399717e-05]
	TIME [epoch: 40.6 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 5.268285619341628e-05		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 5.268285619341628e-05 | validation: 4.103102210832166e-05]
	TIME [epoch: 40.5 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 4.687637846904181e-05		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 4.687637846904181e-05 | validation: 4.5877193785283984e-05]
	TIME [epoch: 40.5 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 5.1056240418995144e-05		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 5.1056240418995144e-05 | validation: 4.8798526100902964e-05]
	TIME [epoch: 40.6 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 4.9962132328569604e-05		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 4.9962132328569604e-05 | validation: 5.162475799888578e-05]
	TIME [epoch: 40.6 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 5.226829966799107e-05		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 5.226829966799107e-05 | validation: 4.4947772647058626e-05]
	TIME [epoch: 40.7 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 4.602900360110429e-05		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 4.602900360110429e-05 | validation: 5.043895283288125e-05]
	TIME [epoch: 40.6 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 4.776536569094303e-05		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 4.776536569094303e-05 | validation: 5.596506065785234e-05]
	TIME [epoch: 40.6 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 4.781136106622608e-05		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 4.781136106622608e-05 | validation: 4.555110128981954e-05]
	TIME [epoch: 40.6 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 5.266917218284928e-05		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 5.266917218284928e-05 | validation: 4.1049665452114195e-05]
	TIME [epoch: 40.5 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 4.7887606169680576e-05		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 4.7887606169680576e-05 | validation: 4.4576267480724407e-05]
	TIME [epoch: 40.6 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 4.5028289920096444e-05		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 4.5028289920096444e-05 | validation: 3.556987761408892e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_314.pth
	Model improved!!!
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 4.884586370802113e-05		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 4.884586370802113e-05 | validation: 4.7988459811243e-05]
	TIME [epoch: 40.6 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 4.890182894105921e-05		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 4.890182894105921e-05 | validation: 4.85133759030254e-05]
	TIME [epoch: 40.6 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 5.050192187052982e-05		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 5.050192187052982e-05 | validation: 2.9415220307931646e-05]
	TIME [epoch: 40.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_317.pth
	Model improved!!!
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 4.592176060048336e-05		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 4.592176060048336e-05 | validation: 4.286237412693739e-05]
	TIME [epoch: 40.6 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 4.79894235081475e-05		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 4.79894235081475e-05 | validation: 3.83968322005075e-05]
	TIME [epoch: 40.5 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 4.136877273414975e-05		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 4.136877273414975e-05 | validation: 5.123263493822239e-05]
	TIME [epoch: 40.6 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 4.748704682647353e-05		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 4.748704682647353e-05 | validation: 4.233536283187456e-05]
	TIME [epoch: 40.6 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3714956837491714e-05		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 4.3714956837491714e-05 | validation: 3.696066754048366e-05]
	TIME [epoch: 40.5 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 4.781946913413238e-05		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 4.781946913413238e-05 | validation: 3.251301170598353e-05]
	TIME [epoch: 40.6 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 4.716486440128054e-05		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 4.716486440128054e-05 | validation: 3.294335241340907e-05]
	TIME [epoch: 40.5 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 4.5096719056737076e-05		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 4.5096719056737076e-05 | validation: 3.719981566188158e-05]
	TIME [epoch: 40.5 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 4.395618081303921e-05		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 4.395618081303921e-05 | validation: 5.6447879353564544e-05]
	TIME [epoch: 40.5 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 4.63240012188878e-05		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 4.63240012188878e-05 | validation: 3.627172950207447e-05]
	TIME [epoch: 40.6 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 4.23653621457214e-05		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 4.23653621457214e-05 | validation: 3.6848728942543205e-05]
	TIME [epoch: 40.5 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 4.907908753967539e-05		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 4.907908753967539e-05 | validation: 4.982147532872916e-05]
	TIME [epoch: 40.6 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 4.401205427136434e-05		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 4.401205427136434e-05 | validation: 3.1336962765304626e-05]
	TIME [epoch: 40.6 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1551350819653e-05		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 4.1551350819653e-05 | validation: 4.140016204159114e-05]
	TIME [epoch: 40.5 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 4.38177195578473e-05		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 4.38177195578473e-05 | validation: 3.753463685079539e-05]
	TIME [epoch: 40.6 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 4.272637193340467e-05		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 4.272637193340467e-05 | validation: 3.600211916854202e-05]
	TIME [epoch: 40.5 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 4.075970507269866e-05		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 4.075970507269866e-05 | validation: 4.2474656668892054e-05]
	TIME [epoch: 40.5 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 4.067713942477768e-05		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 4.067713942477768e-05 | validation: 3.914426468757526e-05]
	TIME [epoch: 40.5 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1014086850935776e-05		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 4.1014086850935776e-05 | validation: 3.7874475857962996e-05]
	TIME [epoch: 40.6 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 4.496040126804668e-05		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 4.496040126804668e-05 | validation: 3.875100998042269e-05]
	TIME [epoch: 40.5 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 3.92814854169592e-05		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 3.92814854169592e-05 | validation: 3.3615212299833443e-05]
	TIME [epoch: 40.7 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 4.48929693728074e-05		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 4.48929693728074e-05 | validation: 3.957459106253492e-05]
	TIME [epoch: 40.5 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 4.368729356376489e-05		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 4.368729356376489e-05 | validation: 4.789129441066908e-05]
	TIME [epoch: 40.6 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 4.4085883744225514e-05		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 4.4085883744225514e-05 | validation: 4.4312055811453365e-05]
	TIME [epoch: 40.5 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 4.078101731974448e-05		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 4.078101731974448e-05 | validation: 4.1840674241588884e-05]
	TIME [epoch: 40.6 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3747171076775616e-05		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 4.3747171076775616e-05 | validation: 4.2919203428107394e-05]
	TIME [epoch: 40.5 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 3.723816024366122e-05		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 3.723816024366122e-05 | validation: 3.708373371596863e-05]
	TIME [epoch: 40.5 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 4.152841230525195e-05		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 4.152841230525195e-05 | validation: 4.302197450468603e-05]
	TIME [epoch: 40.5 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1270533481484685e-05		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 4.1270533481484685e-05 | validation: 3.387396016511368e-05]
	TIME [epoch: 40.5 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3068825640303564e-05		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 4.3068825640303564e-05 | validation: 4.369747951485725e-05]
	TIME [epoch: 40.6 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7489131786258345e-05		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 3.7489131786258345e-05 | validation: 4.6057962731556716e-05]
	TIME [epoch: 40.5 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 3.839163377682065e-05		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 3.839163377682065e-05 | validation: 3.499901148293816e-05]
	TIME [epoch: 40.6 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3373598522549074e-05		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 4.3373598522549074e-05 | validation: 3.522772075654657e-05]
	TIME [epoch: 40.5 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 4.066821312398583e-05		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 4.066821312398583e-05 | validation: 3.486432664144701e-05]
	TIME [epoch: 40.6 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 4.226265402651541e-05		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 4.226265402651541e-05 | validation: 3.517968739873534e-05]
	TIME [epoch: 40.5 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 4.25220864018232e-05		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 4.25220864018232e-05 | validation: 4.334503557398794e-05]
	TIME [epoch: 40.6 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: 4.497870545651505e-05		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: 4.497870545651505e-05 | validation: 2.1940399824766255e-05]
	TIME [epoch: 40.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_354.pth
	Model improved!!!
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3292182573071995e-05		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 4.3292182573071995e-05 | validation: 3.0770917361583105e-05]
	TIME [epoch: 40.6 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 4.140033211026306e-05		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 4.140033211026306e-05 | validation: 3.5308663893934613e-05]
	TIME [epoch: 40.6 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8520825540742324e-05		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 3.8520825540742324e-05 | validation: 4.0970298200611714e-05]
	TIME [epoch: 40.5 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 4.045964602568009e-05		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 4.045964602568009e-05 | validation: 3.5937902670386766e-05]
	TIME [epoch: 40.7 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8414728301392476e-05		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 3.8414728301392476e-05 | validation: 3.714223147213813e-05]
	TIME [epoch: 40.6 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 4.228405334714391e-05		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 4.228405334714391e-05 | validation: 3.812583711821626e-05]
	TIME [epoch: 40.6 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: 4.042494919188467e-05		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: 4.042494919188467e-05 | validation: 3.921439979298746e-05]
	TIME [epoch: 40.5 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6266303307301964e-05		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 3.6266303307301964e-05 | validation: 3.825122046639873e-05]
	TIME [epoch: 40.6 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9115247705006294e-05		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 3.9115247705006294e-05 | validation: 4.056289904682853e-05]
	TIME [epoch: 40.6 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 4.150948921053144e-05		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 4.150948921053144e-05 | validation: 3.067125137521809e-05]
	TIME [epoch: 40.5 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6773567882530946e-05		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 3.6773567882530946e-05 | validation: 4.6228314153694105e-05]
	TIME [epoch: 40.5 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9876456698181736e-05		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 3.9876456698181736e-05 | validation: 4.414105282233538e-05]
	TIME [epoch: 40.6 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 3.966804555648562e-05		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 3.966804555648562e-05 | validation: 4.4227427957667766e-05]
	TIME [epoch: 40.5 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 4.115817357428853e-05		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 4.115817357428853e-05 | validation: 3.680577034603205e-05]
	TIME [epoch: 40.6 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 3.737032867679568e-05		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 3.737032867679568e-05 | validation: 4.2394155093598006e-05]
	TIME [epoch: 40.6 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 3.965196050627929e-05		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 3.965196050627929e-05 | validation: 2.453910011163507e-05]
	TIME [epoch: 40.6 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 3.793926925174174e-05		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 3.793926925174174e-05 | validation: 3.6634873461080186e-05]
	TIME [epoch: 40.6 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 3.801743595974627e-05		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 3.801743595974627e-05 | validation: 3.7584118192675487e-05]
	TIME [epoch: 40.5 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1314498741319875e-05		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 4.1314498741319875e-05 | validation: 2.9979863423451604e-05]
	TIME [epoch: 40.6 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8523408876867006e-05		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 3.8523408876867006e-05 | validation: 3.4298513395161435e-05]
	TIME [epoch: 40.5 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 4.456256598654862e-05		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 4.456256598654862e-05 | validation: 5.300101408003854e-05]
	TIME [epoch: 40.6 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 3.74031555979556e-05		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 3.74031555979556e-05 | validation: 2.9991978322852608e-05]
	TIME [epoch: 40.6 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9710310429533544e-05		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 3.9710310429533544e-05 | validation: 4.571634341417674e-05]
	TIME [epoch: 40.4 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: 3.457256099225892e-05		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: 3.457256099225892e-05 | validation: 3.765822737340474e-05]
	TIME [epoch: 40.5 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 3.775665077327617e-05		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 3.775665077327617e-05 | validation: 3.6346428684399306e-05]
	TIME [epoch: 40.6 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 3.82475163609769e-05		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 3.82475163609769e-05 | validation: 3.121999900112904e-05]
	TIME [epoch: 40.6 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 4.038017065328858e-05		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 4.038017065328858e-05 | validation: 3.342064453541038e-05]
	TIME [epoch: 40.6 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 3.651621450842268e-05		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 3.651621450842268e-05 | validation: 3.2323125059380734e-05]
	TIME [epoch: 40.6 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 3.451157708317698e-05		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 3.451157708317698e-05 | validation: 3.6410495527390725e-05]
	TIME [epoch: 40.6 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7584187033697704e-05		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 3.7584187033697704e-05 | validation: 3.88812492499504e-05]
	TIME [epoch: 40.5 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 3.677682492490275e-05		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 3.677682492490275e-05 | validation: 2.247397251262262e-05]
	TIME [epoch: 40.6 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: 3.346150515590607e-05		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: 3.346150515590607e-05 | validation: 2.6224250290922013e-05]
	TIME [epoch: 40.6 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 3.752774044923768e-05		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 3.752774044923768e-05 | validation: 2.6714019356548267e-05]
	TIME [epoch: 40.6 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8013060909861454e-05		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 3.8013060909861454e-05 | validation: 2.8733916430300655e-05]
	TIME [epoch: 40.6 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 4.194082485418171e-05		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 4.194082485418171e-05 | validation: 3.3706833365244385e-05]
	TIME [epoch: 40.5 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 3.902779701606463e-05		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 3.902779701606463e-05 | validation: 3.3546885899089276e-05]
	TIME [epoch: 40.6 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 3.909793121890548e-05		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 3.909793121890548e-05 | validation: 3.424443243976127e-05]
	TIME [epoch: 40.5 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 3.380793461413778e-05		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 3.380793461413778e-05 | validation: 2.7676661334171017e-05]
	TIME [epoch: 40.7 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3877758384306845e-05		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 3.3877758384306845e-05 | validation: 3.7038344532527745e-05]
	TIME [epoch: 40.5 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 3.437638302032864e-05		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 3.437638302032864e-05 | validation: 3.854495261213131e-05]
	TIME [epoch: 40.5 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 3.741214098026269e-05		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 3.741214098026269e-05 | validation: 2.6423663408883025e-05]
	TIME [epoch: 40.6 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 4.261877882231735e-05		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 4.261877882231735e-05 | validation: 2.5245938937101144e-05]
	TIME [epoch: 40.5 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 3.841436546676225e-05		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 3.841436546676225e-05 | validation: 3.6578139526808775e-05]
	TIME [epoch: 40.6 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 3.979876643609959e-05		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 3.979876643609959e-05 | validation: 3.254099361227958e-05]
	TIME [epoch: 40.5 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 3.640411683926348e-05		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 3.640411683926348e-05 | validation: 3.843396223461592e-05]
	TIME [epoch: 40.6 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7730019790388925e-05		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 3.7730019790388925e-05 | validation: 2.9618686531641236e-05]
	TIME [epoch: 41.7 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 3.687331303089869e-05		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 3.687331303089869e-05 | validation: 4.016795852529098e-05]
	TIME [epoch: 40.5 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2458063495021564e-05		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 3.2458063495021564e-05 | validation: 2.973960637546158e-05]
	TIME [epoch: 40.5 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0408202473772406e-05		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 3.0408202473772406e-05 | validation: 3.38133301538226e-05]
	TIME [epoch: 40.6 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0395835939198813e-05		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 3.0395835939198813e-05 | validation: 2.879412192660813e-05]
	TIME [epoch: 40.6 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 3.397759861048266e-05		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 3.397759861048266e-05 | validation: 2.854599078941278e-05]
	TIME [epoch: 40.5 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 3.610764054795235e-05		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 3.610764054795235e-05 | validation: 2.542408759010839e-05]
	TIME [epoch: 40.5 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5175941301069826e-05		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 3.5175941301069826e-05 | validation: 2.455100977394742e-05]
	TIME [epoch: 40.5 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 3.429652550328521e-05		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 3.429652550328521e-05 | validation: 2.705140096970027e-05]
	TIME [epoch: 40.6 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 3.787431597116142e-05		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 3.787431597116142e-05 | validation: 1.952718860985603e-05]
	TIME [epoch: 40.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_409.pth
	Model improved!!!
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 3.36774240984018e-05		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 3.36774240984018e-05 | validation: 3.263512034133953e-05]
	TIME [epoch: 40.6 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6150402618842614e-05		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 3.6150402618842614e-05 | validation: 4.064016804798265e-05]
	TIME [epoch: 40.5 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 3.332924056598629e-05		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 3.332924056598629e-05 | validation: 3.7947186031382696e-05]
	TIME [epoch: 40.5 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6215473633996685e-05		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 3.6215473633996685e-05 | validation: 3.974410236033488e-05]
	TIME [epoch: 40.5 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 3.564363405503879e-05		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 3.564363405503879e-05 | validation: 2.919500923392304e-05]
	TIME [epoch: 40.6 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4671602991520614e-05		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 3.4671602991520614e-05 | validation: 2.712503807839317e-05]
	TIME [epoch: 40.5 sec]
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: 3.775345240767036e-05		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: 3.775345240767036e-05 | validation: 2.7001621127555444e-05]
	TIME [epoch: 40.6 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0914644834747043e-05		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: 3.0914644834747043e-05 | validation: 3.6285981547452104e-05]
	TIME [epoch: 40.5 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 3.463952108169688e-05		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 3.463952108169688e-05 | validation: 2.1671360323778145e-05]
	TIME [epoch: 40.5 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 3.788531167059017e-05		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 3.788531167059017e-05 | validation: 2.394641888489435e-05]
	TIME [epoch: 40.5 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6551689923862706e-05		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 3.6551689923862706e-05 | validation: 3.8728944569789816e-05]
	TIME [epoch: 40.5 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 3.331885390033062e-05		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 3.331885390033062e-05 | validation: 2.753081565990767e-05]
	TIME [epoch: 40.6 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: 3.311059926884074e-05		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: 3.311059926884074e-05 | validation: 4.0486855608493186e-05]
	TIME [epoch: 40.6 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: 3.564748534261075e-05		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: 3.564748534261075e-05 | validation: 3.718915025266423e-05]
	TIME [epoch: 40.6 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 3.716173163917646e-05		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 3.716173163917646e-05 | validation: 2.76531365176238e-05]
	TIME [epoch: 40.6 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: 3.691047784584667e-05		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: 3.691047784584667e-05 | validation: 4.407408170449068e-05]
	TIME [epoch: 40.6 sec]
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 3.873493379281179e-05		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 3.873493379281179e-05 | validation: 2.9103669598670835e-05]
	TIME [epoch: 40.5 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5080580415360884e-05		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 3.5080580415360884e-05 | validation: 3.543835580668974e-05]
	TIME [epoch: 40.6 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1649779216433194e-05		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 3.1649779216433194e-05 | validation: 2.8253140631479656e-05]
	TIME [epoch: 40.5 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 3.143447947186029e-05		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 3.143447947186029e-05 | validation: 3.638020575086199e-05]
	TIME [epoch: 40.5 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1289321632652963e-05		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 3.1289321632652963e-05 | validation: 3.138379734030017e-05]
	TIME [epoch: 40.5 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 3.639375560105662e-05		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 3.639375560105662e-05 | validation: 3.832642324926794e-05]
	TIME [epoch: 40.5 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4355003672179144e-05		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 3.4355003672179144e-05 | validation: 3.9428740010313934e-05]
	TIME [epoch: 40.6 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 3.590374611907921e-05		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 3.590374611907921e-05 | validation: 3.4785500952721816e-05]
	TIME [epoch: 40.5 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 3.010857276137868e-05		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 3.010857276137868e-05 | validation: 2.6858177040453457e-05]
	TIME [epoch: 40.6 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 3.746924903594118e-05		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 3.746924903594118e-05 | validation: 2.9966081717693438e-05]
	TIME [epoch: 40.5 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 3.846697649957487e-05		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 3.846697649957487e-05 | validation: 2.5315223010117684e-05]
	TIME [epoch: 40.7 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 3.513329665007192e-05		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 3.513329665007192e-05 | validation: 3.089361604805019e-05]
	TIME [epoch: 40.6 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 3.362691471261303e-05		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 3.362691471261303e-05 | validation: 3.80271427572576e-05]
	TIME [epoch: 40.5 sec]
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: 4.046842390252836e-05		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: 4.046842390252836e-05 | validation: 3.28999242650303e-05]
	TIME [epoch: 40.3 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3362498683061517e-05		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 3.3362498683061517e-05 | validation: 2.748097957302709e-05]
	TIME [epoch: 40.4 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2160392522332716e-05		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 3.2160392522332716e-05 | validation: 3.3853780717734814e-05]
	TIME [epoch: 40.6 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 3.533470928861193e-05		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 3.533470928861193e-05 | validation: 3.604731004320483e-05]
	TIME [epoch: 40.5 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: 3.027773389771593e-05		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: 3.027773389771593e-05 | validation: 3.463825136918186e-05]
	TIME [epoch: 40.6 sec]
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 2.9326132890423184e-05		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 2.9326132890423184e-05 | validation: 3.1044284041295445e-05]
	TIME [epoch: 40.5 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4962246014173085e-05		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 3.4962246014173085e-05 | validation: 2.751067417386133e-05]
	TIME [epoch: 40.6 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1196443555122296e-05		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 3.1196443555122296e-05 | validation: 3.503648405548932e-05]
	TIME [epoch: 40.6 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3091735810322325e-05		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: 3.3091735810322325e-05 | validation: 4.350695519166048e-05]
	TIME [epoch: 40.5 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6721802773523325e-05		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: 3.6721802773523325e-05 | validation: 2.6287016610193172e-05]
	TIME [epoch: 40.5 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3990286966335655e-05		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: 3.3990286966335655e-05 | validation: 3.42513546372667e-05]
	TIME [epoch: 40.5 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: 2.934461837555824e-05		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: 2.934461837555824e-05 | validation: 3.094401197896901e-05]
	TIME [epoch: 40.6 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3512909859513605e-05		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: 3.3512909859513605e-05 | validation: 2.7175503731564636e-05]
	TIME [epoch: 40.6 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3904281269676704e-05		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 3.3904281269676704e-05 | validation: 3.7172527897935125e-05]
	TIME [epoch: 40.5 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 3.529460663777617e-05		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 3.529460663777617e-05 | validation: 3.37290629274527e-05]
	TIME [epoch: 40.5 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4162602380027216e-05		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 3.4162602380027216e-05 | validation: 3.531752691028634e-05]
	TIME [epoch: 40.5 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 3.280692682289521e-05		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 3.280692682289521e-05 | validation: 3.234516157126177e-05]
	TIME [epoch: 40.6 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 3.439121286619596e-05		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 3.439121286619596e-05 | validation: 3.2027397154139826e-05]
	TIME [epoch: 40.6 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5221992997156785e-05		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: 3.5221992997156785e-05 | validation: 3.348958556112658e-05]
	TIME [epoch: 40.6 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4498399673020706e-05		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: 3.4498399673020706e-05 | validation: 3.4429366609986406e-05]
	TIME [epoch: 40.6 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4258252579855045e-05		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 3.4258252579855045e-05 | validation: 2.6080426868873685e-05]
	TIME [epoch: 40.5 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 3.83144560288744e-05		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 3.83144560288744e-05 | validation: 2.8100162725479327e-05]
	TIME [epoch: 40.5 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 3.245716326437709e-05		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 3.245716326437709e-05 | validation: 3.5311851671625315e-05]
	TIME [epoch: 40.6 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8206658047110875e-05		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 3.8206658047110875e-05 | validation: 3.0254975942650477e-05]
	TIME [epoch: 40.6 sec]
EPOCH 463/500:
	Training over batches...
		[batch 4/4] avg loss: 3.36038283637724e-05		[learning rate: 1.7715e-05]
	Learning Rate: 1.77147e-05
	LOSS [training: 3.36038283637724e-05 | validation: 2.920279689824912e-05]
	TIME [epoch: 40.6 sec]
EPOCH 464/500:
	Training over batches...
		[batch 4/4] avg loss: 3.714784999756038e-05		[learning rate: 1.7445e-05]
	Learning Rate: 1.74448e-05
	LOSS [training: 3.714784999756038e-05 | validation: 2.634486686213622e-05]
	TIME [epoch: 40.6 sec]
EPOCH 465/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5868250439213824e-05		[learning rate: 1.7179e-05]
	Learning Rate: 1.71791e-05
	LOSS [training: 3.5868250439213824e-05 | validation: 3.742911814731742e-05]
	TIME [epoch: 40.6 sec]
EPOCH 466/500:
	Training over batches...
		[batch 4/4] avg loss: 3.300280299180014e-05		[learning rate: 1.6917e-05]
	Learning Rate: 1.69174e-05
	LOSS [training: 3.300280299180014e-05 | validation: 2.2901616594062537e-05]
	TIME [epoch: 40.6 sec]
EPOCH 467/500:
	Training over batches...
		[batch 4/4] avg loss: 3.582371046754662e-05		[learning rate: 1.666e-05]
	Learning Rate: 1.66597e-05
	LOSS [training: 3.582371046754662e-05 | validation: 3.399663492448091e-05]
	TIME [epoch: 40.6 sec]
EPOCH 468/500:
	Training over batches...
		[batch 4/4] avg loss: 3.213129114222002e-05		[learning rate: 1.6406e-05]
	Learning Rate: 1.64059e-05
	LOSS [training: 3.213129114222002e-05 | validation: 2.866074934510743e-05]
	TIME [epoch: 40.5 sec]
EPOCH 469/500:
	Training over batches...
		[batch 4/4] avg loss: 3.276409813012926e-05		[learning rate: 1.6156e-05]
	Learning Rate: 1.6156e-05
	LOSS [training: 3.276409813012926e-05 | validation: 2.783702249693154e-05]
	TIME [epoch: 40.5 sec]
EPOCH 470/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4843477538183284e-05		[learning rate: 1.591e-05]
	Learning Rate: 1.59099e-05
	LOSS [training: 3.4843477538183284e-05 | validation: 3.0270455505823747e-05]
	TIME [epoch: 40.6 sec]
EPOCH 471/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6155420002702886e-05		[learning rate: 1.5668e-05]
	Learning Rate: 1.56675e-05
	LOSS [training: 3.6155420002702886e-05 | validation: 3.3282972509893095e-05]
	TIME [epoch: 40.5 sec]
EPOCH 472/500:
	Training over batches...
		[batch 4/4] avg loss: 3.510644759226322e-05		[learning rate: 1.5429e-05]
	Learning Rate: 1.54288e-05
	LOSS [training: 3.510644759226322e-05 | validation: 2.5850136665126923e-05]
	TIME [epoch: 40.6 sec]
EPOCH 473/500:
	Training over batches...
		[batch 4/4] avg loss: 3.55562553311648e-05		[learning rate: 1.5194e-05]
	Learning Rate: 1.51938e-05
	LOSS [training: 3.55562553311648e-05 | validation: 4.1584870782166216e-05]
	TIME [epoch: 40.5 sec]
EPOCH 474/500:
	Training over batches...
		[batch 4/4] avg loss: 3.211629396410143e-05		[learning rate: 1.4962e-05]
	Learning Rate: 1.49624e-05
	LOSS [training: 3.211629396410143e-05 | validation: 3.212037787331257e-05]
	TIME [epoch: 40.5 sec]
EPOCH 475/500:
	Training over batches...
		[batch 4/4] avg loss: 3.000811754196797e-05		[learning rate: 1.4734e-05]
	Learning Rate: 1.47344e-05
	LOSS [training: 3.000811754196797e-05 | validation: 3.690560422070366e-05]
	TIME [epoch: 40.6 sec]
EPOCH 476/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3862975658509044e-05		[learning rate: 1.451e-05]
	Learning Rate: 1.451e-05
	LOSS [training: 3.3862975658509044e-05 | validation: 3.6619586896434964e-05]
	TIME [epoch: 40.5 sec]
EPOCH 477/500:
	Training over batches...
		[batch 4/4] avg loss: 3.22979207554166e-05		[learning rate: 1.4289e-05]
	Learning Rate: 1.42889e-05
	LOSS [training: 3.22979207554166e-05 | validation: 3.0282941926705222e-05]
	TIME [epoch: 40.5 sec]
EPOCH 478/500:
	Training over batches...
		[batch 4/4] avg loss: 3.410907118110984e-05		[learning rate: 1.4071e-05]
	Learning Rate: 1.40713e-05
	LOSS [training: 3.410907118110984e-05 | validation: 2.7885770144213676e-05]
	TIME [epoch: 40.5 sec]
EPOCH 479/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7394638327123975e-05		[learning rate: 1.3857e-05]
	Learning Rate: 1.38569e-05
	LOSS [training: 3.7394638327123975e-05 | validation: 3.82948812133892e-05]
	TIME [epoch: 40.6 sec]
EPOCH 480/500:
	Training over batches...
		[batch 4/4] avg loss: 3.523567582968379e-05		[learning rate: 1.3646e-05]
	Learning Rate: 1.36458e-05
	LOSS [training: 3.523567582968379e-05 | validation: 4.4445946847923247e-05]
	TIME [epoch: 40.5 sec]
EPOCH 481/500:
	Training over batches...
		[batch 4/4] avg loss: 3.693822155054472e-05		[learning rate: 1.3438e-05]
	Learning Rate: 1.3438e-05
	LOSS [training: 3.693822155054472e-05 | validation: 3.4340364572526424e-05]
	TIME [epoch: 40.6 sec]
EPOCH 482/500:
	Training over batches...
		[batch 4/4] avg loss: 2.9697824289486528e-05		[learning rate: 1.3233e-05]
	Learning Rate: 1.32333e-05
	LOSS [training: 2.9697824289486528e-05 | validation: 3.458163453859875e-05]
	TIME [epoch: 40.5 sec]
EPOCH 483/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4717473883519204e-05		[learning rate: 1.3032e-05]
	Learning Rate: 1.30317e-05
	LOSS [training: 3.4717473883519204e-05 | validation: 3.476799434537736e-05]
	TIME [epoch: 40.5 sec]
EPOCH 484/500:
	Training over batches...
		[batch 4/4] avg loss: 3.722152688764702e-05		[learning rate: 1.2833e-05]
	Learning Rate: 1.28332e-05
	LOSS [training: 3.722152688764702e-05 | validation: 2.7820682739126303e-05]
	TIME [epoch: 40.5 sec]
EPOCH 485/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6422486590253934e-05		[learning rate: 1.2638e-05]
	Learning Rate: 1.26377e-05
	LOSS [training: 3.6422486590253934e-05 | validation: 3.057670853134376e-05]
	TIME [epoch: 40.5 sec]
EPOCH 486/500:
	Training over batches...
		[batch 4/4] avg loss: 3.357034467816678e-05		[learning rate: 1.2445e-05]
	Learning Rate: 1.24451e-05
	LOSS [training: 3.357034467816678e-05 | validation: 3.074047490308729e-05]
	TIME [epoch: 40.6 sec]
EPOCH 487/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6935581658466e-05		[learning rate: 1.2256e-05]
	Learning Rate: 1.22556e-05
	LOSS [training: 3.6935581658466e-05 | validation: 3.837465514284389e-05]
	TIME [epoch: 40.6 sec]
EPOCH 488/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8449459354888305e-05		[learning rate: 1.2069e-05]
	Learning Rate: 1.20689e-05
	LOSS [training: 3.8449459354888305e-05 | validation: 3.077779074527753e-05]
	TIME [epoch: 40.5 sec]
EPOCH 489/500:
	Training over batches...
		[batch 4/4] avg loss: 3.875332582087043e-05		[learning rate: 1.1885e-05]
	Learning Rate: 1.1885e-05
	LOSS [training: 3.875332582087043e-05 | validation: 3.176550955304291e-05]
	TIME [epoch: 40.6 sec]
EPOCH 490/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3814651223644914e-05		[learning rate: 1.1704e-05]
	Learning Rate: 1.1704e-05
	LOSS [training: 3.3814651223644914e-05 | validation: 3.5614240967117857e-05]
	TIME [epoch: 40.6 sec]
EPOCH 491/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6199943407222876e-05		[learning rate: 1.1526e-05]
	Learning Rate: 1.15257e-05
	LOSS [training: 3.6199943407222876e-05 | validation: 2.947057797994357e-05]
	TIME [epoch: 40.6 sec]
EPOCH 492/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4377616499197016e-05		[learning rate: 1.135e-05]
	Learning Rate: 1.13501e-05
	LOSS [training: 3.4377616499197016e-05 | validation: 2.323386473849376e-05]
	TIME [epoch: 40.5 sec]
EPOCH 493/500:
	Training over batches...
		[batch 4/4] avg loss: 3.501108398303865e-05		[learning rate: 1.1177e-05]
	Learning Rate: 1.11772e-05
	LOSS [training: 3.501108398303865e-05 | validation: 2.969339889420053e-05]
	TIME [epoch: 40.5 sec]
EPOCH 494/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6716521994705875e-05		[learning rate: 1.1007e-05]
	Learning Rate: 1.10069e-05
	LOSS [training: 3.6716521994705875e-05 | validation: 3.527070677311572e-05]
	TIME [epoch: 40.5 sec]
EPOCH 495/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4044018300911963e-05		[learning rate: 1.0839e-05]
	Learning Rate: 1.08393e-05
	LOSS [training: 2.4044018300911963e-05 | validation: 2.1508444906150005e-05]
	TIME [epoch: 40.6 sec]
EPOCH 496/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2238489136895754e-05		[learning rate: 1.0674e-05]
	Learning Rate: 1.06741e-05
	LOSS [training: 3.2238489136895754e-05 | validation: 3.1798323790450225e-05]
	TIME [epoch: 40.5 sec]
EPOCH 497/500:
	Training over batches...
		[batch 4/4] avg loss: 3.400324694285184e-05		[learning rate: 1.0512e-05]
	Learning Rate: 1.05115e-05
	LOSS [training: 3.400324694285184e-05 | validation: 1.800270964560058e-05]
	TIME [epoch: 40.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20241015_180723/states/model_algphiq_1a_v_mmd5_497.pth
	Model improved!!!
EPOCH 498/500:
	Training over batches...
		[batch 4/4] avg loss: 3.684811631828433e-05		[learning rate: 1.0351e-05]
	Learning Rate: 1.03514e-05
	LOSS [training: 3.684811631828433e-05 | validation: 3.3600570806908925e-05]
	TIME [epoch: 40.5 sec]
EPOCH 499/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4215825993661134e-05		[learning rate: 1.0194e-05]
	Learning Rate: 1.01937e-05
	LOSS [training: 3.4215825993661134e-05 | validation: 3.8949215014150164e-05]
	TIME [epoch: 40.6 sec]
EPOCH 500/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8168636426409326e-05		[learning rate: 1.0038e-05]
	Learning Rate: 1.00384e-05
	LOSS [training: 3.8168636426409326e-05 | validation: 2.8465533753537777e-05]
	TIME [epoch: 40.5 sec]
Finished training in 13966.353 seconds.
